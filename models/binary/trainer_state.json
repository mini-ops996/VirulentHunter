{
  "best_metric": 0.7075604207417839,
  "best_model_checkpoint": "models/binary/v7/esm2_t30_150M_UR50D_2024-07-02_22-17-00/run-g06nh0fk/checkpoint-18746",
  "epoch": 13.0,
  "eval_steps": 500,
  "global_step": 18746,
  "is_hyper_param_search": true,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.006934812760055479,
      "grad_norm": 1.5890382528305054,
      "learning_rate": 0.0007369640687640895,
      "loss": 0.7131,
      "step": 10
    },
    {
      "epoch": 0.013869625520110958,
      "grad_norm": 2.8100368976593018,
      "learning_rate": 0.0007367936724476124,
      "loss": 0.695,
      "step": 20
    },
    {
      "epoch": 0.020804438280166437,
      "grad_norm": 1.7598464488983154,
      "learning_rate": 0.000736623276131135,
      "loss": 0.7112,
      "step": 30
    },
    {
      "epoch": 0.027739251040221916,
      "grad_norm": 2.243117094039917,
      "learning_rate": 0.0007364528798146578,
      "loss": 0.6518,
      "step": 40
    },
    {
      "epoch": 0.03467406380027739,
      "grad_norm": 3.108280658721924,
      "learning_rate": 0.0007362824834981806,
      "loss": 0.704,
      "step": 50
    },
    {
      "epoch": 0.04160887656033287,
      "grad_norm": 2.0455281734466553,
      "learning_rate": 0.0007361120871817033,
      "loss": 0.6952,
      "step": 60
    },
    {
      "epoch": 0.04854368932038835,
      "grad_norm": 2.4135780334472656,
      "learning_rate": 0.0007359416908652261,
      "loss": 0.6609,
      "step": 70
    },
    {
      "epoch": 0.05547850208044383,
      "grad_norm": 1.351230263710022,
      "learning_rate": 0.0007357712945487489,
      "loss": 0.6651,
      "step": 80
    },
    {
      "epoch": 0.06241331484049931,
      "grad_norm": 4.684128284454346,
      "learning_rate": 0.0007356008982322715,
      "loss": 0.6729,
      "step": 90
    },
    {
      "epoch": 0.06934812760055478,
      "grad_norm": 2.7937099933624268,
      "learning_rate": 0.0007354305019157943,
      "loss": 0.6946,
      "step": 100
    },
    {
      "epoch": 0.07628294036061026,
      "grad_norm": 2.0052108764648438,
      "learning_rate": 0.0007352601055993171,
      "loss": 0.677,
      "step": 110
    },
    {
      "epoch": 0.08321775312066575,
      "grad_norm": 1.345618486404419,
      "learning_rate": 0.0007350897092828399,
      "loss": 0.6943,
      "step": 120
    },
    {
      "epoch": 0.09015256588072122,
      "grad_norm": 1.6405227184295654,
      "learning_rate": 0.0007349193129663626,
      "loss": 0.6554,
      "step": 130
    },
    {
      "epoch": 0.0970873786407767,
      "grad_norm": 2.2999353408813477,
      "learning_rate": 0.0007347489166498854,
      "loss": 0.6281,
      "step": 140
    },
    {
      "epoch": 0.10402219140083217,
      "grad_norm": 2.5180325508117676,
      "learning_rate": 0.0007345785203334081,
      "loss": 0.6224,
      "step": 150
    },
    {
      "epoch": 0.11095700416088766,
      "grad_norm": 1.3559703826904297,
      "learning_rate": 0.0007344081240169308,
      "loss": 0.7162,
      "step": 160
    },
    {
      "epoch": 0.11789181692094314,
      "grad_norm": 1.5648224353790283,
      "learning_rate": 0.0007342377277004536,
      "loss": 0.6526,
      "step": 170
    },
    {
      "epoch": 0.12482662968099861,
      "grad_norm": 1.1789016723632812,
      "learning_rate": 0.0007340673313839764,
      "loss": 0.6433,
      "step": 180
    },
    {
      "epoch": 0.1317614424410541,
      "grad_norm": 1.3912353515625,
      "learning_rate": 0.0007338969350674991,
      "loss": 0.5936,
      "step": 190
    },
    {
      "epoch": 0.13869625520110956,
      "grad_norm": 2.2674872875213623,
      "learning_rate": 0.0007337265387510219,
      "loss": 0.6609,
      "step": 200
    },
    {
      "epoch": 0.14563106796116504,
      "grad_norm": 2.1406614780426025,
      "learning_rate": 0.0007335561424345446,
      "loss": 0.6581,
      "step": 210
    },
    {
      "epoch": 0.15256588072122051,
      "grad_norm": 1.4766896963119507,
      "learning_rate": 0.0007333857461180673,
      "loss": 0.6177,
      "step": 220
    },
    {
      "epoch": 0.15950069348127602,
      "grad_norm": 1.0789912939071655,
      "learning_rate": 0.0007332153498015902,
      "loss": 0.6743,
      "step": 230
    },
    {
      "epoch": 0.1664355062413315,
      "grad_norm": 1.2781610488891602,
      "learning_rate": 0.0007330449534851129,
      "loss": 0.6766,
      "step": 240
    },
    {
      "epoch": 0.17337031900138697,
      "grad_norm": 1.545387625694275,
      "learning_rate": 0.0007328745571686357,
      "loss": 0.6051,
      "step": 250
    },
    {
      "epoch": 0.18030513176144244,
      "grad_norm": 1.6858530044555664,
      "learning_rate": 0.0007327041608521585,
      "loss": 0.6243,
      "step": 260
    },
    {
      "epoch": 0.18723994452149792,
      "grad_norm": 2.02193021774292,
      "learning_rate": 0.0007325337645356811,
      "loss": 0.6532,
      "step": 270
    },
    {
      "epoch": 0.1941747572815534,
      "grad_norm": 1.8405792713165283,
      "learning_rate": 0.0007323633682192039,
      "loss": 0.6048,
      "step": 280
    },
    {
      "epoch": 0.20110957004160887,
      "grad_norm": 2.3450112342834473,
      "learning_rate": 0.0007321929719027267,
      "loss": 0.6545,
      "step": 290
    },
    {
      "epoch": 0.20804438280166435,
      "grad_norm": 1.5443923473358154,
      "learning_rate": 0.0007320225755862494,
      "loss": 0.605,
      "step": 300
    },
    {
      "epoch": 0.21497919556171982,
      "grad_norm": 2.1796092987060547,
      "learning_rate": 0.0007318521792697722,
      "loss": 0.6234,
      "step": 310
    },
    {
      "epoch": 0.22191400832177532,
      "grad_norm": 1.5369689464569092,
      "learning_rate": 0.000731681782953295,
      "loss": 0.6307,
      "step": 320
    },
    {
      "epoch": 0.2288488210818308,
      "grad_norm": 1.9201831817626953,
      "learning_rate": 0.0007315113866368176,
      "loss": 0.6488,
      "step": 330
    },
    {
      "epoch": 0.23578363384188628,
      "grad_norm": 1.8869940042495728,
      "learning_rate": 0.0007313409903203404,
      "loss": 0.6271,
      "step": 340
    },
    {
      "epoch": 0.24271844660194175,
      "grad_norm": 1.8889130353927612,
      "learning_rate": 0.0007311705940038632,
      "loss": 0.6754,
      "step": 350
    },
    {
      "epoch": 0.24965325936199723,
      "grad_norm": 2.050234317779541,
      "learning_rate": 0.000731000197687386,
      "loss": 0.6301,
      "step": 360
    },
    {
      "epoch": 0.2565880721220527,
      "grad_norm": 2.461827278137207,
      "learning_rate": 0.0007308298013709087,
      "loss": 0.5928,
      "step": 370
    },
    {
      "epoch": 0.2635228848821082,
      "grad_norm": 2.3062527179718018,
      "learning_rate": 0.0007306594050544314,
      "loss": 0.6609,
      "step": 380
    },
    {
      "epoch": 0.27045769764216365,
      "grad_norm": 1.717832326889038,
      "learning_rate": 0.0007304890087379542,
      "loss": 0.6418,
      "step": 390
    },
    {
      "epoch": 0.27739251040221913,
      "grad_norm": 2.1092946529388428,
      "learning_rate": 0.0007303186124214769,
      "loss": 0.6282,
      "step": 400
    },
    {
      "epoch": 0.2843273231622746,
      "grad_norm": 1.6266388893127441,
      "learning_rate": 0.0007301482161049997,
      "loss": 0.6531,
      "step": 410
    },
    {
      "epoch": 0.2912621359223301,
      "grad_norm": 3.584200620651245,
      "learning_rate": 0.0007299778197885225,
      "loss": 0.6133,
      "step": 420
    },
    {
      "epoch": 0.29819694868238555,
      "grad_norm": 2.281277894973755,
      "learning_rate": 0.0007298074234720452,
      "loss": 0.6514,
      "step": 430
    },
    {
      "epoch": 0.30513176144244103,
      "grad_norm": 1.7300721406936646,
      "learning_rate": 0.000729637027155568,
      "loss": 0.6978,
      "step": 440
    },
    {
      "epoch": 0.3120665742024965,
      "grad_norm": 1.1243035793304443,
      "learning_rate": 0.0007294666308390907,
      "loss": 0.6039,
      "step": 450
    },
    {
      "epoch": 0.31900138696255204,
      "grad_norm": 1.6121331453323364,
      "learning_rate": 0.0007292962345226134,
      "loss": 0.625,
      "step": 460
    },
    {
      "epoch": 0.3259361997226075,
      "grad_norm": 1.4088581800460815,
      "learning_rate": 0.0007291258382061363,
      "loss": 0.5989,
      "step": 470
    },
    {
      "epoch": 0.332871012482663,
      "grad_norm": 1.1390002965927124,
      "learning_rate": 0.000728955441889659,
      "loss": 0.6175,
      "step": 480
    },
    {
      "epoch": 0.33980582524271846,
      "grad_norm": 1.9706368446350098,
      "learning_rate": 0.0007287850455731818,
      "loss": 0.5227,
      "step": 490
    },
    {
      "epoch": 0.34674063800277394,
      "grad_norm": 1.7305036783218384,
      "learning_rate": 0.0007286146492567045,
      "loss": 0.648,
      "step": 500
    },
    {
      "epoch": 0.3536754507628294,
      "grad_norm": 1.615557312965393,
      "learning_rate": 0.0007284442529402272,
      "loss": 0.6554,
      "step": 510
    },
    {
      "epoch": 0.3606102635228849,
      "grad_norm": 2.458461046218872,
      "learning_rate": 0.00072827385662375,
      "loss": 0.6253,
      "step": 520
    },
    {
      "epoch": 0.36754507628294036,
      "grad_norm": 1.6763678789138794,
      "learning_rate": 0.0007281034603072728,
      "loss": 0.6316,
      "step": 530
    },
    {
      "epoch": 0.37447988904299584,
      "grad_norm": 2.7585432529449463,
      "learning_rate": 0.0007279330639907955,
      "loss": 0.5953,
      "step": 540
    },
    {
      "epoch": 0.3814147018030513,
      "grad_norm": 2.150087594985962,
      "learning_rate": 0.0007277626676743182,
      "loss": 0.5834,
      "step": 550
    },
    {
      "epoch": 0.3883495145631068,
      "grad_norm": 2.8866007328033447,
      "learning_rate": 0.000727592271357841,
      "loss": 0.673,
      "step": 560
    },
    {
      "epoch": 0.39528432732316227,
      "grad_norm": 1.6257047653198242,
      "learning_rate": 0.0007274218750413638,
      "loss": 0.622,
      "step": 570
    },
    {
      "epoch": 0.40221914008321774,
      "grad_norm": 3.9012250900268555,
      "learning_rate": 0.0007272514787248865,
      "loss": 0.625,
      "step": 580
    },
    {
      "epoch": 0.4091539528432732,
      "grad_norm": 2.122103691101074,
      "learning_rate": 0.0007270810824084093,
      "loss": 0.5833,
      "step": 590
    },
    {
      "epoch": 0.4160887656033287,
      "grad_norm": 2.956308364868164,
      "learning_rate": 0.0007269106860919321,
      "loss": 0.6679,
      "step": 600
    },
    {
      "epoch": 0.42302357836338417,
      "grad_norm": 3.2308640480041504,
      "learning_rate": 0.0007267402897754547,
      "loss": 0.6028,
      "step": 610
    },
    {
      "epoch": 0.42995839112343964,
      "grad_norm": 2.6356117725372314,
      "learning_rate": 0.0007265698934589775,
      "loss": 0.5124,
      "step": 620
    },
    {
      "epoch": 0.4368932038834951,
      "grad_norm": 4.340152740478516,
      "learning_rate": 0.0007263994971425003,
      "loss": 0.6326,
      "step": 630
    },
    {
      "epoch": 0.44382801664355065,
      "grad_norm": 4.025601387023926,
      "learning_rate": 0.000726229100826023,
      "loss": 0.5737,
      "step": 640
    },
    {
      "epoch": 0.4507628294036061,
      "grad_norm": 2.7130985260009766,
      "learning_rate": 0.0007260587045095459,
      "loss": 0.6584,
      "step": 650
    },
    {
      "epoch": 0.4576976421636616,
      "grad_norm": 2.153038263320923,
      "learning_rate": 0.0007258883081930686,
      "loss": 0.619,
      "step": 660
    },
    {
      "epoch": 0.4646324549237171,
      "grad_norm": 1.8749818801879883,
      "learning_rate": 0.0007257179118765912,
      "loss": 0.6183,
      "step": 670
    },
    {
      "epoch": 0.47156726768377255,
      "grad_norm": 2.951012134552002,
      "learning_rate": 0.0007255475155601141,
      "loss": 0.6247,
      "step": 680
    },
    {
      "epoch": 0.478502080443828,
      "grad_norm": 1.8457492589950562,
      "learning_rate": 0.0007253771192436368,
      "loss": 0.5649,
      "step": 690
    },
    {
      "epoch": 0.4854368932038835,
      "grad_norm": 1.5670965909957886,
      "learning_rate": 0.0007252067229271596,
      "loss": 0.6232,
      "step": 700
    },
    {
      "epoch": 0.492371705963939,
      "grad_norm": 1.8021811246871948,
      "learning_rate": 0.0007250363266106824,
      "loss": 0.5345,
      "step": 710
    },
    {
      "epoch": 0.49930651872399445,
      "grad_norm": 1.7138193845748901,
      "learning_rate": 0.000724865930294205,
      "loss": 0.5961,
      "step": 720
    },
    {
      "epoch": 0.5062413314840499,
      "grad_norm": 2.035149574279785,
      "learning_rate": 0.0007246955339777278,
      "loss": 0.6475,
      "step": 730
    },
    {
      "epoch": 0.5131761442441054,
      "grad_norm": 3.0088982582092285,
      "learning_rate": 0.0007245251376612506,
      "loss": 0.659,
      "step": 740
    },
    {
      "epoch": 0.5201109570041609,
      "grad_norm": 2.5691397190093994,
      "learning_rate": 0.0007243547413447733,
      "loss": 0.6471,
      "step": 750
    },
    {
      "epoch": 0.5270457697642164,
      "grad_norm": 1.3427138328552246,
      "learning_rate": 0.0007241843450282961,
      "loss": 0.6036,
      "step": 760
    },
    {
      "epoch": 0.5339805825242718,
      "grad_norm": 1.5753782987594604,
      "learning_rate": 0.0007240139487118189,
      "loss": 0.6264,
      "step": 770
    },
    {
      "epoch": 0.5409153952843273,
      "grad_norm": 1.8555415868759155,
      "learning_rate": 0.0007238435523953417,
      "loss": 0.577,
      "step": 780
    },
    {
      "epoch": 0.5478502080443828,
      "grad_norm": 1.918079137802124,
      "learning_rate": 0.0007236731560788643,
      "loss": 0.5526,
      "step": 790
    },
    {
      "epoch": 0.5547850208044383,
      "grad_norm": 1.6750211715698242,
      "learning_rate": 0.0007235027597623871,
      "loss": 0.6018,
      "step": 800
    },
    {
      "epoch": 0.5617198335644937,
      "grad_norm": 1.7467564344406128,
      "learning_rate": 0.0007233323634459099,
      "loss": 0.6854,
      "step": 810
    },
    {
      "epoch": 0.5686546463245492,
      "grad_norm": 1.6134198904037476,
      "learning_rate": 0.0007231619671294326,
      "loss": 0.5726,
      "step": 820
    },
    {
      "epoch": 0.5755894590846047,
      "grad_norm": 2.0468997955322266,
      "learning_rate": 0.0007229915708129554,
      "loss": 0.6452,
      "step": 830
    },
    {
      "epoch": 0.5825242718446602,
      "grad_norm": 1.5845178365707397,
      "learning_rate": 0.0007228211744964781,
      "loss": 0.6251,
      "step": 840
    },
    {
      "epoch": 0.5894590846047156,
      "grad_norm": 1.1244055032730103,
      "learning_rate": 0.0007226507781800008,
      "loss": 0.5943,
      "step": 850
    },
    {
      "epoch": 0.5963938973647711,
      "grad_norm": 3.258410930633545,
      "learning_rate": 0.0007224803818635236,
      "loss": 0.53,
      "step": 860
    },
    {
      "epoch": 0.6033287101248266,
      "grad_norm": 1.9709196090698242,
      "learning_rate": 0.0007223099855470464,
      "loss": 0.6577,
      "step": 870
    },
    {
      "epoch": 0.6102635228848821,
      "grad_norm": 1.949977159500122,
      "learning_rate": 0.0007221395892305691,
      "loss": 0.6564,
      "step": 880
    },
    {
      "epoch": 0.6171983356449375,
      "grad_norm": 2.730783700942993,
      "learning_rate": 0.000721969192914092,
      "loss": 0.595,
      "step": 890
    },
    {
      "epoch": 0.624133148404993,
      "grad_norm": 2.679492235183716,
      "learning_rate": 0.0007217987965976146,
      "loss": 0.6041,
      "step": 900
    },
    {
      "epoch": 0.6310679611650486,
      "grad_norm": 1.3136887550354004,
      "learning_rate": 0.0007216284002811374,
      "loss": 0.5936,
      "step": 910
    },
    {
      "epoch": 0.6380027739251041,
      "grad_norm": 3.5647947788238525,
      "learning_rate": 0.0007214580039646602,
      "loss": 0.605,
      "step": 920
    },
    {
      "epoch": 0.6449375866851595,
      "grad_norm": 1.846119999885559,
      "learning_rate": 0.0007212876076481829,
      "loss": 0.5753,
      "step": 930
    },
    {
      "epoch": 0.651872399445215,
      "grad_norm": 1.6637492179870605,
      "learning_rate": 0.0007211172113317057,
      "loss": 0.5614,
      "step": 940
    },
    {
      "epoch": 0.6588072122052705,
      "grad_norm": 1.9314595460891724,
      "learning_rate": 0.0007209468150152285,
      "loss": 0.551,
      "step": 950
    },
    {
      "epoch": 0.665742024965326,
      "grad_norm": 1.0798652172088623,
      "learning_rate": 0.0007207764186987511,
      "loss": 0.6335,
      "step": 960
    },
    {
      "epoch": 0.6726768377253814,
      "grad_norm": 1.281287431716919,
      "learning_rate": 0.0007206060223822739,
      "loss": 0.6179,
      "step": 970
    },
    {
      "epoch": 0.6796116504854369,
      "grad_norm": 1.7910341024398804,
      "learning_rate": 0.0007204356260657967,
      "loss": 0.6131,
      "step": 980
    },
    {
      "epoch": 0.6865464632454924,
      "grad_norm": 1.9097037315368652,
      "learning_rate": 0.0007202652297493194,
      "loss": 0.659,
      "step": 990
    },
    {
      "epoch": 0.6934812760055479,
      "grad_norm": 1.5531821250915527,
      "learning_rate": 0.0007200948334328422,
      "loss": 0.6065,
      "step": 1000
    },
    {
      "epoch": 0.7004160887656034,
      "grad_norm": 1.6307481527328491,
      "learning_rate": 0.000719924437116365,
      "loss": 0.5896,
      "step": 1010
    },
    {
      "epoch": 0.7073509015256588,
      "grad_norm": 2.0719165802001953,
      "learning_rate": 0.0007197540407998877,
      "loss": 0.5692,
      "step": 1020
    },
    {
      "epoch": 0.7142857142857143,
      "grad_norm": 1.9951426982879639,
      "learning_rate": 0.0007195836444834104,
      "loss": 0.5275,
      "step": 1030
    },
    {
      "epoch": 0.7212205270457698,
      "grad_norm": 3.193829298019409,
      "learning_rate": 0.0007194132481669332,
      "loss": 0.5445,
      "step": 1040
    },
    {
      "epoch": 0.7281553398058253,
      "grad_norm": 3.2532830238342285,
      "learning_rate": 0.000719242851850456,
      "loss": 0.5904,
      "step": 1050
    },
    {
      "epoch": 0.7350901525658807,
      "grad_norm": 1.4234899282455444,
      "learning_rate": 0.0007190724555339787,
      "loss": 0.6347,
      "step": 1060
    },
    {
      "epoch": 0.7420249653259362,
      "grad_norm": 1.900368332862854,
      "learning_rate": 0.0007189020592175014,
      "loss": 0.6255,
      "step": 1070
    },
    {
      "epoch": 0.7489597780859917,
      "grad_norm": 1.6611855030059814,
      "learning_rate": 0.0007187316629010242,
      "loss": 0.5396,
      "step": 1080
    },
    {
      "epoch": 0.7558945908460472,
      "grad_norm": 2.870448350906372,
      "learning_rate": 0.0007185612665845469,
      "loss": 0.5216,
      "step": 1090
    },
    {
      "epoch": 0.7628294036061026,
      "grad_norm": 2.4680678844451904,
      "learning_rate": 0.0007183908702680698,
      "loss": 0.6053,
      "step": 1100
    },
    {
      "epoch": 0.7697642163661581,
      "grad_norm": 2.5220787525177,
      "learning_rate": 0.0007182204739515925,
      "loss": 0.5723,
      "step": 1110
    },
    {
      "epoch": 0.7766990291262136,
      "grad_norm": 1.9417688846588135,
      "learning_rate": 0.0007180500776351152,
      "loss": 0.5573,
      "step": 1120
    },
    {
      "epoch": 0.7836338418862691,
      "grad_norm": 2.9397199153900146,
      "learning_rate": 0.000717879681318638,
      "loss": 0.6498,
      "step": 1130
    },
    {
      "epoch": 0.7905686546463245,
      "grad_norm": 1.7134032249450684,
      "learning_rate": 0.0007177092850021607,
      "loss": 0.6127,
      "step": 1140
    },
    {
      "epoch": 0.79750346740638,
      "grad_norm": 2.5208303928375244,
      "learning_rate": 0.0007175388886856835,
      "loss": 0.5853,
      "step": 1150
    },
    {
      "epoch": 0.8044382801664355,
      "grad_norm": 2.3165721893310547,
      "learning_rate": 0.0007173684923692063,
      "loss": 0.5147,
      "step": 1160
    },
    {
      "epoch": 0.811373092926491,
      "grad_norm": 2.7437775135040283,
      "learning_rate": 0.000717198096052729,
      "loss": 0.512,
      "step": 1170
    },
    {
      "epoch": 0.8183079056865464,
      "grad_norm": 2.9137792587280273,
      "learning_rate": 0.0007170276997362518,
      "loss": 0.591,
      "step": 1180
    },
    {
      "epoch": 0.8252427184466019,
      "grad_norm": 3.3406155109405518,
      "learning_rate": 0.0007168573034197745,
      "loss": 0.6153,
      "step": 1190
    },
    {
      "epoch": 0.8321775312066574,
      "grad_norm": 2.2046868801116943,
      "learning_rate": 0.0007166869071032972,
      "loss": 0.5716,
      "step": 1200
    },
    {
      "epoch": 0.8391123439667129,
      "grad_norm": 1.555625081062317,
      "learning_rate": 0.00071651651078682,
      "loss": 0.577,
      "step": 1210
    },
    {
      "epoch": 0.8460471567267683,
      "grad_norm": 3.8800413608551025,
      "learning_rate": 0.0007163461144703428,
      "loss": 0.5742,
      "step": 1220
    },
    {
      "epoch": 0.8529819694868238,
      "grad_norm": 3.0525739192962646,
      "learning_rate": 0.0007161757181538656,
      "loss": 0.5761,
      "step": 1230
    },
    {
      "epoch": 0.8599167822468793,
      "grad_norm": 2.3487977981567383,
      "learning_rate": 0.0007160053218373883,
      "loss": 0.5814,
      "step": 1240
    },
    {
      "epoch": 0.8668515950069348,
      "grad_norm": 1.793400764465332,
      "learning_rate": 0.000715834925520911,
      "loss": 0.5813,
      "step": 1250
    },
    {
      "epoch": 0.8737864077669902,
      "grad_norm": 1.5315253734588623,
      "learning_rate": 0.0007156645292044338,
      "loss": 0.5671,
      "step": 1260
    },
    {
      "epoch": 0.8807212205270458,
      "grad_norm": 3.0051984786987305,
      "learning_rate": 0.0007154941328879565,
      "loss": 0.5783,
      "step": 1270
    },
    {
      "epoch": 0.8876560332871013,
      "grad_norm": 2.4097423553466797,
      "learning_rate": 0.0007153237365714793,
      "loss": 0.5665,
      "step": 1280
    },
    {
      "epoch": 0.8945908460471568,
      "grad_norm": 3.5170323848724365,
      "learning_rate": 0.0007151533402550021,
      "loss": 0.5649,
      "step": 1290
    },
    {
      "epoch": 0.9015256588072122,
      "grad_norm": 1.316186547279358,
      "learning_rate": 0.0007149829439385247,
      "loss": 0.5654,
      "step": 1300
    },
    {
      "epoch": 0.9084604715672677,
      "grad_norm": 1.8796473741531372,
      "learning_rate": 0.0007148125476220476,
      "loss": 0.5609,
      "step": 1310
    },
    {
      "epoch": 0.9153952843273232,
      "grad_norm": 2.545046806335449,
      "learning_rate": 0.0007146421513055703,
      "loss": 0.5827,
      "step": 1320
    },
    {
      "epoch": 0.9223300970873787,
      "grad_norm": 2.2025063037872314,
      "learning_rate": 0.000714471754989093,
      "loss": 0.5754,
      "step": 1330
    },
    {
      "epoch": 0.9292649098474342,
      "grad_norm": 1.7214314937591553,
      "learning_rate": 0.0007143013586726159,
      "loss": 0.5649,
      "step": 1340
    },
    {
      "epoch": 0.9361997226074896,
      "grad_norm": 2.047328233718872,
      "learning_rate": 0.0007141309623561386,
      "loss": 0.5573,
      "step": 1350
    },
    {
      "epoch": 0.9431345353675451,
      "grad_norm": 2.159379243850708,
      "learning_rate": 0.0007139605660396614,
      "loss": 0.5547,
      "step": 1360
    },
    {
      "epoch": 0.9500693481276006,
      "grad_norm": 1.532646894454956,
      "learning_rate": 0.0007137901697231841,
      "loss": 0.5682,
      "step": 1370
    },
    {
      "epoch": 0.957004160887656,
      "grad_norm": 1.4741765260696411,
      "learning_rate": 0.0007136197734067068,
      "loss": 0.5614,
      "step": 1380
    },
    {
      "epoch": 0.9639389736477115,
      "grad_norm": 2.0172033309936523,
      "learning_rate": 0.0007134493770902296,
      "loss": 0.539,
      "step": 1390
    },
    {
      "epoch": 0.970873786407767,
      "grad_norm": 1.6740018129348755,
      "learning_rate": 0.0007132789807737524,
      "loss": 0.5844,
      "step": 1400
    },
    {
      "epoch": 0.9778085991678225,
      "grad_norm": 2.2035698890686035,
      "learning_rate": 0.0007131085844572751,
      "loss": 0.5248,
      "step": 1410
    },
    {
      "epoch": 0.984743411927878,
      "grad_norm": 2.5941500663757324,
      "learning_rate": 0.0007129381881407978,
      "loss": 0.5504,
      "step": 1420
    },
    {
      "epoch": 0.9916782246879334,
      "grad_norm": 2.4517078399658203,
      "learning_rate": 0.0007127677918243206,
      "loss": 0.5606,
      "step": 1430
    },
    {
      "epoch": 0.9986130374479889,
      "grad_norm": 1.4538261890411377,
      "learning_rate": 0.0007125973955078434,
      "loss": 0.6099,
      "step": 1440
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.7363857093305585,
      "eval_auc": 0.736355358863326,
      "eval_f1": 0.7110266159695817,
      "eval_loss": 0.537916362285614,
      "eval_mcc": 0.4801339761499603,
      "eval_precision": 0.7863751051303617,
      "eval_recall": 0.648854961832061,
      "eval_runtime": 79.4606,
      "eval_samples_per_second": 36.282,
      "eval_steps_per_second": 2.278,
      "step": 1442
    },
    {
      "epoch": 1.0055478502080444,
      "grad_norm": 2.7876057624816895,
      "learning_rate": 0.0007124269991913661,
      "loss": 0.5635,
      "step": 1450
    },
    {
      "epoch": 1.0124826629680999,
      "grad_norm": 1.4708120822906494,
      "learning_rate": 0.0007122566028748889,
      "loss": 0.6234,
      "step": 1460
    },
    {
      "epoch": 1.0194174757281553,
      "grad_norm": 1.8800930976867676,
      "learning_rate": 0.0007120862065584117,
      "loss": 0.5183,
      "step": 1470
    },
    {
      "epoch": 1.0263522884882108,
      "grad_norm": 1.9016766548156738,
      "learning_rate": 0.0007119158102419343,
      "loss": 0.662,
      "step": 1480
    },
    {
      "epoch": 1.0332871012482663,
      "grad_norm": 1.8894697427749634,
      "learning_rate": 0.0007117454139254571,
      "loss": 0.5803,
      "step": 1490
    },
    {
      "epoch": 1.0402219140083218,
      "grad_norm": 1.8083550930023193,
      "learning_rate": 0.0007115750176089799,
      "loss": 0.5332,
      "step": 1500
    },
    {
      "epoch": 1.0471567267683772,
      "grad_norm": 1.6332694292068481,
      "learning_rate": 0.0007114046212925026,
      "loss": 0.5143,
      "step": 1510
    },
    {
      "epoch": 1.0540915395284327,
      "grad_norm": 2.6451003551483154,
      "learning_rate": 0.0007112342249760254,
      "loss": 0.535,
      "step": 1520
    },
    {
      "epoch": 1.0610263522884882,
      "grad_norm": 2.5727221965789795,
      "learning_rate": 0.0007110638286595482,
      "loss": 0.5042,
      "step": 1530
    },
    {
      "epoch": 1.0679611650485437,
      "grad_norm": 2.6946074962615967,
      "learning_rate": 0.0007108934323430708,
      "loss": 0.5661,
      "step": 1540
    },
    {
      "epoch": 1.0748959778085991,
      "grad_norm": 3.1888279914855957,
      "learning_rate": 0.0007107230360265937,
      "loss": 0.5758,
      "step": 1550
    },
    {
      "epoch": 1.0818307905686546,
      "grad_norm": 2.801206111907959,
      "learning_rate": 0.0007105526397101164,
      "loss": 0.6385,
      "step": 1560
    },
    {
      "epoch": 1.08876560332871,
      "grad_norm": 2.1025471687316895,
      "learning_rate": 0.0007103822433936392,
      "loss": 0.5718,
      "step": 1570
    },
    {
      "epoch": 1.0957004160887656,
      "grad_norm": 1.941351056098938,
      "learning_rate": 0.000710211847077162,
      "loss": 0.5883,
      "step": 1580
    },
    {
      "epoch": 1.102635228848821,
      "grad_norm": 3.6474251747131348,
      "learning_rate": 0.0007100414507606847,
      "loss": 0.6924,
      "step": 1590
    },
    {
      "epoch": 1.1095700416088765,
      "grad_norm": 2.3120651245117188,
      "learning_rate": 0.0007098710544442074,
      "loss": 0.4987,
      "step": 1600
    },
    {
      "epoch": 1.116504854368932,
      "grad_norm": 1.1999437808990479,
      "learning_rate": 0.0007097006581277302,
      "loss": 0.5144,
      "step": 1610
    },
    {
      "epoch": 1.1234396671289875,
      "grad_norm": 2.3694236278533936,
      "learning_rate": 0.0007095302618112529,
      "loss": 0.5545,
      "step": 1620
    },
    {
      "epoch": 1.130374479889043,
      "grad_norm": 1.700730800628662,
      "learning_rate": 0.0007093598654947757,
      "loss": 0.5308,
      "step": 1630
    },
    {
      "epoch": 1.1373092926490984,
      "grad_norm": 1.5046824216842651,
      "learning_rate": 0.0007091894691782985,
      "loss": 0.5935,
      "step": 1640
    },
    {
      "epoch": 1.1442441054091539,
      "grad_norm": 4.287773132324219,
      "learning_rate": 0.0007090190728618211,
      "loss": 0.5382,
      "step": 1650
    },
    {
      "epoch": 1.1511789181692094,
      "grad_norm": 1.697439193725586,
      "learning_rate": 0.0007088486765453439,
      "loss": 0.6469,
      "step": 1660
    },
    {
      "epoch": 1.1581137309292648,
      "grad_norm": 1.6978397369384766,
      "learning_rate": 0.0007086782802288667,
      "loss": 0.6099,
      "step": 1670
    },
    {
      "epoch": 1.1650485436893203,
      "grad_norm": 1.6651055812835693,
      "learning_rate": 0.0007085078839123895,
      "loss": 0.5429,
      "step": 1680
    },
    {
      "epoch": 1.1719833564493758,
      "grad_norm": 1.7127766609191895,
      "learning_rate": 0.0007083374875959122,
      "loss": 0.5117,
      "step": 1690
    },
    {
      "epoch": 1.1789181692094313,
      "grad_norm": 2.6024105548858643,
      "learning_rate": 0.000708167091279435,
      "loss": 0.6183,
      "step": 1700
    },
    {
      "epoch": 1.1858529819694867,
      "grad_norm": 1.4135406017303467,
      "learning_rate": 0.0007079966949629577,
      "loss": 0.4855,
      "step": 1710
    },
    {
      "epoch": 1.1927877947295422,
      "grad_norm": 6.258447170257568,
      "learning_rate": 0.0007078262986464804,
      "loss": 0.5614,
      "step": 1720
    },
    {
      "epoch": 1.1997226074895977,
      "grad_norm": 2.07316517829895,
      "learning_rate": 0.0007076559023300032,
      "loss": 0.5061,
      "step": 1730
    },
    {
      "epoch": 1.2066574202496532,
      "grad_norm": 3.1060404777526855,
      "learning_rate": 0.000707485506013526,
      "loss": 0.5347,
      "step": 1740
    },
    {
      "epoch": 1.2135922330097086,
      "grad_norm": 2.3107166290283203,
      "learning_rate": 0.0007073151096970487,
      "loss": 0.5728,
      "step": 1750
    },
    {
      "epoch": 1.2205270457697641,
      "grad_norm": 2.2695417404174805,
      "learning_rate": 0.0007071447133805716,
      "loss": 0.5184,
      "step": 1760
    },
    {
      "epoch": 1.2274618585298196,
      "grad_norm": 4.32050085067749,
      "learning_rate": 0.0007069743170640942,
      "loss": 0.5168,
      "step": 1770
    },
    {
      "epoch": 1.234396671289875,
      "grad_norm": 2.302875518798828,
      "learning_rate": 0.0007068039207476169,
      "loss": 0.5919,
      "step": 1780
    },
    {
      "epoch": 1.2413314840499305,
      "grad_norm": 1.5479061603546143,
      "learning_rate": 0.0007066335244311398,
      "loss": 0.6003,
      "step": 1790
    },
    {
      "epoch": 1.248266296809986,
      "grad_norm": 2.2617950439453125,
      "learning_rate": 0.0007064631281146625,
      "loss": 0.6124,
      "step": 1800
    },
    {
      "epoch": 1.2552011095700415,
      "grad_norm": 1.2390611171722412,
      "learning_rate": 0.0007062927317981853,
      "loss": 0.5364,
      "step": 1810
    },
    {
      "epoch": 1.262135922330097,
      "grad_norm": 2.795217990875244,
      "learning_rate": 0.000706122335481708,
      "loss": 0.6253,
      "step": 1820
    },
    {
      "epoch": 1.2690707350901524,
      "grad_norm": 2.528383493423462,
      "learning_rate": 0.0007059519391652307,
      "loss": 0.4923,
      "step": 1830
    },
    {
      "epoch": 1.276005547850208,
      "grad_norm": 2.669663429260254,
      "learning_rate": 0.0007057815428487535,
      "loss": 0.6237,
      "step": 1840
    },
    {
      "epoch": 1.2829403606102634,
      "grad_norm": 3.1345224380493164,
      "learning_rate": 0.0007056111465322762,
      "loss": 0.556,
      "step": 1850
    },
    {
      "epoch": 1.2898751733703189,
      "grad_norm": 1.7842116355895996,
      "learning_rate": 0.000705440750215799,
      "loss": 0.596,
      "step": 1860
    },
    {
      "epoch": 1.2968099861303743,
      "grad_norm": 1.774960994720459,
      "learning_rate": 0.0007052703538993218,
      "loss": 0.6144,
      "step": 1870
    },
    {
      "epoch": 1.3037447988904298,
      "grad_norm": 2.0222949981689453,
      "learning_rate": 0.0007050999575828444,
      "loss": 0.6013,
      "step": 1880
    },
    {
      "epoch": 1.3106796116504853,
      "grad_norm": 1.6631412506103516,
      "learning_rate": 0.0007049295612663673,
      "loss": 0.5611,
      "step": 1890
    },
    {
      "epoch": 1.317614424410541,
      "grad_norm": 1.6647992134094238,
      "learning_rate": 0.00070475916494989,
      "loss": 0.5982,
      "step": 1900
    },
    {
      "epoch": 1.3245492371705965,
      "grad_norm": 2.8905482292175293,
      "learning_rate": 0.0007045887686334127,
      "loss": 0.4912,
      "step": 1910
    },
    {
      "epoch": 1.331484049930652,
      "grad_norm": 2.566073179244995,
      "learning_rate": 0.0007044183723169356,
      "loss": 0.6047,
      "step": 1920
    },
    {
      "epoch": 1.3384188626907074,
      "grad_norm": 2.408493995666504,
      "learning_rate": 0.0007042479760004583,
      "loss": 0.6056,
      "step": 1930
    },
    {
      "epoch": 1.345353675450763,
      "grad_norm": 1.9968335628509521,
      "learning_rate": 0.000704077579683981,
      "loss": 0.5193,
      "step": 1940
    },
    {
      "epoch": 1.3522884882108184,
      "grad_norm": 2.305631160736084,
      "learning_rate": 0.0007039071833675038,
      "loss": 0.5294,
      "step": 1950
    },
    {
      "epoch": 1.3592233009708738,
      "grad_norm": 1.8598814010620117,
      "learning_rate": 0.0007037367870510265,
      "loss": 0.4701,
      "step": 1960
    },
    {
      "epoch": 1.3661581137309293,
      "grad_norm": 3.2956125736236572,
      "learning_rate": 0.0007035663907345493,
      "loss": 0.5574,
      "step": 1970
    },
    {
      "epoch": 1.3730929264909848,
      "grad_norm": 1.3706260919570923,
      "learning_rate": 0.0007033959944180721,
      "loss": 0.6666,
      "step": 1980
    },
    {
      "epoch": 1.3800277392510403,
      "grad_norm": 2.346447229385376,
      "learning_rate": 0.0007032255981015948,
      "loss": 0.6109,
      "step": 1990
    },
    {
      "epoch": 1.3869625520110958,
      "grad_norm": 2.628058671951294,
      "learning_rate": 0.0007030552017851175,
      "loss": 0.6293,
      "step": 2000
    },
    {
      "epoch": 1.3938973647711512,
      "grad_norm": 1.6011104583740234,
      "learning_rate": 0.0007028848054686403,
      "loss": 0.5571,
      "step": 2010
    },
    {
      "epoch": 1.4008321775312067,
      "grad_norm": 1.6973141431808472,
      "learning_rate": 0.0007027144091521631,
      "loss": 0.6276,
      "step": 2020
    },
    {
      "epoch": 1.4077669902912622,
      "grad_norm": 3.0826778411865234,
      "learning_rate": 0.0007025440128356858,
      "loss": 0.498,
      "step": 2030
    },
    {
      "epoch": 1.4147018030513177,
      "grad_norm": 2.1531379222869873,
      "learning_rate": 0.0007023736165192086,
      "loss": 0.6124,
      "step": 2040
    },
    {
      "epoch": 1.4216366158113731,
      "grad_norm": 0.9851161241531372,
      "learning_rate": 0.0007022032202027314,
      "loss": 0.5854,
      "step": 2050
    },
    {
      "epoch": 1.4285714285714286,
      "grad_norm": 1.9198627471923828,
      "learning_rate": 0.000702032823886254,
      "loss": 0.4833,
      "step": 2060
    },
    {
      "epoch": 1.435506241331484,
      "grad_norm": 2.695017099380493,
      "learning_rate": 0.0007018624275697768,
      "loss": 0.5691,
      "step": 2070
    },
    {
      "epoch": 1.4424410540915396,
      "grad_norm": 1.7325072288513184,
      "learning_rate": 0.0007016920312532996,
      "loss": 0.5247,
      "step": 2080
    },
    {
      "epoch": 1.449375866851595,
      "grad_norm": 1.6717928647994995,
      "learning_rate": 0.0007015216349368223,
      "loss": 0.4844,
      "step": 2090
    },
    {
      "epoch": 1.4563106796116505,
      "grad_norm": 4.0889573097229,
      "learning_rate": 0.0007013512386203452,
      "loss": 0.4941,
      "step": 2100
    },
    {
      "epoch": 1.463245492371706,
      "grad_norm": 1.3658497333526611,
      "learning_rate": 0.0007011808423038679,
      "loss": 0.5572,
      "step": 2110
    },
    {
      "epoch": 1.4701803051317615,
      "grad_norm": 2.0944907665252686,
      "learning_rate": 0.0007010104459873905,
      "loss": 0.5529,
      "step": 2120
    },
    {
      "epoch": 1.477115117891817,
      "grad_norm": 1.5355232954025269,
      "learning_rate": 0.0007008400496709134,
      "loss": 0.5583,
      "step": 2130
    },
    {
      "epoch": 1.4840499306518724,
      "grad_norm": 1.4672600030899048,
      "learning_rate": 0.0007006696533544361,
      "loss": 0.5092,
      "step": 2140
    },
    {
      "epoch": 1.4909847434119279,
      "grad_norm": 3.459277629852295,
      "learning_rate": 0.0007004992570379589,
      "loss": 0.4936,
      "step": 2150
    },
    {
      "epoch": 1.4979195561719834,
      "grad_norm": 1.8154933452606201,
      "learning_rate": 0.0007003288607214817,
      "loss": 0.5401,
      "step": 2160
    },
    {
      "epoch": 1.5048543689320388,
      "grad_norm": 1.910453200340271,
      "learning_rate": 0.0007001584644050043,
      "loss": 0.5103,
      "step": 2170
    },
    {
      "epoch": 1.5117891816920943,
      "grad_norm": 2.1034059524536133,
      "learning_rate": 0.0006999880680885271,
      "loss": 0.5113,
      "step": 2180
    },
    {
      "epoch": 1.5187239944521498,
      "grad_norm": 1.7697261571884155,
      "learning_rate": 0.0006998176717720499,
      "loss": 0.5365,
      "step": 2190
    },
    {
      "epoch": 1.5256588072122053,
      "grad_norm": 2.1367037296295166,
      "learning_rate": 0.0006996472754555726,
      "loss": 0.4834,
      "step": 2200
    },
    {
      "epoch": 1.5325936199722607,
      "grad_norm": 2.7221455574035645,
      "learning_rate": 0.0006994768791390954,
      "loss": 0.5491,
      "step": 2210
    },
    {
      "epoch": 1.5395284327323162,
      "grad_norm": 3.4878714084625244,
      "learning_rate": 0.0006993064828226182,
      "loss": 0.4747,
      "step": 2220
    },
    {
      "epoch": 1.5464632454923717,
      "grad_norm": 2.299077272415161,
      "learning_rate": 0.000699136086506141,
      "loss": 0.5769,
      "step": 2230
    },
    {
      "epoch": 1.5533980582524272,
      "grad_norm": 2.8016011714935303,
      "learning_rate": 0.0006989656901896636,
      "loss": 0.5164,
      "step": 2240
    },
    {
      "epoch": 1.5603328710124826,
      "grad_norm": 1.633975863456726,
      "learning_rate": 0.0006987952938731864,
      "loss": 0.639,
      "step": 2250
    },
    {
      "epoch": 1.5672676837725381,
      "grad_norm": 1.3793514966964722,
      "learning_rate": 0.0006986248975567092,
      "loss": 0.4742,
      "step": 2260
    },
    {
      "epoch": 1.5742024965325936,
      "grad_norm": 3.0750863552093506,
      "learning_rate": 0.0006984545012402319,
      "loss": 0.5816,
      "step": 2270
    },
    {
      "epoch": 1.581137309292649,
      "grad_norm": 2.3973515033721924,
      "learning_rate": 0.0006982841049237547,
      "loss": 0.5326,
      "step": 2280
    },
    {
      "epoch": 1.5880721220527045,
      "grad_norm": 1.337443470954895,
      "learning_rate": 0.0006981137086072774,
      "loss": 0.538,
      "step": 2290
    },
    {
      "epoch": 1.59500693481276,
      "grad_norm": 3.0328261852264404,
      "learning_rate": 0.0006979433122908001,
      "loss": 0.5614,
      "step": 2300
    },
    {
      "epoch": 1.6019417475728155,
      "grad_norm": 2.183640480041504,
      "learning_rate": 0.0006977729159743229,
      "loss": 0.5271,
      "step": 2310
    },
    {
      "epoch": 1.608876560332871,
      "grad_norm": 3.075843572616577,
      "learning_rate": 0.0006976025196578457,
      "loss": 0.5044,
      "step": 2320
    },
    {
      "epoch": 1.6158113730929264,
      "grad_norm": 1.4819968938827515,
      "learning_rate": 0.0006974321233413684,
      "loss": 0.5343,
      "step": 2330
    },
    {
      "epoch": 1.622746185852982,
      "grad_norm": 2.4521737098693848,
      "learning_rate": 0.0006972617270248913,
      "loss": 0.5936,
      "step": 2340
    },
    {
      "epoch": 1.6296809986130376,
      "grad_norm": 2.384857177734375,
      "learning_rate": 0.0006970913307084139,
      "loss": 0.4828,
      "step": 2350
    },
    {
      "epoch": 1.636615811373093,
      "grad_norm": 2.6075525283813477,
      "learning_rate": 0.0006969209343919367,
      "loss": 0.5832,
      "step": 2360
    },
    {
      "epoch": 1.6435506241331486,
      "grad_norm": 2.5495543479919434,
      "learning_rate": 0.0006967505380754595,
      "loss": 0.5382,
      "step": 2370
    },
    {
      "epoch": 1.650485436893204,
      "grad_norm": 1.9038037061691284,
      "learning_rate": 0.0006965801417589822,
      "loss": 0.4944,
      "step": 2380
    },
    {
      "epoch": 1.6574202496532595,
      "grad_norm": 2.782874345779419,
      "learning_rate": 0.000696409745442505,
      "loss": 0.4677,
      "step": 2390
    },
    {
      "epoch": 1.664355062413315,
      "grad_norm": 3.0683300495147705,
      "learning_rate": 0.0006962393491260278,
      "loss": 0.5118,
      "step": 2400
    },
    {
      "epoch": 1.6712898751733705,
      "grad_norm": 2.3804686069488525,
      "learning_rate": 0.0006960689528095504,
      "loss": 0.4802,
      "step": 2410
    },
    {
      "epoch": 1.678224687933426,
      "grad_norm": 2.6652302742004395,
      "learning_rate": 0.0006958985564930732,
      "loss": 0.5584,
      "step": 2420
    },
    {
      "epoch": 1.6851595006934814,
      "grad_norm": 1.2096168994903564,
      "learning_rate": 0.000695728160176596,
      "loss": 0.5235,
      "step": 2430
    },
    {
      "epoch": 1.692094313453537,
      "grad_norm": 1.3147146701812744,
      "learning_rate": 0.0006955577638601187,
      "loss": 0.4837,
      "step": 2440
    },
    {
      "epoch": 1.6990291262135924,
      "grad_norm": 3.734110116958618,
      "learning_rate": 0.0006953873675436415,
      "loss": 0.5188,
      "step": 2450
    },
    {
      "epoch": 1.7059639389736478,
      "grad_norm": 3.369192361831665,
      "learning_rate": 0.0006952169712271643,
      "loss": 0.511,
      "step": 2460
    },
    {
      "epoch": 1.7128987517337033,
      "grad_norm": 1.415438175201416,
      "learning_rate": 0.000695046574910687,
      "loss": 0.5939,
      "step": 2470
    },
    {
      "epoch": 1.7198335644937588,
      "grad_norm": 2.1924355030059814,
      "learning_rate": 0.0006948761785942097,
      "loss": 0.5101,
      "step": 2480
    },
    {
      "epoch": 1.7267683772538143,
      "grad_norm": 5.85518741607666,
      "learning_rate": 0.0006947057822777325,
      "loss": 0.4484,
      "step": 2490
    },
    {
      "epoch": 1.7337031900138697,
      "grad_norm": 3.140679121017456,
      "learning_rate": 0.0006945353859612553,
      "loss": 0.7145,
      "step": 2500
    },
    {
      "epoch": 1.7406380027739252,
      "grad_norm": 1.6221200227737427,
      "learning_rate": 0.000694364989644778,
      "loss": 0.5472,
      "step": 2510
    },
    {
      "epoch": 1.7475728155339807,
      "grad_norm": 1.7740696668624878,
      "learning_rate": 0.0006941945933283007,
      "loss": 0.5473,
      "step": 2520
    },
    {
      "epoch": 1.7545076282940362,
      "grad_norm": 1.9236488342285156,
      "learning_rate": 0.0006940241970118235,
      "loss": 0.4903,
      "step": 2530
    },
    {
      "epoch": 1.7614424410540916,
      "grad_norm": 2.109079122543335,
      "learning_rate": 0.0006938538006953462,
      "loss": 0.617,
      "step": 2540
    },
    {
      "epoch": 1.7683772538141471,
      "grad_norm": 1.2786304950714111,
      "learning_rate": 0.0006936834043788691,
      "loss": 0.503,
      "step": 2550
    },
    {
      "epoch": 1.7753120665742026,
      "grad_norm": 4.098536968231201,
      "learning_rate": 0.0006935130080623918,
      "loss": 0.5148,
      "step": 2560
    },
    {
      "epoch": 1.782246879334258,
      "grad_norm": 1.78255033493042,
      "learning_rate": 0.0006933426117459145,
      "loss": 0.5583,
      "step": 2570
    },
    {
      "epoch": 1.7891816920943135,
      "grad_norm": 1.3088197708129883,
      "learning_rate": 0.0006931722154294373,
      "loss": 0.5259,
      "step": 2580
    },
    {
      "epoch": 1.796116504854369,
      "grad_norm": 1.9535555839538574,
      "learning_rate": 0.00069300181911296,
      "loss": 0.4903,
      "step": 2590
    },
    {
      "epoch": 1.8030513176144245,
      "grad_norm": 3.4315145015716553,
      "learning_rate": 0.0006928314227964828,
      "loss": 0.528,
      "step": 2600
    },
    {
      "epoch": 1.80998613037448,
      "grad_norm": 1.6843382120132446,
      "learning_rate": 0.0006926610264800056,
      "loss": 0.5731,
      "step": 2610
    },
    {
      "epoch": 1.8169209431345354,
      "grad_norm": 1.6237990856170654,
      "learning_rate": 0.0006924906301635283,
      "loss": 0.5853,
      "step": 2620
    },
    {
      "epoch": 1.823855755894591,
      "grad_norm": 1.3465051651000977,
      "learning_rate": 0.0006923202338470511,
      "loss": 0.5135,
      "step": 2630
    },
    {
      "epoch": 1.8307905686546464,
      "grad_norm": 2.9365384578704834,
      "learning_rate": 0.0006921498375305738,
      "loss": 0.4907,
      "step": 2640
    },
    {
      "epoch": 1.8377253814147019,
      "grad_norm": 1.5624287128448486,
      "learning_rate": 0.0006919794412140965,
      "loss": 0.7024,
      "step": 2650
    },
    {
      "epoch": 1.8446601941747574,
      "grad_norm": 2.4223153591156006,
      "learning_rate": 0.0006918090448976193,
      "loss": 0.5549,
      "step": 2660
    },
    {
      "epoch": 1.8515950069348128,
      "grad_norm": 1.3072487115859985,
      "learning_rate": 0.0006916386485811421,
      "loss": 0.5683,
      "step": 2670
    },
    {
      "epoch": 1.8585298196948683,
      "grad_norm": 2.224229097366333,
      "learning_rate": 0.0006914682522646649,
      "loss": 0.4997,
      "step": 2680
    },
    {
      "epoch": 1.8654646324549238,
      "grad_norm": 1.4773465394973755,
      "learning_rate": 0.0006912978559481876,
      "loss": 0.5316,
      "step": 2690
    },
    {
      "epoch": 1.8723994452149793,
      "grad_norm": 3.6490516662597656,
      "learning_rate": 0.0006911274596317103,
      "loss": 0.5245,
      "step": 2700
    },
    {
      "epoch": 1.8793342579750347,
      "grad_norm": 2.1510701179504395,
      "learning_rate": 0.0006909570633152331,
      "loss": 0.5477,
      "step": 2710
    },
    {
      "epoch": 1.8862690707350902,
      "grad_norm": 3.3832509517669678,
      "learning_rate": 0.0006907866669987558,
      "loss": 0.5595,
      "step": 2720
    },
    {
      "epoch": 1.8932038834951457,
      "grad_norm": 1.4651689529418945,
      "learning_rate": 0.0006906162706822786,
      "loss": 0.5801,
      "step": 2730
    },
    {
      "epoch": 1.9001386962552012,
      "grad_norm": 2.4308760166168213,
      "learning_rate": 0.0006904458743658014,
      "loss": 0.5279,
      "step": 2740
    },
    {
      "epoch": 1.9070735090152566,
      "grad_norm": 1.5944020748138428,
      "learning_rate": 0.000690275478049324,
      "loss": 0.5845,
      "step": 2750
    },
    {
      "epoch": 1.914008321775312,
      "grad_norm": 2.4133236408233643,
      "learning_rate": 0.0006901050817328469,
      "loss": 0.5527,
      "step": 2760
    },
    {
      "epoch": 1.9209431345353676,
      "grad_norm": 1.48746919631958,
      "learning_rate": 0.0006899346854163696,
      "loss": 0.59,
      "step": 2770
    },
    {
      "epoch": 1.927877947295423,
      "grad_norm": 1.755818247795105,
      "learning_rate": 0.0006897642890998923,
      "loss": 0.4552,
      "step": 2780
    },
    {
      "epoch": 1.9348127600554785,
      "grad_norm": 1.8207330703735352,
      "learning_rate": 0.0006895938927834152,
      "loss": 0.4919,
      "step": 2790
    },
    {
      "epoch": 1.941747572815534,
      "grad_norm": 1.9271297454833984,
      "learning_rate": 0.0006894234964669379,
      "loss": 0.4872,
      "step": 2800
    },
    {
      "epoch": 1.9486823855755895,
      "grad_norm": 2.3742661476135254,
      "learning_rate": 0.0006892531001504606,
      "loss": 0.5924,
      "step": 2810
    },
    {
      "epoch": 1.955617198335645,
      "grad_norm": 2.8435704708099365,
      "learning_rate": 0.0006890827038339834,
      "loss": 0.4793,
      "step": 2820
    },
    {
      "epoch": 1.9625520110957004,
      "grad_norm": 1.4708276987075806,
      "learning_rate": 0.0006889123075175061,
      "loss": 0.6115,
      "step": 2830
    },
    {
      "epoch": 1.969486823855756,
      "grad_norm": 1.8421176671981812,
      "learning_rate": 0.0006887419112010289,
      "loss": 0.5572,
      "step": 2840
    },
    {
      "epoch": 1.9764216366158114,
      "grad_norm": 1.5779128074645996,
      "learning_rate": 0.0006885715148845517,
      "loss": 0.4812,
      "step": 2850
    },
    {
      "epoch": 1.9833564493758669,
      "grad_norm": 1.5726453065872192,
      "learning_rate": 0.0006884011185680744,
      "loss": 0.5916,
      "step": 2860
    },
    {
      "epoch": 1.9902912621359223,
      "grad_norm": 2.031681776046753,
      "learning_rate": 0.0006882307222515971,
      "loss": 0.4861,
      "step": 2870
    },
    {
      "epoch": 1.9972260748959778,
      "grad_norm": 3.706212043762207,
      "learning_rate": 0.0006880603259351199,
      "loss": 0.5301,
      "step": 2880
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.7596253902185224,
      "eval_auc": 0.7595966066098727,
      "eval_f1": 0.7377979568671964,
      "eval_loss": 0.5107985138893127,
      "eval_mcc": 0.5265111065241217,
      "eval_precision": 0.8111480865224625,
      "eval_recall": 0.6766134628730048,
      "eval_runtime": 78.9507,
      "eval_samples_per_second": 36.516,
      "eval_steps_per_second": 2.293,
      "step": 2884
    },
    {
      "epoch": 2.0041608876560333,
      "grad_norm": 2.9376046657562256,
      "learning_rate": 0.0006878899296186427,
      "loss": 0.5614,
      "step": 2890
    },
    {
      "epoch": 2.0110957004160888,
      "grad_norm": 1.317083716392517,
      "learning_rate": 0.0006877195333021654,
      "loss": 0.5607,
      "step": 2900
    },
    {
      "epoch": 2.0180305131761442,
      "grad_norm": 1.6867114305496216,
      "learning_rate": 0.0006875491369856882,
      "loss": 0.5469,
      "step": 2910
    },
    {
      "epoch": 2.0249653259361997,
      "grad_norm": 1.906951904296875,
      "learning_rate": 0.000687378740669211,
      "loss": 0.6033,
      "step": 2920
    },
    {
      "epoch": 2.031900138696255,
      "grad_norm": 2.266693115234375,
      "learning_rate": 0.0006872083443527336,
      "loss": 0.4852,
      "step": 2930
    },
    {
      "epoch": 2.0388349514563107,
      "grad_norm": 4.022829055786133,
      "learning_rate": 0.0006870379480362564,
      "loss": 0.5063,
      "step": 2940
    },
    {
      "epoch": 2.045769764216366,
      "grad_norm": 3.2651145458221436,
      "learning_rate": 0.0006868675517197792,
      "loss": 0.5156,
      "step": 2950
    },
    {
      "epoch": 2.0527045769764216,
      "grad_norm": 2.3161797523498535,
      "learning_rate": 0.0006866971554033019,
      "loss": 0.5526,
      "step": 2960
    },
    {
      "epoch": 2.059639389736477,
      "grad_norm": 3.1921908855438232,
      "learning_rate": 0.0006865267590868247,
      "loss": 0.5413,
      "step": 2970
    },
    {
      "epoch": 2.0665742024965326,
      "grad_norm": 2.179884433746338,
      "learning_rate": 0.0006863563627703475,
      "loss": 0.5275,
      "step": 2980
    },
    {
      "epoch": 2.073509015256588,
      "grad_norm": 4.942862510681152,
      "learning_rate": 0.0006861859664538701,
      "loss": 0.4787,
      "step": 2990
    },
    {
      "epoch": 2.0804438280166435,
      "grad_norm": 4.593550205230713,
      "learning_rate": 0.000686015570137393,
      "loss": 0.5515,
      "step": 3000
    },
    {
      "epoch": 2.087378640776699,
      "grad_norm": 1.4622410535812378,
      "learning_rate": 0.0006858451738209157,
      "loss": 0.4857,
      "step": 3010
    },
    {
      "epoch": 2.0943134535367545,
      "grad_norm": 1.4503401517868042,
      "learning_rate": 0.0006856747775044385,
      "loss": 0.5219,
      "step": 3020
    },
    {
      "epoch": 2.10124826629681,
      "grad_norm": 3.624464511871338,
      "learning_rate": 0.0006855043811879613,
      "loss": 0.473,
      "step": 3030
    },
    {
      "epoch": 2.1081830790568654,
      "grad_norm": 2.0608274936676025,
      "learning_rate": 0.000685333984871484,
      "loss": 0.6441,
      "step": 3040
    },
    {
      "epoch": 2.115117891816921,
      "grad_norm": 2.423114538192749,
      "learning_rate": 0.0006851635885550067,
      "loss": 0.5131,
      "step": 3050
    },
    {
      "epoch": 2.1220527045769764,
      "grad_norm": 2.9890594482421875,
      "learning_rate": 0.0006849931922385295,
      "loss": 0.6047,
      "step": 3060
    },
    {
      "epoch": 2.128987517337032,
      "grad_norm": 1.4115532636642456,
      "learning_rate": 0.0006848227959220522,
      "loss": 0.5978,
      "step": 3070
    },
    {
      "epoch": 2.1359223300970873,
      "grad_norm": 3.662229061126709,
      "learning_rate": 0.000684652399605575,
      "loss": 0.6016,
      "step": 3080
    },
    {
      "epoch": 2.142857142857143,
      "grad_norm": 1.1830841302871704,
      "learning_rate": 0.0006844820032890978,
      "loss": 0.5295,
      "step": 3090
    },
    {
      "epoch": 2.1497919556171983,
      "grad_norm": 2.8046491146087646,
      "learning_rate": 0.0006843116069726204,
      "loss": 0.5327,
      "step": 3100
    },
    {
      "epoch": 2.1567267683772537,
      "grad_norm": 1.4210965633392334,
      "learning_rate": 0.0006841412106561432,
      "loss": 0.5392,
      "step": 3110
    },
    {
      "epoch": 2.163661581137309,
      "grad_norm": 1.4437533617019653,
      "learning_rate": 0.000683970814339666,
      "loss": 0.5701,
      "step": 3120
    },
    {
      "epoch": 2.1705963938973647,
      "grad_norm": 1.537818193435669,
      "learning_rate": 0.0006838004180231888,
      "loss": 0.5656,
      "step": 3130
    },
    {
      "epoch": 2.17753120665742,
      "grad_norm": 2.0930209159851074,
      "learning_rate": 0.0006836300217067115,
      "loss": 0.4953,
      "step": 3140
    },
    {
      "epoch": 2.1844660194174756,
      "grad_norm": 1.8169419765472412,
      "learning_rate": 0.0006834596253902343,
      "loss": 0.5183,
      "step": 3150
    },
    {
      "epoch": 2.191400832177531,
      "grad_norm": 2.318708896636963,
      "learning_rate": 0.000683289229073757,
      "loss": 0.5739,
      "step": 3160
    },
    {
      "epoch": 2.1983356449375866,
      "grad_norm": 2.092993974685669,
      "learning_rate": 0.0006831188327572797,
      "loss": 0.5182,
      "step": 3170
    },
    {
      "epoch": 2.205270457697642,
      "grad_norm": 2.4479730129241943,
      "learning_rate": 0.0006829484364408025,
      "loss": 0.5112,
      "step": 3180
    },
    {
      "epoch": 2.2122052704576975,
      "grad_norm": 1.7749762535095215,
      "learning_rate": 0.0006827780401243253,
      "loss": 0.4648,
      "step": 3190
    },
    {
      "epoch": 2.219140083217753,
      "grad_norm": 1.6999132633209229,
      "learning_rate": 0.000682607643807848,
      "loss": 0.5039,
      "step": 3200
    },
    {
      "epoch": 2.2260748959778085,
      "grad_norm": 2.5017452239990234,
      "learning_rate": 0.0006824372474913709,
      "loss": 0.4799,
      "step": 3210
    },
    {
      "epoch": 2.233009708737864,
      "grad_norm": 3.483443260192871,
      "learning_rate": 0.0006822668511748935,
      "loss": 0.5713,
      "step": 3220
    },
    {
      "epoch": 2.2399445214979194,
      "grad_norm": 1.78951895236969,
      "learning_rate": 0.0006820964548584162,
      "loss": 0.5728,
      "step": 3230
    },
    {
      "epoch": 2.246879334257975,
      "grad_norm": 2.3487045764923096,
      "learning_rate": 0.0006819260585419391,
      "loss": 0.4756,
      "step": 3240
    },
    {
      "epoch": 2.2538141470180304,
      "grad_norm": 1.842257022857666,
      "learning_rate": 0.0006817556622254618,
      "loss": 0.5411,
      "step": 3250
    },
    {
      "epoch": 2.260748959778086,
      "grad_norm": 2.64607834815979,
      "learning_rate": 0.0006815852659089846,
      "loss": 0.5095,
      "step": 3260
    },
    {
      "epoch": 2.2676837725381414,
      "grad_norm": 1.9640790224075317,
      "learning_rate": 0.0006814148695925074,
      "loss": 0.4615,
      "step": 3270
    },
    {
      "epoch": 2.274618585298197,
      "grad_norm": 2.350407123565674,
      "learning_rate": 0.00068124447327603,
      "loss": 0.5298,
      "step": 3280
    },
    {
      "epoch": 2.2815533980582523,
      "grad_norm": 3.0320889949798584,
      "learning_rate": 0.0006810740769595528,
      "loss": 0.5594,
      "step": 3290
    },
    {
      "epoch": 2.2884882108183078,
      "grad_norm": 1.7141276597976685,
      "learning_rate": 0.0006809036806430756,
      "loss": 0.5306,
      "step": 3300
    },
    {
      "epoch": 2.2954230235783633,
      "grad_norm": 1.9611659049987793,
      "learning_rate": 0.0006807332843265983,
      "loss": 0.4333,
      "step": 3310
    },
    {
      "epoch": 2.3023578363384187,
      "grad_norm": 3.4424142837524414,
      "learning_rate": 0.0006805628880101211,
      "loss": 0.5115,
      "step": 3320
    },
    {
      "epoch": 2.309292649098474,
      "grad_norm": 2.4004926681518555,
      "learning_rate": 0.0006803924916936438,
      "loss": 0.4319,
      "step": 3330
    },
    {
      "epoch": 2.3162274618585297,
      "grad_norm": 2.398047924041748,
      "learning_rate": 0.0006802220953771666,
      "loss": 0.528,
      "step": 3340
    },
    {
      "epoch": 2.323162274618585,
      "grad_norm": 2.609149694442749,
      "learning_rate": 0.0006800516990606893,
      "loss": 0.5294,
      "step": 3350
    },
    {
      "epoch": 2.3300970873786406,
      "grad_norm": 2.004409074783325,
      "learning_rate": 0.0006798813027442121,
      "loss": 0.5621,
      "step": 3360
    },
    {
      "epoch": 2.337031900138696,
      "grad_norm": 1.3165234327316284,
      "learning_rate": 0.0006797109064277349,
      "loss": 0.5471,
      "step": 3370
    },
    {
      "epoch": 2.3439667128987516,
      "grad_norm": 4.447317600250244,
      "learning_rate": 0.0006795405101112576,
      "loss": 0.4588,
      "step": 3380
    },
    {
      "epoch": 2.350901525658807,
      "grad_norm": 4.768912315368652,
      "learning_rate": 0.0006793701137947803,
      "loss": 0.4966,
      "step": 3390
    },
    {
      "epoch": 2.3578363384188625,
      "grad_norm": 1.50713312625885,
      "learning_rate": 0.0006791997174783031,
      "loss": 0.5675,
      "step": 3400
    },
    {
      "epoch": 2.364771151178918,
      "grad_norm": 3.4926607608795166,
      "learning_rate": 0.0006790293211618258,
      "loss": 0.4478,
      "step": 3410
    },
    {
      "epoch": 2.3717059639389735,
      "grad_norm": 2.037506341934204,
      "learning_rate": 0.0006788589248453486,
      "loss": 0.5073,
      "step": 3420
    },
    {
      "epoch": 2.378640776699029,
      "grad_norm": 1.9119045734405518,
      "learning_rate": 0.0006786885285288714,
      "loss": 0.5843,
      "step": 3430
    },
    {
      "epoch": 2.3855755894590844,
      "grad_norm": 2.2035388946533203,
      "learning_rate": 0.000678518132212394,
      "loss": 0.5061,
      "step": 3440
    },
    {
      "epoch": 2.39251040221914,
      "grad_norm": 1.645658254623413,
      "learning_rate": 0.000678347735895917,
      "loss": 0.4772,
      "step": 3450
    },
    {
      "epoch": 2.3994452149791954,
      "grad_norm": 1.7305666208267212,
      "learning_rate": 0.0006781773395794396,
      "loss": 0.4755,
      "step": 3460
    },
    {
      "epoch": 2.406380027739251,
      "grad_norm": 1.4343022108078003,
      "learning_rate": 0.0006780069432629624,
      "loss": 0.5284,
      "step": 3470
    },
    {
      "epoch": 2.4133148404993063,
      "grad_norm": 1.4467483758926392,
      "learning_rate": 0.0006778365469464852,
      "loss": 0.4448,
      "step": 3480
    },
    {
      "epoch": 2.420249653259362,
      "grad_norm": 3.3418774604797363,
      "learning_rate": 0.0006776661506300079,
      "loss": 0.4885,
      "step": 3490
    },
    {
      "epoch": 2.4271844660194173,
      "grad_norm": 2.2839572429656982,
      "learning_rate": 0.0006774957543135307,
      "loss": 0.6205,
      "step": 3500
    },
    {
      "epoch": 2.4341192787794728,
      "grad_norm": 1.3095102310180664,
      "learning_rate": 0.0006773253579970534,
      "loss": 0.5207,
      "step": 3510
    },
    {
      "epoch": 2.4410540915395282,
      "grad_norm": 2.9148049354553223,
      "learning_rate": 0.0006771549616805761,
      "loss": 0.5102,
      "step": 3520
    },
    {
      "epoch": 2.447988904299584,
      "grad_norm": 2.1358492374420166,
      "learning_rate": 0.0006769845653640989,
      "loss": 0.5612,
      "step": 3530
    },
    {
      "epoch": 2.454923717059639,
      "grad_norm": 1.255211591720581,
      "learning_rate": 0.0006768141690476217,
      "loss": 0.5462,
      "step": 3540
    },
    {
      "epoch": 2.461858529819695,
      "grad_norm": 1.2182673215866089,
      "learning_rate": 0.0006766437727311445,
      "loss": 0.5097,
      "step": 3550
    },
    {
      "epoch": 2.46879334257975,
      "grad_norm": 4.506428241729736,
      "learning_rate": 0.0006764733764146671,
      "loss": 0.5121,
      "step": 3560
    },
    {
      "epoch": 2.475728155339806,
      "grad_norm": 2.379492998123169,
      "learning_rate": 0.0006763029800981899,
      "loss": 0.5643,
      "step": 3570
    },
    {
      "epoch": 2.482662968099861,
      "grad_norm": 2.2784485816955566,
      "learning_rate": 0.0006761325837817127,
      "loss": 0.4843,
      "step": 3580
    },
    {
      "epoch": 2.489597780859917,
      "grad_norm": 4.302100658416748,
      "learning_rate": 0.0006759621874652354,
      "loss": 0.4841,
      "step": 3590
    },
    {
      "epoch": 2.496532593619972,
      "grad_norm": 2.842172145843506,
      "learning_rate": 0.0006757917911487582,
      "loss": 0.5044,
      "step": 3600
    },
    {
      "epoch": 2.503467406380028,
      "grad_norm": 1.67918860912323,
      "learning_rate": 0.000675621394832281,
      "loss": 0.51,
      "step": 3610
    },
    {
      "epoch": 2.510402219140083,
      "grad_norm": 2.181461811065674,
      "learning_rate": 0.0006754509985158036,
      "loss": 0.5652,
      "step": 3620
    },
    {
      "epoch": 2.517337031900139,
      "grad_norm": 1.598584771156311,
      "learning_rate": 0.0006752806021993264,
      "loss": 0.5128,
      "step": 3630
    },
    {
      "epoch": 2.524271844660194,
      "grad_norm": 4.11177921295166,
      "learning_rate": 0.0006751102058828492,
      "loss": 0.5809,
      "step": 3640
    },
    {
      "epoch": 2.53120665742025,
      "grad_norm": 1.681580662727356,
      "learning_rate": 0.0006749398095663719,
      "loss": 0.4421,
      "step": 3650
    },
    {
      "epoch": 2.538141470180305,
      "grad_norm": 2.8141472339630127,
      "learning_rate": 0.0006747694132498948,
      "loss": 0.5125,
      "step": 3660
    },
    {
      "epoch": 2.545076282940361,
      "grad_norm": 1.448059320449829,
      "learning_rate": 0.0006745990169334175,
      "loss": 0.4921,
      "step": 3670
    },
    {
      "epoch": 2.552011095700416,
      "grad_norm": 2.3050100803375244,
      "learning_rate": 0.0006744286206169402,
      "loss": 0.5241,
      "step": 3680
    },
    {
      "epoch": 2.5589459084604718,
      "grad_norm": 1.4639695882797241,
      "learning_rate": 0.000674258224300463,
      "loss": 0.5835,
      "step": 3690
    },
    {
      "epoch": 2.565880721220527,
      "grad_norm": 1.501936674118042,
      "learning_rate": 0.0006740878279839857,
      "loss": 0.4413,
      "step": 3700
    },
    {
      "epoch": 2.5728155339805827,
      "grad_norm": 2.7524383068084717,
      "learning_rate": 0.0006739174316675085,
      "loss": 0.654,
      "step": 3710
    },
    {
      "epoch": 2.5797503467406377,
      "grad_norm": 1.6254639625549316,
      "learning_rate": 0.0006737470353510313,
      "loss": 0.554,
      "step": 3720
    },
    {
      "epoch": 2.5866851595006937,
      "grad_norm": 1.9200307130813599,
      "learning_rate": 0.000673576639034554,
      "loss": 0.544,
      "step": 3730
    },
    {
      "epoch": 2.5936199722607487,
      "grad_norm": 1.233124017715454,
      "learning_rate": 0.0006734062427180767,
      "loss": 0.5678,
      "step": 3740
    },
    {
      "epoch": 2.6005547850208046,
      "grad_norm": 1.406691551208496,
      "learning_rate": 0.0006732358464015995,
      "loss": 0.5532,
      "step": 3750
    },
    {
      "epoch": 2.6074895977808596,
      "grad_norm": 1.4530609846115112,
      "learning_rate": 0.0006730654500851222,
      "loss": 0.5048,
      "step": 3760
    },
    {
      "epoch": 2.6144244105409156,
      "grad_norm": 1.2101929187774658,
      "learning_rate": 0.000672895053768645,
      "loss": 0.5093,
      "step": 3770
    },
    {
      "epoch": 2.6213592233009706,
      "grad_norm": 1.8052048683166504,
      "learning_rate": 0.0006727246574521678,
      "loss": 0.5658,
      "step": 3780
    },
    {
      "epoch": 2.6282940360610265,
      "grad_norm": 1.45345938205719,
      "learning_rate": 0.0006725542611356906,
      "loss": 0.5392,
      "step": 3790
    },
    {
      "epoch": 2.635228848821082,
      "grad_norm": 1.6566511392593384,
      "learning_rate": 0.0006723838648192132,
      "loss": 0.5515,
      "step": 3800
    },
    {
      "epoch": 2.6421636615811375,
      "grad_norm": 2.5798499584198,
      "learning_rate": 0.000672213468502736,
      "loss": 0.5318,
      "step": 3810
    },
    {
      "epoch": 2.649098474341193,
      "grad_norm": 1.9323498010635376,
      "learning_rate": 0.0006720430721862588,
      "loss": 0.5604,
      "step": 3820
    },
    {
      "epoch": 2.6560332871012484,
      "grad_norm": 2.7718288898468018,
      "learning_rate": 0.0006718726758697815,
      "loss": 0.4869,
      "step": 3830
    },
    {
      "epoch": 2.662968099861304,
      "grad_norm": 1.9202797412872314,
      "learning_rate": 0.0006717022795533043,
      "loss": 0.5315,
      "step": 3840
    },
    {
      "epoch": 2.6699029126213594,
      "grad_norm": 4.521284103393555,
      "learning_rate": 0.000671531883236827,
      "loss": 0.4566,
      "step": 3850
    },
    {
      "epoch": 2.676837725381415,
      "grad_norm": 3.1131927967071533,
      "learning_rate": 0.0006713614869203497,
      "loss": 0.4399,
      "step": 3860
    },
    {
      "epoch": 2.6837725381414703,
      "grad_norm": 1.961970329284668,
      "learning_rate": 0.0006711910906038726,
      "loss": 0.5431,
      "step": 3870
    },
    {
      "epoch": 2.690707350901526,
      "grad_norm": 1.7107720375061035,
      "learning_rate": 0.0006710206942873953,
      "loss": 0.5133,
      "step": 3880
    },
    {
      "epoch": 2.6976421636615813,
      "grad_norm": 1.2820453643798828,
      "learning_rate": 0.000670850297970918,
      "loss": 0.5566,
      "step": 3890
    },
    {
      "epoch": 2.7045769764216367,
      "grad_norm": 1.3946800231933594,
      "learning_rate": 0.0006706799016544409,
      "loss": 0.4613,
      "step": 3900
    },
    {
      "epoch": 2.7115117891816922,
      "grad_norm": 3.647977590560913,
      "learning_rate": 0.0006705095053379635,
      "loss": 0.4752,
      "step": 3910
    },
    {
      "epoch": 2.7184466019417477,
      "grad_norm": 1.401490330696106,
      "learning_rate": 0.0006703391090214863,
      "loss": 0.5363,
      "step": 3920
    },
    {
      "epoch": 2.725381414701803,
      "grad_norm": 1.6901662349700928,
      "learning_rate": 0.0006701687127050091,
      "loss": 0.4882,
      "step": 3930
    },
    {
      "epoch": 2.7323162274618586,
      "grad_norm": 2.3615734577178955,
      "learning_rate": 0.0006699983163885318,
      "loss": 0.4421,
      "step": 3940
    },
    {
      "epoch": 2.739251040221914,
      "grad_norm": 1.7621430158615112,
      "learning_rate": 0.0006698279200720546,
      "loss": 0.5819,
      "step": 3950
    },
    {
      "epoch": 2.7461858529819696,
      "grad_norm": 1.2061339616775513,
      "learning_rate": 0.0006696575237555774,
      "loss": 0.51,
      "step": 3960
    },
    {
      "epoch": 2.753120665742025,
      "grad_norm": 3.0981955528259277,
      "learning_rate": 0.0006694871274391,
      "loss": 0.6098,
      "step": 3970
    },
    {
      "epoch": 2.7600554785020806,
      "grad_norm": 2.9153831005096436,
      "learning_rate": 0.0006693167311226228,
      "loss": 0.5087,
      "step": 3980
    },
    {
      "epoch": 2.766990291262136,
      "grad_norm": 1.7158527374267578,
      "learning_rate": 0.0006691463348061456,
      "loss": 0.5032,
      "step": 3990
    },
    {
      "epoch": 2.7739251040221915,
      "grad_norm": 2.416811943054199,
      "learning_rate": 0.0006689759384896684,
      "loss": 0.542,
      "step": 4000
    },
    {
      "epoch": 2.780859916782247,
      "grad_norm": 1.7076170444488525,
      "learning_rate": 0.0006688055421731911,
      "loss": 0.5313,
      "step": 4010
    },
    {
      "epoch": 2.7877947295423025,
      "grad_norm": 2.693986654281616,
      "learning_rate": 0.0006686351458567139,
      "loss": 0.5043,
      "step": 4020
    },
    {
      "epoch": 2.794729542302358,
      "grad_norm": 1.7893235683441162,
      "learning_rate": 0.0006684647495402366,
      "loss": 0.5028,
      "step": 4030
    },
    {
      "epoch": 2.8016643550624134,
      "grad_norm": 1.7661175727844238,
      "learning_rate": 0.0006682943532237593,
      "loss": 0.4847,
      "step": 4040
    },
    {
      "epoch": 2.808599167822469,
      "grad_norm": 1.655048131942749,
      "learning_rate": 0.0006681239569072821,
      "loss": 0.4663,
      "step": 4050
    },
    {
      "epoch": 2.8155339805825244,
      "grad_norm": 3.030679941177368,
      "learning_rate": 0.0006679535605908049,
      "loss": 0.4599,
      "step": 4060
    },
    {
      "epoch": 2.82246879334258,
      "grad_norm": 1.5405709743499756,
      "learning_rate": 0.0006677831642743276,
      "loss": 0.5045,
      "step": 4070
    },
    {
      "epoch": 2.8294036061026353,
      "grad_norm": 1.5226161479949951,
      "learning_rate": 0.0006676127679578504,
      "loss": 0.5025,
      "step": 4080
    },
    {
      "epoch": 2.836338418862691,
      "grad_norm": 2.0343401432037354,
      "learning_rate": 0.0006674423716413731,
      "loss": 0.546,
      "step": 4090
    },
    {
      "epoch": 2.8432732316227463,
      "grad_norm": 4.060346603393555,
      "learning_rate": 0.0006672719753248958,
      "loss": 0.393,
      "step": 4100
    },
    {
      "epoch": 2.8502080443828017,
      "grad_norm": 3.1155412197113037,
      "learning_rate": 0.0006671015790084187,
      "loss": 0.5073,
      "step": 4110
    },
    {
      "epoch": 2.857142857142857,
      "grad_norm": 2.166045904159546,
      "learning_rate": 0.0006669311826919414,
      "loss": 0.4345,
      "step": 4120
    },
    {
      "epoch": 2.8640776699029127,
      "grad_norm": 4.6675639152526855,
      "learning_rate": 0.0006667607863754642,
      "loss": 0.5105,
      "step": 4130
    },
    {
      "epoch": 2.871012482662968,
      "grad_norm": 1.0988012552261353,
      "learning_rate": 0.000666590390058987,
      "loss": 0.5974,
      "step": 4140
    },
    {
      "epoch": 2.8779472954230236,
      "grad_norm": 1.1819608211517334,
      "learning_rate": 0.0006664199937425096,
      "loss": 0.4737,
      "step": 4150
    },
    {
      "epoch": 2.884882108183079,
      "grad_norm": 2.365739107131958,
      "learning_rate": 0.0006662495974260324,
      "loss": 0.4996,
      "step": 4160
    },
    {
      "epoch": 2.8918169209431346,
      "grad_norm": 2.9419634342193604,
      "learning_rate": 0.0006660792011095552,
      "loss": 0.5003,
      "step": 4170
    },
    {
      "epoch": 2.89875173370319,
      "grad_norm": 1.1952985525131226,
      "learning_rate": 0.0006659088047930779,
      "loss": 0.494,
      "step": 4180
    },
    {
      "epoch": 2.9056865464632455,
      "grad_norm": 1.312381386756897,
      "learning_rate": 0.0006657384084766007,
      "loss": 0.4995,
      "step": 4190
    },
    {
      "epoch": 2.912621359223301,
      "grad_norm": 1.350519061088562,
      "learning_rate": 0.0006655680121601234,
      "loss": 0.5017,
      "step": 4200
    },
    {
      "epoch": 2.9195561719833565,
      "grad_norm": 2.8000900745391846,
      "learning_rate": 0.0006653976158436461,
      "loss": 0.5439,
      "step": 4210
    },
    {
      "epoch": 2.926490984743412,
      "grad_norm": 1.599941611289978,
      "learning_rate": 0.0006652272195271689,
      "loss": 0.5296,
      "step": 4220
    },
    {
      "epoch": 2.9334257975034674,
      "grad_norm": 2.1716978549957275,
      "learning_rate": 0.0006650568232106917,
      "loss": 0.4589,
      "step": 4230
    },
    {
      "epoch": 2.940360610263523,
      "grad_norm": 3.4585015773773193,
      "learning_rate": 0.0006648864268942145,
      "loss": 0.516,
      "step": 4240
    },
    {
      "epoch": 2.9472954230235784,
      "grad_norm": 2.548281192779541,
      "learning_rate": 0.0006647160305777372,
      "loss": 0.4956,
      "step": 4250
    },
    {
      "epoch": 2.954230235783634,
      "grad_norm": 2.4814085960388184,
      "learning_rate": 0.0006645456342612599,
      "loss": 0.5223,
      "step": 4260
    },
    {
      "epoch": 2.9611650485436893,
      "grad_norm": 1.3330512046813965,
      "learning_rate": 0.0006643752379447827,
      "loss": 0.448,
      "step": 4270
    },
    {
      "epoch": 2.968099861303745,
      "grad_norm": 2.444063186645508,
      "learning_rate": 0.0006642048416283054,
      "loss": 0.5374,
      "step": 4280
    },
    {
      "epoch": 2.9750346740638003,
      "grad_norm": 2.785417079925537,
      "learning_rate": 0.0006640344453118282,
      "loss": 0.5783,
      "step": 4290
    },
    {
      "epoch": 2.9819694868238558,
      "grad_norm": 1.7761352062225342,
      "learning_rate": 0.000663864048995351,
      "loss": 0.5329,
      "step": 4300
    },
    {
      "epoch": 2.9889042995839112,
      "grad_norm": 2.158060312271118,
      "learning_rate": 0.0006636936526788737,
      "loss": 0.6241,
      "step": 4310
    },
    {
      "epoch": 2.9958391123439667,
      "grad_norm": 1.538686752319336,
      "learning_rate": 0.0006635232563623965,
      "loss": 0.5893,
      "step": 4320
    },
    {
      "epoch": 3.0,
      "eval_accuracy": 0.6985778702740201,
      "eval_auc": 0.6984889230683347,
      "eval_f1": 0.594493700419972,
      "eval_loss": 0.5682880878448486,
      "eval_mcc": 0.4624709979061853,
      "eval_precision": 0.9074074074074074,
      "eval_recall": 0.44205412907702984,
      "eval_runtime": 79.0161,
      "eval_samples_per_second": 36.486,
      "eval_steps_per_second": 2.291,
      "step": 4326
    },
    {
      "epoch": 3.002773925104022,
      "grad_norm": 2.4202992916107178,
      "learning_rate": 0.0006633528600459192,
      "loss": 0.5294,
      "step": 4330
    },
    {
      "epoch": 3.0097087378640777,
      "grad_norm": 2.760228395462036,
      "learning_rate": 0.0006631824637294419,
      "loss": 0.5447,
      "step": 4340
    },
    {
      "epoch": 3.016643550624133,
      "grad_norm": 2.470874786376953,
      "learning_rate": 0.0006630120674129648,
      "loss": 0.5448,
      "step": 4350
    },
    {
      "epoch": 3.0235783633841886,
      "grad_norm": 1.7631680965423584,
      "learning_rate": 0.0006628416710964875,
      "loss": 0.4168,
      "step": 4360
    },
    {
      "epoch": 3.030513176144244,
      "grad_norm": 2.692415952682495,
      "learning_rate": 0.0006626712747800103,
      "loss": 0.4985,
      "step": 4370
    },
    {
      "epoch": 3.0374479889042996,
      "grad_norm": 2.034578561782837,
      "learning_rate": 0.000662500878463533,
      "loss": 0.6216,
      "step": 4380
    },
    {
      "epoch": 3.044382801664355,
      "grad_norm": 1.1037670373916626,
      "learning_rate": 0.0006623304821470557,
      "loss": 0.5075,
      "step": 4390
    },
    {
      "epoch": 3.0513176144244105,
      "grad_norm": 2.342862844467163,
      "learning_rate": 0.0006621600858305785,
      "loss": 0.584,
      "step": 4400
    },
    {
      "epoch": 3.058252427184466,
      "grad_norm": 2.2949609756469727,
      "learning_rate": 0.0006619896895141013,
      "loss": 0.4865,
      "step": 4410
    },
    {
      "epoch": 3.0651872399445215,
      "grad_norm": 1.708966851234436,
      "learning_rate": 0.000661819293197624,
      "loss": 0.4493,
      "step": 4420
    },
    {
      "epoch": 3.072122052704577,
      "grad_norm": 1.4939913749694824,
      "learning_rate": 0.0006616488968811467,
      "loss": 0.4527,
      "step": 4430
    },
    {
      "epoch": 3.0790568654646324,
      "grad_norm": 1.172658085823059,
      "learning_rate": 0.0006614785005646695,
      "loss": 0.4859,
      "step": 4440
    },
    {
      "epoch": 3.085991678224688,
      "grad_norm": 1.5364608764648438,
      "learning_rate": 0.0006613081042481923,
      "loss": 0.437,
      "step": 4450
    },
    {
      "epoch": 3.0929264909847434,
      "grad_norm": 2.6704626083374023,
      "learning_rate": 0.000661137707931715,
      "loss": 0.4989,
      "step": 4460
    },
    {
      "epoch": 3.099861303744799,
      "grad_norm": 1.3709462881088257,
      "learning_rate": 0.0006609673116152378,
      "loss": 0.5313,
      "step": 4470
    },
    {
      "epoch": 3.1067961165048543,
      "grad_norm": 2.929011344909668,
      "learning_rate": 0.0006607969152987606,
      "loss": 0.5068,
      "step": 4480
    },
    {
      "epoch": 3.11373092926491,
      "grad_norm": 1.3217432498931885,
      "learning_rate": 0.0006606265189822832,
      "loss": 0.4081,
      "step": 4490
    },
    {
      "epoch": 3.1206657420249653,
      "grad_norm": 3.018068790435791,
      "learning_rate": 0.000660456122665806,
      "loss": 0.4277,
      "step": 4500
    },
    {
      "epoch": 3.1276005547850207,
      "grad_norm": 2.0386316776275635,
      "learning_rate": 0.0006602857263493288,
      "loss": 0.4866,
      "step": 4510
    },
    {
      "epoch": 3.1345353675450762,
      "grad_norm": 3.1054019927978516,
      "learning_rate": 0.0006601153300328515,
      "loss": 0.56,
      "step": 4520
    },
    {
      "epoch": 3.1414701803051317,
      "grad_norm": 2.2777957916259766,
      "learning_rate": 0.0006599449337163744,
      "loss": 0.4778,
      "step": 4530
    },
    {
      "epoch": 3.148404993065187,
      "grad_norm": 4.157938003540039,
      "learning_rate": 0.0006597745373998971,
      "loss": 0.5126,
      "step": 4540
    },
    {
      "epoch": 3.1553398058252426,
      "grad_norm": 3.0733249187469482,
      "learning_rate": 0.0006596041410834197,
      "loss": 0.5455,
      "step": 4550
    },
    {
      "epoch": 3.162274618585298,
      "grad_norm": 1.8554762601852417,
      "learning_rate": 0.0006594337447669426,
      "loss": 0.4423,
      "step": 4560
    },
    {
      "epoch": 3.1692094313453536,
      "grad_norm": 1.6188188791275024,
      "learning_rate": 0.0006592633484504653,
      "loss": 0.5469,
      "step": 4570
    },
    {
      "epoch": 3.176144244105409,
      "grad_norm": 2.329759120941162,
      "learning_rate": 0.0006590929521339881,
      "loss": 0.4949,
      "step": 4580
    },
    {
      "epoch": 3.1830790568654646,
      "grad_norm": 1.800742745399475,
      "learning_rate": 0.0006589225558175109,
      "loss": 0.4288,
      "step": 4590
    },
    {
      "epoch": 3.19001386962552,
      "grad_norm": 2.3120830059051514,
      "learning_rate": 0.0006587521595010336,
      "loss": 0.557,
      "step": 4600
    },
    {
      "epoch": 3.1969486823855755,
      "grad_norm": 2.1347107887268066,
      "learning_rate": 0.0006585817631845563,
      "loss": 0.5268,
      "step": 4610
    },
    {
      "epoch": 3.203883495145631,
      "grad_norm": 1.412336826324463,
      "learning_rate": 0.0006584113668680791,
      "loss": 0.4651,
      "step": 4620
    },
    {
      "epoch": 3.2108183079056865,
      "grad_norm": 2.367347478866577,
      "learning_rate": 0.0006582409705516018,
      "loss": 0.5818,
      "step": 4630
    },
    {
      "epoch": 3.217753120665742,
      "grad_norm": 1.6946889162063599,
      "learning_rate": 0.0006580705742351246,
      "loss": 0.5222,
      "step": 4640
    },
    {
      "epoch": 3.2246879334257974,
      "grad_norm": 1.641872525215149,
      "learning_rate": 0.0006579001779186474,
      "loss": 0.4703,
      "step": 4650
    },
    {
      "epoch": 3.231622746185853,
      "grad_norm": 2.56396746635437,
      "learning_rate": 0.0006577297816021702,
      "loss": 0.5805,
      "step": 4660
    },
    {
      "epoch": 3.2385575589459084,
      "grad_norm": 2.187238931655884,
      "learning_rate": 0.0006575593852856928,
      "loss": 0.4857,
      "step": 4670
    },
    {
      "epoch": 3.245492371705964,
      "grad_norm": 3.9512429237365723,
      "learning_rate": 0.0006573889889692156,
      "loss": 0.4889,
      "step": 4680
    },
    {
      "epoch": 3.2524271844660193,
      "grad_norm": 3.078899383544922,
      "learning_rate": 0.0006572185926527384,
      "loss": 0.5447,
      "step": 4690
    },
    {
      "epoch": 3.259361997226075,
      "grad_norm": 1.7697066068649292,
      "learning_rate": 0.0006570481963362611,
      "loss": 0.4256,
      "step": 4700
    },
    {
      "epoch": 3.2662968099861303,
      "grad_norm": 2.631469488143921,
      "learning_rate": 0.0006568778000197839,
      "loss": 0.5228,
      "step": 4710
    },
    {
      "epoch": 3.2732316227461857,
      "grad_norm": 2.4766194820404053,
      "learning_rate": 0.0006567074037033067,
      "loss": 0.5137,
      "step": 4720
    },
    {
      "epoch": 3.280166435506241,
      "grad_norm": 3.20540189743042,
      "learning_rate": 0.0006565370073868293,
      "loss": 0.5079,
      "step": 4730
    },
    {
      "epoch": 3.2871012482662967,
      "grad_norm": 3.662189245223999,
      "learning_rate": 0.0006563666110703521,
      "loss": 0.5511,
      "step": 4740
    },
    {
      "epoch": 3.294036061026352,
      "grad_norm": 2.1821329593658447,
      "learning_rate": 0.0006561962147538749,
      "loss": 0.4616,
      "step": 4750
    },
    {
      "epoch": 3.3009708737864076,
      "grad_norm": 2.887545585632324,
      "learning_rate": 0.0006560258184373976,
      "loss": 0.5464,
      "step": 4760
    },
    {
      "epoch": 3.307905686546463,
      "grad_norm": 1.6960879564285278,
      "learning_rate": 0.0006558554221209205,
      "loss": 0.4057,
      "step": 4770
    },
    {
      "epoch": 3.3148404993065186,
      "grad_norm": 2.3717048168182373,
      "learning_rate": 0.0006556850258044431,
      "loss": 0.4612,
      "step": 4780
    },
    {
      "epoch": 3.321775312066574,
      "grad_norm": 3.38777494430542,
      "learning_rate": 0.0006555146294879659,
      "loss": 0.4036,
      "step": 4790
    },
    {
      "epoch": 3.3287101248266295,
      "grad_norm": 0.9422338604927063,
      "learning_rate": 0.0006553442331714887,
      "loss": 0.4742,
      "step": 4800
    },
    {
      "epoch": 3.335644937586685,
      "grad_norm": 3.056089162826538,
      "learning_rate": 0.0006551738368550114,
      "loss": 0.5026,
      "step": 4810
    },
    {
      "epoch": 3.3425797503467405,
      "grad_norm": 3.677644729614258,
      "learning_rate": 0.0006550034405385342,
      "loss": 0.4212,
      "step": 4820
    },
    {
      "epoch": 3.349514563106796,
      "grad_norm": 3.3898446559906006,
      "learning_rate": 0.000654833044222057,
      "loss": 0.5337,
      "step": 4830
    },
    {
      "epoch": 3.3564493758668514,
      "grad_norm": 1.407984733581543,
      "learning_rate": 0.0006546626479055796,
      "loss": 0.5109,
      "step": 4840
    },
    {
      "epoch": 3.363384188626907,
      "grad_norm": 2.2779805660247803,
      "learning_rate": 0.0006544922515891024,
      "loss": 0.4537,
      "step": 4850
    },
    {
      "epoch": 3.3703190013869624,
      "grad_norm": 1.5162807703018188,
      "learning_rate": 0.0006543218552726252,
      "loss": 0.4762,
      "step": 4860
    },
    {
      "epoch": 3.377253814147018,
      "grad_norm": 1.6534308195114136,
      "learning_rate": 0.0006541514589561479,
      "loss": 0.5853,
      "step": 4870
    },
    {
      "epoch": 3.3841886269070733,
      "grad_norm": 1.1959632635116577,
      "learning_rate": 0.0006539810626396707,
      "loss": 0.4803,
      "step": 4880
    },
    {
      "epoch": 3.391123439667129,
      "grad_norm": 2.2977190017700195,
      "learning_rate": 0.0006538106663231935,
      "loss": 0.431,
      "step": 4890
    },
    {
      "epoch": 3.3980582524271843,
      "grad_norm": 3.0259077548980713,
      "learning_rate": 0.0006536402700067162,
      "loss": 0.4133,
      "step": 4900
    },
    {
      "epoch": 3.4049930651872398,
      "grad_norm": 2.365050792694092,
      "learning_rate": 0.0006534698736902389,
      "loss": 0.427,
      "step": 4910
    },
    {
      "epoch": 3.4119278779472952,
      "grad_norm": 1.084250569343567,
      "learning_rate": 0.0006532994773737617,
      "loss": 0.4584,
      "step": 4920
    },
    {
      "epoch": 3.4188626907073507,
      "grad_norm": 3.5922179222106934,
      "learning_rate": 0.0006531290810572845,
      "loss": 0.508,
      "step": 4930
    },
    {
      "epoch": 3.425797503467406,
      "grad_norm": 3.1317288875579834,
      "learning_rate": 0.0006529586847408072,
      "loss": 0.454,
      "step": 4940
    },
    {
      "epoch": 3.4327323162274617,
      "grad_norm": 3.2808356285095215,
      "learning_rate": 0.00065278828842433,
      "loss": 0.4325,
      "step": 4950
    },
    {
      "epoch": 3.4396671289875176,
      "grad_norm": 1.7898467779159546,
      "learning_rate": 0.0006526178921078527,
      "loss": 0.5376,
      "step": 4960
    },
    {
      "epoch": 3.4466019417475726,
      "grad_norm": 2.686356782913208,
      "learning_rate": 0.0006524474957913754,
      "loss": 0.5112,
      "step": 4970
    },
    {
      "epoch": 3.4535367545076285,
      "grad_norm": 1.0982630252838135,
      "learning_rate": 0.0006522770994748983,
      "loss": 0.4337,
      "step": 4980
    },
    {
      "epoch": 3.4604715672676836,
      "grad_norm": 1.7776333093643188,
      "learning_rate": 0.000652106703158421,
      "loss": 0.6084,
      "step": 4990
    },
    {
      "epoch": 3.4674063800277395,
      "grad_norm": 1.5480074882507324,
      "learning_rate": 0.0006519363068419437,
      "loss": 0.453,
      "step": 5000
    },
    {
      "epoch": 3.4743411927877945,
      "grad_norm": 2.413275718688965,
      "learning_rate": 0.0006517659105254666,
      "loss": 0.5219,
      "step": 5010
    },
    {
      "epoch": 3.4812760055478504,
      "grad_norm": 2.562786102294922,
      "learning_rate": 0.0006515955142089892,
      "loss": 0.4173,
      "step": 5020
    },
    {
      "epoch": 3.4882108183079055,
      "grad_norm": 4.2402024269104,
      "learning_rate": 0.000651425117892512,
      "loss": 0.5464,
      "step": 5030
    },
    {
      "epoch": 3.4951456310679614,
      "grad_norm": 3.511176824569702,
      "learning_rate": 0.0006512547215760348,
      "loss": 0.4546,
      "step": 5040
    },
    {
      "epoch": 3.5020804438280164,
      "grad_norm": 1.1758594512939453,
      "learning_rate": 0.0006510843252595575,
      "loss": 0.5372,
      "step": 5050
    },
    {
      "epoch": 3.5090152565880723,
      "grad_norm": 1.7843220233917236,
      "learning_rate": 0.0006509139289430803,
      "loss": 0.3985,
      "step": 5060
    },
    {
      "epoch": 3.5159500693481274,
      "grad_norm": 3.1197011470794678,
      "learning_rate": 0.000650743532626603,
      "loss": 0.415,
      "step": 5070
    },
    {
      "epoch": 3.5228848821081833,
      "grad_norm": 4.201744079589844,
      "learning_rate": 0.0006505731363101257,
      "loss": 0.468,
      "step": 5080
    },
    {
      "epoch": 3.5298196948682383,
      "grad_norm": 1.2371577024459839,
      "learning_rate": 0.0006504027399936485,
      "loss": 0.4172,
      "step": 5090
    },
    {
      "epoch": 3.5367545076282942,
      "grad_norm": 2.270930767059326,
      "learning_rate": 0.0006502323436771713,
      "loss": 0.5645,
      "step": 5100
    },
    {
      "epoch": 3.5436893203883493,
      "grad_norm": 2.8321421146392822,
      "learning_rate": 0.0006500619473606941,
      "loss": 0.5783,
      "step": 5110
    },
    {
      "epoch": 3.550624133148405,
      "grad_norm": 1.5579631328582764,
      "learning_rate": 0.0006498915510442168,
      "loss": 0.5124,
      "step": 5120
    },
    {
      "epoch": 3.5575589459084602,
      "grad_norm": 1.8635655641555786,
      "learning_rate": 0.0006497211547277395,
      "loss": 0.5508,
      "step": 5130
    },
    {
      "epoch": 3.564493758668516,
      "grad_norm": 2.512904405593872,
      "learning_rate": 0.0006495507584112623,
      "loss": 0.5134,
      "step": 5140
    },
    {
      "epoch": 3.571428571428571,
      "grad_norm": 1.618710994720459,
      "learning_rate": 0.000649380362094785,
      "loss": 0.4864,
      "step": 5150
    },
    {
      "epoch": 3.578363384188627,
      "grad_norm": 1.5866749286651611,
      "learning_rate": 0.0006492099657783078,
      "loss": 0.4665,
      "step": 5160
    },
    {
      "epoch": 3.585298196948682,
      "grad_norm": 2.246417284011841,
      "learning_rate": 0.0006490395694618306,
      "loss": 0.4502,
      "step": 5170
    },
    {
      "epoch": 3.592233009708738,
      "grad_norm": 2.1368441581726074,
      "learning_rate": 0.0006488691731453532,
      "loss": 0.4515,
      "step": 5180
    },
    {
      "epoch": 3.599167822468793,
      "grad_norm": 2.5997345447540283,
      "learning_rate": 0.0006486987768288761,
      "loss": 0.4773,
      "step": 5190
    },
    {
      "epoch": 3.606102635228849,
      "grad_norm": 4.477546215057373,
      "learning_rate": 0.0006485283805123988,
      "loss": 0.4403,
      "step": 5200
    },
    {
      "epoch": 3.613037447988904,
      "grad_norm": 4.018711566925049,
      "learning_rate": 0.0006483579841959215,
      "loss": 0.5598,
      "step": 5210
    },
    {
      "epoch": 3.61997226074896,
      "grad_norm": 1.2130476236343384,
      "learning_rate": 0.0006481875878794444,
      "loss": 0.4464,
      "step": 5220
    },
    {
      "epoch": 3.6269070735090154,
      "grad_norm": 1.78815758228302,
      "learning_rate": 0.0006480171915629671,
      "loss": 0.5251,
      "step": 5230
    },
    {
      "epoch": 3.633841886269071,
      "grad_norm": 3.3007380962371826,
      "learning_rate": 0.0006478467952464899,
      "loss": 0.5357,
      "step": 5240
    },
    {
      "epoch": 3.6407766990291264,
      "grad_norm": 1.668614387512207,
      "learning_rate": 0.0006476763989300126,
      "loss": 0.5355,
      "step": 5250
    },
    {
      "epoch": 3.647711511789182,
      "grad_norm": 2.2288901805877686,
      "learning_rate": 0.0006475060026135353,
      "loss": 0.6291,
      "step": 5260
    },
    {
      "epoch": 3.6546463245492373,
      "grad_norm": 1.627251148223877,
      "learning_rate": 0.0006473356062970581,
      "loss": 0.476,
      "step": 5270
    },
    {
      "epoch": 3.661581137309293,
      "grad_norm": 2.3475327491760254,
      "learning_rate": 0.0006471652099805809,
      "loss": 0.4857,
      "step": 5280
    },
    {
      "epoch": 3.6685159500693483,
      "grad_norm": 1.8800597190856934,
      "learning_rate": 0.0006469948136641036,
      "loss": 0.4816,
      "step": 5290
    },
    {
      "epoch": 3.6754507628294038,
      "grad_norm": 1.777796745300293,
      "learning_rate": 0.0006468244173476263,
      "loss": 0.4221,
      "step": 5300
    },
    {
      "epoch": 3.6823855755894592,
      "grad_norm": 2.409590005874634,
      "learning_rate": 0.0006466540210311491,
      "loss": 0.4093,
      "step": 5310
    },
    {
      "epoch": 3.6893203883495147,
      "grad_norm": 7.860659122467041,
      "learning_rate": 0.0006464836247146719,
      "loss": 0.5853,
      "step": 5320
    },
    {
      "epoch": 3.69625520110957,
      "grad_norm": 1.4758121967315674,
      "learning_rate": 0.0006463132283981946,
      "loss": 0.4954,
      "step": 5330
    },
    {
      "epoch": 3.7031900138696257,
      "grad_norm": 1.548561453819275,
      "learning_rate": 0.0006461428320817174,
      "loss": 0.4843,
      "step": 5340
    },
    {
      "epoch": 3.710124826629681,
      "grad_norm": 1.9566524028778076,
      "learning_rate": 0.0006459724357652402,
      "loss": 0.4665,
      "step": 5350
    },
    {
      "epoch": 3.7170596393897366,
      "grad_norm": 3.76861310005188,
      "learning_rate": 0.0006458020394487628,
      "loss": 0.5053,
      "step": 5360
    },
    {
      "epoch": 3.723994452149792,
      "grad_norm": 1.33755624294281,
      "learning_rate": 0.0006456316431322856,
      "loss": 0.4442,
      "step": 5370
    },
    {
      "epoch": 3.7309292649098476,
      "grad_norm": 1.4193209409713745,
      "learning_rate": 0.0006454612468158084,
      "loss": 0.4392,
      "step": 5380
    },
    {
      "epoch": 3.737864077669903,
      "grad_norm": 2.200732707977295,
      "learning_rate": 0.0006452908504993311,
      "loss": 0.4365,
      "step": 5390
    },
    {
      "epoch": 3.7447988904299585,
      "grad_norm": 1.625002145767212,
      "learning_rate": 0.0006451204541828539,
      "loss": 0.5552,
      "step": 5400
    },
    {
      "epoch": 3.751733703190014,
      "grad_norm": 1.0994077920913696,
      "learning_rate": 0.0006449500578663767,
      "loss": 0.4251,
      "step": 5410
    },
    {
      "epoch": 3.7586685159500695,
      "grad_norm": 3.700880765914917,
      "learning_rate": 0.0006447796615498993,
      "loss": 0.485,
      "step": 5420
    },
    {
      "epoch": 3.765603328710125,
      "grad_norm": 1.6034718751907349,
      "learning_rate": 0.0006446092652334221,
      "loss": 0.6372,
      "step": 5430
    },
    {
      "epoch": 3.7725381414701804,
      "grad_norm": 2.271404504776001,
      "learning_rate": 0.0006444388689169449,
      "loss": 0.4682,
      "step": 5440
    },
    {
      "epoch": 3.779472954230236,
      "grad_norm": 2.089026927947998,
      "learning_rate": 0.0006442684726004677,
      "loss": 0.4639,
      "step": 5450
    },
    {
      "epoch": 3.7864077669902914,
      "grad_norm": 1.3405252695083618,
      "learning_rate": 0.0006440980762839904,
      "loss": 0.4161,
      "step": 5460
    },
    {
      "epoch": 3.793342579750347,
      "grad_norm": 1.8175231218338013,
      "learning_rate": 0.0006439276799675132,
      "loss": 0.5218,
      "step": 5470
    },
    {
      "epoch": 3.8002773925104023,
      "grad_norm": 1.463603138923645,
      "learning_rate": 0.0006437572836510359,
      "loss": 0.4512,
      "step": 5480
    },
    {
      "epoch": 3.807212205270458,
      "grad_norm": 2.017132043838501,
      "learning_rate": 0.0006435868873345586,
      "loss": 0.48,
      "step": 5490
    },
    {
      "epoch": 3.8141470180305133,
      "grad_norm": 2.590273857116699,
      "learning_rate": 0.0006434164910180814,
      "loss": 0.4553,
      "step": 5500
    },
    {
      "epoch": 3.8210818307905687,
      "grad_norm": 1.1282334327697754,
      "learning_rate": 0.0006432460947016042,
      "loss": 0.4793,
      "step": 5510
    },
    {
      "epoch": 3.828016643550624,
      "grad_norm": 1.2765222787857056,
      "learning_rate": 0.0006430756983851269,
      "loss": 0.4688,
      "step": 5520
    },
    {
      "epoch": 3.8349514563106797,
      "grad_norm": 2.780904531478882,
      "learning_rate": 0.0006429053020686496,
      "loss": 0.486,
      "step": 5530
    },
    {
      "epoch": 3.841886269070735,
      "grad_norm": 2.803724765777588,
      "learning_rate": 0.0006427349057521724,
      "loss": 0.4152,
      "step": 5540
    },
    {
      "epoch": 3.8488210818307906,
      "grad_norm": 1.6503218412399292,
      "learning_rate": 0.0006425645094356951,
      "loss": 0.51,
      "step": 5550
    },
    {
      "epoch": 3.855755894590846,
      "grad_norm": 1.1545231342315674,
      "learning_rate": 0.000642394113119218,
      "loss": 0.5197,
      "step": 5560
    },
    {
      "epoch": 3.8626907073509016,
      "grad_norm": 1.4724732637405396,
      "learning_rate": 0.0006422237168027407,
      "loss": 0.5174,
      "step": 5570
    },
    {
      "epoch": 3.869625520110957,
      "grad_norm": 3.6948084831237793,
      "learning_rate": 0.0006420533204862635,
      "loss": 0.4415,
      "step": 5580
    },
    {
      "epoch": 3.8765603328710125,
      "grad_norm": 8.698366165161133,
      "learning_rate": 0.0006418829241697862,
      "loss": 0.4067,
      "step": 5590
    },
    {
      "epoch": 3.883495145631068,
      "grad_norm": 8.053170204162598,
      "learning_rate": 0.0006417125278533089,
      "loss": 0.4781,
      "step": 5600
    },
    {
      "epoch": 3.8904299583911235,
      "grad_norm": 2.275675058364868,
      "learning_rate": 0.0006415421315368317,
      "loss": 0.5854,
      "step": 5610
    },
    {
      "epoch": 3.897364771151179,
      "grad_norm": 2.089224338531494,
      "learning_rate": 0.0006413717352203545,
      "loss": 0.4637,
      "step": 5620
    },
    {
      "epoch": 3.9042995839112344,
      "grad_norm": 3.156386375427246,
      "learning_rate": 0.0006412013389038772,
      "loss": 0.4367,
      "step": 5630
    },
    {
      "epoch": 3.91123439667129,
      "grad_norm": 1.3124927282333374,
      "learning_rate": 0.0006410309425874,
      "loss": 0.6144,
      "step": 5640
    },
    {
      "epoch": 3.9181692094313454,
      "grad_norm": 1.568651795387268,
      "learning_rate": 0.0006408605462709227,
      "loss": 0.4882,
      "step": 5650
    },
    {
      "epoch": 3.925104022191401,
      "grad_norm": 1.3711516857147217,
      "learning_rate": 0.0006406901499544454,
      "loss": 0.4408,
      "step": 5660
    },
    {
      "epoch": 3.9320388349514563,
      "grad_norm": 2.547321081161499,
      "learning_rate": 0.0006405197536379682,
      "loss": 0.5544,
      "step": 5670
    },
    {
      "epoch": 3.938973647711512,
      "grad_norm": 2.489896059036255,
      "learning_rate": 0.000640349357321491,
      "loss": 0.4239,
      "step": 5680
    },
    {
      "epoch": 3.9459084604715673,
      "grad_norm": 2.0257925987243652,
      "learning_rate": 0.0006401789610050138,
      "loss": 0.493,
      "step": 5690
    },
    {
      "epoch": 3.9528432732316228,
      "grad_norm": 2.5405330657958984,
      "learning_rate": 0.0006400085646885365,
      "loss": 0.4195,
      "step": 5700
    },
    {
      "epoch": 3.9597780859916782,
      "grad_norm": 1.436928153038025,
      "learning_rate": 0.0006398381683720592,
      "loss": 0.439,
      "step": 5710
    },
    {
      "epoch": 3.9667128987517337,
      "grad_norm": 3.3839447498321533,
      "learning_rate": 0.000639667772055582,
      "loss": 0.4348,
      "step": 5720
    },
    {
      "epoch": 3.973647711511789,
      "grad_norm": 2.3077900409698486,
      "learning_rate": 0.0006394973757391047,
      "loss": 0.4037,
      "step": 5730
    },
    {
      "epoch": 3.9805825242718447,
      "grad_norm": 1.7581305503845215,
      "learning_rate": 0.0006393269794226275,
      "loss": 0.5826,
      "step": 5740
    },
    {
      "epoch": 3.9875173370319,
      "grad_norm": 1.9170114994049072,
      "learning_rate": 0.0006391565831061503,
      "loss": 0.4739,
      "step": 5750
    },
    {
      "epoch": 3.9944521497919556,
      "grad_norm": 2.7418997287750244,
      "learning_rate": 0.000638986186789673,
      "loss": 0.4743,
      "step": 5760
    },
    {
      "epoch": 4.0,
      "eval_accuracy": 0.7610128338536247,
      "eval_auc": 0.7609546941608012,
      "eval_f1": 0.7127969987494789,
      "eval_loss": 0.5099405646324158,
      "eval_mcc": 0.5540023517641695,
      "eval_precision": 0.8924843423799582,
      "eval_recall": 0.5933379597501734,
      "eval_runtime": 79.0394,
      "eval_samples_per_second": 36.476,
      "eval_steps_per_second": 2.29,
      "step": 5768
    },
    {
      "epoch": 4.0013869625520115,
      "grad_norm": 2.0073978900909424,
      "learning_rate": 0.0006388157904731958,
      "loss": 0.473,
      "step": 5770
    },
    {
      "epoch": 4.008321775312067,
      "grad_norm": 1.9666794538497925,
      "learning_rate": 0.0006386453941567185,
      "loss": 0.4373,
      "step": 5780
    },
    {
      "epoch": 4.0152565880721225,
      "grad_norm": 2.595735788345337,
      "learning_rate": 0.0006384749978402412,
      "loss": 0.4289,
      "step": 5790
    },
    {
      "epoch": 4.0221914008321775,
      "grad_norm": 2.9218924045562744,
      "learning_rate": 0.0006383046015237641,
      "loss": 0.5083,
      "step": 5800
    },
    {
      "epoch": 4.029126213592233,
      "grad_norm": 1.5948861837387085,
      "learning_rate": 0.0006381342052072868,
      "loss": 0.5151,
      "step": 5810
    },
    {
      "epoch": 4.0360610263522885,
      "grad_norm": 2.6722402572631836,
      "learning_rate": 0.0006379638088908095,
      "loss": 0.426,
      "step": 5820
    },
    {
      "epoch": 4.042995839112344,
      "grad_norm": 2.751376152038574,
      "learning_rate": 0.0006377934125743323,
      "loss": 0.5634,
      "step": 5830
    },
    {
      "epoch": 4.049930651872399,
      "grad_norm": 2.279200315475464,
      "learning_rate": 0.000637623016257855,
      "loss": 0.4544,
      "step": 5840
    },
    {
      "epoch": 4.056865464632455,
      "grad_norm": 2.924055337905884,
      "learning_rate": 0.0006374526199413778,
      "loss": 0.5027,
      "step": 5850
    },
    {
      "epoch": 4.06380027739251,
      "grad_norm": 1.8971749544143677,
      "learning_rate": 0.0006372822236249006,
      "loss": 0.4654,
      "step": 5860
    },
    {
      "epoch": 4.070735090152566,
      "grad_norm": 2.4636621475219727,
      "learning_rate": 0.0006371118273084233,
      "loss": 0.483,
      "step": 5870
    },
    {
      "epoch": 4.077669902912621,
      "grad_norm": 2.102008104324341,
      "learning_rate": 0.000636941430991946,
      "loss": 0.4132,
      "step": 5880
    },
    {
      "epoch": 4.084604715672677,
      "grad_norm": 1.526235580444336,
      "learning_rate": 0.0006367710346754688,
      "loss": 0.4407,
      "step": 5890
    },
    {
      "epoch": 4.091539528432732,
      "grad_norm": 3.3520326614379883,
      "learning_rate": 0.0006366006383589916,
      "loss": 0.516,
      "step": 5900
    },
    {
      "epoch": 4.098474341192788,
      "grad_norm": 3.1630868911743164,
      "learning_rate": 0.0006364302420425143,
      "loss": 0.492,
      "step": 5910
    },
    {
      "epoch": 4.105409153952843,
      "grad_norm": 1.1256197690963745,
      "learning_rate": 0.0006362598457260371,
      "loss": 0.4239,
      "step": 5920
    },
    {
      "epoch": 4.112343966712899,
      "grad_norm": 3.8853611946105957,
      "learning_rate": 0.0006360894494095599,
      "loss": 0.4011,
      "step": 5930
    },
    {
      "epoch": 4.119278779472954,
      "grad_norm": 2.397554874420166,
      "learning_rate": 0.0006359190530930825,
      "loss": 0.4964,
      "step": 5940
    },
    {
      "epoch": 4.12621359223301,
      "grad_norm": 1.8362406492233276,
      "learning_rate": 0.0006357486567766053,
      "loss": 0.4643,
      "step": 5950
    },
    {
      "epoch": 4.133148404993065,
      "grad_norm": 1.244858980178833,
      "learning_rate": 0.0006355782604601281,
      "loss": 0.4965,
      "step": 5960
    },
    {
      "epoch": 4.140083217753121,
      "grad_norm": 1.073968529701233,
      "learning_rate": 0.0006354078641436508,
      "loss": 0.3868,
      "step": 5970
    },
    {
      "epoch": 4.147018030513176,
      "grad_norm": 3.82761287689209,
      "learning_rate": 0.0006352374678271737,
      "loss": 0.4821,
      "step": 5980
    },
    {
      "epoch": 4.153952843273232,
      "grad_norm": 2.2693259716033936,
      "learning_rate": 0.0006350670715106964,
      "loss": 0.5218,
      "step": 5990
    },
    {
      "epoch": 4.160887656033287,
      "grad_norm": 2.3139772415161133,
      "learning_rate": 0.000634896675194219,
      "loss": 0.3985,
      "step": 6000
    },
    {
      "epoch": 4.167822468793343,
      "grad_norm": 1.8920190334320068,
      "learning_rate": 0.0006347262788777419,
      "loss": 0.4098,
      "step": 6010
    },
    {
      "epoch": 4.174757281553398,
      "grad_norm": 2.791619062423706,
      "learning_rate": 0.0006345558825612646,
      "loss": 0.5003,
      "step": 6020
    },
    {
      "epoch": 4.181692094313454,
      "grad_norm": 1.8766151666641235,
      "learning_rate": 0.0006343854862447874,
      "loss": 0.4283,
      "step": 6030
    },
    {
      "epoch": 4.188626907073509,
      "grad_norm": 1.888370156288147,
      "learning_rate": 0.0006342150899283102,
      "loss": 0.3972,
      "step": 6040
    },
    {
      "epoch": 4.195561719833565,
      "grad_norm": 2.822065830230713,
      "learning_rate": 0.0006340446936118328,
      "loss": 0.4094,
      "step": 6050
    },
    {
      "epoch": 4.20249653259362,
      "grad_norm": 2.12227725982666,
      "learning_rate": 0.0006338742972953556,
      "loss": 0.4816,
      "step": 6060
    },
    {
      "epoch": 4.209431345353676,
      "grad_norm": 1.8528791666030884,
      "learning_rate": 0.0006337039009788784,
      "loss": 0.4747,
      "step": 6070
    },
    {
      "epoch": 4.216366158113731,
      "grad_norm": 2.531660556793213,
      "learning_rate": 0.0006335335046624011,
      "loss": 0.4226,
      "step": 6080
    },
    {
      "epoch": 4.223300970873787,
      "grad_norm": 5.085011005401611,
      "learning_rate": 0.0006333631083459239,
      "loss": 0.386,
      "step": 6090
    },
    {
      "epoch": 4.230235783633842,
      "grad_norm": 3.4808623790740967,
      "learning_rate": 0.0006331927120294467,
      "loss": 0.5648,
      "step": 6100
    },
    {
      "epoch": 4.237170596393898,
      "grad_norm": 1.496383786201477,
      "learning_rate": 0.0006330223157129695,
      "loss": 0.4217,
      "step": 6110
    },
    {
      "epoch": 4.244105409153953,
      "grad_norm": 1.5480331182479858,
      "learning_rate": 0.0006328519193964921,
      "loss": 0.5637,
      "step": 6120
    },
    {
      "epoch": 4.251040221914009,
      "grad_norm": 2.1749069690704346,
      "learning_rate": 0.0006326815230800149,
      "loss": 0.5087,
      "step": 6130
    },
    {
      "epoch": 4.257975034674064,
      "grad_norm": 3.428591251373291,
      "learning_rate": 0.0006325111267635377,
      "loss": 0.4267,
      "step": 6140
    },
    {
      "epoch": 4.26490984743412,
      "grad_norm": 9.649093627929688,
      "learning_rate": 0.0006323407304470604,
      "loss": 0.3909,
      "step": 6150
    },
    {
      "epoch": 4.271844660194175,
      "grad_norm": 2.472080707550049,
      "learning_rate": 0.0006321703341305832,
      "loss": 0.4438,
      "step": 6160
    },
    {
      "epoch": 4.278779472954231,
      "grad_norm": 1.8915846347808838,
      "learning_rate": 0.0006319999378141059,
      "loss": 0.4591,
      "step": 6170
    },
    {
      "epoch": 4.285714285714286,
      "grad_norm": 1.8886587619781494,
      "learning_rate": 0.0006318295414976286,
      "loss": 0.465,
      "step": 6180
    },
    {
      "epoch": 4.2926490984743415,
      "grad_norm": 1.2088332176208496,
      "learning_rate": 0.0006316591451811514,
      "loss": 0.4329,
      "step": 6190
    },
    {
      "epoch": 4.2995839112343965,
      "grad_norm": 2.213299512863159,
      "learning_rate": 0.0006314887488646742,
      "loss": 0.4319,
      "step": 6200
    },
    {
      "epoch": 4.3065187239944525,
      "grad_norm": 1.0777864456176758,
      "learning_rate": 0.0006313183525481969,
      "loss": 0.3258,
      "step": 6210
    },
    {
      "epoch": 4.3134535367545075,
      "grad_norm": 2.238741159439087,
      "learning_rate": 0.0006311479562317198,
      "loss": 0.4545,
      "step": 6220
    },
    {
      "epoch": 4.320388349514563,
      "grad_norm": 1.9071441888809204,
      "learning_rate": 0.0006309775599152424,
      "loss": 0.4883,
      "step": 6230
    },
    {
      "epoch": 4.327323162274618,
      "grad_norm": 2.9153590202331543,
      "learning_rate": 0.0006308071635987652,
      "loss": 0.5024,
      "step": 6240
    },
    {
      "epoch": 4.334257975034674,
      "grad_norm": 1.5599230527877808,
      "learning_rate": 0.000630636767282288,
      "loss": 0.4833,
      "step": 6250
    },
    {
      "epoch": 4.341192787794729,
      "grad_norm": 4.754970550537109,
      "learning_rate": 0.0006304663709658107,
      "loss": 0.4214,
      "step": 6260
    },
    {
      "epoch": 4.348127600554785,
      "grad_norm": 2.8280200958251953,
      "learning_rate": 0.0006302959746493335,
      "loss": 0.487,
      "step": 6270
    },
    {
      "epoch": 4.35506241331484,
      "grad_norm": 1.3143839836120605,
      "learning_rate": 0.0006301255783328563,
      "loss": 0.5138,
      "step": 6280
    },
    {
      "epoch": 4.361997226074896,
      "grad_norm": 1.486795425415039,
      "learning_rate": 0.0006299551820163789,
      "loss": 0.4868,
      "step": 6290
    },
    {
      "epoch": 4.368932038834951,
      "grad_norm": 1.4822580814361572,
      "learning_rate": 0.0006297847856999017,
      "loss": 0.4421,
      "step": 6300
    },
    {
      "epoch": 4.375866851595007,
      "grad_norm": 2.5525310039520264,
      "learning_rate": 0.0006296143893834245,
      "loss": 0.4322,
      "step": 6310
    },
    {
      "epoch": 4.382801664355062,
      "grad_norm": 3.0068044662475586,
      "learning_rate": 0.0006294439930669472,
      "loss": 0.3536,
      "step": 6320
    },
    {
      "epoch": 4.389736477115118,
      "grad_norm": 2.4847335815429688,
      "learning_rate": 0.00062927359675047,
      "loss": 0.3912,
      "step": 6330
    },
    {
      "epoch": 4.396671289875173,
      "grad_norm": 1.542629361152649,
      "learning_rate": 0.0006291032004339928,
      "loss": 0.4688,
      "step": 6340
    },
    {
      "epoch": 4.403606102635229,
      "grad_norm": 4.963046073913574,
      "learning_rate": 0.0006289328041175155,
      "loss": 0.5073,
      "step": 6350
    },
    {
      "epoch": 4.410540915395284,
      "grad_norm": 1.1102527379989624,
      "learning_rate": 0.0006287624078010382,
      "loss": 0.3948,
      "step": 6360
    },
    {
      "epoch": 4.41747572815534,
      "grad_norm": 2.6029772758483887,
      "learning_rate": 0.000628592011484561,
      "loss": 0.543,
      "step": 6370
    },
    {
      "epoch": 4.424410540915395,
      "grad_norm": 1.7966147661209106,
      "learning_rate": 0.0006284216151680838,
      "loss": 0.55,
      "step": 6380
    },
    {
      "epoch": 4.431345353675451,
      "grad_norm": 1.9996083974838257,
      "learning_rate": 0.0006282512188516065,
      "loss": 0.4062,
      "step": 6390
    },
    {
      "epoch": 4.438280166435506,
      "grad_norm": 3.886847972869873,
      "learning_rate": 0.0006280808225351292,
      "loss": 0.4011,
      "step": 6400
    },
    {
      "epoch": 4.445214979195562,
      "grad_norm": 1.9373633861541748,
      "learning_rate": 0.000627910426218652,
      "loss": 0.5603,
      "step": 6410
    },
    {
      "epoch": 4.452149791955617,
      "grad_norm": 1.241135835647583,
      "learning_rate": 0.0006277400299021747,
      "loss": 0.4317,
      "step": 6420
    },
    {
      "epoch": 4.459084604715673,
      "grad_norm": 1.002285122871399,
      "learning_rate": 0.0006275696335856976,
      "loss": 0.5073,
      "step": 6430
    },
    {
      "epoch": 4.466019417475728,
      "grad_norm": 3.5151989459991455,
      "learning_rate": 0.0006273992372692203,
      "loss": 0.3843,
      "step": 6440
    },
    {
      "epoch": 4.472954230235784,
      "grad_norm": 2.197671413421631,
      "learning_rate": 0.000627228840952743,
      "loss": 0.4274,
      "step": 6450
    },
    {
      "epoch": 4.479889042995839,
      "grad_norm": 3.330260753631592,
      "learning_rate": 0.0006270584446362658,
      "loss": 0.4115,
      "step": 6460
    },
    {
      "epoch": 4.486823855755895,
      "grad_norm": 2.810825824737549,
      "learning_rate": 0.0006268880483197885,
      "loss": 0.4773,
      "step": 6470
    },
    {
      "epoch": 4.49375866851595,
      "grad_norm": 2.2014245986938477,
      "learning_rate": 0.0006267176520033113,
      "loss": 0.4736,
      "step": 6480
    },
    {
      "epoch": 4.500693481276006,
      "grad_norm": 2.616919755935669,
      "learning_rate": 0.0006265472556868341,
      "loss": 0.4365,
      "step": 6490
    },
    {
      "epoch": 4.507628294036061,
      "grad_norm": 1.927514672279358,
      "learning_rate": 0.0006263768593703568,
      "loss": 0.4208,
      "step": 6500
    },
    {
      "epoch": 4.514563106796117,
      "grad_norm": 2.0042734146118164,
      "learning_rate": 0.0006262064630538796,
      "loss": 0.5186,
      "step": 6510
    },
    {
      "epoch": 4.521497919556172,
      "grad_norm": 1.8971288204193115,
      "learning_rate": 0.0006260360667374023,
      "loss": 0.4821,
      "step": 6520
    },
    {
      "epoch": 4.528432732316228,
      "grad_norm": 3.6475226879119873,
      "learning_rate": 0.000625865670420925,
      "loss": 0.4428,
      "step": 6530
    },
    {
      "epoch": 4.535367545076283,
      "grad_norm": 1.388890027999878,
      "learning_rate": 0.0006256952741044478,
      "loss": 0.4554,
      "step": 6540
    },
    {
      "epoch": 4.542302357836339,
      "grad_norm": 2.123138189315796,
      "learning_rate": 0.0006255248777879706,
      "loss": 0.5426,
      "step": 6550
    },
    {
      "epoch": 4.549237170596394,
      "grad_norm": 1.3982750177383423,
      "learning_rate": 0.0006253544814714934,
      "loss": 0.4541,
      "step": 6560
    },
    {
      "epoch": 4.55617198335645,
      "grad_norm": 3.246824264526367,
      "learning_rate": 0.000625184085155016,
      "loss": 0.4141,
      "step": 6570
    },
    {
      "epoch": 4.563106796116505,
      "grad_norm": 2.586426258087158,
      "learning_rate": 0.0006250136888385388,
      "loss": 0.4117,
      "step": 6580
    },
    {
      "epoch": 4.5700416088765605,
      "grad_norm": 2.0345141887664795,
      "learning_rate": 0.0006248432925220616,
      "loss": 0.376,
      "step": 6590
    },
    {
      "epoch": 4.5769764216366156,
      "grad_norm": 1.3333426713943481,
      "learning_rate": 0.0006246728962055843,
      "loss": 0.3925,
      "step": 6600
    },
    {
      "epoch": 4.5839112343966715,
      "grad_norm": 3.7577593326568604,
      "learning_rate": 0.0006245024998891071,
      "loss": 0.3736,
      "step": 6610
    },
    {
      "epoch": 4.5908460471567265,
      "grad_norm": 1.7988249063491821,
      "learning_rate": 0.0006243321035726299,
      "loss": 0.4352,
      "step": 6620
    },
    {
      "epoch": 4.597780859916782,
      "grad_norm": 2.321552276611328,
      "learning_rate": 0.0006241617072561525,
      "loss": 0.4048,
      "step": 6630
    },
    {
      "epoch": 4.6047156726768375,
      "grad_norm": 3.550649642944336,
      "learning_rate": 0.0006239913109396754,
      "loss": 0.5817,
      "step": 6640
    },
    {
      "epoch": 4.611650485436893,
      "grad_norm": 1.6055386066436768,
      "learning_rate": 0.0006238209146231981,
      "loss": 0.4729,
      "step": 6650
    },
    {
      "epoch": 4.618585298196948,
      "grad_norm": 2.5274839401245117,
      "learning_rate": 0.0006236505183067208,
      "loss": 0.4703,
      "step": 6660
    },
    {
      "epoch": 4.625520110957004,
      "grad_norm": 3.2128472328186035,
      "learning_rate": 0.0006234801219902437,
      "loss": 0.5904,
      "step": 6670
    },
    {
      "epoch": 4.632454923717059,
      "grad_norm": 1.3505196571350098,
      "learning_rate": 0.0006233097256737664,
      "loss": 0.4887,
      "step": 6680
    },
    {
      "epoch": 4.639389736477115,
      "grad_norm": 1.3049132823944092,
      "learning_rate": 0.0006231393293572891,
      "loss": 0.4841,
      "step": 6690
    },
    {
      "epoch": 4.64632454923717,
      "grad_norm": 1.296612024307251,
      "learning_rate": 0.0006229689330408119,
      "loss": 0.3942,
      "step": 6700
    },
    {
      "epoch": 4.653259361997226,
      "grad_norm": 1.6288869380950928,
      "learning_rate": 0.0006227985367243346,
      "loss": 0.4504,
      "step": 6710
    },
    {
      "epoch": 4.660194174757281,
      "grad_norm": 2.334202527999878,
      "learning_rate": 0.0006226281404078574,
      "loss": 0.4434,
      "step": 6720
    },
    {
      "epoch": 4.667128987517337,
      "grad_norm": 1.5011372566223145,
      "learning_rate": 0.0006224577440913802,
      "loss": 0.3946,
      "step": 6730
    },
    {
      "epoch": 4.674063800277392,
      "grad_norm": 1.3529236316680908,
      "learning_rate": 0.0006222873477749029,
      "loss": 0.4963,
      "step": 6740
    },
    {
      "epoch": 4.680998613037448,
      "grad_norm": 3.225029230117798,
      "learning_rate": 0.0006221169514584256,
      "loss": 0.3814,
      "step": 6750
    },
    {
      "epoch": 4.687933425797503,
      "grad_norm": 2.3265373706817627,
      "learning_rate": 0.0006219465551419484,
      "loss": 0.4523,
      "step": 6760
    },
    {
      "epoch": 4.694868238557559,
      "grad_norm": 2.072357654571533,
      "learning_rate": 0.0006217761588254712,
      "loss": 0.5436,
      "step": 6770
    },
    {
      "epoch": 4.701803051317614,
      "grad_norm": 2.8022680282592773,
      "learning_rate": 0.0006216057625089939,
      "loss": 0.48,
      "step": 6780
    },
    {
      "epoch": 4.70873786407767,
      "grad_norm": 1.847780704498291,
      "learning_rate": 0.0006214353661925167,
      "loss": 0.4692,
      "step": 6790
    },
    {
      "epoch": 4.715672676837725,
      "grad_norm": 1.444065809249878,
      "learning_rate": 0.0006212649698760395,
      "loss": 0.5489,
      "step": 6800
    },
    {
      "epoch": 4.722607489597781,
      "grad_norm": 2.8575210571289062,
      "learning_rate": 0.0006210945735595621,
      "loss": 0.4489,
      "step": 6810
    },
    {
      "epoch": 4.729542302357836,
      "grad_norm": 3.7806522846221924,
      "learning_rate": 0.0006209241772430849,
      "loss": 0.5148,
      "step": 6820
    },
    {
      "epoch": 4.736477115117892,
      "grad_norm": 2.4596617221832275,
      "learning_rate": 0.0006207537809266077,
      "loss": 0.4536,
      "step": 6830
    },
    {
      "epoch": 4.743411927877947,
      "grad_norm": 1.7756648063659668,
      "learning_rate": 0.0006205833846101304,
      "loss": 0.5059,
      "step": 6840
    },
    {
      "epoch": 4.750346740638003,
      "grad_norm": 0.9519446492195129,
      "learning_rate": 0.0006204129882936532,
      "loss": 0.4111,
      "step": 6850
    },
    {
      "epoch": 4.757281553398058,
      "grad_norm": 1.5904114246368408,
      "learning_rate": 0.000620242591977176,
      "loss": 0.4153,
      "step": 6860
    },
    {
      "epoch": 4.764216366158114,
      "grad_norm": 2.069126844406128,
      "learning_rate": 0.0006200721956606986,
      "loss": 0.5589,
      "step": 6870
    },
    {
      "epoch": 4.771151178918169,
      "grad_norm": 1.2252964973449707,
      "learning_rate": 0.0006199017993442215,
      "loss": 0.4655,
      "step": 6880
    },
    {
      "epoch": 4.778085991678225,
      "grad_norm": 1.8109869956970215,
      "learning_rate": 0.0006197314030277442,
      "loss": 0.4201,
      "step": 6890
    },
    {
      "epoch": 4.78502080443828,
      "grad_norm": 1.7545182704925537,
      "learning_rate": 0.000619561006711267,
      "loss": 0.4973,
      "step": 6900
    },
    {
      "epoch": 4.791955617198336,
      "grad_norm": 2.1535775661468506,
      "learning_rate": 0.0006193906103947898,
      "loss": 0.5287,
      "step": 6910
    },
    {
      "epoch": 4.798890429958391,
      "grad_norm": 1.8850717544555664,
      "learning_rate": 0.0006192202140783124,
      "loss": 0.4912,
      "step": 6920
    },
    {
      "epoch": 4.805825242718447,
      "grad_norm": 1.81786048412323,
      "learning_rate": 0.0006190498177618352,
      "loss": 0.3707,
      "step": 6930
    },
    {
      "epoch": 4.812760055478502,
      "grad_norm": 7.620935440063477,
      "learning_rate": 0.000618879421445358,
      "loss": 0.3174,
      "step": 6940
    },
    {
      "epoch": 4.819694868238558,
      "grad_norm": 2.249272346496582,
      "learning_rate": 0.0006187090251288807,
      "loss": 0.458,
      "step": 6950
    },
    {
      "epoch": 4.826629680998613,
      "grad_norm": 5.8604021072387695,
      "learning_rate": 0.0006185386288124035,
      "loss": 0.3833,
      "step": 6960
    },
    {
      "epoch": 4.833564493758669,
      "grad_norm": 2.2178683280944824,
      "learning_rate": 0.0006183682324959263,
      "loss": 0.5292,
      "step": 6970
    },
    {
      "epoch": 4.840499306518724,
      "grad_norm": 1.448951005935669,
      "learning_rate": 0.0006181978361794489,
      "loss": 0.4865,
      "step": 6980
    },
    {
      "epoch": 4.8474341192787795,
      "grad_norm": 1.2120790481567383,
      "learning_rate": 0.0006180274398629717,
      "loss": 0.3485,
      "step": 6990
    },
    {
      "epoch": 4.854368932038835,
      "grad_norm": 2.2167699337005615,
      "learning_rate": 0.0006178570435464945,
      "loss": 0.4593,
      "step": 7000
    },
    {
      "epoch": 4.8613037447988905,
      "grad_norm": 1.2982487678527832,
      "learning_rate": 0.0006176866472300173,
      "loss": 0.4926,
      "step": 7010
    },
    {
      "epoch": 4.8682385575589455,
      "grad_norm": 1.879092812538147,
      "learning_rate": 0.00061751625091354,
      "loss": 0.5218,
      "step": 7020
    },
    {
      "epoch": 4.875173370319001,
      "grad_norm": 1.3247733116149902,
      "learning_rate": 0.0006173458545970628,
      "loss": 0.4479,
      "step": 7030
    },
    {
      "epoch": 4.8821081830790565,
      "grad_norm": 1.6367135047912598,
      "learning_rate": 0.0006171754582805855,
      "loss": 0.4441,
      "step": 7040
    },
    {
      "epoch": 4.889042995839112,
      "grad_norm": 1.7401832342147827,
      "learning_rate": 0.0006170050619641082,
      "loss": 0.4431,
      "step": 7050
    },
    {
      "epoch": 4.895977808599168,
      "grad_norm": 3.0170576572418213,
      "learning_rate": 0.000616834665647631,
      "loss": 0.4592,
      "step": 7060
    },
    {
      "epoch": 4.902912621359223,
      "grad_norm": 3.6918411254882812,
      "learning_rate": 0.0006166642693311538,
      "loss": 0.4673,
      "step": 7070
    },
    {
      "epoch": 4.909847434119278,
      "grad_norm": 1.9928017854690552,
      "learning_rate": 0.0006164938730146765,
      "loss": 0.4136,
      "step": 7080
    },
    {
      "epoch": 4.916782246879334,
      "grad_norm": 2.1579482555389404,
      "learning_rate": 0.0006163234766981994,
      "loss": 0.4251,
      "step": 7090
    },
    {
      "epoch": 4.92371705963939,
      "grad_norm": 1.8288991451263428,
      "learning_rate": 0.000616153080381722,
      "loss": 0.3962,
      "step": 7100
    },
    {
      "epoch": 4.930651872399445,
      "grad_norm": 4.345067977905273,
      "learning_rate": 0.0006159826840652447,
      "loss": 0.4795,
      "step": 7110
    },
    {
      "epoch": 4.9375866851595,
      "grad_norm": 1.6402918100357056,
      "learning_rate": 0.0006158122877487676,
      "loss": 0.4168,
      "step": 7120
    },
    {
      "epoch": 4.944521497919556,
      "grad_norm": 1.8533927202224731,
      "learning_rate": 0.0006156418914322903,
      "loss": 0.4983,
      "step": 7130
    },
    {
      "epoch": 4.951456310679612,
      "grad_norm": 1.8565903902053833,
      "learning_rate": 0.0006154714951158131,
      "loss": 0.4084,
      "step": 7140
    },
    {
      "epoch": 4.958391123439667,
      "grad_norm": 2.0361883640289307,
      "learning_rate": 0.0006153010987993359,
      "loss": 0.4029,
      "step": 7150
    },
    {
      "epoch": 4.965325936199722,
      "grad_norm": 1.146431803703308,
      "learning_rate": 0.0006151307024828585,
      "loss": 0.4552,
      "step": 7160
    },
    {
      "epoch": 4.972260748959778,
      "grad_norm": 2.149606227874756,
      "learning_rate": 0.0006149603061663813,
      "loss": 0.407,
      "step": 7170
    },
    {
      "epoch": 4.979195561719834,
      "grad_norm": 1.8984460830688477,
      "learning_rate": 0.0006147899098499041,
      "loss": 0.4364,
      "step": 7180
    },
    {
      "epoch": 4.986130374479889,
      "grad_norm": 2.5682806968688965,
      "learning_rate": 0.0006146195135334268,
      "loss": 0.4179,
      "step": 7190
    },
    {
      "epoch": 4.993065187239944,
      "grad_norm": 1.8387982845306396,
      "learning_rate": 0.0006144491172169496,
      "loss": 0.4707,
      "step": 7200
    },
    {
      "epoch": 5.0,
      "grad_norm": 2.1547505855560303,
      "learning_rate": 0.0006142787209004723,
      "loss": 0.5056,
      "step": 7210
    },
    {
      "epoch": 5.0,
      "eval_accuracy": 0.8144294138050642,
      "eval_auc": 0.8144148336655562,
      "eval_f1": 0.8062296269467584,
      "eval_loss": 0.4277656376361847,
      "eval_mcc": 0.631075309454819,
      "eval_precision": 0.8431818181818181,
      "eval_recall": 0.7723802914642609,
      "eval_runtime": 79.4032,
      "eval_samples_per_second": 36.308,
      "eval_steps_per_second": 2.28,
      "step": 7210
    },
    {
      "epoch": 5.006934812760056,
      "grad_norm": 1.2094141244888306,
      "learning_rate": 0.0006141083245839951,
      "loss": 0.436,
      "step": 7220
    },
    {
      "epoch": 5.013869625520111,
      "grad_norm": 1.747249722480774,
      "learning_rate": 0.0006139379282675178,
      "loss": 0.4739,
      "step": 7230
    },
    {
      "epoch": 5.020804438280167,
      "grad_norm": 1.4988282918930054,
      "learning_rate": 0.0006137675319510406,
      "loss": 0.4227,
      "step": 7240
    },
    {
      "epoch": 5.027739251040222,
      "grad_norm": 1.3628500699996948,
      "learning_rate": 0.0006135971356345634,
      "loss": 0.4595,
      "step": 7250
    },
    {
      "epoch": 5.034674063800278,
      "grad_norm": 3.325648307800293,
      "learning_rate": 0.0006134267393180861,
      "loss": 0.4447,
      "step": 7260
    },
    {
      "epoch": 5.041608876560333,
      "grad_norm": 1.8880966901779175,
      "learning_rate": 0.0006132563430016088,
      "loss": 0.4822,
      "step": 7270
    },
    {
      "epoch": 5.048543689320389,
      "grad_norm": 3.126227378845215,
      "learning_rate": 0.0006130859466851316,
      "loss": 0.3819,
      "step": 7280
    },
    {
      "epoch": 5.055478502080444,
      "grad_norm": 1.66195809841156,
      "learning_rate": 0.0006129155503686543,
      "loss": 0.489,
      "step": 7290
    },
    {
      "epoch": 5.0624133148405,
      "grad_norm": 1.4522366523742676,
      "learning_rate": 0.0006127451540521771,
      "loss": 0.3884,
      "step": 7300
    },
    {
      "epoch": 5.069348127600555,
      "grad_norm": 1.8220655918121338,
      "learning_rate": 0.0006125747577356999,
      "loss": 0.4112,
      "step": 7310
    },
    {
      "epoch": 5.076282940360611,
      "grad_norm": 1.4909368753433228,
      "learning_rate": 0.0006124043614192226,
      "loss": 0.2883,
      "step": 7320
    },
    {
      "epoch": 5.083217753120666,
      "grad_norm": 1.6731541156768799,
      "learning_rate": 0.0006122339651027454,
      "loss": 0.3222,
      "step": 7330
    },
    {
      "epoch": 5.090152565880722,
      "grad_norm": 5.971959114074707,
      "learning_rate": 0.0006120635687862681,
      "loss": 0.4765,
      "step": 7340
    },
    {
      "epoch": 5.097087378640777,
      "grad_norm": 2.614423990249634,
      "learning_rate": 0.0006118931724697909,
      "loss": 0.5926,
      "step": 7350
    },
    {
      "epoch": 5.104022191400833,
      "grad_norm": 2.659708023071289,
      "learning_rate": 0.0006117227761533137,
      "loss": 0.4296,
      "step": 7360
    },
    {
      "epoch": 5.110957004160888,
      "grad_norm": 3.862757444381714,
      "learning_rate": 0.0006115523798368364,
      "loss": 0.4444,
      "step": 7370
    },
    {
      "epoch": 5.1178918169209435,
      "grad_norm": 1.6139533519744873,
      "learning_rate": 0.0006113819835203592,
      "loss": 0.4053,
      "step": 7380
    },
    {
      "epoch": 5.124826629680999,
      "grad_norm": 1.787762999534607,
      "learning_rate": 0.0006112115872038819,
      "loss": 0.356,
      "step": 7390
    },
    {
      "epoch": 5.1317614424410545,
      "grad_norm": 1.3079233169555664,
      "learning_rate": 0.0006110411908874046,
      "loss": 0.5495,
      "step": 7400
    },
    {
      "epoch": 5.1386962552011095,
      "grad_norm": 3.133399248123169,
      "learning_rate": 0.0006108707945709274,
      "loss": 0.3926,
      "step": 7410
    },
    {
      "epoch": 5.145631067961165,
      "grad_norm": 4.6354079246521,
      "learning_rate": 0.0006107003982544502,
      "loss": 0.3964,
      "step": 7420
    },
    {
      "epoch": 5.1525658807212205,
      "grad_norm": 1.1136431694030762,
      "learning_rate": 0.0006105300019379729,
      "loss": 0.384,
      "step": 7430
    },
    {
      "epoch": 5.159500693481276,
      "grad_norm": 1.5048021078109741,
      "learning_rate": 0.0006103596056214956,
      "loss": 0.4167,
      "step": 7440
    },
    {
      "epoch": 5.166435506241331,
      "grad_norm": 2.1999902725219727,
      "learning_rate": 0.0006101892093050184,
      "loss": 0.4366,
      "step": 7450
    },
    {
      "epoch": 5.173370319001387,
      "grad_norm": 1.5014945268630981,
      "learning_rate": 0.0006100188129885412,
      "loss": 0.4369,
      "step": 7460
    },
    {
      "epoch": 5.180305131761442,
      "grad_norm": 2.3609137535095215,
      "learning_rate": 0.0006098484166720639,
      "loss": 0.4021,
      "step": 7470
    },
    {
      "epoch": 5.187239944521498,
      "grad_norm": 1.9455125331878662,
      "learning_rate": 0.0006096780203555867,
      "loss": 0.4164,
      "step": 7480
    },
    {
      "epoch": 5.194174757281553,
      "grad_norm": 1.7184991836547852,
      "learning_rate": 0.0006095076240391095,
      "loss": 0.5063,
      "step": 7490
    },
    {
      "epoch": 5.201109570041609,
      "grad_norm": 1.4482421875,
      "learning_rate": 0.0006093372277226321,
      "loss": 0.4917,
      "step": 7500
    },
    {
      "epoch": 5.208044382801664,
      "grad_norm": 1.3541792631149292,
      "learning_rate": 0.0006091668314061549,
      "loss": 0.4365,
      "step": 7510
    },
    {
      "epoch": 5.21497919556172,
      "grad_norm": 1.8855928182601929,
      "learning_rate": 0.0006089964350896777,
      "loss": 0.4482,
      "step": 7520
    },
    {
      "epoch": 5.221914008321775,
      "grad_norm": 2.326178789138794,
      "learning_rate": 0.0006088260387732004,
      "loss": 0.4802,
      "step": 7530
    },
    {
      "epoch": 5.228848821081831,
      "grad_norm": 3.92842435836792,
      "learning_rate": 0.0006086556424567233,
      "loss": 0.5499,
      "step": 7540
    },
    {
      "epoch": 5.235783633841886,
      "grad_norm": 1.1180675029754639,
      "learning_rate": 0.000608485246140246,
      "loss": 0.492,
      "step": 7550
    },
    {
      "epoch": 5.242718446601942,
      "grad_norm": 1.4822999238967896,
      "learning_rate": 0.0006083148498237687,
      "loss": 0.441,
      "step": 7560
    },
    {
      "epoch": 5.249653259361997,
      "grad_norm": 1.8555994033813477,
      "learning_rate": 0.0006081444535072915,
      "loss": 0.48,
      "step": 7570
    },
    {
      "epoch": 5.256588072122053,
      "grad_norm": 1.9529341459274292,
      "learning_rate": 0.0006079740571908142,
      "loss": 0.4357,
      "step": 7580
    },
    {
      "epoch": 5.263522884882108,
      "grad_norm": 2.2560832500457764,
      "learning_rate": 0.000607803660874337,
      "loss": 0.475,
      "step": 7590
    },
    {
      "epoch": 5.270457697642164,
      "grad_norm": 2.180328369140625,
      "learning_rate": 0.0006076332645578598,
      "loss": 0.3518,
      "step": 7600
    },
    {
      "epoch": 5.277392510402219,
      "grad_norm": 2.076875686645508,
      "learning_rate": 0.0006074628682413825,
      "loss": 0.451,
      "step": 7610
    },
    {
      "epoch": 5.284327323162275,
      "grad_norm": 2.469593048095703,
      "learning_rate": 0.0006072924719249052,
      "loss": 0.4176,
      "step": 7620
    },
    {
      "epoch": 5.29126213592233,
      "grad_norm": 1.6388379335403442,
      "learning_rate": 0.000607122075608428,
      "loss": 0.4468,
      "step": 7630
    },
    {
      "epoch": 5.298196948682386,
      "grad_norm": 2.2160558700561523,
      "learning_rate": 0.0006069516792919507,
      "loss": 0.3896,
      "step": 7640
    },
    {
      "epoch": 5.305131761442441,
      "grad_norm": 2.46968412399292,
      "learning_rate": 0.0006067812829754735,
      "loss": 0.4515,
      "step": 7650
    },
    {
      "epoch": 5.312066574202497,
      "grad_norm": 2.3297855854034424,
      "learning_rate": 0.0006066108866589963,
      "loss": 0.4874,
      "step": 7660
    },
    {
      "epoch": 5.319001386962552,
      "grad_norm": 1.922400951385498,
      "learning_rate": 0.0006064404903425191,
      "loss": 0.4403,
      "step": 7670
    },
    {
      "epoch": 5.325936199722608,
      "grad_norm": 1.5347956418991089,
      "learning_rate": 0.0006062700940260417,
      "loss": 0.4459,
      "step": 7680
    },
    {
      "epoch": 5.332871012482663,
      "grad_norm": 4.6107988357543945,
      "learning_rate": 0.0006060996977095645,
      "loss": 0.4081,
      "step": 7690
    },
    {
      "epoch": 5.339805825242719,
      "grad_norm": 2.0724844932556152,
      "learning_rate": 0.0006059293013930873,
      "loss": 0.3514,
      "step": 7700
    },
    {
      "epoch": 5.346740638002774,
      "grad_norm": 2.098862409591675,
      "learning_rate": 0.00060575890507661,
      "loss": 0.4035,
      "step": 7710
    },
    {
      "epoch": 5.35367545076283,
      "grad_norm": 2.068887233734131,
      "learning_rate": 0.0006055885087601328,
      "loss": 0.3871,
      "step": 7720
    },
    {
      "epoch": 5.360610263522885,
      "grad_norm": 1.5460946559906006,
      "learning_rate": 0.0006054181124436556,
      "loss": 0.4256,
      "step": 7730
    },
    {
      "epoch": 5.367545076282941,
      "grad_norm": 2.4513587951660156,
      "learning_rate": 0.0006052477161271782,
      "loss": 0.4807,
      "step": 7740
    },
    {
      "epoch": 5.374479889042996,
      "grad_norm": 1.880887746810913,
      "learning_rate": 0.0006050773198107011,
      "loss": 0.4293,
      "step": 7750
    },
    {
      "epoch": 5.381414701803052,
      "grad_norm": 1.8038541078567505,
      "learning_rate": 0.0006049069234942238,
      "loss": 0.4637,
      "step": 7760
    },
    {
      "epoch": 5.388349514563107,
      "grad_norm": 1.7539420127868652,
      "learning_rate": 0.0006047365271777465,
      "loss": 0.4101,
      "step": 7770
    },
    {
      "epoch": 5.3952843273231625,
      "grad_norm": 1.5812309980392456,
      "learning_rate": 0.0006045661308612694,
      "loss": 0.5293,
      "step": 7780
    },
    {
      "epoch": 5.402219140083218,
      "grad_norm": 1.3509527444839478,
      "learning_rate": 0.000604395734544792,
      "loss": 0.4396,
      "step": 7790
    },
    {
      "epoch": 5.4091539528432735,
      "grad_norm": 2.332331895828247,
      "learning_rate": 0.0006042253382283148,
      "loss": 0.4556,
      "step": 7800
    },
    {
      "epoch": 5.4160887656033285,
      "grad_norm": 0.8542439937591553,
      "learning_rate": 0.0006040549419118376,
      "loss": 0.3282,
      "step": 7810
    },
    {
      "epoch": 5.4230235783633844,
      "grad_norm": 3.4630095958709717,
      "learning_rate": 0.0006038845455953603,
      "loss": 0.3967,
      "step": 7820
    },
    {
      "epoch": 5.4299583911234395,
      "grad_norm": 1.6077748537063599,
      "learning_rate": 0.0006037141492788831,
      "loss": 0.4938,
      "step": 7830
    },
    {
      "epoch": 5.436893203883495,
      "grad_norm": 2.067304849624634,
      "learning_rate": 0.0006035437529624059,
      "loss": 0.5874,
      "step": 7840
    },
    {
      "epoch": 5.44382801664355,
      "grad_norm": 1.4926538467407227,
      "learning_rate": 0.0006033733566459285,
      "loss": 0.4674,
      "step": 7850
    },
    {
      "epoch": 5.450762829403606,
      "grad_norm": 1.598062515258789,
      "learning_rate": 0.0006032029603294513,
      "loss": 0.5305,
      "step": 7860
    },
    {
      "epoch": 5.457697642163661,
      "grad_norm": 1.5569182634353638,
      "learning_rate": 0.0006030325640129741,
      "loss": 0.4896,
      "step": 7870
    },
    {
      "epoch": 5.464632454923717,
      "grad_norm": 2.18359637260437,
      "learning_rate": 0.0006028621676964969,
      "loss": 0.4491,
      "step": 7880
    },
    {
      "epoch": 5.471567267683772,
      "grad_norm": 2.783336639404297,
      "learning_rate": 0.0006026917713800196,
      "loss": 0.4197,
      "step": 7890
    },
    {
      "epoch": 5.478502080443828,
      "grad_norm": 1.12771475315094,
      "learning_rate": 0.0006025213750635424,
      "loss": 0.4561,
      "step": 7900
    },
    {
      "epoch": 5.485436893203883,
      "grad_norm": 1.988283395767212,
      "learning_rate": 0.0006023509787470651,
      "loss": 0.4872,
      "step": 7910
    },
    {
      "epoch": 5.492371705963939,
      "grad_norm": 2.108180046081543,
      "learning_rate": 0.0006021805824305878,
      "loss": 0.435,
      "step": 7920
    },
    {
      "epoch": 5.499306518723994,
      "grad_norm": 1.600124716758728,
      "learning_rate": 0.0006020101861141106,
      "loss": 0.4342,
      "step": 7930
    },
    {
      "epoch": 5.50624133148405,
      "grad_norm": 2.4484469890594482,
      "learning_rate": 0.0006018397897976334,
      "loss": 0.3956,
      "step": 7940
    },
    {
      "epoch": 5.513176144244105,
      "grad_norm": 2.4787323474884033,
      "learning_rate": 0.0006016693934811561,
      "loss": 0.4986,
      "step": 7950
    },
    {
      "epoch": 5.520110957004161,
      "grad_norm": 1.596699833869934,
      "learning_rate": 0.0006014989971646789,
      "loss": 0.3723,
      "step": 7960
    },
    {
      "epoch": 5.527045769764216,
      "grad_norm": 2.2880499362945557,
      "learning_rate": 0.0006013286008482016,
      "loss": 0.3308,
      "step": 7970
    },
    {
      "epoch": 5.533980582524272,
      "grad_norm": 2.4674246311187744,
      "learning_rate": 0.0006011582045317243,
      "loss": 0.5894,
      "step": 7980
    },
    {
      "epoch": 5.540915395284327,
      "grad_norm": 1.432444453239441,
      "learning_rate": 0.0006009878082152472,
      "loss": 0.3871,
      "step": 7990
    },
    {
      "epoch": 5.547850208044383,
      "grad_norm": 1.6022967100143433,
      "learning_rate": 0.0006008174118987699,
      "loss": 0.4714,
      "step": 8000
    },
    {
      "epoch": 5.554785020804438,
      "grad_norm": 3.081559419631958,
      "learning_rate": 0.0006006470155822927,
      "loss": 0.3931,
      "step": 8010
    },
    {
      "epoch": 5.561719833564494,
      "grad_norm": 2.3382787704467773,
      "learning_rate": 0.0006004766192658155,
      "loss": 0.4147,
      "step": 8020
    },
    {
      "epoch": 5.568654646324549,
      "grad_norm": 1.7394781112670898,
      "learning_rate": 0.0006003062229493381,
      "loss": 0.3827,
      "step": 8030
    },
    {
      "epoch": 5.575589459084605,
      "grad_norm": 2.653791904449463,
      "learning_rate": 0.0006001358266328609,
      "loss": 0.5066,
      "step": 8040
    },
    {
      "epoch": 5.58252427184466,
      "grad_norm": 2.9032461643218994,
      "learning_rate": 0.0005999654303163837,
      "loss": 0.3498,
      "step": 8050
    },
    {
      "epoch": 5.589459084604716,
      "grad_norm": 2.3067591190338135,
      "learning_rate": 0.0005997950339999064,
      "loss": 0.3967,
      "step": 8060
    },
    {
      "epoch": 5.596393897364771,
      "grad_norm": 11.956409454345703,
      "learning_rate": 0.0005996246376834292,
      "loss": 0.49,
      "step": 8070
    },
    {
      "epoch": 5.603328710124827,
      "grad_norm": 4.2596235275268555,
      "learning_rate": 0.000599454241366952,
      "loss": 0.4513,
      "step": 8080
    },
    {
      "epoch": 5.610263522884882,
      "grad_norm": 3.2109487056732178,
      "learning_rate": 0.0005992838450504746,
      "loss": 0.4074,
      "step": 8090
    },
    {
      "epoch": 5.617198335644938,
      "grad_norm": 11.223039627075195,
      "learning_rate": 0.0005991134487339974,
      "loss": 0.3802,
      "step": 8100
    },
    {
      "epoch": 5.624133148404993,
      "grad_norm": 1.5793651342391968,
      "learning_rate": 0.0005989430524175202,
      "loss": 0.4474,
      "step": 8110
    },
    {
      "epoch": 5.631067961165049,
      "grad_norm": 1.6835554838180542,
      "learning_rate": 0.000598772656101043,
      "loss": 0.4208,
      "step": 8120
    },
    {
      "epoch": 5.638002773925104,
      "grad_norm": 1.5992389917373657,
      "learning_rate": 0.0005986022597845657,
      "loss": 0.4298,
      "step": 8130
    },
    {
      "epoch": 5.64493758668516,
      "grad_norm": 4.678886413574219,
      "learning_rate": 0.0005984318634680884,
      "loss": 0.4374,
      "step": 8140
    },
    {
      "epoch": 5.651872399445215,
      "grad_norm": 1.5230365991592407,
      "learning_rate": 0.0005982614671516112,
      "loss": 0.3305,
      "step": 8150
    },
    {
      "epoch": 5.658807212205271,
      "grad_norm": 6.662856101989746,
      "learning_rate": 0.0005980910708351339,
      "loss": 0.4186,
      "step": 8160
    },
    {
      "epoch": 5.665742024965326,
      "grad_norm": 1.677159070968628,
      "learning_rate": 0.0005979206745186567,
      "loss": 0.4532,
      "step": 8170
    },
    {
      "epoch": 5.672676837725382,
      "grad_norm": 1.5008214712142944,
      "learning_rate": 0.0005977502782021795,
      "loss": 0.4346,
      "step": 8180
    },
    {
      "epoch": 5.679611650485437,
      "grad_norm": 4.380478858947754,
      "learning_rate": 0.0005975798818857022,
      "loss": 0.4637,
      "step": 8190
    },
    {
      "epoch": 5.6865464632454925,
      "grad_norm": 2.174090623855591,
      "learning_rate": 0.000597409485569225,
      "loss": 0.4486,
      "step": 8200
    },
    {
      "epoch": 5.6934812760055475,
      "grad_norm": 1.314306616783142,
      "learning_rate": 0.0005972390892527477,
      "loss": 0.4667,
      "step": 8210
    },
    {
      "epoch": 5.7004160887656035,
      "grad_norm": 2.9807369709014893,
      "learning_rate": 0.0005970686929362704,
      "loss": 0.3799,
      "step": 8220
    },
    {
      "epoch": 5.7073509015256585,
      "grad_norm": 3.2776365280151367,
      "learning_rate": 0.0005968982966197933,
      "loss": 0.4033,
      "step": 8230
    },
    {
      "epoch": 5.714285714285714,
      "grad_norm": 1.1769098043441772,
      "learning_rate": 0.000596727900303316,
      "loss": 0.3947,
      "step": 8240
    },
    {
      "epoch": 5.721220527045769,
      "grad_norm": 4.843662738800049,
      "learning_rate": 0.0005965575039868388,
      "loss": 0.396,
      "step": 8250
    },
    {
      "epoch": 5.728155339805825,
      "grad_norm": 2.1351852416992188,
      "learning_rate": 0.0005963871076703615,
      "loss": 0.5021,
      "step": 8260
    },
    {
      "epoch": 5.73509015256588,
      "grad_norm": 2.8203530311584473,
      "learning_rate": 0.0005962167113538842,
      "loss": 0.366,
      "step": 8270
    },
    {
      "epoch": 5.742024965325936,
      "grad_norm": 2.3683247566223145,
      "learning_rate": 0.000596046315037407,
      "loss": 0.4427,
      "step": 8280
    },
    {
      "epoch": 5.748959778085991,
      "grad_norm": 2.297788143157959,
      "learning_rate": 0.0005958759187209298,
      "loss": 0.466,
      "step": 8290
    },
    {
      "epoch": 5.755894590846047,
      "grad_norm": 2.1224045753479004,
      "learning_rate": 0.0005957055224044525,
      "loss": 0.4736,
      "step": 8300
    },
    {
      "epoch": 5.762829403606102,
      "grad_norm": 1.2459636926651,
      "learning_rate": 0.0005955351260879752,
      "loss": 0.4393,
      "step": 8310
    },
    {
      "epoch": 5.769764216366158,
      "grad_norm": 1.8316906690597534,
      "learning_rate": 0.000595364729771498,
      "loss": 0.4687,
      "step": 8320
    },
    {
      "epoch": 5.776699029126213,
      "grad_norm": 2.032071828842163,
      "learning_rate": 0.0005951943334550208,
      "loss": 0.4565,
      "step": 8330
    },
    {
      "epoch": 5.783633841886269,
      "grad_norm": 2.2232468128204346,
      "learning_rate": 0.0005950239371385435,
      "loss": 0.4937,
      "step": 8340
    },
    {
      "epoch": 5.790568654646324,
      "grad_norm": 2.523650646209717,
      "learning_rate": 0.0005948535408220663,
      "loss": 0.4361,
      "step": 8350
    },
    {
      "epoch": 5.79750346740638,
      "grad_norm": 1.3036432266235352,
      "learning_rate": 0.0005946831445055891,
      "loss": 0.3494,
      "step": 8360
    },
    {
      "epoch": 5.804438280166435,
      "grad_norm": 3.946251392364502,
      "learning_rate": 0.0005945127481891117,
      "loss": 0.5047,
      "step": 8370
    },
    {
      "epoch": 5.811373092926491,
      "grad_norm": 2.547445774078369,
      "learning_rate": 0.0005943423518726345,
      "loss": 0.3378,
      "step": 8380
    },
    {
      "epoch": 5.818307905686546,
      "grad_norm": 2.2184832096099854,
      "learning_rate": 0.0005941719555561573,
      "loss": 0.4594,
      "step": 8390
    },
    {
      "epoch": 5.825242718446602,
      "grad_norm": 2.115577220916748,
      "learning_rate": 0.00059400155923968,
      "loss": 0.4348,
      "step": 8400
    },
    {
      "epoch": 5.832177531206657,
      "grad_norm": 1.7057965993881226,
      "learning_rate": 0.0005938311629232029,
      "loss": 0.474,
      "step": 8410
    },
    {
      "epoch": 5.839112343966713,
      "grad_norm": 4.242212772369385,
      "learning_rate": 0.0005936607666067256,
      "loss": 0.4119,
      "step": 8420
    },
    {
      "epoch": 5.846047156726768,
      "grad_norm": 5.619433879852295,
      "learning_rate": 0.0005934903702902482,
      "loss": 0.4662,
      "step": 8430
    },
    {
      "epoch": 5.852981969486824,
      "grad_norm": 1.7264742851257324,
      "learning_rate": 0.0005933199739737711,
      "loss": 0.4552,
      "step": 8440
    },
    {
      "epoch": 5.859916782246879,
      "grad_norm": 1.5444989204406738,
      "learning_rate": 0.0005931495776572938,
      "loss": 0.4608,
      "step": 8450
    },
    {
      "epoch": 5.866851595006935,
      "grad_norm": 1.6752759218215942,
      "learning_rate": 0.0005929791813408166,
      "loss": 0.3912,
      "step": 8460
    },
    {
      "epoch": 5.87378640776699,
      "grad_norm": 3.055236339569092,
      "learning_rate": 0.0005928087850243394,
      "loss": 0.4215,
      "step": 8470
    },
    {
      "epoch": 5.880721220527046,
      "grad_norm": 2.302016496658325,
      "learning_rate": 0.000592638388707862,
      "loss": 0.3847,
      "step": 8480
    },
    {
      "epoch": 5.887656033287101,
      "grad_norm": 1.5769407749176025,
      "learning_rate": 0.0005924679923913848,
      "loss": 0.3749,
      "step": 8490
    },
    {
      "epoch": 5.894590846047157,
      "grad_norm": 2.2493202686309814,
      "learning_rate": 0.0005922975960749076,
      "loss": 0.4372,
      "step": 8500
    },
    {
      "epoch": 5.901525658807213,
      "grad_norm": 1.6385022401809692,
      "learning_rate": 0.0005921271997584303,
      "loss": 0.3937,
      "step": 8510
    },
    {
      "epoch": 5.908460471567268,
      "grad_norm": 0.7675570845603943,
      "learning_rate": 0.0005919568034419531,
      "loss": 0.4971,
      "step": 8520
    },
    {
      "epoch": 5.915395284327323,
      "grad_norm": 1.626524806022644,
      "learning_rate": 0.0005917864071254759,
      "loss": 0.4588,
      "step": 8530
    },
    {
      "epoch": 5.922330097087379,
      "grad_norm": 1.795169472694397,
      "learning_rate": 0.0005916160108089987,
      "loss": 0.4156,
      "step": 8540
    },
    {
      "epoch": 5.929264909847435,
      "grad_norm": 2.828415632247925,
      "learning_rate": 0.0005914456144925213,
      "loss": 0.4638,
      "step": 8550
    },
    {
      "epoch": 5.93619972260749,
      "grad_norm": 1.9432305097579956,
      "learning_rate": 0.0005912752181760441,
      "loss": 0.3686,
      "step": 8560
    },
    {
      "epoch": 5.943134535367545,
      "grad_norm": 2.0942420959472656,
      "learning_rate": 0.0005911048218595669,
      "loss": 0.3962,
      "step": 8570
    },
    {
      "epoch": 5.950069348127601,
      "grad_norm": 2.824538230895996,
      "learning_rate": 0.0005909344255430896,
      "loss": 0.3584,
      "step": 8580
    },
    {
      "epoch": 5.9570041608876565,
      "grad_norm": 2.0686542987823486,
      "learning_rate": 0.0005907640292266124,
      "loss": 0.4128,
      "step": 8590
    },
    {
      "epoch": 5.9639389736477115,
      "grad_norm": 2.751047372817993,
      "learning_rate": 0.0005905936329101352,
      "loss": 0.3895,
      "step": 8600
    },
    {
      "epoch": 5.970873786407767,
      "grad_norm": 2.581878900527954,
      "learning_rate": 0.0005904232365936578,
      "loss": 0.2757,
      "step": 8610
    },
    {
      "epoch": 5.9778085991678225,
      "grad_norm": 2.285287857055664,
      "learning_rate": 0.0005902528402771806,
      "loss": 0.408,
      "step": 8620
    },
    {
      "epoch": 5.984743411927878,
      "grad_norm": 3.223942756652832,
      "learning_rate": 0.0005900824439607034,
      "loss": 0.4729,
      "step": 8630
    },
    {
      "epoch": 5.991678224687933,
      "grad_norm": 2.6449759006500244,
      "learning_rate": 0.0005899120476442261,
      "loss": 0.4912,
      "step": 8640
    },
    {
      "epoch": 5.9986130374479885,
      "grad_norm": 1.0914078950881958,
      "learning_rate": 0.000589741651327749,
      "loss": 0.4351,
      "step": 8650
    },
    {
      "epoch": 6.0,
      "eval_accuracy": 0.8144294138050642,
      "eval_auc": 0.8144376930414136,
      "eval_f1": 0.8187055235513386,
      "eval_loss": 0.4170539975166321,
      "eval_mcc": 0.6295865989605676,
      "eval_precision": 0.8,
      "eval_recall": 0.8383067314365025,
      "eval_runtime": 79.276,
      "eval_samples_per_second": 36.367,
      "eval_steps_per_second": 2.283,
      "step": 8652
    },
    {
      "epoch": 6.005547850208044,
      "grad_norm": 1.3222397565841675,
      "learning_rate": 0.0005895712550112716,
      "loss": 0.3306,
      "step": 8660
    },
    {
      "epoch": 6.0124826629681,
      "grad_norm": 3.5268208980560303,
      "learning_rate": 0.0005894008586947944,
      "loss": 0.4804,
      "step": 8670
    },
    {
      "epoch": 6.019417475728155,
      "grad_norm": 2.1784636974334717,
      "learning_rate": 0.0005892304623783172,
      "loss": 0.3708,
      "step": 8680
    },
    {
      "epoch": 6.026352288488211,
      "grad_norm": 0.9424761533737183,
      "learning_rate": 0.0005890600660618399,
      "loss": 0.3583,
      "step": 8690
    },
    {
      "epoch": 6.033287101248266,
      "grad_norm": 2.9395511150360107,
      "learning_rate": 0.0005888896697453627,
      "loss": 0.5381,
      "step": 8700
    },
    {
      "epoch": 6.040221914008322,
      "grad_norm": 2.043602705001831,
      "learning_rate": 0.0005887192734288855,
      "loss": 0.3566,
      "step": 8710
    },
    {
      "epoch": 6.047156726768377,
      "grad_norm": 6.395673751831055,
      "learning_rate": 0.0005885488771124081,
      "loss": 0.3371,
      "step": 8720
    },
    {
      "epoch": 6.054091539528433,
      "grad_norm": 2.601484775543213,
      "learning_rate": 0.0005883784807959309,
      "loss": 0.4794,
      "step": 8730
    },
    {
      "epoch": 6.061026352288488,
      "grad_norm": 3.041565418243408,
      "learning_rate": 0.0005882080844794537,
      "loss": 0.3964,
      "step": 8740
    },
    {
      "epoch": 6.067961165048544,
      "grad_norm": 2.033353328704834,
      "learning_rate": 0.0005880376881629764,
      "loss": 0.4625,
      "step": 8750
    },
    {
      "epoch": 6.074895977808599,
      "grad_norm": 3.233166217803955,
      "learning_rate": 0.0005878672918464992,
      "loss": 0.4759,
      "step": 8760
    },
    {
      "epoch": 6.081830790568655,
      "grad_norm": 2.3132872581481934,
      "learning_rate": 0.000587696895530022,
      "loss": 0.4323,
      "step": 8770
    },
    {
      "epoch": 6.08876560332871,
      "grad_norm": 1.562626838684082,
      "learning_rate": 0.0005875264992135447,
      "loss": 0.3323,
      "step": 8780
    },
    {
      "epoch": 6.095700416088766,
      "grad_norm": 3.813582420349121,
      "learning_rate": 0.0005873561028970674,
      "loss": 0.4247,
      "step": 8790
    },
    {
      "epoch": 6.102635228848821,
      "grad_norm": 1.383955955505371,
      "learning_rate": 0.0005871857065805902,
      "loss": 0.397,
      "step": 8800
    },
    {
      "epoch": 6.109570041608877,
      "grad_norm": 2.0046958923339844,
      "learning_rate": 0.000587015310264113,
      "loss": 0.3933,
      "step": 8810
    },
    {
      "epoch": 6.116504854368932,
      "grad_norm": 2.6197733879089355,
      "learning_rate": 0.0005868449139476357,
      "loss": 0.3854,
      "step": 8820
    },
    {
      "epoch": 6.123439667128988,
      "grad_norm": 1.7615418434143066,
      "learning_rate": 0.0005866745176311585,
      "loss": 0.4295,
      "step": 8830
    },
    {
      "epoch": 6.130374479889043,
      "grad_norm": 1.4512289762496948,
      "learning_rate": 0.0005865041213146812,
      "loss": 0.4024,
      "step": 8840
    },
    {
      "epoch": 6.137309292649099,
      "grad_norm": 2.3147482872009277,
      "learning_rate": 0.0005863337249982039,
      "loss": 0.3989,
      "step": 8850
    },
    {
      "epoch": 6.144244105409154,
      "grad_norm": 1.9737647771835327,
      "learning_rate": 0.0005861633286817268,
      "loss": 0.3099,
      "step": 8860
    },
    {
      "epoch": 6.15117891816921,
      "grad_norm": 2.269742488861084,
      "learning_rate": 0.0005859929323652495,
      "loss": 0.429,
      "step": 8870
    },
    {
      "epoch": 6.158113730929265,
      "grad_norm": 1.7113690376281738,
      "learning_rate": 0.0005858225360487722,
      "loss": 0.3969,
      "step": 8880
    },
    {
      "epoch": 6.165048543689321,
      "grad_norm": 3.233011484146118,
      "learning_rate": 0.000585652139732295,
      "loss": 0.387,
      "step": 8890
    },
    {
      "epoch": 6.171983356449376,
      "grad_norm": 1.9180216789245605,
      "learning_rate": 0.0005854817434158177,
      "loss": 0.3314,
      "step": 8900
    },
    {
      "epoch": 6.178918169209432,
      "grad_norm": 2.562878131866455,
      "learning_rate": 0.0005853113470993405,
      "loss": 0.3746,
      "step": 8910
    },
    {
      "epoch": 6.185852981969487,
      "grad_norm": 2.338667869567871,
      "learning_rate": 0.0005851409507828633,
      "loss": 0.3705,
      "step": 8920
    },
    {
      "epoch": 6.192787794729543,
      "grad_norm": 4.351855278015137,
      "learning_rate": 0.000584970554466386,
      "loss": 0.4157,
      "step": 8930
    },
    {
      "epoch": 6.199722607489598,
      "grad_norm": 2.3825783729553223,
      "learning_rate": 0.0005848001581499088,
      "loss": 0.3737,
      "step": 8940
    },
    {
      "epoch": 6.206657420249654,
      "grad_norm": 2.379699945449829,
      "learning_rate": 0.0005846297618334315,
      "loss": 0.3958,
      "step": 8950
    },
    {
      "epoch": 6.213592233009709,
      "grad_norm": 1.8666636943817139,
      "learning_rate": 0.0005844593655169542,
      "loss": 0.4265,
      "step": 8960
    },
    {
      "epoch": 6.220527045769765,
      "grad_norm": 11.132712364196777,
      "learning_rate": 0.000584288969200477,
      "loss": 0.3614,
      "step": 8970
    },
    {
      "epoch": 6.22746185852982,
      "grad_norm": 2.9253318309783936,
      "learning_rate": 0.0005841185728839998,
      "loss": 0.3835,
      "step": 8980
    },
    {
      "epoch": 6.2343966712898755,
      "grad_norm": 2.0667712688446045,
      "learning_rate": 0.0005839481765675226,
      "loss": 0.4316,
      "step": 8990
    },
    {
      "epoch": 6.2413314840499305,
      "grad_norm": 1.906478762626648,
      "learning_rate": 0.0005837777802510453,
      "loss": 0.5313,
      "step": 9000
    },
    {
      "epoch": 6.2482662968099865,
      "grad_norm": 1.597078800201416,
      "learning_rate": 0.000583607383934568,
      "loss": 0.368,
      "step": 9010
    },
    {
      "epoch": 6.2552011095700415,
      "grad_norm": 2.167771339416504,
      "learning_rate": 0.0005834369876180908,
      "loss": 0.3799,
      "step": 9020
    },
    {
      "epoch": 6.262135922330097,
      "grad_norm": 4.106772422790527,
      "learning_rate": 0.0005832665913016135,
      "loss": 0.4681,
      "step": 9030
    },
    {
      "epoch": 6.2690707350901524,
      "grad_norm": 3.7582411766052246,
      "learning_rate": 0.0005830961949851363,
      "loss": 0.4421,
      "step": 9040
    },
    {
      "epoch": 6.276005547850208,
      "grad_norm": 1.9253530502319336,
      "learning_rate": 0.0005829257986686591,
      "loss": 0.374,
      "step": 9050
    },
    {
      "epoch": 6.282940360610263,
      "grad_norm": 2.0622904300689697,
      "learning_rate": 0.0005827554023521818,
      "loss": 0.3599,
      "step": 9060
    },
    {
      "epoch": 6.289875173370319,
      "grad_norm": 2.12603497505188,
      "learning_rate": 0.0005825850060357045,
      "loss": 0.4224,
      "step": 9070
    },
    {
      "epoch": 6.296809986130374,
      "grad_norm": 1.9913594722747803,
      "learning_rate": 0.0005824146097192273,
      "loss": 0.4008,
      "step": 9080
    },
    {
      "epoch": 6.30374479889043,
      "grad_norm": 1.4749820232391357,
      "learning_rate": 0.00058224421340275,
      "loss": 0.4432,
      "step": 9090
    },
    {
      "epoch": 6.310679611650485,
      "grad_norm": 1.3787710666656494,
      "learning_rate": 0.0005820738170862728,
      "loss": 0.4574,
      "step": 9100
    },
    {
      "epoch": 6.317614424410541,
      "grad_norm": 1.9849066734313965,
      "learning_rate": 0.0005819034207697956,
      "loss": 0.2899,
      "step": 9110
    },
    {
      "epoch": 6.324549237170596,
      "grad_norm": 2.8028573989868164,
      "learning_rate": 0.0005817330244533184,
      "loss": 0.4366,
      "step": 9120
    },
    {
      "epoch": 6.331484049930652,
      "grad_norm": 1.5017080307006836,
      "learning_rate": 0.000581562628136841,
      "loss": 0.3585,
      "step": 9130
    },
    {
      "epoch": 6.338418862690707,
      "grad_norm": 3.8947577476501465,
      "learning_rate": 0.0005813922318203638,
      "loss": 0.4104,
      "step": 9140
    },
    {
      "epoch": 6.345353675450763,
      "grad_norm": 2.085768461227417,
      "learning_rate": 0.0005812218355038866,
      "loss": 0.3961,
      "step": 9150
    },
    {
      "epoch": 6.352288488210818,
      "grad_norm": 1.2946785688400269,
      "learning_rate": 0.0005810514391874093,
      "loss": 0.4467,
      "step": 9160
    },
    {
      "epoch": 6.359223300970874,
      "grad_norm": 2.5664846897125244,
      "learning_rate": 0.0005808810428709321,
      "loss": 0.3673,
      "step": 9170
    },
    {
      "epoch": 6.366158113730929,
      "grad_norm": 1.9022217988967896,
      "learning_rate": 0.0005807106465544548,
      "loss": 0.4693,
      "step": 9180
    },
    {
      "epoch": 6.373092926490985,
      "grad_norm": 1.6316486597061157,
      "learning_rate": 0.0005805402502379775,
      "loss": 0.4715,
      "step": 9190
    },
    {
      "epoch": 6.38002773925104,
      "grad_norm": 1.7898298501968384,
      "learning_rate": 0.0005803698539215004,
      "loss": 0.424,
      "step": 9200
    },
    {
      "epoch": 6.386962552011096,
      "grad_norm": 1.7648286819458008,
      "learning_rate": 0.0005801994576050231,
      "loss": 0.3803,
      "step": 9210
    },
    {
      "epoch": 6.393897364771151,
      "grad_norm": 1.7030869722366333,
      "learning_rate": 0.0005800290612885458,
      "loss": 0.3877,
      "step": 9220
    },
    {
      "epoch": 6.400832177531207,
      "grad_norm": 1.7033442258834839,
      "learning_rate": 0.0005798586649720687,
      "loss": 0.4101,
      "step": 9230
    },
    {
      "epoch": 6.407766990291262,
      "grad_norm": 2.13881778717041,
      "learning_rate": 0.0005796882686555913,
      "loss": 0.4292,
      "step": 9240
    },
    {
      "epoch": 6.414701803051318,
      "grad_norm": 1.3683987855911255,
      "learning_rate": 0.0005795178723391141,
      "loss": 0.4096,
      "step": 9250
    },
    {
      "epoch": 6.421636615811373,
      "grad_norm": 1.6306273937225342,
      "learning_rate": 0.0005793474760226369,
      "loss": 0.3304,
      "step": 9260
    },
    {
      "epoch": 6.428571428571429,
      "grad_norm": 1.5109069347381592,
      "learning_rate": 0.0005791770797061596,
      "loss": 0.4215,
      "step": 9270
    },
    {
      "epoch": 6.435506241331484,
      "grad_norm": 2.3550350666046143,
      "learning_rate": 0.0005790066833896824,
      "loss": 0.4107,
      "step": 9280
    },
    {
      "epoch": 6.44244105409154,
      "grad_norm": 4.084063529968262,
      "learning_rate": 0.0005788362870732052,
      "loss": 0.3996,
      "step": 9290
    },
    {
      "epoch": 6.449375866851595,
      "grad_norm": 1.2336716651916504,
      "learning_rate": 0.0005786658907567278,
      "loss": 0.4348,
      "step": 9300
    },
    {
      "epoch": 6.456310679611651,
      "grad_norm": 2.5266168117523193,
      "learning_rate": 0.0005784954944402506,
      "loss": 0.3545,
      "step": 9310
    },
    {
      "epoch": 6.463245492371706,
      "grad_norm": 6.431821346282959,
      "learning_rate": 0.0005783250981237734,
      "loss": 0.4102,
      "step": 9320
    },
    {
      "epoch": 6.470180305131762,
      "grad_norm": 1.0397429466247559,
      "learning_rate": 0.0005781547018072962,
      "loss": 0.3471,
      "step": 9330
    },
    {
      "epoch": 6.477115117891817,
      "grad_norm": 3.581655502319336,
      "learning_rate": 0.0005779843054908189,
      "loss": 0.3879,
      "step": 9340
    },
    {
      "epoch": 6.484049930651873,
      "grad_norm": 3.700284957885742,
      "learning_rate": 0.0005778139091743417,
      "loss": 0.361,
      "step": 9350
    },
    {
      "epoch": 6.490984743411928,
      "grad_norm": 2.7091970443725586,
      "learning_rate": 0.0005776435128578644,
      "loss": 0.3447,
      "step": 9360
    },
    {
      "epoch": 6.497919556171984,
      "grad_norm": 2.1885178089141846,
      "learning_rate": 0.0005774731165413871,
      "loss": 0.4605,
      "step": 9370
    },
    {
      "epoch": 6.504854368932039,
      "grad_norm": 1.7867693901062012,
      "learning_rate": 0.0005773027202249099,
      "loss": 0.4294,
      "step": 9380
    },
    {
      "epoch": 6.5117891816920945,
      "grad_norm": 2.0839219093322754,
      "learning_rate": 0.0005771323239084327,
      "loss": 0.4033,
      "step": 9390
    },
    {
      "epoch": 6.51872399445215,
      "grad_norm": 3.610915422439575,
      "learning_rate": 0.0005769619275919554,
      "loss": 0.4147,
      "step": 9400
    },
    {
      "epoch": 6.5256588072122055,
      "grad_norm": 1.2788825035095215,
      "learning_rate": 0.0005767915312754781,
      "loss": 0.3808,
      "step": 9410
    },
    {
      "epoch": 6.5325936199722605,
      "grad_norm": 2.179187297821045,
      "learning_rate": 0.0005766211349590009,
      "loss": 0.3852,
      "step": 9420
    },
    {
      "epoch": 6.539528432732316,
      "grad_norm": 7.057796001434326,
      "learning_rate": 0.0005764507386425236,
      "loss": 0.3949,
      "step": 9430
    },
    {
      "epoch": 6.5464632454923715,
      "grad_norm": 1.4511759281158447,
      "learning_rate": 0.0005762803423260465,
      "loss": 0.4332,
      "step": 9440
    },
    {
      "epoch": 6.553398058252427,
      "grad_norm": 2.0909299850463867,
      "learning_rate": 0.0005761099460095692,
      "loss": 0.2972,
      "step": 9450
    },
    {
      "epoch": 6.560332871012482,
      "grad_norm": 1.447161078453064,
      "learning_rate": 0.000575939549693092,
      "loss": 0.4343,
      "step": 9460
    },
    {
      "epoch": 6.567267683772538,
      "grad_norm": 2.1877658367156982,
      "learning_rate": 0.0005757691533766147,
      "loss": 0.4135,
      "step": 9470
    },
    {
      "epoch": 6.574202496532593,
      "grad_norm": 1.432533860206604,
      "learning_rate": 0.0005755987570601374,
      "loss": 0.4389,
      "step": 9480
    },
    {
      "epoch": 6.581137309292649,
      "grad_norm": 1.338769793510437,
      "learning_rate": 0.0005754283607436602,
      "loss": 0.3937,
      "step": 9490
    },
    {
      "epoch": 6.588072122052704,
      "grad_norm": 2.2582969665527344,
      "learning_rate": 0.000575257964427183,
      "loss": 0.3551,
      "step": 9500
    },
    {
      "epoch": 6.59500693481276,
      "grad_norm": 2.1240017414093018,
      "learning_rate": 0.0005750875681107057,
      "loss": 0.342,
      "step": 9510
    },
    {
      "epoch": 6.601941747572815,
      "grad_norm": 2.2384605407714844,
      "learning_rate": 0.0005749171717942285,
      "loss": 0.3678,
      "step": 9520
    },
    {
      "epoch": 6.608876560332871,
      "grad_norm": 6.489050388336182,
      "learning_rate": 0.0005747467754777512,
      "loss": 0.3991,
      "step": 9530
    },
    {
      "epoch": 6.615811373092926,
      "grad_norm": 1.5288820266723633,
      "learning_rate": 0.0005745763791612739,
      "loss": 0.4396,
      "step": 9540
    },
    {
      "epoch": 6.622746185852982,
      "grad_norm": 2.304807662963867,
      "learning_rate": 0.0005744059828447967,
      "loss": 0.3801,
      "step": 9550
    },
    {
      "epoch": 6.629680998613037,
      "grad_norm": 2.9987783432006836,
      "learning_rate": 0.0005742355865283195,
      "loss": 0.4,
      "step": 9560
    },
    {
      "epoch": 6.636615811373093,
      "grad_norm": 1.2711131572723389,
      "learning_rate": 0.0005740651902118423,
      "loss": 0.4029,
      "step": 9570
    },
    {
      "epoch": 6.643550624133148,
      "grad_norm": 1.5965855121612549,
      "learning_rate": 0.000573894793895365,
      "loss": 0.4447,
      "step": 9580
    },
    {
      "epoch": 6.650485436893204,
      "grad_norm": 2.8470864295959473,
      "learning_rate": 0.0005737243975788877,
      "loss": 0.4405,
      "step": 9590
    },
    {
      "epoch": 6.657420249653259,
      "grad_norm": 1.72786545753479,
      "learning_rate": 0.0005735540012624105,
      "loss": 0.4217,
      "step": 9600
    },
    {
      "epoch": 6.664355062413315,
      "grad_norm": 1.4922518730163574,
      "learning_rate": 0.0005733836049459332,
      "loss": 0.4318,
      "step": 9610
    },
    {
      "epoch": 6.67128987517337,
      "grad_norm": 1.6026973724365234,
      "learning_rate": 0.000573213208629456,
      "loss": 0.3958,
      "step": 9620
    },
    {
      "epoch": 6.678224687933426,
      "grad_norm": 1.6776659488677979,
      "learning_rate": 0.0005730428123129788,
      "loss": 0.3817,
      "step": 9630
    },
    {
      "epoch": 6.685159500693481,
      "grad_norm": 4.300652503967285,
      "learning_rate": 0.0005728724159965014,
      "loss": 0.4155,
      "step": 9640
    },
    {
      "epoch": 6.692094313453537,
      "grad_norm": 8.516692161560059,
      "learning_rate": 0.0005727020196800243,
      "loss": 0.3722,
      "step": 9650
    },
    {
      "epoch": 6.699029126213592,
      "grad_norm": 2.4584271907806396,
      "learning_rate": 0.000572531623363547,
      "loss": 0.3911,
      "step": 9660
    },
    {
      "epoch": 6.705963938973648,
      "grad_norm": 2.1684834957122803,
      "learning_rate": 0.0005723612270470697,
      "loss": 0.4911,
      "step": 9670
    },
    {
      "epoch": 6.712898751733703,
      "grad_norm": 1.8386075496673584,
      "learning_rate": 0.0005721908307305926,
      "loss": 0.5312,
      "step": 9680
    },
    {
      "epoch": 6.719833564493759,
      "grad_norm": 2.557222604751587,
      "learning_rate": 0.0005720204344141153,
      "loss": 0.4856,
      "step": 9690
    },
    {
      "epoch": 6.726768377253814,
      "grad_norm": 1.898003339767456,
      "learning_rate": 0.000571850038097638,
      "loss": 0.4444,
      "step": 9700
    },
    {
      "epoch": 6.73370319001387,
      "grad_norm": 4.250784873962402,
      "learning_rate": 0.0005716796417811608,
      "loss": 0.4554,
      "step": 9710
    },
    {
      "epoch": 6.740638002773925,
      "grad_norm": 2.4798121452331543,
      "learning_rate": 0.0005715092454646835,
      "loss": 0.3342,
      "step": 9720
    },
    {
      "epoch": 6.747572815533981,
      "grad_norm": 2.283907651901245,
      "learning_rate": 0.0005713388491482063,
      "loss": 0.4173,
      "step": 9730
    },
    {
      "epoch": 6.754507628294036,
      "grad_norm": 2.4758718013763428,
      "learning_rate": 0.0005711684528317291,
      "loss": 0.416,
      "step": 9740
    },
    {
      "epoch": 6.761442441054092,
      "grad_norm": 1.0000624656677246,
      "learning_rate": 0.0005709980565152518,
      "loss": 0.4107,
      "step": 9750
    },
    {
      "epoch": 6.768377253814147,
      "grad_norm": 1.7800253629684448,
      "learning_rate": 0.0005708276601987745,
      "loss": 0.4394,
      "step": 9760
    },
    {
      "epoch": 6.775312066574203,
      "grad_norm": 1.6310489177703857,
      "learning_rate": 0.0005706572638822973,
      "loss": 0.4032,
      "step": 9770
    },
    {
      "epoch": 6.782246879334258,
      "grad_norm": 5.567906856536865,
      "learning_rate": 0.0005704868675658201,
      "loss": 0.4999,
      "step": 9780
    },
    {
      "epoch": 6.7891816920943135,
      "grad_norm": 1.9887279272079468,
      "learning_rate": 0.0005703164712493428,
      "loss": 0.4152,
      "step": 9790
    },
    {
      "epoch": 6.796116504854369,
      "grad_norm": 1.6741015911102295,
      "learning_rate": 0.0005701460749328656,
      "loss": 0.4951,
      "step": 9800
    },
    {
      "epoch": 6.8030513176144245,
      "grad_norm": 3.7210724353790283,
      "learning_rate": 0.0005699756786163884,
      "loss": 0.4632,
      "step": 9810
    },
    {
      "epoch": 6.8099861303744795,
      "grad_norm": 1.9083510637283325,
      "learning_rate": 0.000569805282299911,
      "loss": 0.3796,
      "step": 9820
    },
    {
      "epoch": 6.8169209431345354,
      "grad_norm": 2.2920422554016113,
      "learning_rate": 0.0005696348859834338,
      "loss": 0.4289,
      "step": 9830
    },
    {
      "epoch": 6.8238557558945905,
      "grad_norm": 4.080992221832275,
      "learning_rate": 0.0005694644896669566,
      "loss": 0.3451,
      "step": 9840
    },
    {
      "epoch": 6.830790568654646,
      "grad_norm": 3.027292013168335,
      "learning_rate": 0.0005692940933504793,
      "loss": 0.4209,
      "step": 9850
    },
    {
      "epoch": 6.837725381414701,
      "grad_norm": 1.6523901224136353,
      "learning_rate": 0.0005691236970340022,
      "loss": 0.4826,
      "step": 9860
    },
    {
      "epoch": 6.844660194174757,
      "grad_norm": 1.5317877531051636,
      "learning_rate": 0.0005689533007175249,
      "loss": 0.3462,
      "step": 9870
    },
    {
      "epoch": 6.851595006934812,
      "grad_norm": 3.532932758331299,
      "learning_rate": 0.0005687829044010475,
      "loss": 0.4258,
      "step": 9880
    },
    {
      "epoch": 6.858529819694868,
      "grad_norm": 2.0219876766204834,
      "learning_rate": 0.0005686125080845704,
      "loss": 0.3622,
      "step": 9890
    },
    {
      "epoch": 6.865464632454923,
      "grad_norm": 2.2083449363708496,
      "learning_rate": 0.0005684421117680931,
      "loss": 0.4819,
      "step": 9900
    },
    {
      "epoch": 6.872399445214979,
      "grad_norm": 2.337285041809082,
      "learning_rate": 0.0005682717154516159,
      "loss": 0.443,
      "step": 9910
    },
    {
      "epoch": 6.879334257975035,
      "grad_norm": 1.7074137926101685,
      "learning_rate": 0.0005681013191351387,
      "loss": 0.387,
      "step": 9920
    },
    {
      "epoch": 6.88626907073509,
      "grad_norm": 1.542110562324524,
      "learning_rate": 0.0005679309228186613,
      "loss": 0.4094,
      "step": 9930
    },
    {
      "epoch": 6.893203883495145,
      "grad_norm": 3.471726417541504,
      "learning_rate": 0.0005677605265021841,
      "loss": 0.4838,
      "step": 9940
    },
    {
      "epoch": 6.900138696255201,
      "grad_norm": 2.197906255722046,
      "learning_rate": 0.0005675901301857069,
      "loss": 0.3421,
      "step": 9950
    },
    {
      "epoch": 6.907073509015257,
      "grad_norm": 3.4605653285980225,
      "learning_rate": 0.0005674197338692296,
      "loss": 0.4033,
      "step": 9960
    },
    {
      "epoch": 6.914008321775312,
      "grad_norm": 2.1622164249420166,
      "learning_rate": 0.0005672493375527524,
      "loss": 0.521,
      "step": 9970
    },
    {
      "epoch": 6.920943134535367,
      "grad_norm": 1.6954472064971924,
      "learning_rate": 0.0005670789412362752,
      "loss": 0.3311,
      "step": 9980
    },
    {
      "epoch": 6.927877947295423,
      "grad_norm": 1.685346245765686,
      "learning_rate": 0.000566908544919798,
      "loss": 0.4964,
      "step": 9990
    },
    {
      "epoch": 6.934812760055479,
      "grad_norm": 2.3829402923583984,
      "learning_rate": 0.0005667381486033206,
      "loss": 0.4379,
      "step": 10000
    },
    {
      "epoch": 6.941747572815534,
      "grad_norm": 2.160423994064331,
      "learning_rate": 0.0005665677522868434,
      "loss": 0.4021,
      "step": 10010
    },
    {
      "epoch": 6.948682385575589,
      "grad_norm": 1.7462122440338135,
      "learning_rate": 0.0005663973559703662,
      "loss": 0.4428,
      "step": 10020
    },
    {
      "epoch": 6.955617198335645,
      "grad_norm": 1.9450708627700806,
      "learning_rate": 0.0005662269596538889,
      "loss": 0.3902,
      "step": 10030
    },
    {
      "epoch": 6.962552011095701,
      "grad_norm": 3.937772512435913,
      "learning_rate": 0.0005660565633374117,
      "loss": 0.4034,
      "step": 10040
    },
    {
      "epoch": 6.969486823855756,
      "grad_norm": 1.7396972179412842,
      "learning_rate": 0.0005658861670209344,
      "loss": 0.3772,
      "step": 10050
    },
    {
      "epoch": 6.976421636615811,
      "grad_norm": 1.920174241065979,
      "learning_rate": 0.0005657157707044571,
      "loss": 0.424,
      "step": 10060
    },
    {
      "epoch": 6.983356449375867,
      "grad_norm": 2.8559231758117676,
      "learning_rate": 0.0005655453743879799,
      "loss": 0.4447,
      "step": 10070
    },
    {
      "epoch": 6.990291262135923,
      "grad_norm": 2.799462080001831,
      "learning_rate": 0.0005653749780715027,
      "loss": 0.3665,
      "step": 10080
    },
    {
      "epoch": 6.997226074895978,
      "grad_norm": 1.8849740028381348,
      "learning_rate": 0.0005652045817550254,
      "loss": 0.4023,
      "step": 10090
    },
    {
      "epoch": 7.0,
      "eval_accuracy": 0.8314255983350677,
      "eval_auc": 0.8314164343031163,
      "eval_f1": 0.82679971489665,
      "eval_loss": 0.4026888608932495,
      "eval_mcc": 0.6637682045465557,
      "eval_precision": 0.8498168498168498,
      "eval_recall": 0.8049965301873698,
      "eval_runtime": 79.0828,
      "eval_samples_per_second": 36.455,
      "eval_steps_per_second": 2.289,
      "step": 10094
    },
    {
      "epoch": 7.004160887656034,
      "grad_norm": 2.6147429943084717,
      "learning_rate": 0.0005650341854385483,
      "loss": 0.374,
      "step": 10100
    },
    {
      "epoch": 7.011095700416089,
      "grad_norm": 2.282193660736084,
      "learning_rate": 0.0005648637891220709,
      "loss": 0.4606,
      "step": 10110
    },
    {
      "epoch": 7.018030513176145,
      "grad_norm": 1.4123047590255737,
      "learning_rate": 0.0005646933928055937,
      "loss": 0.378,
      "step": 10120
    },
    {
      "epoch": 7.0249653259362,
      "grad_norm": 3.988966941833496,
      "learning_rate": 0.0005645229964891165,
      "loss": 0.3897,
      "step": 10130
    },
    {
      "epoch": 7.031900138696256,
      "grad_norm": 1.3258975744247437,
      "learning_rate": 0.0005643526001726392,
      "loss": 0.3457,
      "step": 10140
    },
    {
      "epoch": 7.038834951456311,
      "grad_norm": 4.3620147705078125,
      "learning_rate": 0.000564182203856162,
      "loss": 0.4106,
      "step": 10150
    },
    {
      "epoch": 7.045769764216367,
      "grad_norm": 1.2053862810134888,
      "learning_rate": 0.0005640118075396848,
      "loss": 0.4286,
      "step": 10160
    },
    {
      "epoch": 7.052704576976422,
      "grad_norm": 5.012759685516357,
      "learning_rate": 0.0005638414112232074,
      "loss": 0.4596,
      "step": 10170
    },
    {
      "epoch": 7.0596393897364775,
      "grad_norm": 3.855634927749634,
      "learning_rate": 0.0005636710149067302,
      "loss": 0.4281,
      "step": 10180
    },
    {
      "epoch": 7.066574202496533,
      "grad_norm": 1.8668614625930786,
      "learning_rate": 0.000563500618590253,
      "loss": 0.437,
      "step": 10190
    },
    {
      "epoch": 7.0735090152565885,
      "grad_norm": 2.031689405441284,
      "learning_rate": 0.0005633302222737757,
      "loss": 0.3538,
      "step": 10200
    },
    {
      "epoch": 7.0804438280166435,
      "grad_norm": 1.9231215715408325,
      "learning_rate": 0.0005631598259572985,
      "loss": 0.4223,
      "step": 10210
    },
    {
      "epoch": 7.087378640776699,
      "grad_norm": 1.3778109550476074,
      "learning_rate": 0.0005629894296408213,
      "loss": 0.4146,
      "step": 10220
    },
    {
      "epoch": 7.0943134535367545,
      "grad_norm": 2.230111598968506,
      "learning_rate": 0.000562819033324344,
      "loss": 0.3829,
      "step": 10230
    },
    {
      "epoch": 7.10124826629681,
      "grad_norm": 2.3289260864257812,
      "learning_rate": 0.0005626486370078667,
      "loss": 0.2586,
      "step": 10240
    },
    {
      "epoch": 7.108183079056865,
      "grad_norm": 2.3755006790161133,
      "learning_rate": 0.0005624782406913895,
      "loss": 0.4319,
      "step": 10250
    },
    {
      "epoch": 7.115117891816921,
      "grad_norm": 3.1990320682525635,
      "learning_rate": 0.0005623078443749123,
      "loss": 0.4517,
      "step": 10260
    },
    {
      "epoch": 7.122052704576976,
      "grad_norm": 2.3986833095550537,
      "learning_rate": 0.000562137448058435,
      "loss": 0.3341,
      "step": 10270
    },
    {
      "epoch": 7.128987517337032,
      "grad_norm": 1.7616441249847412,
      "learning_rate": 0.0005619670517419577,
      "loss": 0.3531,
      "step": 10280
    },
    {
      "epoch": 7.135922330097087,
      "grad_norm": 1.7171188592910767,
      "learning_rate": 0.0005617966554254805,
      "loss": 0.3587,
      "step": 10290
    },
    {
      "epoch": 7.142857142857143,
      "grad_norm": 2.671583652496338,
      "learning_rate": 0.0005616262591090032,
      "loss": 0.4096,
      "step": 10300
    },
    {
      "epoch": 7.149791955617198,
      "grad_norm": 3.2795803546905518,
      "learning_rate": 0.0005614558627925261,
      "loss": 0.4613,
      "step": 10310
    },
    {
      "epoch": 7.156726768377254,
      "grad_norm": 2.898047685623169,
      "learning_rate": 0.0005612854664760488,
      "loss": 0.4849,
      "step": 10320
    },
    {
      "epoch": 7.163661581137309,
      "grad_norm": 2.642622709274292,
      "learning_rate": 0.0005611150701595715,
      "loss": 0.3681,
      "step": 10330
    },
    {
      "epoch": 7.170596393897365,
      "grad_norm": 3.0743041038513184,
      "learning_rate": 0.0005609446738430943,
      "loss": 0.4681,
      "step": 10340
    },
    {
      "epoch": 7.17753120665742,
      "grad_norm": 1.5489122867584229,
      "learning_rate": 0.000560774277526617,
      "loss": 0.4987,
      "step": 10350
    },
    {
      "epoch": 7.184466019417476,
      "grad_norm": 1.2059555053710938,
      "learning_rate": 0.0005606038812101398,
      "loss": 0.4138,
      "step": 10360
    },
    {
      "epoch": 7.191400832177531,
      "grad_norm": 2.895603895187378,
      "learning_rate": 0.0005604334848936626,
      "loss": 0.3545,
      "step": 10370
    },
    {
      "epoch": 7.198335644937587,
      "grad_norm": 1.5402847528457642,
      "learning_rate": 0.0005602630885771853,
      "loss": 0.4144,
      "step": 10380
    },
    {
      "epoch": 7.205270457697642,
      "grad_norm": 1.4682658910751343,
      "learning_rate": 0.0005600926922607081,
      "loss": 0.3855,
      "step": 10390
    },
    {
      "epoch": 7.212205270457698,
      "grad_norm": 2.2491140365600586,
      "learning_rate": 0.0005599222959442308,
      "loss": 0.3704,
      "step": 10400
    },
    {
      "epoch": 7.219140083217753,
      "grad_norm": 1.8250139951705933,
      "learning_rate": 0.0005597518996277535,
      "loss": 0.3548,
      "step": 10410
    },
    {
      "epoch": 7.226074895977809,
      "grad_norm": 5.600576400756836,
      "learning_rate": 0.0005595815033112763,
      "loss": 0.3793,
      "step": 10420
    },
    {
      "epoch": 7.233009708737864,
      "grad_norm": 2.2739360332489014,
      "learning_rate": 0.0005594111069947991,
      "loss": 0.4247,
      "step": 10430
    },
    {
      "epoch": 7.23994452149792,
      "grad_norm": 2.691450357437134,
      "learning_rate": 0.0005592407106783219,
      "loss": 0.3856,
      "step": 10440
    },
    {
      "epoch": 7.246879334257975,
      "grad_norm": 2.904118299484253,
      "learning_rate": 0.0005590703143618446,
      "loss": 0.3702,
      "step": 10450
    },
    {
      "epoch": 7.253814147018031,
      "grad_norm": 1.227813720703125,
      "learning_rate": 0.0005588999180453673,
      "loss": 0.4016,
      "step": 10460
    },
    {
      "epoch": 7.260748959778086,
      "grad_norm": 1.916284203529358,
      "learning_rate": 0.0005587295217288901,
      "loss": 0.3392,
      "step": 10470
    },
    {
      "epoch": 7.267683772538142,
      "grad_norm": 0.9465125203132629,
      "learning_rate": 0.0005585591254124128,
      "loss": 0.3606,
      "step": 10480
    },
    {
      "epoch": 7.274618585298197,
      "grad_norm": 2.293972969055176,
      "learning_rate": 0.0005583887290959356,
      "loss": 0.341,
      "step": 10490
    },
    {
      "epoch": 7.281553398058253,
      "grad_norm": 1.8034106492996216,
      "learning_rate": 0.0005582183327794584,
      "loss": 0.4127,
      "step": 10500
    },
    {
      "epoch": 7.288488210818308,
      "grad_norm": 8.374595642089844,
      "learning_rate": 0.000558047936462981,
      "loss": 0.3863,
      "step": 10510
    },
    {
      "epoch": 7.295423023578364,
      "grad_norm": 1.8073891401290894,
      "learning_rate": 0.0005578775401465038,
      "loss": 0.3982,
      "step": 10520
    },
    {
      "epoch": 7.302357836338419,
      "grad_norm": 2.621682643890381,
      "learning_rate": 0.0005577071438300266,
      "loss": 0.4019,
      "step": 10530
    },
    {
      "epoch": 7.309292649098475,
      "grad_norm": 1.8395267724990845,
      "learning_rate": 0.0005575367475135493,
      "loss": 0.4919,
      "step": 10540
    },
    {
      "epoch": 7.31622746185853,
      "grad_norm": 1.9799952507019043,
      "learning_rate": 0.0005573663511970722,
      "loss": 0.4038,
      "step": 10550
    },
    {
      "epoch": 7.323162274618586,
      "grad_norm": 2.0426228046417236,
      "learning_rate": 0.0005571959548805949,
      "loss": 0.4168,
      "step": 10560
    },
    {
      "epoch": 7.330097087378641,
      "grad_norm": 3.2548303604125977,
      "learning_rate": 0.0005570255585641176,
      "loss": 0.3329,
      "step": 10570
    },
    {
      "epoch": 7.3370319001386965,
      "grad_norm": 1.182203769683838,
      "learning_rate": 0.0005568551622476404,
      "loss": 0.4256,
      "step": 10580
    },
    {
      "epoch": 7.343966712898752,
      "grad_norm": 2.802940845489502,
      "learning_rate": 0.0005566847659311631,
      "loss": 0.3956,
      "step": 10590
    },
    {
      "epoch": 7.3509015256588075,
      "grad_norm": 1.394371747970581,
      "learning_rate": 0.0005565143696146859,
      "loss": 0.4009,
      "step": 10600
    },
    {
      "epoch": 7.3578363384188625,
      "grad_norm": 1.680108904838562,
      "learning_rate": 0.0005563439732982087,
      "loss": 0.3319,
      "step": 10610
    },
    {
      "epoch": 7.3647711511789185,
      "grad_norm": 1.803851842880249,
      "learning_rate": 0.0005561735769817314,
      "loss": 0.3229,
      "step": 10620
    },
    {
      "epoch": 7.3717059639389735,
      "grad_norm": 1.284591555595398,
      "learning_rate": 0.0005560031806652541,
      "loss": 0.4378,
      "step": 10630
    },
    {
      "epoch": 7.378640776699029,
      "grad_norm": 4.530837059020996,
      "learning_rate": 0.0005558327843487769,
      "loss": 0.4514,
      "step": 10640
    },
    {
      "epoch": 7.385575589459084,
      "grad_norm": 1.3903554677963257,
      "learning_rate": 0.0005556623880322997,
      "loss": 0.4537,
      "step": 10650
    },
    {
      "epoch": 7.39251040221914,
      "grad_norm": 4.101833343505859,
      "learning_rate": 0.0005554919917158224,
      "loss": 0.3461,
      "step": 10660
    },
    {
      "epoch": 7.399445214979195,
      "grad_norm": 2.7717692852020264,
      "learning_rate": 0.0005553215953993452,
      "loss": 0.4242,
      "step": 10670
    },
    {
      "epoch": 7.406380027739251,
      "grad_norm": 2.7680063247680664,
      "learning_rate": 0.000555151199082868,
      "loss": 0.3512,
      "step": 10680
    },
    {
      "epoch": 7.413314840499306,
      "grad_norm": 1.2801547050476074,
      "learning_rate": 0.0005549808027663906,
      "loss": 0.5787,
      "step": 10690
    },
    {
      "epoch": 7.420249653259362,
      "grad_norm": 1.8367927074432373,
      "learning_rate": 0.0005548104064499134,
      "loss": 0.3764,
      "step": 10700
    },
    {
      "epoch": 7.427184466019417,
      "grad_norm": 4.163532733917236,
      "learning_rate": 0.0005546400101334362,
      "loss": 0.3459,
      "step": 10710
    },
    {
      "epoch": 7.434119278779473,
      "grad_norm": 4.520393371582031,
      "learning_rate": 0.0005544696138169589,
      "loss": 0.5035,
      "step": 10720
    },
    {
      "epoch": 7.441054091539528,
      "grad_norm": 3.241564989089966,
      "learning_rate": 0.0005542992175004817,
      "loss": 0.3619,
      "step": 10730
    },
    {
      "epoch": 7.447988904299584,
      "grad_norm": 1.9028189182281494,
      "learning_rate": 0.0005541288211840045,
      "loss": 0.302,
      "step": 10740
    },
    {
      "epoch": 7.454923717059639,
      "grad_norm": 1.1120392084121704,
      "learning_rate": 0.0005539584248675271,
      "loss": 0.3954,
      "step": 10750
    },
    {
      "epoch": 7.461858529819695,
      "grad_norm": 2.4022328853607178,
      "learning_rate": 0.00055378802855105,
      "loss": 0.3622,
      "step": 10760
    },
    {
      "epoch": 7.46879334257975,
      "grad_norm": 1.768134355545044,
      "learning_rate": 0.0005536176322345727,
      "loss": 0.4394,
      "step": 10770
    },
    {
      "epoch": 7.475728155339806,
      "grad_norm": 1.6337523460388184,
      "learning_rate": 0.0005534472359180955,
      "loss": 0.4664,
      "step": 10780
    },
    {
      "epoch": 7.482662968099861,
      "grad_norm": 1.4074969291687012,
      "learning_rate": 0.0005532768396016183,
      "loss": 0.3476,
      "step": 10790
    },
    {
      "epoch": 7.489597780859917,
      "grad_norm": 2.2771682739257812,
      "learning_rate": 0.000553106443285141,
      "loss": 0.401,
      "step": 10800
    },
    {
      "epoch": 7.496532593619972,
      "grad_norm": 1.9135940074920654,
      "learning_rate": 0.0005529360469686637,
      "loss": 0.4336,
      "step": 10810
    },
    {
      "epoch": 7.503467406380028,
      "grad_norm": 1.2409917116165161,
      "learning_rate": 0.0005527656506521865,
      "loss": 0.3806,
      "step": 10820
    },
    {
      "epoch": 7.510402219140083,
      "grad_norm": 1.2204771041870117,
      "learning_rate": 0.0005525952543357092,
      "loss": 0.4267,
      "step": 10830
    },
    {
      "epoch": 7.517337031900139,
      "grad_norm": 3.3586270809173584,
      "learning_rate": 0.000552424858019232,
      "loss": 0.2928,
      "step": 10840
    },
    {
      "epoch": 7.524271844660194,
      "grad_norm": 4.968681335449219,
      "learning_rate": 0.0005522544617027548,
      "loss": 0.325,
      "step": 10850
    },
    {
      "epoch": 7.53120665742025,
      "grad_norm": 2.368809700012207,
      "learning_rate": 0.0005520840653862774,
      "loss": 0.3118,
      "step": 10860
    },
    {
      "epoch": 7.538141470180305,
      "grad_norm": 1.7237228155136108,
      "learning_rate": 0.0005519136690698002,
      "loss": 0.4289,
      "step": 10870
    },
    {
      "epoch": 7.545076282940361,
      "grad_norm": 2.509476661682129,
      "learning_rate": 0.000551743272753323,
      "loss": 0.4528,
      "step": 10880
    },
    {
      "epoch": 7.552011095700416,
      "grad_norm": 1.562368392944336,
      "learning_rate": 0.0005515728764368458,
      "loss": 0.3105,
      "step": 10890
    },
    {
      "epoch": 7.558945908460472,
      "grad_norm": 2.6396877765655518,
      "learning_rate": 0.0005514024801203685,
      "loss": 0.4107,
      "step": 10900
    },
    {
      "epoch": 7.565880721220527,
      "grad_norm": 1.9786789417266846,
      "learning_rate": 0.0005512320838038913,
      "loss": 0.3679,
      "step": 10910
    },
    {
      "epoch": 7.572815533980583,
      "grad_norm": 3.4326114654541016,
      "learning_rate": 0.000551061687487414,
      "loss": 0.4225,
      "step": 10920
    },
    {
      "epoch": 7.579750346740638,
      "grad_norm": 5.124345302581787,
      "learning_rate": 0.0005508912911709367,
      "loss": 0.4085,
      "step": 10930
    },
    {
      "epoch": 7.586685159500694,
      "grad_norm": 3.9273111820220947,
      "learning_rate": 0.0005507208948544595,
      "loss": 0.4239,
      "step": 10940
    },
    {
      "epoch": 7.593619972260749,
      "grad_norm": 2.452086925506592,
      "learning_rate": 0.0005505504985379823,
      "loss": 0.3869,
      "step": 10950
    },
    {
      "epoch": 7.600554785020805,
      "grad_norm": 1.4268109798431396,
      "learning_rate": 0.000550380102221505,
      "loss": 0.3293,
      "step": 10960
    },
    {
      "epoch": 7.60748959778086,
      "grad_norm": 1.42532217502594,
      "learning_rate": 0.0005502097059050279,
      "loss": 0.468,
      "step": 10970
    },
    {
      "epoch": 7.614424410540916,
      "grad_norm": 1.4411460161209106,
      "learning_rate": 0.0005500393095885505,
      "loss": 0.4088,
      "step": 10980
    },
    {
      "epoch": 7.621359223300971,
      "grad_norm": 1.5911134481430054,
      "learning_rate": 0.0005498689132720732,
      "loss": 0.441,
      "step": 10990
    },
    {
      "epoch": 7.6282940360610265,
      "grad_norm": 1.4159607887268066,
      "learning_rate": 0.0005496985169555961,
      "loss": 0.4001,
      "step": 11000
    },
    {
      "epoch": 7.6352288488210815,
      "grad_norm": 3.3275997638702393,
      "learning_rate": 0.0005495281206391188,
      "loss": 0.4243,
      "step": 11010
    },
    {
      "epoch": 7.6421636615811375,
      "grad_norm": 2.770416021347046,
      "learning_rate": 0.0005493577243226416,
      "loss": 0.327,
      "step": 11020
    },
    {
      "epoch": 7.6490984743411925,
      "grad_norm": 2.463960647583008,
      "learning_rate": 0.0005491873280061644,
      "loss": 0.3932,
      "step": 11030
    },
    {
      "epoch": 7.656033287101248,
      "grad_norm": 2.1186225414276123,
      "learning_rate": 0.000549016931689687,
      "loss": 0.3461,
      "step": 11040
    },
    {
      "epoch": 7.6629680998613035,
      "grad_norm": 2.641502857208252,
      "learning_rate": 0.0005488465353732098,
      "loss": 0.3939,
      "step": 11050
    },
    {
      "epoch": 7.669902912621359,
      "grad_norm": 1.6860970258712769,
      "learning_rate": 0.0005486761390567326,
      "loss": 0.5007,
      "step": 11060
    },
    {
      "epoch": 7.676837725381414,
      "grad_norm": 1.8505449295043945,
      "learning_rate": 0.0005485057427402553,
      "loss": 0.4122,
      "step": 11070
    },
    {
      "epoch": 7.68377253814147,
      "grad_norm": 2.2380573749542236,
      "learning_rate": 0.0005483353464237781,
      "loss": 0.4278,
      "step": 11080
    },
    {
      "epoch": 7.690707350901525,
      "grad_norm": 3.35992693901062,
      "learning_rate": 0.0005481649501073009,
      "loss": 0.4926,
      "step": 11090
    },
    {
      "epoch": 7.697642163661581,
      "grad_norm": 2.0350029468536377,
      "learning_rate": 0.0005479945537908236,
      "loss": 0.4034,
      "step": 11100
    },
    {
      "epoch": 7.704576976421636,
      "grad_norm": 1.7679182291030884,
      "learning_rate": 0.0005478241574743463,
      "loss": 0.3814,
      "step": 11110
    },
    {
      "epoch": 7.711511789181692,
      "grad_norm": 2.0820140838623047,
      "learning_rate": 0.0005476537611578691,
      "loss": 0.4529,
      "step": 11120
    },
    {
      "epoch": 7.718446601941747,
      "grad_norm": 1.5510255098342896,
      "learning_rate": 0.0005474833648413919,
      "loss": 0.4045,
      "step": 11130
    },
    {
      "epoch": 7.725381414701803,
      "grad_norm": 1.4964678287506104,
      "learning_rate": 0.0005473129685249146,
      "loss": 0.3726,
      "step": 11140
    },
    {
      "epoch": 7.732316227461858,
      "grad_norm": 2.0279598236083984,
      "learning_rate": 0.0005471425722084373,
      "loss": 0.4111,
      "step": 11150
    },
    {
      "epoch": 7.739251040221914,
      "grad_norm": 2.934704065322876,
      "learning_rate": 0.0005469721758919601,
      "loss": 0.3405,
      "step": 11160
    },
    {
      "epoch": 7.746185852981969,
      "grad_norm": 1.9990520477294922,
      "learning_rate": 0.0005468017795754828,
      "loss": 0.2909,
      "step": 11170
    },
    {
      "epoch": 7.753120665742025,
      "grad_norm": 1.2161147594451904,
      "learning_rate": 0.0005466313832590056,
      "loss": 0.3785,
      "step": 11180
    },
    {
      "epoch": 7.76005547850208,
      "grad_norm": 2.322727918624878,
      "learning_rate": 0.0005464609869425284,
      "loss": 0.3923,
      "step": 11190
    },
    {
      "epoch": 7.766990291262136,
      "grad_norm": 1.6544499397277832,
      "learning_rate": 0.000546290590626051,
      "loss": 0.4117,
      "step": 11200
    },
    {
      "epoch": 7.773925104022191,
      "grad_norm": 1.1662157773971558,
      "learning_rate": 0.000546120194309574,
      "loss": 0.3345,
      "step": 11210
    },
    {
      "epoch": 7.780859916782247,
      "grad_norm": 2.4316678047180176,
      "learning_rate": 0.0005459497979930966,
      "loss": 0.3349,
      "step": 11220
    },
    {
      "epoch": 7.787794729542302,
      "grad_norm": 4.591095447540283,
      "learning_rate": 0.0005457794016766194,
      "loss": 0.4591,
      "step": 11230
    },
    {
      "epoch": 7.794729542302358,
      "grad_norm": 4.769387245178223,
      "learning_rate": 0.0005456090053601422,
      "loss": 0.5197,
      "step": 11240
    },
    {
      "epoch": 7.801664355062413,
      "grad_norm": 1.3960453271865845,
      "learning_rate": 0.0005454386090436649,
      "loss": 0.3763,
      "step": 11250
    },
    {
      "epoch": 7.808599167822469,
      "grad_norm": 1.6974098682403564,
      "learning_rate": 0.0005452682127271877,
      "loss": 0.481,
      "step": 11260
    },
    {
      "epoch": 7.815533980582524,
      "grad_norm": 2.7978925704956055,
      "learning_rate": 0.0005450978164107104,
      "loss": 0.4324,
      "step": 11270
    },
    {
      "epoch": 7.82246879334258,
      "grad_norm": 1.414576768875122,
      "learning_rate": 0.0005449274200942331,
      "loss": 0.3649,
      "step": 11280
    },
    {
      "epoch": 7.829403606102635,
      "grad_norm": 1.7298016548156738,
      "learning_rate": 0.0005447570237777559,
      "loss": 0.2909,
      "step": 11290
    },
    {
      "epoch": 7.836338418862691,
      "grad_norm": 3.104755401611328,
      "learning_rate": 0.0005445866274612787,
      "loss": 0.4344,
      "step": 11300
    },
    {
      "epoch": 7.843273231622746,
      "grad_norm": 1.268936038017273,
      "learning_rate": 0.0005444162311448014,
      "loss": 0.3548,
      "step": 11310
    },
    {
      "epoch": 7.850208044382802,
      "grad_norm": 2.374995708465576,
      "learning_rate": 0.0005442458348283242,
      "loss": 0.332,
      "step": 11320
    },
    {
      "epoch": 7.857142857142857,
      "grad_norm": 2.43530011177063,
      "learning_rate": 0.0005440754385118469,
      "loss": 0.349,
      "step": 11330
    },
    {
      "epoch": 7.864077669902913,
      "grad_norm": 4.398040771484375,
      "learning_rate": 0.0005439050421953697,
      "loss": 0.4558,
      "step": 11340
    },
    {
      "epoch": 7.871012482662968,
      "grad_norm": 4.604262828826904,
      "learning_rate": 0.0005437346458788924,
      "loss": 0.3661,
      "step": 11350
    },
    {
      "epoch": 7.877947295423024,
      "grad_norm": 1.2674894332885742,
      "learning_rate": 0.0005435642495624152,
      "loss": 0.3636,
      "step": 11360
    },
    {
      "epoch": 7.8848821081830796,
      "grad_norm": 2.7897539138793945,
      "learning_rate": 0.000543393853245938,
      "loss": 0.3554,
      "step": 11370
    },
    {
      "epoch": 7.891816920943135,
      "grad_norm": 2.949486017227173,
      "learning_rate": 0.0005432234569294606,
      "loss": 0.3655,
      "step": 11380
    },
    {
      "epoch": 7.89875173370319,
      "grad_norm": 9.83043384552002,
      "learning_rate": 0.0005430530606129834,
      "loss": 0.3768,
      "step": 11390
    },
    {
      "epoch": 7.9056865464632455,
      "grad_norm": 2.460426092147827,
      "learning_rate": 0.0005428826642965062,
      "loss": 0.4068,
      "step": 11400
    },
    {
      "epoch": 7.9126213592233015,
      "grad_norm": 2.23079252243042,
      "learning_rate": 0.0005427122679800289,
      "loss": 0.3274,
      "step": 11410
    },
    {
      "epoch": 7.9195561719833565,
      "grad_norm": 1.0822539329528809,
      "learning_rate": 0.0005425418716635518,
      "loss": 0.4092,
      "step": 11420
    },
    {
      "epoch": 7.9264909847434115,
      "grad_norm": 3.6907570362091064,
      "learning_rate": 0.0005423714753470745,
      "loss": 0.3621,
      "step": 11430
    },
    {
      "epoch": 7.933425797503467,
      "grad_norm": 1.2645816802978516,
      "learning_rate": 0.0005422010790305971,
      "loss": 0.4811,
      "step": 11440
    },
    {
      "epoch": 7.940360610263523,
      "grad_norm": 0.8443061113357544,
      "learning_rate": 0.00054203068271412,
      "loss": 0.3525,
      "step": 11450
    },
    {
      "epoch": 7.947295423023578,
      "grad_norm": 1.3708909749984741,
      "learning_rate": 0.0005418602863976427,
      "loss": 0.447,
      "step": 11460
    },
    {
      "epoch": 7.954230235783633,
      "grad_norm": 5.1739091873168945,
      "learning_rate": 0.0005416898900811655,
      "loss": 0.3278,
      "step": 11470
    },
    {
      "epoch": 7.961165048543689,
      "grad_norm": 7.897181987762451,
      "learning_rate": 0.0005415194937646883,
      "loss": 0.3604,
      "step": 11480
    },
    {
      "epoch": 7.968099861303745,
      "grad_norm": 3.0078885555267334,
      "learning_rate": 0.000541349097448211,
      "loss": 0.3821,
      "step": 11490
    },
    {
      "epoch": 7.9750346740638,
      "grad_norm": 2.524364471435547,
      "learning_rate": 0.0005411787011317337,
      "loss": 0.3904,
      "step": 11500
    },
    {
      "epoch": 7.981969486823855,
      "grad_norm": 2.1378135681152344,
      "learning_rate": 0.0005410083048152565,
      "loss": 0.4242,
      "step": 11510
    },
    {
      "epoch": 7.988904299583911,
      "grad_norm": 1.764784336090088,
      "learning_rate": 0.0005408379084987792,
      "loss": 0.3874,
      "step": 11520
    },
    {
      "epoch": 7.995839112343967,
      "grad_norm": 1.3505703210830688,
      "learning_rate": 0.000540667512182302,
      "loss": 0.3182,
      "step": 11530
    },
    {
      "epoch": 8.0,
      "eval_accuracy": 0.8255289628858827,
      "eval_auc": 0.8255283403323127,
      "eval_f1": 0.8251651025373653,
      "eval_loss": 0.41798585653305054,
      "eval_mcc": 0.6510613805311062,
      "eval_precision": 0.8266016713091922,
      "eval_recall": 0.823733518390007,
      "eval_runtime": 78.9309,
      "eval_samples_per_second": 36.526,
      "eval_steps_per_second": 2.293,
      "step": 11536
    },
    {
      "epoch": 8.002773925104023,
      "grad_norm": 2.108480930328369,
      "learning_rate": 0.0005404971158658248,
      "loss": 0.4371,
      "step": 11540
    },
    {
      "epoch": 8.009708737864077,
      "grad_norm": 1.8993586301803589,
      "learning_rate": 0.0005403267195493476,
      "loss": 0.3748,
      "step": 11550
    },
    {
      "epoch": 8.016643550624133,
      "grad_norm": 2.1350948810577393,
      "learning_rate": 0.0005401563232328702,
      "loss": 0.3309,
      "step": 11560
    },
    {
      "epoch": 8.023578363384189,
      "grad_norm": 3.8948159217834473,
      "learning_rate": 0.000539985926916393,
      "loss": 0.3361,
      "step": 11570
    },
    {
      "epoch": 8.030513176144245,
      "grad_norm": 2.435030460357666,
      "learning_rate": 0.0005398155305999158,
      "loss": 0.3688,
      "step": 11580
    },
    {
      "epoch": 8.0374479889043,
      "grad_norm": 1.5759025812149048,
      "learning_rate": 0.0005396451342834385,
      "loss": 0.3866,
      "step": 11590
    },
    {
      "epoch": 8.044382801664355,
      "grad_norm": 4.275597095489502,
      "learning_rate": 0.0005394747379669613,
      "loss": 0.3512,
      "step": 11600
    },
    {
      "epoch": 8.051317614424411,
      "grad_norm": 2.2931628227233887,
      "learning_rate": 0.000539304341650484,
      "loss": 0.3345,
      "step": 11610
    },
    {
      "epoch": 8.058252427184467,
      "grad_norm": 4.854591369628906,
      "learning_rate": 0.0005391339453340067,
      "loss": 0.3515,
      "step": 11620
    },
    {
      "epoch": 8.065187239944521,
      "grad_norm": 2.1686065196990967,
      "learning_rate": 0.0005389635490175296,
      "loss": 0.2718,
      "step": 11630
    },
    {
      "epoch": 8.072122052704577,
      "grad_norm": 4.7320027351379395,
      "learning_rate": 0.0005387931527010523,
      "loss": 0.3907,
      "step": 11640
    },
    {
      "epoch": 8.079056865464633,
      "grad_norm": 2.3305482864379883,
      "learning_rate": 0.000538622756384575,
      "loss": 0.3594,
      "step": 11650
    },
    {
      "epoch": 8.085991678224689,
      "grad_norm": 2.6055479049682617,
      "learning_rate": 0.0005384523600680979,
      "loss": 0.3439,
      "step": 11660
    },
    {
      "epoch": 8.092926490984743,
      "grad_norm": 3.2118852138519287,
      "learning_rate": 0.0005382819637516205,
      "loss": 0.3165,
      "step": 11670
    },
    {
      "epoch": 8.099861303744799,
      "grad_norm": 1.5255264043807983,
      "learning_rate": 0.0005381115674351433,
      "loss": 0.4187,
      "step": 11680
    },
    {
      "epoch": 8.106796116504855,
      "grad_norm": 3.096341371536255,
      "learning_rate": 0.0005379411711186661,
      "loss": 0.4097,
      "step": 11690
    },
    {
      "epoch": 8.11373092926491,
      "grad_norm": 4.164670944213867,
      "learning_rate": 0.0005377707748021888,
      "loss": 0.3833,
      "step": 11700
    },
    {
      "epoch": 8.120665742024965,
      "grad_norm": 1.9024736881256104,
      "learning_rate": 0.0005376003784857116,
      "loss": 0.3153,
      "step": 11710
    },
    {
      "epoch": 8.12760055478502,
      "grad_norm": 6.1786322593688965,
      "learning_rate": 0.0005374299821692344,
      "loss": 0.3573,
      "step": 11720
    },
    {
      "epoch": 8.134535367545077,
      "grad_norm": 1.7566484212875366,
      "learning_rate": 0.000537259585852757,
      "loss": 0.3921,
      "step": 11730
    },
    {
      "epoch": 8.141470180305133,
      "grad_norm": 3.804603099822998,
      "learning_rate": 0.0005370891895362798,
      "loss": 0.4141,
      "step": 11740
    },
    {
      "epoch": 8.148404993065187,
      "grad_norm": 1.447417140007019,
      "learning_rate": 0.0005369187932198026,
      "loss": 0.4647,
      "step": 11750
    },
    {
      "epoch": 8.155339805825243,
      "grad_norm": 3.2603254318237305,
      "learning_rate": 0.0005367483969033254,
      "loss": 0.4247,
      "step": 11760
    },
    {
      "epoch": 8.162274618585299,
      "grad_norm": 1.5253962278366089,
      "learning_rate": 0.0005365780005868481,
      "loss": 0.3005,
      "step": 11770
    },
    {
      "epoch": 8.169209431345354,
      "grad_norm": 1.0474590063095093,
      "learning_rate": 0.0005364076042703709,
      "loss": 0.2668,
      "step": 11780
    },
    {
      "epoch": 8.176144244105409,
      "grad_norm": 2.275907039642334,
      "learning_rate": 0.0005362372079538936,
      "loss": 0.3853,
      "step": 11790
    },
    {
      "epoch": 8.183079056865465,
      "grad_norm": 1.2860034704208374,
      "learning_rate": 0.0005360668116374163,
      "loss": 0.4008,
      "step": 11800
    },
    {
      "epoch": 8.19001386962552,
      "grad_norm": 6.636435508728027,
      "learning_rate": 0.0005358964153209391,
      "loss": 0.4011,
      "step": 11810
    },
    {
      "epoch": 8.196948682385576,
      "grad_norm": 1.5490158796310425,
      "learning_rate": 0.0005357260190044619,
      "loss": 0.3487,
      "step": 11820
    },
    {
      "epoch": 8.20388349514563,
      "grad_norm": 3.2012670040130615,
      "learning_rate": 0.0005355556226879846,
      "loss": 0.315,
      "step": 11830
    },
    {
      "epoch": 8.210818307905686,
      "grad_norm": 2.755129814147949,
      "learning_rate": 0.0005353852263715074,
      "loss": 0.342,
      "step": 11840
    },
    {
      "epoch": 8.217753120665742,
      "grad_norm": 0.967983067035675,
      "learning_rate": 0.0005352148300550301,
      "loss": 0.2751,
      "step": 11850
    },
    {
      "epoch": 8.224687933425798,
      "grad_norm": 2.096721649169922,
      "learning_rate": 0.0005350444337385528,
      "loss": 0.4383,
      "step": 11860
    },
    {
      "epoch": 8.231622746185852,
      "grad_norm": 2.5520379543304443,
      "learning_rate": 0.0005348740374220757,
      "loss": 0.4007,
      "step": 11870
    },
    {
      "epoch": 8.238557558945908,
      "grad_norm": 2.790283679962158,
      "learning_rate": 0.0005347036411055984,
      "loss": 0.3394,
      "step": 11880
    },
    {
      "epoch": 8.245492371705964,
      "grad_norm": 3.8303775787353516,
      "learning_rate": 0.0005345332447891212,
      "loss": 0.3413,
      "step": 11890
    },
    {
      "epoch": 8.25242718446602,
      "grad_norm": 2.057633399963379,
      "learning_rate": 0.000534362848472644,
      "loss": 0.3408,
      "step": 11900
    },
    {
      "epoch": 8.259361997226074,
      "grad_norm": 1.3953125476837158,
      "learning_rate": 0.0005341924521561666,
      "loss": 0.3818,
      "step": 11910
    },
    {
      "epoch": 8.26629680998613,
      "grad_norm": 1.5685069561004639,
      "learning_rate": 0.0005340220558396894,
      "loss": 0.3032,
      "step": 11920
    },
    {
      "epoch": 8.273231622746186,
      "grad_norm": 3.1725659370422363,
      "learning_rate": 0.0005338516595232122,
      "loss": 0.3309,
      "step": 11930
    },
    {
      "epoch": 8.280166435506242,
      "grad_norm": 1.8551100492477417,
      "learning_rate": 0.0005336812632067349,
      "loss": 0.4078,
      "step": 11940
    },
    {
      "epoch": 8.287101248266296,
      "grad_norm": 4.163368225097656,
      "learning_rate": 0.0005335108668902577,
      "loss": 0.4219,
      "step": 11950
    },
    {
      "epoch": 8.294036061026352,
      "grad_norm": 1.750062108039856,
      "learning_rate": 0.0005333404705737804,
      "loss": 0.3185,
      "step": 11960
    },
    {
      "epoch": 8.300970873786408,
      "grad_norm": 1.3351283073425293,
      "learning_rate": 0.0005331700742573031,
      "loss": 0.4358,
      "step": 11970
    },
    {
      "epoch": 8.307905686546464,
      "grad_norm": 2.4073002338409424,
      "learning_rate": 0.0005329996779408259,
      "loss": 0.4257,
      "step": 11980
    },
    {
      "epoch": 8.314840499306518,
      "grad_norm": 2.171841621398926,
      "learning_rate": 0.0005328292816243487,
      "loss": 0.3792,
      "step": 11990
    },
    {
      "epoch": 8.321775312066574,
      "grad_norm": 1.501577377319336,
      "learning_rate": 0.0005326588853078715,
      "loss": 0.4002,
      "step": 12000
    },
    {
      "epoch": 8.32871012482663,
      "grad_norm": 1.507516860961914,
      "learning_rate": 0.0005324884889913942,
      "loss": 0.3446,
      "step": 12010
    },
    {
      "epoch": 8.335644937586686,
      "grad_norm": 2.505694627761841,
      "learning_rate": 0.0005323180926749169,
      "loss": 0.3682,
      "step": 12020
    },
    {
      "epoch": 8.34257975034674,
      "grad_norm": 1.749446153640747,
      "learning_rate": 0.0005321476963584397,
      "loss": 0.2921,
      "step": 12030
    },
    {
      "epoch": 8.349514563106796,
      "grad_norm": 4.834962844848633,
      "learning_rate": 0.0005319773000419624,
      "loss": 0.5405,
      "step": 12040
    },
    {
      "epoch": 8.356449375866852,
      "grad_norm": 2.2613627910614014,
      "learning_rate": 0.0005318069037254852,
      "loss": 0.4661,
      "step": 12050
    },
    {
      "epoch": 8.363384188626908,
      "grad_norm": 1.544443964958191,
      "learning_rate": 0.000531636507409008,
      "loss": 0.3924,
      "step": 12060
    },
    {
      "epoch": 8.370319001386962,
      "grad_norm": 1.8477858304977417,
      "learning_rate": 0.0005314661110925307,
      "loss": 0.3748,
      "step": 12070
    },
    {
      "epoch": 8.377253814147018,
      "grad_norm": 1.8055615425109863,
      "learning_rate": 0.0005312957147760535,
      "loss": 0.4089,
      "step": 12080
    },
    {
      "epoch": 8.384188626907074,
      "grad_norm": 1.3460993766784668,
      "learning_rate": 0.0005311253184595762,
      "loss": 0.4473,
      "step": 12090
    },
    {
      "epoch": 8.39112343966713,
      "grad_norm": 2.2805263996124268,
      "learning_rate": 0.0005309549221430989,
      "loss": 0.348,
      "step": 12100
    },
    {
      "epoch": 8.398058252427184,
      "grad_norm": 3.6716253757476807,
      "learning_rate": 0.0005307845258266218,
      "loss": 0.4169,
      "step": 12110
    },
    {
      "epoch": 8.40499306518724,
      "grad_norm": 4.244953155517578,
      "learning_rate": 0.0005306141295101445,
      "loss": 0.3512,
      "step": 12120
    },
    {
      "epoch": 8.411927877947296,
      "grad_norm": 1.6765763759613037,
      "learning_rate": 0.0005304437331936673,
      "loss": 0.3035,
      "step": 12130
    },
    {
      "epoch": 8.418862690707352,
      "grad_norm": 1.6606755256652832,
      "learning_rate": 0.00053027333687719,
      "loss": 0.3392,
      "step": 12140
    },
    {
      "epoch": 8.425797503467406,
      "grad_norm": 0.8481556177139282,
      "learning_rate": 0.0005301029405607127,
      "loss": 0.3717,
      "step": 12150
    },
    {
      "epoch": 8.432732316227462,
      "grad_norm": 13.016922950744629,
      "learning_rate": 0.0005299325442442355,
      "loss": 0.3161,
      "step": 12160
    },
    {
      "epoch": 8.439667128987518,
      "grad_norm": 1.5210602283477783,
      "learning_rate": 0.0005297621479277583,
      "loss": 0.3551,
      "step": 12170
    },
    {
      "epoch": 8.446601941747574,
      "grad_norm": 1.5416958332061768,
      "learning_rate": 0.000529591751611281,
      "loss": 0.3586,
      "step": 12180
    },
    {
      "epoch": 8.453536754507628,
      "grad_norm": 1.1705001592636108,
      "learning_rate": 0.0005294213552948037,
      "loss": 0.34,
      "step": 12190
    },
    {
      "epoch": 8.460471567267684,
      "grad_norm": 1.9202864170074463,
      "learning_rate": 0.0005292509589783265,
      "loss": 0.3956,
      "step": 12200
    },
    {
      "epoch": 8.46740638002774,
      "grad_norm": 1.8826721906661987,
      "learning_rate": 0.0005290805626618493,
      "loss": 0.4027,
      "step": 12210
    },
    {
      "epoch": 8.474341192787795,
      "grad_norm": 1.603375792503357,
      "learning_rate": 0.000528910166345372,
      "loss": 0.4561,
      "step": 12220
    },
    {
      "epoch": 8.48127600554785,
      "grad_norm": 2.1849493980407715,
      "learning_rate": 0.0005287397700288948,
      "loss": 0.3382,
      "step": 12230
    },
    {
      "epoch": 8.488210818307905,
      "grad_norm": 1.1327522993087769,
      "learning_rate": 0.0005285693737124176,
      "loss": 0.3727,
      "step": 12240
    },
    {
      "epoch": 8.495145631067961,
      "grad_norm": 3.3574888706207275,
      "learning_rate": 0.0005283989773959402,
      "loss": 0.5217,
      "step": 12250
    },
    {
      "epoch": 8.502080443828017,
      "grad_norm": 2.7405636310577393,
      "learning_rate": 0.000528228581079463,
      "loss": 0.459,
      "step": 12260
    },
    {
      "epoch": 8.509015256588071,
      "grad_norm": 2.144792318344116,
      "learning_rate": 0.0005280581847629858,
      "loss": 0.3504,
      "step": 12270
    },
    {
      "epoch": 8.515950069348127,
      "grad_norm": 1.171225666999817,
      "learning_rate": 0.0005278877884465085,
      "loss": 0.3971,
      "step": 12280
    },
    {
      "epoch": 8.522884882108183,
      "grad_norm": 3.3823704719543457,
      "learning_rate": 0.0005277173921300314,
      "loss": 0.3175,
      "step": 12290
    },
    {
      "epoch": 8.52981969486824,
      "grad_norm": 1.1869102716445923,
      "learning_rate": 0.0005275469958135541,
      "loss": 0.3758,
      "step": 12300
    },
    {
      "epoch": 8.536754507628293,
      "grad_norm": 4.5070576667785645,
      "learning_rate": 0.0005273765994970767,
      "loss": 0.435,
      "step": 12310
    },
    {
      "epoch": 8.54368932038835,
      "grad_norm": 1.4865487813949585,
      "learning_rate": 0.0005272062031805996,
      "loss": 0.4205,
      "step": 12320
    },
    {
      "epoch": 8.550624133148405,
      "grad_norm": 1.8524177074432373,
      "learning_rate": 0.0005270358068641223,
      "loss": 0.4375,
      "step": 12330
    },
    {
      "epoch": 8.557558945908461,
      "grad_norm": 1.6682565212249756,
      "learning_rate": 0.0005268654105476451,
      "loss": 0.3955,
      "step": 12340
    },
    {
      "epoch": 8.564493758668515,
      "grad_norm": 1.800808072090149,
      "learning_rate": 0.0005266950142311679,
      "loss": 0.3552,
      "step": 12350
    },
    {
      "epoch": 8.571428571428571,
      "grad_norm": 2.3000283241271973,
      "learning_rate": 0.0005265246179146906,
      "loss": 0.3971,
      "step": 12360
    },
    {
      "epoch": 8.578363384188627,
      "grad_norm": 1.9382869005203247,
      "learning_rate": 0.0005263542215982133,
      "loss": 0.3376,
      "step": 12370
    },
    {
      "epoch": 8.585298196948683,
      "grad_norm": 1.5602525472640991,
      "learning_rate": 0.0005261838252817361,
      "loss": 0.5078,
      "step": 12380
    },
    {
      "epoch": 8.592233009708737,
      "grad_norm": 3.1720712184906006,
      "learning_rate": 0.0005260134289652588,
      "loss": 0.3368,
      "step": 12390
    },
    {
      "epoch": 8.599167822468793,
      "grad_norm": 1.6968613862991333,
      "learning_rate": 0.0005258430326487816,
      "loss": 0.3939,
      "step": 12400
    },
    {
      "epoch": 8.606102635228849,
      "grad_norm": 2.4660604000091553,
      "learning_rate": 0.0005256726363323044,
      "loss": 0.5176,
      "step": 12410
    },
    {
      "epoch": 8.613037447988905,
      "grad_norm": 2.1340856552124023,
      "learning_rate": 0.0005255022400158272,
      "loss": 0.4171,
      "step": 12420
    },
    {
      "epoch": 8.619972260748959,
      "grad_norm": 1.6882216930389404,
      "learning_rate": 0.0005253318436993498,
      "loss": 0.4029,
      "step": 12430
    },
    {
      "epoch": 8.626907073509015,
      "grad_norm": 1.8355271816253662,
      "learning_rate": 0.0005251614473828726,
      "loss": 0.3718,
      "step": 12440
    },
    {
      "epoch": 8.633841886269071,
      "grad_norm": 2.177919387817383,
      "learning_rate": 0.0005249910510663954,
      "loss": 0.3861,
      "step": 12450
    },
    {
      "epoch": 8.640776699029127,
      "grad_norm": 1.7592121362686157,
      "learning_rate": 0.0005248206547499181,
      "loss": 0.3903,
      "step": 12460
    },
    {
      "epoch": 8.647711511789181,
      "grad_norm": 5.552389144897461,
      "learning_rate": 0.0005246502584334409,
      "loss": 0.423,
      "step": 12470
    },
    {
      "epoch": 8.654646324549237,
      "grad_norm": 4.834184169769287,
      "learning_rate": 0.0005244798621169637,
      "loss": 0.503,
      "step": 12480
    },
    {
      "epoch": 8.661581137309293,
      "grad_norm": 1.8755228519439697,
      "learning_rate": 0.0005243094658004863,
      "loss": 0.2685,
      "step": 12490
    },
    {
      "epoch": 8.668515950069349,
      "grad_norm": 2.8028955459594727,
      "learning_rate": 0.0005241390694840091,
      "loss": 0.3745,
      "step": 12500
    },
    {
      "epoch": 8.675450762829403,
      "grad_norm": 3.498154640197754,
      "learning_rate": 0.0005239686731675319,
      "loss": 0.3324,
      "step": 12510
    },
    {
      "epoch": 8.682385575589459,
      "grad_norm": 2.1253323554992676,
      "learning_rate": 0.0005237982768510546,
      "loss": 0.3047,
      "step": 12520
    },
    {
      "epoch": 8.689320388349515,
      "grad_norm": 1.8475706577301025,
      "learning_rate": 0.0005236278805345775,
      "loss": 0.4908,
      "step": 12530
    },
    {
      "epoch": 8.69625520110957,
      "grad_norm": 1.7737667560577393,
      "learning_rate": 0.0005234574842181001,
      "loss": 0.286,
      "step": 12540
    },
    {
      "epoch": 8.703190013869625,
      "grad_norm": 2.886013984680176,
      "learning_rate": 0.0005232870879016229,
      "loss": 0.3608,
      "step": 12550
    },
    {
      "epoch": 8.71012482662968,
      "grad_norm": 2.2348501682281494,
      "learning_rate": 0.0005231166915851457,
      "loss": 0.4308,
      "step": 12560
    },
    {
      "epoch": 8.717059639389737,
      "grad_norm": 1.815970778465271,
      "learning_rate": 0.0005229462952686684,
      "loss": 0.3839,
      "step": 12570
    },
    {
      "epoch": 8.723994452149793,
      "grad_norm": 2.2905168533325195,
      "learning_rate": 0.0005227758989521912,
      "loss": 0.3889,
      "step": 12580
    },
    {
      "epoch": 8.730929264909847,
      "grad_norm": 2.311406373977661,
      "learning_rate": 0.000522605502635714,
      "loss": 0.3102,
      "step": 12590
    },
    {
      "epoch": 8.737864077669903,
      "grad_norm": 2.5230178833007812,
      "learning_rate": 0.0005224351063192366,
      "loss": 0.3072,
      "step": 12600
    },
    {
      "epoch": 8.744798890429959,
      "grad_norm": 2.122338056564331,
      "learning_rate": 0.0005222647100027594,
      "loss": 0.4453,
      "step": 12610
    },
    {
      "epoch": 8.751733703190014,
      "grad_norm": 2.5149013996124268,
      "learning_rate": 0.0005220943136862821,
      "loss": 0.4029,
      "step": 12620
    },
    {
      "epoch": 8.758668515950069,
      "grad_norm": 1.4328670501708984,
      "learning_rate": 0.0005219239173698049,
      "loss": 0.3148,
      "step": 12630
    },
    {
      "epoch": 8.765603328710124,
      "grad_norm": 2.61484432220459,
      "learning_rate": 0.0005217535210533277,
      "loss": 0.4894,
      "step": 12640
    },
    {
      "epoch": 8.77253814147018,
      "grad_norm": 1.3017611503601074,
      "learning_rate": 0.0005215831247368503,
      "loss": 0.3402,
      "step": 12650
    },
    {
      "epoch": 8.779472954230236,
      "grad_norm": 2.217266321182251,
      "learning_rate": 0.0005214127284203732,
      "loss": 0.3107,
      "step": 12660
    },
    {
      "epoch": 8.78640776699029,
      "grad_norm": 4.334647178649902,
      "learning_rate": 0.0005212423321038959,
      "loss": 0.4459,
      "step": 12670
    },
    {
      "epoch": 8.793342579750346,
      "grad_norm": 2.2835073471069336,
      "learning_rate": 0.0005210719357874187,
      "loss": 0.43,
      "step": 12680
    },
    {
      "epoch": 8.800277392510402,
      "grad_norm": 1.8427221775054932,
      "learning_rate": 0.0005209015394709415,
      "loss": 0.3489,
      "step": 12690
    },
    {
      "epoch": 8.807212205270458,
      "grad_norm": 1.5308314561843872,
      "learning_rate": 0.0005207311431544642,
      "loss": 0.4304,
      "step": 12700
    },
    {
      "epoch": 8.814147018030512,
      "grad_norm": 3.8636629581451416,
      "learning_rate": 0.000520560746837987,
      "loss": 0.2806,
      "step": 12710
    },
    {
      "epoch": 8.821081830790568,
      "grad_norm": 2.7729082107543945,
      "learning_rate": 0.0005203903505215097,
      "loss": 0.4946,
      "step": 12720
    },
    {
      "epoch": 8.828016643550624,
      "grad_norm": 2.3525993824005127,
      "learning_rate": 0.0005202199542050324,
      "loss": 0.3456,
      "step": 12730
    },
    {
      "epoch": 8.83495145631068,
      "grad_norm": 1.8227635622024536,
      "learning_rate": 0.0005200495578885552,
      "loss": 0.4262,
      "step": 12740
    },
    {
      "epoch": 8.841886269070734,
      "grad_norm": 3.113494873046875,
      "learning_rate": 0.000519879161572078,
      "loss": 0.4418,
      "step": 12750
    },
    {
      "epoch": 8.84882108183079,
      "grad_norm": 1.4953590631484985,
      "learning_rate": 0.0005197087652556007,
      "loss": 0.3243,
      "step": 12760
    },
    {
      "epoch": 8.855755894590846,
      "grad_norm": 2.4754068851470947,
      "learning_rate": 0.0005195383689391234,
      "loss": 0.3882,
      "step": 12770
    },
    {
      "epoch": 8.862690707350902,
      "grad_norm": 1.151562213897705,
      "learning_rate": 0.0005193679726226462,
      "loss": 0.325,
      "step": 12780
    },
    {
      "epoch": 8.869625520110956,
      "grad_norm": 2.393202543258667,
      "learning_rate": 0.000519197576306169,
      "loss": 0.3738,
      "step": 12790
    },
    {
      "epoch": 8.876560332871012,
      "grad_norm": 2.150709629058838,
      "learning_rate": 0.0005190271799896917,
      "loss": 0.3933,
      "step": 12800
    },
    {
      "epoch": 8.883495145631068,
      "grad_norm": 1.2722628116607666,
      "learning_rate": 0.0005188567836732145,
      "loss": 0.2761,
      "step": 12810
    },
    {
      "epoch": 8.890429958391124,
      "grad_norm": 2.2726564407348633,
      "learning_rate": 0.0005186863873567373,
      "loss": 0.3437,
      "step": 12820
    },
    {
      "epoch": 8.897364771151178,
      "grad_norm": 4.425238132476807,
      "learning_rate": 0.0005185159910402599,
      "loss": 0.5078,
      "step": 12830
    },
    {
      "epoch": 8.904299583911234,
      "grad_norm": 1.4052609205245972,
      "learning_rate": 0.0005183455947237827,
      "loss": 0.4315,
      "step": 12840
    },
    {
      "epoch": 8.91123439667129,
      "grad_norm": 1.5511153936386108,
      "learning_rate": 0.0005181751984073055,
      "loss": 0.3801,
      "step": 12850
    },
    {
      "epoch": 8.918169209431346,
      "grad_norm": 5.0106425285339355,
      "learning_rate": 0.0005180048020908282,
      "loss": 0.3728,
      "step": 12860
    },
    {
      "epoch": 8.925104022191402,
      "grad_norm": 2.356534719467163,
      "learning_rate": 0.0005178344057743511,
      "loss": 0.4821,
      "step": 12870
    },
    {
      "epoch": 8.932038834951456,
      "grad_norm": 1.4585506916046143,
      "learning_rate": 0.0005176640094578738,
      "loss": 0.3975,
      "step": 12880
    },
    {
      "epoch": 8.938973647711512,
      "grad_norm": 1.2256726026535034,
      "learning_rate": 0.0005174936131413964,
      "loss": 0.3106,
      "step": 12890
    },
    {
      "epoch": 8.945908460471568,
      "grad_norm": 1.6233893632888794,
      "learning_rate": 0.0005173232168249193,
      "loss": 0.3393,
      "step": 12900
    },
    {
      "epoch": 8.952843273231622,
      "grad_norm": 1.9601269960403442,
      "learning_rate": 0.000517152820508442,
      "loss": 0.3833,
      "step": 12910
    },
    {
      "epoch": 8.959778085991678,
      "grad_norm": 1.2726930379867554,
      "learning_rate": 0.0005169824241919648,
      "loss": 0.3997,
      "step": 12920
    },
    {
      "epoch": 8.966712898751734,
      "grad_norm": 2.3663580417633057,
      "learning_rate": 0.0005168120278754876,
      "loss": 0.3831,
      "step": 12930
    },
    {
      "epoch": 8.97364771151179,
      "grad_norm": 1.9956424236297607,
      "learning_rate": 0.0005166416315590103,
      "loss": 0.398,
      "step": 12940
    },
    {
      "epoch": 8.980582524271846,
      "grad_norm": 1.7555065155029297,
      "learning_rate": 0.000516471235242533,
      "loss": 0.2778,
      "step": 12950
    },
    {
      "epoch": 8.9875173370319,
      "grad_norm": 2.853212833404541,
      "learning_rate": 0.0005163008389260558,
      "loss": 0.3382,
      "step": 12960
    },
    {
      "epoch": 8.994452149791956,
      "grad_norm": 2.294429063796997,
      "learning_rate": 0.0005161304426095785,
      "loss": 0.4662,
      "step": 12970
    },
    {
      "epoch": 9.0,
      "eval_accuracy": 0.8352410683315991,
      "eval_auc": 0.8352286563210747,
      "eval_f1": 0.8290752069089602,
      "eval_loss": 0.391831636428833,
      "eval_mcc": 0.6721921772191987,
      "eval_precision": 0.8609865470852018,
      "eval_recall": 0.7994448299791811,
      "eval_runtime": 79.1117,
      "eval_samples_per_second": 36.442,
      "eval_steps_per_second": 2.288,
      "step": 12978
    },
    {
      "epoch": 9.001386962552012,
      "grad_norm": 1.1461753845214844,
      "learning_rate": 0.0005159600462931013,
      "loss": 0.4746,
      "step": 12980
    },
    {
      "epoch": 9.008321775312067,
      "grad_norm": 3.336315631866455,
      "learning_rate": 0.0005157896499766241,
      "loss": 0.3662,
      "step": 12990
    },
    {
      "epoch": 9.015256588072122,
      "grad_norm": 1.2574615478515625,
      "learning_rate": 0.0005156192536601469,
      "loss": 0.3036,
      "step": 13000
    },
    {
      "epoch": 9.022191400832178,
      "grad_norm": 1.1690455675125122,
      "learning_rate": 0.0005154488573436695,
      "loss": 0.4505,
      "step": 13010
    },
    {
      "epoch": 9.029126213592233,
      "grad_norm": 3.923189163208008,
      "learning_rate": 0.0005152784610271923,
      "loss": 0.3335,
      "step": 13020
    },
    {
      "epoch": 9.03606102635229,
      "grad_norm": 1.8026610612869263,
      "learning_rate": 0.0005151080647107151,
      "loss": 0.3395,
      "step": 13030
    },
    {
      "epoch": 9.042995839112344,
      "grad_norm": 2.2744598388671875,
      "learning_rate": 0.0005149376683942378,
      "loss": 0.3556,
      "step": 13040
    },
    {
      "epoch": 9.0499306518724,
      "grad_norm": 1.7644340991973877,
      "learning_rate": 0.0005147672720777606,
      "loss": 0.3135,
      "step": 13050
    },
    {
      "epoch": 9.056865464632455,
      "grad_norm": 2.2738687992095947,
      "learning_rate": 0.0005145968757612833,
      "loss": 0.3188,
      "step": 13060
    },
    {
      "epoch": 9.063800277392511,
      "grad_norm": 2.1040778160095215,
      "learning_rate": 0.000514426479444806,
      "loss": 0.383,
      "step": 13070
    },
    {
      "epoch": 9.070735090152565,
      "grad_norm": 3.4436404705047607,
      "learning_rate": 0.0005142560831283289,
      "loss": 0.3593,
      "step": 13080
    },
    {
      "epoch": 9.077669902912621,
      "grad_norm": 5.23781156539917,
      "learning_rate": 0.0005140856868118516,
      "loss": 0.3861,
      "step": 13090
    },
    {
      "epoch": 9.084604715672677,
      "grad_norm": 11.83591079711914,
      "learning_rate": 0.0005139152904953743,
      "loss": 0.3335,
      "step": 13100
    },
    {
      "epoch": 9.091539528432733,
      "grad_norm": 2.1867852210998535,
      "learning_rate": 0.0005137448941788972,
      "loss": 0.3239,
      "step": 13110
    },
    {
      "epoch": 9.098474341192787,
      "grad_norm": 2.021683931350708,
      "learning_rate": 0.0005135744978624198,
      "loss": 0.4279,
      "step": 13120
    },
    {
      "epoch": 9.105409153952843,
      "grad_norm": 1.7682936191558838,
      "learning_rate": 0.0005134041015459426,
      "loss": 0.2696,
      "step": 13130
    },
    {
      "epoch": 9.1123439667129,
      "grad_norm": 1.1352136135101318,
      "learning_rate": 0.0005132337052294654,
      "loss": 0.4884,
      "step": 13140
    },
    {
      "epoch": 9.119278779472955,
      "grad_norm": 3.149479627609253,
      "learning_rate": 0.0005130633089129881,
      "loss": 0.3187,
      "step": 13150
    },
    {
      "epoch": 9.12621359223301,
      "grad_norm": 2.8330769538879395,
      "learning_rate": 0.0005128929125965109,
      "loss": 0.347,
      "step": 13160
    },
    {
      "epoch": 9.133148404993065,
      "grad_norm": 6.928891658782959,
      "learning_rate": 0.0005127225162800337,
      "loss": 0.331,
      "step": 13170
    },
    {
      "epoch": 9.140083217753121,
      "grad_norm": 2.1457667350769043,
      "learning_rate": 0.0005125521199635563,
      "loss": 0.3844,
      "step": 13180
    },
    {
      "epoch": 9.147018030513177,
      "grad_norm": 2.558335542678833,
      "learning_rate": 0.0005123817236470791,
      "loss": 0.3388,
      "step": 13190
    },
    {
      "epoch": 9.153952843273231,
      "grad_norm": 1.6902250051498413,
      "learning_rate": 0.0005122113273306019,
      "loss": 0.321,
      "step": 13200
    },
    {
      "epoch": 9.160887656033287,
      "grad_norm": 2.29986834526062,
      "learning_rate": 0.0005120409310141247,
      "loss": 0.3358,
      "step": 13210
    },
    {
      "epoch": 9.167822468793343,
      "grad_norm": 3.736276865005493,
      "learning_rate": 0.0005118705346976474,
      "loss": 0.3998,
      "step": 13220
    },
    {
      "epoch": 9.174757281553399,
      "grad_norm": 2.3061156272888184,
      "learning_rate": 0.0005117001383811702,
      "loss": 0.4348,
      "step": 13230
    },
    {
      "epoch": 9.181692094313453,
      "grad_norm": 2.47560453414917,
      "learning_rate": 0.0005115297420646929,
      "loss": 0.3582,
      "step": 13240
    },
    {
      "epoch": 9.188626907073509,
      "grad_norm": 1.7711824178695679,
      "learning_rate": 0.0005113593457482156,
      "loss": 0.3267,
      "step": 13250
    },
    {
      "epoch": 9.195561719833565,
      "grad_norm": 3.938457727432251,
      "learning_rate": 0.0005111889494317384,
      "loss": 0.2697,
      "step": 13260
    },
    {
      "epoch": 9.20249653259362,
      "grad_norm": 2.2135112285614014,
      "learning_rate": 0.0005110185531152612,
      "loss": 0.399,
      "step": 13270
    },
    {
      "epoch": 9.209431345353675,
      "grad_norm": 3.3043456077575684,
      "learning_rate": 0.0005108481567987839,
      "loss": 0.3,
      "step": 13280
    },
    {
      "epoch": 9.21636615811373,
      "grad_norm": 2.1745541095733643,
      "learning_rate": 0.0005106777604823066,
      "loss": 0.3776,
      "step": 13290
    },
    {
      "epoch": 9.223300970873787,
      "grad_norm": 1.283407211303711,
      "learning_rate": 0.0005105073641658294,
      "loss": 0.2854,
      "step": 13300
    },
    {
      "epoch": 9.230235783633843,
      "grad_norm": 4.194600582122803,
      "learning_rate": 0.0005103369678493521,
      "loss": 0.4079,
      "step": 13310
    },
    {
      "epoch": 9.237170596393897,
      "grad_norm": 1.8632076978683472,
      "learning_rate": 0.000510166571532875,
      "loss": 0.2858,
      "step": 13320
    },
    {
      "epoch": 9.244105409153953,
      "grad_norm": 1.6316484212875366,
      "learning_rate": 0.0005099961752163977,
      "loss": 0.397,
      "step": 13330
    },
    {
      "epoch": 9.251040221914009,
      "grad_norm": 2.1217002868652344,
      "learning_rate": 0.0005098257788999205,
      "loss": 0.4209,
      "step": 13340
    },
    {
      "epoch": 9.257975034674065,
      "grad_norm": 3.6780004501342773,
      "learning_rate": 0.0005096553825834432,
      "loss": 0.3224,
      "step": 13350
    },
    {
      "epoch": 9.264909847434119,
      "grad_norm": 3.0344951152801514,
      "learning_rate": 0.0005094849862669659,
      "loss": 0.3783,
      "step": 13360
    },
    {
      "epoch": 9.271844660194175,
      "grad_norm": 2.0209836959838867,
      "learning_rate": 0.0005093145899504887,
      "loss": 0.3372,
      "step": 13370
    },
    {
      "epoch": 9.27877947295423,
      "grad_norm": 1.9657328128814697,
      "learning_rate": 0.0005091441936340115,
      "loss": 0.3468,
      "step": 13380
    },
    {
      "epoch": 9.285714285714286,
      "grad_norm": 2.423318386077881,
      "learning_rate": 0.0005089737973175342,
      "loss": 0.3836,
      "step": 13390
    },
    {
      "epoch": 9.29264909847434,
      "grad_norm": 2.7406275272369385,
      "learning_rate": 0.000508803401001057,
      "loss": 0.3905,
      "step": 13400
    },
    {
      "epoch": 9.299583911234397,
      "grad_norm": 1.1438816785812378,
      "learning_rate": 0.0005086330046845797,
      "loss": 0.3909,
      "step": 13410
    },
    {
      "epoch": 9.306518723994452,
      "grad_norm": 1.9162334203720093,
      "learning_rate": 0.0005084626083681024,
      "loss": 0.3782,
      "step": 13420
    },
    {
      "epoch": 9.313453536754508,
      "grad_norm": 1.9074406623840332,
      "learning_rate": 0.0005082922120516252,
      "loss": 0.3187,
      "step": 13430
    },
    {
      "epoch": 9.320388349514563,
      "grad_norm": 2.3667654991149902,
      "learning_rate": 0.000508121815735148,
      "loss": 0.3447,
      "step": 13440
    },
    {
      "epoch": 9.327323162274618,
      "grad_norm": 1.699293851852417,
      "learning_rate": 0.0005079514194186708,
      "loss": 0.3031,
      "step": 13450
    },
    {
      "epoch": 9.334257975034674,
      "grad_norm": 4.1235032081604,
      "learning_rate": 0.0005077810231021935,
      "loss": 0.2807,
      "step": 13460
    },
    {
      "epoch": 9.34119278779473,
      "grad_norm": 5.321574687957764,
      "learning_rate": 0.0005076106267857162,
      "loss": 0.3299,
      "step": 13470
    },
    {
      "epoch": 9.348127600554784,
      "grad_norm": 2.170668125152588,
      "learning_rate": 0.000507440230469239,
      "loss": 0.2719,
      "step": 13480
    },
    {
      "epoch": 9.35506241331484,
      "grad_norm": 2.246804714202881,
      "learning_rate": 0.0005072698341527617,
      "loss": 0.3631,
      "step": 13490
    },
    {
      "epoch": 9.361997226074896,
      "grad_norm": 2.2982969284057617,
      "learning_rate": 0.0005070994378362845,
      "loss": 0.4078,
      "step": 13500
    },
    {
      "epoch": 9.368932038834952,
      "grad_norm": 1.9415169954299927,
      "learning_rate": 0.0005069290415198073,
      "loss": 0.3667,
      "step": 13510
    },
    {
      "epoch": 9.375866851595006,
      "grad_norm": 0.7770586013793945,
      "learning_rate": 0.00050675864520333,
      "loss": 0.3363,
      "step": 13520
    },
    {
      "epoch": 9.382801664355062,
      "grad_norm": 1.1777514219284058,
      "learning_rate": 0.0005065882488868528,
      "loss": 0.3663,
      "step": 13530
    },
    {
      "epoch": 9.389736477115118,
      "grad_norm": 3.626580238342285,
      "learning_rate": 0.0005064178525703755,
      "loss": 0.3522,
      "step": 13540
    },
    {
      "epoch": 9.396671289875174,
      "grad_norm": 3.652256965637207,
      "learning_rate": 0.0005062474562538982,
      "loss": 0.3758,
      "step": 13550
    },
    {
      "epoch": 9.403606102635228,
      "grad_norm": 1.6708470582962036,
      "learning_rate": 0.0005060770599374211,
      "loss": 0.4417,
      "step": 13560
    },
    {
      "epoch": 9.410540915395284,
      "grad_norm": 1.2901537418365479,
      "learning_rate": 0.0005059066636209438,
      "loss": 0.3681,
      "step": 13570
    },
    {
      "epoch": 9.41747572815534,
      "grad_norm": 1.2346731424331665,
      "learning_rate": 0.0005057362673044665,
      "loss": 0.4698,
      "step": 13580
    },
    {
      "epoch": 9.424410540915396,
      "grad_norm": 1.0593137741088867,
      "learning_rate": 0.0005055658709879893,
      "loss": 0.3057,
      "step": 13590
    },
    {
      "epoch": 9.43134535367545,
      "grad_norm": 0.7406230568885803,
      "learning_rate": 0.000505395474671512,
      "loss": 0.292,
      "step": 13600
    },
    {
      "epoch": 9.438280166435506,
      "grad_norm": 1.502685785293579,
      "learning_rate": 0.0005052250783550348,
      "loss": 0.4845,
      "step": 13610
    },
    {
      "epoch": 9.445214979195562,
      "grad_norm": 1.7007657289505005,
      "learning_rate": 0.0005050546820385576,
      "loss": 0.3474,
      "step": 13620
    },
    {
      "epoch": 9.452149791955618,
      "grad_norm": 2.0396552085876465,
      "learning_rate": 0.0005048842857220803,
      "loss": 0.3688,
      "step": 13630
    },
    {
      "epoch": 9.459084604715672,
      "grad_norm": 1.2241147756576538,
      "learning_rate": 0.000504713889405603,
      "loss": 0.3719,
      "step": 13640
    },
    {
      "epoch": 9.466019417475728,
      "grad_norm": 2.081721782684326,
      "learning_rate": 0.0005045434930891258,
      "loss": 0.4366,
      "step": 13650
    },
    {
      "epoch": 9.472954230235784,
      "grad_norm": 0.9838783144950867,
      "learning_rate": 0.0005043730967726486,
      "loss": 0.3422,
      "step": 13660
    },
    {
      "epoch": 9.47988904299584,
      "grad_norm": 4.792089462280273,
      "learning_rate": 0.0005042027004561713,
      "loss": 0.3908,
      "step": 13670
    },
    {
      "epoch": 9.486823855755894,
      "grad_norm": 2.7137484550476074,
      "learning_rate": 0.0005040323041396941,
      "loss": 0.3983,
      "step": 13680
    },
    {
      "epoch": 9.49375866851595,
      "grad_norm": 9.383441925048828,
      "learning_rate": 0.0005038619078232169,
      "loss": 0.422,
      "step": 13690
    },
    {
      "epoch": 9.500693481276006,
      "grad_norm": 1.494536280632019,
      "learning_rate": 0.0005036915115067395,
      "loss": 0.3557,
      "step": 13700
    },
    {
      "epoch": 9.507628294036062,
      "grad_norm": 1.5814002752304077,
      "learning_rate": 0.0005035211151902623,
      "loss": 0.4888,
      "step": 13710
    },
    {
      "epoch": 9.514563106796116,
      "grad_norm": 1.7461382150650024,
      "learning_rate": 0.0005033507188737851,
      "loss": 0.4223,
      "step": 13720
    },
    {
      "epoch": 9.521497919556172,
      "grad_norm": 1.793674349784851,
      "learning_rate": 0.0005031803225573078,
      "loss": 0.4294,
      "step": 13730
    },
    {
      "epoch": 9.528432732316228,
      "grad_norm": 1.818513035774231,
      "learning_rate": 0.0005030099262408307,
      "loss": 0.3356,
      "step": 13740
    },
    {
      "epoch": 9.535367545076284,
      "grad_norm": 1.3986862897872925,
      "learning_rate": 0.0005028395299243534,
      "loss": 0.4981,
      "step": 13750
    },
    {
      "epoch": 9.542302357836338,
      "grad_norm": 1.5857691764831543,
      "learning_rate": 0.000502669133607876,
      "loss": 0.3183,
      "step": 13760
    },
    {
      "epoch": 9.549237170596394,
      "grad_norm": 9.334427833557129,
      "learning_rate": 0.0005024987372913989,
      "loss": 0.3632,
      "step": 13770
    },
    {
      "epoch": 9.55617198335645,
      "grad_norm": 3.042078971862793,
      "learning_rate": 0.0005023283409749216,
      "loss": 0.3574,
      "step": 13780
    },
    {
      "epoch": 9.563106796116505,
      "grad_norm": 2.1742467880249023,
      "learning_rate": 0.0005021579446584444,
      "loss": 0.3227,
      "step": 13790
    },
    {
      "epoch": 9.57004160887656,
      "grad_norm": 3.447248935699463,
      "learning_rate": 0.0005019875483419672,
      "loss": 0.3889,
      "step": 13800
    },
    {
      "epoch": 9.576976421636616,
      "grad_norm": 3.9415011405944824,
      "learning_rate": 0.0005018171520254898,
      "loss": 0.3144,
      "step": 13810
    },
    {
      "epoch": 9.583911234396671,
      "grad_norm": 2.2533609867095947,
      "learning_rate": 0.0005016467557090126,
      "loss": 0.3263,
      "step": 13820
    },
    {
      "epoch": 9.590846047156727,
      "grad_norm": 1.9557095766067505,
      "learning_rate": 0.0005014763593925354,
      "loss": 0.3197,
      "step": 13830
    },
    {
      "epoch": 9.597780859916782,
      "grad_norm": 1.592512607574463,
      "learning_rate": 0.0005013059630760581,
      "loss": 0.4041,
      "step": 13840
    },
    {
      "epoch": 9.604715672676837,
      "grad_norm": 1.1930376291275024,
      "learning_rate": 0.0005011355667595809,
      "loss": 0.3003,
      "step": 13850
    },
    {
      "epoch": 9.611650485436893,
      "grad_norm": 3.77653169631958,
      "learning_rate": 0.0005009651704431037,
      "loss": 0.3693,
      "step": 13860
    },
    {
      "epoch": 9.61858529819695,
      "grad_norm": 1.1405448913574219,
      "learning_rate": 0.0005007947741266265,
      "loss": 0.3511,
      "step": 13870
    },
    {
      "epoch": 9.625520110957003,
      "grad_norm": 1.1301065683364868,
      "learning_rate": 0.0005006243778101491,
      "loss": 0.3092,
      "step": 13880
    },
    {
      "epoch": 9.63245492371706,
      "grad_norm": 1.9344936609268188,
      "learning_rate": 0.0005004539814936719,
      "loss": 0.3255,
      "step": 13890
    },
    {
      "epoch": 9.639389736477115,
      "grad_norm": 5.26539945602417,
      "learning_rate": 0.0005002835851771947,
      "loss": 0.4058,
      "step": 13900
    },
    {
      "epoch": 9.646324549237171,
      "grad_norm": 2.072795867919922,
      "learning_rate": 0.0005001131888607174,
      "loss": 0.3819,
      "step": 13910
    },
    {
      "epoch": 9.653259361997225,
      "grad_norm": 1.7357065677642822,
      "learning_rate": 0.0004999427925442402,
      "loss": 0.4055,
      "step": 13920
    },
    {
      "epoch": 9.660194174757281,
      "grad_norm": 1.3548532724380493,
      "learning_rate": 0.000499772396227763,
      "loss": 0.3479,
      "step": 13930
    },
    {
      "epoch": 9.667128987517337,
      "grad_norm": 3.7898828983306885,
      "learning_rate": 0.0004996019999112856,
      "loss": 0.4268,
      "step": 13940
    },
    {
      "epoch": 9.674063800277393,
      "grad_norm": 2.4035732746124268,
      "learning_rate": 0.0004994316035948084,
      "loss": 0.3999,
      "step": 13950
    },
    {
      "epoch": 9.680998613037447,
      "grad_norm": 1.5689631700515747,
      "learning_rate": 0.0004992612072783312,
      "loss": 0.2854,
      "step": 13960
    },
    {
      "epoch": 9.687933425797503,
      "grad_norm": 0.8671950101852417,
      "learning_rate": 0.0004990908109618539,
      "loss": 0.3802,
      "step": 13970
    },
    {
      "epoch": 9.694868238557559,
      "grad_norm": 1.2414195537567139,
      "learning_rate": 0.0004989204146453768,
      "loss": 0.3444,
      "step": 13980
    },
    {
      "epoch": 9.701803051317615,
      "grad_norm": 2.758528232574463,
      "learning_rate": 0.0004987500183288994,
      "loss": 0.387,
      "step": 13990
    },
    {
      "epoch": 9.70873786407767,
      "grad_norm": 2.494633197784424,
      "learning_rate": 0.0004985796220124222,
      "loss": 0.385,
      "step": 14000
    },
    {
      "epoch": 9.715672676837725,
      "grad_norm": 1.4655263423919678,
      "learning_rate": 0.000498409225695945,
      "loss": 0.2932,
      "step": 14010
    },
    {
      "epoch": 9.722607489597781,
      "grad_norm": 2.3658108711242676,
      "learning_rate": 0.0004982388293794677,
      "loss": 0.3887,
      "step": 14020
    },
    {
      "epoch": 9.729542302357837,
      "grad_norm": 1.7866511344909668,
      "learning_rate": 0.0004980684330629905,
      "loss": 0.4569,
      "step": 14030
    },
    {
      "epoch": 9.736477115117891,
      "grad_norm": 4.532111167907715,
      "learning_rate": 0.0004978980367465133,
      "loss": 0.4107,
      "step": 14040
    },
    {
      "epoch": 9.743411927877947,
      "grad_norm": 2.085275888442993,
      "learning_rate": 0.0004977276404300359,
      "loss": 0.4195,
      "step": 14050
    },
    {
      "epoch": 9.750346740638003,
      "grad_norm": 1.1515300273895264,
      "learning_rate": 0.0004975572441135587,
      "loss": 0.3684,
      "step": 14060
    },
    {
      "epoch": 9.757281553398059,
      "grad_norm": 1.3013852834701538,
      "learning_rate": 0.0004973868477970815,
      "loss": 0.3714,
      "step": 14070
    },
    {
      "epoch": 9.764216366158113,
      "grad_norm": 1.6482588052749634,
      "learning_rate": 0.0004972164514806042,
      "loss": 0.3554,
      "step": 14080
    },
    {
      "epoch": 9.771151178918169,
      "grad_norm": 1.42021906375885,
      "learning_rate": 0.000497046055164127,
      "loss": 0.4554,
      "step": 14090
    },
    {
      "epoch": 9.778085991678225,
      "grad_norm": 1.0186623334884644,
      "learning_rate": 0.0004968756588476498,
      "loss": 0.3157,
      "step": 14100
    },
    {
      "epoch": 9.78502080443828,
      "grad_norm": 4.203078269958496,
      "learning_rate": 0.0004967052625311725,
      "loss": 0.3162,
      "step": 14110
    },
    {
      "epoch": 9.791955617198335,
      "grad_norm": 1.183915615081787,
      "learning_rate": 0.0004965348662146952,
      "loss": 0.3322,
      "step": 14120
    },
    {
      "epoch": 9.79889042995839,
      "grad_norm": 1.7934774160385132,
      "learning_rate": 0.000496364469898218,
      "loss": 0.3301,
      "step": 14130
    },
    {
      "epoch": 9.805825242718447,
      "grad_norm": 1.5460848808288574,
      "learning_rate": 0.0004961940735817408,
      "loss": 0.4121,
      "step": 14140
    },
    {
      "epoch": 9.812760055478503,
      "grad_norm": 1.374524712562561,
      "learning_rate": 0.0004960236772652635,
      "loss": 0.4192,
      "step": 14150
    },
    {
      "epoch": 9.819694868238557,
      "grad_norm": 1.3660826683044434,
      "learning_rate": 0.0004958532809487862,
      "loss": 0.4378,
      "step": 14160
    },
    {
      "epoch": 9.826629680998613,
      "grad_norm": 1.3570038080215454,
      "learning_rate": 0.000495682884632309,
      "loss": 0.3174,
      "step": 14170
    },
    {
      "epoch": 9.833564493758669,
      "grad_norm": 17.649547576904297,
      "learning_rate": 0.0004955124883158317,
      "loss": 0.3672,
      "step": 14180
    },
    {
      "epoch": 9.840499306518725,
      "grad_norm": 2.3998513221740723,
      "learning_rate": 0.0004953420919993546,
      "loss": 0.3865,
      "step": 14190
    },
    {
      "epoch": 9.847434119278779,
      "grad_norm": 2.528547525405884,
      "learning_rate": 0.0004951716956828773,
      "loss": 0.3657,
      "step": 14200
    },
    {
      "epoch": 9.854368932038835,
      "grad_norm": 3.951964855194092,
      "learning_rate": 0.0004950012993664,
      "loss": 0.3402,
      "step": 14210
    },
    {
      "epoch": 9.86130374479889,
      "grad_norm": 4.395044803619385,
      "learning_rate": 0.0004948309030499228,
      "loss": 0.4564,
      "step": 14220
    },
    {
      "epoch": 9.868238557558946,
      "grad_norm": 1.7346558570861816,
      "learning_rate": 0.0004946605067334455,
      "loss": 0.4389,
      "step": 14230
    },
    {
      "epoch": 9.875173370319,
      "grad_norm": 1.100234866142273,
      "learning_rate": 0.0004944901104169683,
      "loss": 0.3069,
      "step": 14240
    },
    {
      "epoch": 9.882108183079056,
      "grad_norm": 1.1500149965286255,
      "learning_rate": 0.0004943197141004911,
      "loss": 0.3161,
      "step": 14250
    },
    {
      "epoch": 9.889042995839112,
      "grad_norm": 3.650437355041504,
      "learning_rate": 0.0004941493177840138,
      "loss": 0.3629,
      "step": 14260
    },
    {
      "epoch": 9.895977808599168,
      "grad_norm": 1.9486221075057983,
      "learning_rate": 0.0004939789214675366,
      "loss": 0.3253,
      "step": 14270
    },
    {
      "epoch": 9.902912621359224,
      "grad_norm": 2.468585968017578,
      "learning_rate": 0.0004938085251510593,
      "loss": 0.3846,
      "step": 14280
    },
    {
      "epoch": 9.909847434119278,
      "grad_norm": 3.697932481765747,
      "learning_rate": 0.000493638128834582,
      "loss": 0.4198,
      "step": 14290
    },
    {
      "epoch": 9.916782246879334,
      "grad_norm": 0.9306303858757019,
      "learning_rate": 0.0004934677325181048,
      "loss": 0.3157,
      "step": 14300
    },
    {
      "epoch": 9.92371705963939,
      "grad_norm": 1.0038955211639404,
      "learning_rate": 0.0004932973362016276,
      "loss": 0.2773,
      "step": 14310
    },
    {
      "epoch": 9.930651872399444,
      "grad_norm": 1.7166498899459839,
      "learning_rate": 0.0004931269398851504,
      "loss": 0.3236,
      "step": 14320
    },
    {
      "epoch": 9.9375866851595,
      "grad_norm": 1.0358264446258545,
      "learning_rate": 0.000492956543568673,
      "loss": 0.4889,
      "step": 14330
    },
    {
      "epoch": 9.944521497919556,
      "grad_norm": 1.406248927116394,
      "learning_rate": 0.0004927861472521958,
      "loss": 0.4324,
      "step": 14340
    },
    {
      "epoch": 9.951456310679612,
      "grad_norm": 2.3073229789733887,
      "learning_rate": 0.0004926157509357186,
      "loss": 0.3893,
      "step": 14350
    },
    {
      "epoch": 9.958391123439668,
      "grad_norm": 2.0755691528320312,
      "learning_rate": 0.0004924453546192413,
      "loss": 0.4243,
      "step": 14360
    },
    {
      "epoch": 9.965325936199722,
      "grad_norm": 1.6413722038269043,
      "learning_rate": 0.0004922749583027641,
      "loss": 0.3571,
      "step": 14370
    },
    {
      "epoch": 9.972260748959778,
      "grad_norm": 1.3462361097335815,
      "learning_rate": 0.0004921045619862869,
      "loss": 0.4132,
      "step": 14380
    },
    {
      "epoch": 9.979195561719834,
      "grad_norm": 1.3216619491577148,
      "learning_rate": 0.0004919341656698095,
      "loss": 0.3324,
      "step": 14390
    },
    {
      "epoch": 9.986130374479888,
      "grad_norm": 1.8980848789215088,
      "learning_rate": 0.0004917637693533323,
      "loss": 0.2984,
      "step": 14400
    },
    {
      "epoch": 9.993065187239944,
      "grad_norm": 1.3352717161178589,
      "learning_rate": 0.0004915933730368551,
      "loss": 0.3836,
      "step": 14410
    },
    {
      "epoch": 10.0,
      "grad_norm": 2.2471232414245605,
      "learning_rate": 0.0004914229767203778,
      "loss": 0.4292,
      "step": 14420
    },
    {
      "epoch": 10.0,
      "eval_accuracy": 0.8265695456122095,
      "eval_auc": 0.8265341528700307,
      "eval_f1": 0.8068006182380216,
      "eval_loss": 0.4207351803779602,
      "eval_mcc": 0.6671395053699662,
      "eval_precision": 0.9102005231037489,
      "eval_recall": 0.7244968771686329,
      "eval_runtime": 79.0681,
      "eval_samples_per_second": 36.462,
      "eval_steps_per_second": 2.289,
      "step": 14420
    },
    {
      "epoch": 10.006934812760056,
      "grad_norm": 1.0834848880767822,
      "learning_rate": 0.0004912525804039007,
      "loss": 0.2668,
      "step": 14430
    },
    {
      "epoch": 10.013869625520112,
      "grad_norm": 4.727875709533691,
      "learning_rate": 0.0004910821840874234,
      "loss": 0.3149,
      "step": 14440
    },
    {
      "epoch": 10.020804438280166,
      "grad_norm": 1.950827717781067,
      "learning_rate": 0.0004909117877709461,
      "loss": 0.4084,
      "step": 14450
    },
    {
      "epoch": 10.027739251040222,
      "grad_norm": 1.5828882455825806,
      "learning_rate": 0.0004907413914544689,
      "loss": 0.3686,
      "step": 14460
    },
    {
      "epoch": 10.034674063800278,
      "grad_norm": 1.848160743713379,
      "learning_rate": 0.0004905709951379916,
      "loss": 0.3019,
      "step": 14470
    },
    {
      "epoch": 10.041608876560334,
      "grad_norm": 2.532456636428833,
      "learning_rate": 0.0004904005988215144,
      "loss": 0.2985,
      "step": 14480
    },
    {
      "epoch": 10.048543689320388,
      "grad_norm": 2.318066358566284,
      "learning_rate": 0.0004902302025050372,
      "loss": 0.3549,
      "step": 14490
    },
    {
      "epoch": 10.055478502080444,
      "grad_norm": 2.7087950706481934,
      "learning_rate": 0.0004900598061885599,
      "loss": 0.3505,
      "step": 14500
    },
    {
      "epoch": 10.0624133148405,
      "grad_norm": 1.9840965270996094,
      "learning_rate": 0.0004898894098720826,
      "loss": 0.2085,
      "step": 14510
    },
    {
      "epoch": 10.069348127600556,
      "grad_norm": 4.389285087585449,
      "learning_rate": 0.0004897190135556054,
      "loss": 0.3817,
      "step": 14520
    },
    {
      "epoch": 10.07628294036061,
      "grad_norm": 2.3919312953948975,
      "learning_rate": 0.0004895486172391281,
      "loss": 0.4513,
      "step": 14530
    },
    {
      "epoch": 10.083217753120666,
      "grad_norm": 1.9867936372756958,
      "learning_rate": 0.0004893782209226509,
      "loss": 0.3055,
      "step": 14540
    },
    {
      "epoch": 10.090152565880722,
      "grad_norm": 1.5427907705307007,
      "learning_rate": 0.0004892078246061737,
      "loss": 0.3439,
      "step": 14550
    },
    {
      "epoch": 10.097087378640778,
      "grad_norm": 4.66163444519043,
      "learning_rate": 0.0004890374282896965,
      "loss": 0.368,
      "step": 14560
    },
    {
      "epoch": 10.104022191400832,
      "grad_norm": 2.4831466674804688,
      "learning_rate": 0.0004888670319732191,
      "loss": 0.3003,
      "step": 14570
    },
    {
      "epoch": 10.110957004160888,
      "grad_norm": 5.5555338859558105,
      "learning_rate": 0.0004886966356567419,
      "loss": 0.3999,
      "step": 14580
    },
    {
      "epoch": 10.117891816920944,
      "grad_norm": 1.1381129026412964,
      "learning_rate": 0.0004885262393402647,
      "loss": 0.3991,
      "step": 14590
    },
    {
      "epoch": 10.124826629681,
      "grad_norm": 1.9320930242538452,
      "learning_rate": 0.0004883558430237874,
      "loss": 0.3635,
      "step": 14600
    },
    {
      "epoch": 10.131761442441054,
      "grad_norm": 3.0702710151672363,
      "learning_rate": 0.0004881854467073102,
      "loss": 0.4027,
      "step": 14610
    },
    {
      "epoch": 10.13869625520111,
      "grad_norm": 1.5901497602462769,
      "learning_rate": 0.00048801505039083295,
      "loss": 0.3162,
      "step": 14620
    },
    {
      "epoch": 10.145631067961165,
      "grad_norm": 1.8883209228515625,
      "learning_rate": 0.0004878446540743557,
      "loss": 0.3339,
      "step": 14630
    },
    {
      "epoch": 10.152565880721221,
      "grad_norm": 2.6773674488067627,
      "learning_rate": 0.00048767425775787847,
      "loss": 0.3842,
      "step": 14640
    },
    {
      "epoch": 10.159500693481275,
      "grad_norm": 2.132161855697632,
      "learning_rate": 0.0004875038614414012,
      "loss": 0.3952,
      "step": 14650
    },
    {
      "epoch": 10.166435506241331,
      "grad_norm": 1.9191335439682007,
      "learning_rate": 0.00048733346512492393,
      "loss": 0.3956,
      "step": 14660
    },
    {
      "epoch": 10.173370319001387,
      "grad_norm": 1.3830757141113281,
      "learning_rate": 0.0004871630688084467,
      "loss": 0.3427,
      "step": 14670
    },
    {
      "epoch": 10.180305131761443,
      "grad_norm": 1.4975323677062988,
      "learning_rate": 0.00048699267249196945,
      "loss": 0.3936,
      "step": 14680
    },
    {
      "epoch": 10.187239944521497,
      "grad_norm": 2.5231540203094482,
      "learning_rate": 0.0004868222761754922,
      "loss": 0.3906,
      "step": 14690
    },
    {
      "epoch": 10.194174757281553,
      "grad_norm": 3.3214855194091797,
      "learning_rate": 0.000486651879859015,
      "loss": 0.3858,
      "step": 14700
    },
    {
      "epoch": 10.20110957004161,
      "grad_norm": 1.800502061843872,
      "learning_rate": 0.00048648148354253775,
      "loss": 0.3108,
      "step": 14710
    },
    {
      "epoch": 10.208044382801665,
      "grad_norm": 1.1635222434997559,
      "learning_rate": 0.0004863110872260605,
      "loss": 0.4113,
      "step": 14720
    },
    {
      "epoch": 10.21497919556172,
      "grad_norm": 3.1318440437316895,
      "learning_rate": 0.00048614069090958327,
      "loss": 0.4081,
      "step": 14730
    },
    {
      "epoch": 10.221914008321775,
      "grad_norm": 2.577958583831787,
      "learning_rate": 0.000485970294593106,
      "loss": 0.3841,
      "step": 14740
    },
    {
      "epoch": 10.228848821081831,
      "grad_norm": 0.9107095003128052,
      "learning_rate": 0.00048579989827662873,
      "loss": 0.3215,
      "step": 14750
    },
    {
      "epoch": 10.235783633841887,
      "grad_norm": 1.6465755701065063,
      "learning_rate": 0.0004856295019601515,
      "loss": 0.3205,
      "step": 14760
    },
    {
      "epoch": 10.242718446601941,
      "grad_norm": 1.3096221685409546,
      "learning_rate": 0.00048545910564367424,
      "loss": 0.3382,
      "step": 14770
    },
    {
      "epoch": 10.249653259361997,
      "grad_norm": 3.49599552154541,
      "learning_rate": 0.000485288709327197,
      "loss": 0.2874,
      "step": 14780
    },
    {
      "epoch": 10.256588072122053,
      "grad_norm": 1.4873961210250854,
      "learning_rate": 0.0004851183130107198,
      "loss": 0.3082,
      "step": 14790
    },
    {
      "epoch": 10.263522884882109,
      "grad_norm": 3.347844362258911,
      "learning_rate": 0.0004849479166942425,
      "loss": 0.4135,
      "step": 14800
    },
    {
      "epoch": 10.270457697642163,
      "grad_norm": 2.4829494953155518,
      "learning_rate": 0.0004847775203777652,
      "loss": 0.29,
      "step": 14810
    },
    {
      "epoch": 10.277392510402219,
      "grad_norm": 1.7676942348480225,
      "learning_rate": 0.00048460712406128806,
      "loss": 0.4197,
      "step": 14820
    },
    {
      "epoch": 10.284327323162275,
      "grad_norm": 3.3054986000061035,
      "learning_rate": 0.0004844367277448108,
      "loss": 0.379,
      "step": 14830
    },
    {
      "epoch": 10.29126213592233,
      "grad_norm": 2.1177010536193848,
      "learning_rate": 0.0004842663314283335,
      "loss": 0.3633,
      "step": 14840
    },
    {
      "epoch": 10.298196948682385,
      "grad_norm": 2.3354909420013428,
      "learning_rate": 0.0004840959351118563,
      "loss": 0.3122,
      "step": 14850
    },
    {
      "epoch": 10.305131761442441,
      "grad_norm": 4.039310932159424,
      "learning_rate": 0.00048392553879537904,
      "loss": 0.3996,
      "step": 14860
    },
    {
      "epoch": 10.312066574202497,
      "grad_norm": 1.0988491773605347,
      "learning_rate": 0.00048375514247890177,
      "loss": 0.3527,
      "step": 14870
    },
    {
      "epoch": 10.319001386962553,
      "grad_norm": 3.935760259628296,
      "learning_rate": 0.00048358474616242456,
      "loss": 0.3468,
      "step": 14880
    },
    {
      "epoch": 10.325936199722607,
      "grad_norm": 1.624815821647644,
      "learning_rate": 0.0004834143498459473,
      "loss": 0.3507,
      "step": 14890
    },
    {
      "epoch": 10.332871012482663,
      "grad_norm": 1.3985227346420288,
      "learning_rate": 0.00048324395352947,
      "loss": 0.3003,
      "step": 14900
    },
    {
      "epoch": 10.339805825242719,
      "grad_norm": 2.4418323040008545,
      "learning_rate": 0.00048307355721299286,
      "loss": 0.3708,
      "step": 14910
    },
    {
      "epoch": 10.346740638002775,
      "grad_norm": 2.503105878829956,
      "learning_rate": 0.0004829031608965156,
      "loss": 0.3698,
      "step": 14920
    },
    {
      "epoch": 10.353675450762829,
      "grad_norm": 3.864449977874756,
      "learning_rate": 0.00048273276458003826,
      "loss": 0.4518,
      "step": 14930
    },
    {
      "epoch": 10.360610263522885,
      "grad_norm": 1.7602611780166626,
      "learning_rate": 0.0004825623682635611,
      "loss": 0.3398,
      "step": 14940
    },
    {
      "epoch": 10.36754507628294,
      "grad_norm": 1.9024051427841187,
      "learning_rate": 0.00048239197194708383,
      "loss": 0.3746,
      "step": 14950
    },
    {
      "epoch": 10.374479889042997,
      "grad_norm": 2.112753391265869,
      "learning_rate": 0.00048222157563060657,
      "loss": 0.3951,
      "step": 14960
    },
    {
      "epoch": 10.38141470180305,
      "grad_norm": 2.4075536727905273,
      "learning_rate": 0.00048205117931412935,
      "loss": 0.3135,
      "step": 14970
    },
    {
      "epoch": 10.388349514563107,
      "grad_norm": 0.9768823981285095,
      "learning_rate": 0.0004818807829976521,
      "loss": 0.3764,
      "step": 14980
    },
    {
      "epoch": 10.395284327323163,
      "grad_norm": 2.6819515228271484,
      "learning_rate": 0.0004817103866811748,
      "loss": 0.4379,
      "step": 14990
    },
    {
      "epoch": 10.402219140083218,
      "grad_norm": 2.2239186763763428,
      "learning_rate": 0.0004815399903646976,
      "loss": 0.3806,
      "step": 15000
    },
    {
      "epoch": 10.409153952843273,
      "grad_norm": 1.4776203632354736,
      "learning_rate": 0.00048136959404822033,
      "loss": 0.4262,
      "step": 15010
    },
    {
      "epoch": 10.416088765603329,
      "grad_norm": 1.4430034160614014,
      "learning_rate": 0.00048119919773174306,
      "loss": 0.3029,
      "step": 15020
    },
    {
      "epoch": 10.423023578363384,
      "grad_norm": 1.885054588317871,
      "learning_rate": 0.0004810288014152659,
      "loss": 0.2701,
      "step": 15030
    },
    {
      "epoch": 10.42995839112344,
      "grad_norm": 2.1675479412078857,
      "learning_rate": 0.00048085840509878863,
      "loss": 0.3433,
      "step": 15040
    },
    {
      "epoch": 10.436893203883495,
      "grad_norm": 1.8897030353546143,
      "learning_rate": 0.00048068800878231136,
      "loss": 0.2703,
      "step": 15050
    },
    {
      "epoch": 10.44382801664355,
      "grad_norm": 1.3262995481491089,
      "learning_rate": 0.00048051761246583415,
      "loss": 0.3169,
      "step": 15060
    },
    {
      "epoch": 10.450762829403606,
      "grad_norm": 1.7960177659988403,
      "learning_rate": 0.0004803472161493569,
      "loss": 0.3154,
      "step": 15070
    },
    {
      "epoch": 10.457697642163662,
      "grad_norm": 1.4365754127502441,
      "learning_rate": 0.0004801768198328796,
      "loss": 0.3171,
      "step": 15080
    },
    {
      "epoch": 10.464632454923716,
      "grad_norm": 4.841055393218994,
      "learning_rate": 0.0004800064235164024,
      "loss": 0.3603,
      "step": 15090
    },
    {
      "epoch": 10.471567267683772,
      "grad_norm": 3.5681676864624023,
      "learning_rate": 0.0004798360271999251,
      "loss": 0.2738,
      "step": 15100
    },
    {
      "epoch": 10.478502080443828,
      "grad_norm": 2.25223445892334,
      "learning_rate": 0.00047966563088344786,
      "loss": 0.3261,
      "step": 15110
    },
    {
      "epoch": 10.485436893203884,
      "grad_norm": 0.8731325268745422,
      "learning_rate": 0.0004794952345669707,
      "loss": 0.2244,
      "step": 15120
    },
    {
      "epoch": 10.492371705963938,
      "grad_norm": 2.217352867126465,
      "learning_rate": 0.00047932483825049337,
      "loss": 0.3824,
      "step": 15130
    },
    {
      "epoch": 10.499306518723994,
      "grad_norm": 3.776013135910034,
      "learning_rate": 0.0004791544419340161,
      "loss": 0.2734,
      "step": 15140
    },
    {
      "epoch": 10.50624133148405,
      "grad_norm": 1.8110384941101074,
      "learning_rate": 0.00047898404561753894,
      "loss": 0.427,
      "step": 15150
    },
    {
      "epoch": 10.513176144244106,
      "grad_norm": 3.1097452640533447,
      "learning_rate": 0.00047881364930106167,
      "loss": 0.3868,
      "step": 15160
    },
    {
      "epoch": 10.52011095700416,
      "grad_norm": 2.9463346004486084,
      "learning_rate": 0.0004786432529845844,
      "loss": 0.31,
      "step": 15170
    },
    {
      "epoch": 10.527045769764216,
      "grad_norm": 2.008950710296631,
      "learning_rate": 0.0004784728566681072,
      "loss": 0.2143,
      "step": 15180
    },
    {
      "epoch": 10.533980582524272,
      "grad_norm": 1.2994518280029297,
      "learning_rate": 0.0004783024603516299,
      "loss": 0.3775,
      "step": 15190
    },
    {
      "epoch": 10.540915395284328,
      "grad_norm": 1.9332072734832764,
      "learning_rate": 0.00047813206403515265,
      "loss": 0.4362,
      "step": 15200
    },
    {
      "epoch": 10.547850208044382,
      "grad_norm": 2.2731399536132812,
      "learning_rate": 0.00047796166771867544,
      "loss": 0.3906,
      "step": 15210
    },
    {
      "epoch": 10.554785020804438,
      "grad_norm": 1.4090139865875244,
      "learning_rate": 0.00047779127140219817,
      "loss": 0.3232,
      "step": 15220
    },
    {
      "epoch": 10.561719833564494,
      "grad_norm": 1.7293747663497925,
      "learning_rate": 0.0004776208750857209,
      "loss": 0.3422,
      "step": 15230
    },
    {
      "epoch": 10.56865464632455,
      "grad_norm": 2.722996711730957,
      "learning_rate": 0.00047745047876924374,
      "loss": 0.4114,
      "step": 15240
    },
    {
      "epoch": 10.575589459084604,
      "grad_norm": 0.9678450226783752,
      "learning_rate": 0.00047728008245276647,
      "loss": 0.3036,
      "step": 15250
    },
    {
      "epoch": 10.58252427184466,
      "grad_norm": 2.0240132808685303,
      "learning_rate": 0.00047710968613628914,
      "loss": 0.3624,
      "step": 15260
    },
    {
      "epoch": 10.589459084604716,
      "grad_norm": 8.947754859924316,
      "learning_rate": 0.000476939289819812,
      "loss": 0.3894,
      "step": 15270
    },
    {
      "epoch": 10.596393897364772,
      "grad_norm": 1.6934250593185425,
      "learning_rate": 0.0004767688935033347,
      "loss": 0.3099,
      "step": 15280
    },
    {
      "epoch": 10.603328710124826,
      "grad_norm": 3.1191041469573975,
      "learning_rate": 0.00047659849718685745,
      "loss": 0.396,
      "step": 15290
    },
    {
      "epoch": 10.610263522884882,
      "grad_norm": 4.563358783721924,
      "learning_rate": 0.00047642810087038023,
      "loss": 0.364,
      "step": 15300
    },
    {
      "epoch": 10.617198335644938,
      "grad_norm": 2.47841477394104,
      "learning_rate": 0.00047625770455390296,
      "loss": 0.3772,
      "step": 15310
    },
    {
      "epoch": 10.624133148404994,
      "grad_norm": 1.2614964246749878,
      "learning_rate": 0.0004760873082374257,
      "loss": 0.3411,
      "step": 15320
    },
    {
      "epoch": 10.631067961165048,
      "grad_norm": 2.118474006652832,
      "learning_rate": 0.0004759169119209485,
      "loss": 0.3106,
      "step": 15330
    },
    {
      "epoch": 10.638002773925104,
      "grad_norm": 1.7095392942428589,
      "learning_rate": 0.0004757465156044712,
      "loss": 0.3025,
      "step": 15340
    },
    {
      "epoch": 10.64493758668516,
      "grad_norm": 1.1532878875732422,
      "learning_rate": 0.00047557611928799394,
      "loss": 0.3435,
      "step": 15350
    },
    {
      "epoch": 10.651872399445216,
      "grad_norm": 2.0603790283203125,
      "learning_rate": 0.0004754057229715168,
      "loss": 0.4319,
      "step": 15360
    },
    {
      "epoch": 10.65880721220527,
      "grad_norm": 3.406825542449951,
      "learning_rate": 0.0004752353266550395,
      "loss": 0.3486,
      "step": 15370
    },
    {
      "epoch": 10.665742024965326,
      "grad_norm": 1.5047317743301392,
      "learning_rate": 0.00047506493033856224,
      "loss": 0.2427,
      "step": 15380
    },
    {
      "epoch": 10.672676837725382,
      "grad_norm": 3.8730905055999756,
      "learning_rate": 0.000474894534022085,
      "loss": 0.4132,
      "step": 15390
    },
    {
      "epoch": 10.679611650485437,
      "grad_norm": 2.58634877204895,
      "learning_rate": 0.00047472413770560776,
      "loss": 0.3125,
      "step": 15400
    },
    {
      "epoch": 10.686546463245492,
      "grad_norm": 1.5509048700332642,
      "learning_rate": 0.0004745537413891305,
      "loss": 0.4062,
      "step": 15410
    },
    {
      "epoch": 10.693481276005548,
      "grad_norm": 2.522782564163208,
      "learning_rate": 0.0004743833450726533,
      "loss": 0.3833,
      "step": 15420
    },
    {
      "epoch": 10.700416088765603,
      "grad_norm": 1.3345929384231567,
      "learning_rate": 0.000474212948756176,
      "loss": 0.3496,
      "step": 15430
    },
    {
      "epoch": 10.70735090152566,
      "grad_norm": 1.993358850479126,
      "learning_rate": 0.00047404255243969874,
      "loss": 0.3287,
      "step": 15440
    },
    {
      "epoch": 10.714285714285714,
      "grad_norm": 1.3083611726760864,
      "learning_rate": 0.0004738721561232216,
      "loss": 0.2784,
      "step": 15450
    },
    {
      "epoch": 10.72122052704577,
      "grad_norm": 3.747314929962158,
      "learning_rate": 0.00047370175980674425,
      "loss": 0.562,
      "step": 15460
    },
    {
      "epoch": 10.728155339805825,
      "grad_norm": 2.5899336338043213,
      "learning_rate": 0.000473531363490267,
      "loss": 0.4079,
      "step": 15470
    },
    {
      "epoch": 10.735090152565881,
      "grad_norm": 1.6896545886993408,
      "learning_rate": 0.0004733609671737898,
      "loss": 0.3753,
      "step": 15480
    },
    {
      "epoch": 10.742024965325935,
      "grad_norm": 4.629326820373535,
      "learning_rate": 0.00047319057085731255,
      "loss": 0.3458,
      "step": 15490
    },
    {
      "epoch": 10.748959778085991,
      "grad_norm": 1.7517963647842407,
      "learning_rate": 0.0004730201745408353,
      "loss": 0.4384,
      "step": 15500
    },
    {
      "epoch": 10.755894590846047,
      "grad_norm": 1.2633873224258423,
      "learning_rate": 0.00047284977822435807,
      "loss": 0.3275,
      "step": 15510
    },
    {
      "epoch": 10.762829403606103,
      "grad_norm": 2.14632511138916,
      "learning_rate": 0.0004726793819078808,
      "loss": 0.3756,
      "step": 15520
    },
    {
      "epoch": 10.769764216366157,
      "grad_norm": 4.44227409362793,
      "learning_rate": 0.00047250898559140353,
      "loss": 0.3343,
      "step": 15530
    },
    {
      "epoch": 10.776699029126213,
      "grad_norm": 1.0689220428466797,
      "learning_rate": 0.0004723385892749263,
      "loss": 0.2904,
      "step": 15540
    },
    {
      "epoch": 10.78363384188627,
      "grad_norm": 3.1294074058532715,
      "learning_rate": 0.00047216819295844905,
      "loss": 0.3385,
      "step": 15550
    },
    {
      "epoch": 10.790568654646325,
      "grad_norm": 2.490936040878296,
      "learning_rate": 0.0004719977966419718,
      "loss": 0.3709,
      "step": 15560
    },
    {
      "epoch": 10.79750346740638,
      "grad_norm": 1.2628028392791748,
      "learning_rate": 0.0004718274003254946,
      "loss": 0.3226,
      "step": 15570
    },
    {
      "epoch": 10.804438280166435,
      "grad_norm": 1.7881672382354736,
      "learning_rate": 0.00047165700400901735,
      "loss": 0.2493,
      "step": 15580
    },
    {
      "epoch": 10.811373092926491,
      "grad_norm": 2.272646188735962,
      "learning_rate": 0.00047148660769254,
      "loss": 0.3932,
      "step": 15590
    },
    {
      "epoch": 10.818307905686547,
      "grad_norm": 1.7672266960144043,
      "learning_rate": 0.00047131621137606286,
      "loss": 0.4576,
      "step": 15600
    },
    {
      "epoch": 10.825242718446601,
      "grad_norm": 1.7020705938339233,
      "learning_rate": 0.0004711458150595856,
      "loss": 0.3568,
      "step": 15610
    },
    {
      "epoch": 10.832177531206657,
      "grad_norm": 1.4410967826843262,
      "learning_rate": 0.0004709754187431083,
      "loss": 0.4302,
      "step": 15620
    },
    {
      "epoch": 10.839112343966713,
      "grad_norm": 1.9119404554367065,
      "learning_rate": 0.0004708050224266311,
      "loss": 0.3452,
      "step": 15630
    },
    {
      "epoch": 10.846047156726769,
      "grad_norm": 1.929624080657959,
      "learning_rate": 0.00047063462611015384,
      "loss": 0.3102,
      "step": 15640
    },
    {
      "epoch": 10.852981969486823,
      "grad_norm": 2.3950397968292236,
      "learning_rate": 0.0004704642297936766,
      "loss": 0.4087,
      "step": 15650
    },
    {
      "epoch": 10.859916782246879,
      "grad_norm": 1.6832526922225952,
      "learning_rate": 0.00047029383347719936,
      "loss": 0.3169,
      "step": 15660
    },
    {
      "epoch": 10.866851595006935,
      "grad_norm": 0.8160895109176636,
      "learning_rate": 0.0004701234371607221,
      "loss": 0.2779,
      "step": 15670
    },
    {
      "epoch": 10.87378640776699,
      "grad_norm": 2.520287036895752,
      "learning_rate": 0.0004699530408442448,
      "loss": 0.3485,
      "step": 15680
    },
    {
      "epoch": 10.880721220527047,
      "grad_norm": 1.1282145977020264,
      "learning_rate": 0.00046978264452776766,
      "loss": 0.2413,
      "step": 15690
    },
    {
      "epoch": 10.8876560332871,
      "grad_norm": 2.373502254486084,
      "learning_rate": 0.0004696122482112904,
      "loss": 0.3845,
      "step": 15700
    },
    {
      "epoch": 10.894590846047157,
      "grad_norm": 1.4946885108947754,
      "learning_rate": 0.0004694418518948131,
      "loss": 0.3394,
      "step": 15710
    },
    {
      "epoch": 10.901525658807213,
      "grad_norm": 4.443148612976074,
      "learning_rate": 0.0004692714555783359,
      "loss": 0.3996,
      "step": 15720
    },
    {
      "epoch": 10.908460471567267,
      "grad_norm": 1.4096999168395996,
      "learning_rate": 0.00046910105926185864,
      "loss": 0.3425,
      "step": 15730
    },
    {
      "epoch": 10.915395284327323,
      "grad_norm": 3.4100661277770996,
      "learning_rate": 0.00046893066294538137,
      "loss": 0.3862,
      "step": 15740
    },
    {
      "epoch": 10.922330097087379,
      "grad_norm": 1.5198428630828857,
      "learning_rate": 0.00046876026662890415,
      "loss": 0.3211,
      "step": 15750
    },
    {
      "epoch": 10.929264909847435,
      "grad_norm": 0.9299490451812744,
      "learning_rate": 0.0004685898703124269,
      "loss": 0.3293,
      "step": 15760
    },
    {
      "epoch": 10.93619972260749,
      "grad_norm": 2.5501434803009033,
      "learning_rate": 0.0004684194739959496,
      "loss": 0.3633,
      "step": 15770
    },
    {
      "epoch": 10.943134535367545,
      "grad_norm": 1.9820560216903687,
      "learning_rate": 0.0004682490776794724,
      "loss": 0.3862,
      "step": 15780
    },
    {
      "epoch": 10.9500693481276,
      "grad_norm": 1.8966988325119019,
      "learning_rate": 0.00046807868136299513,
      "loss": 0.3881,
      "step": 15790
    },
    {
      "epoch": 10.957004160887656,
      "grad_norm": 1.169661521911621,
      "learning_rate": 0.00046790828504651786,
      "loss": 0.3305,
      "step": 15800
    },
    {
      "epoch": 10.96393897364771,
      "grad_norm": 1.521786093711853,
      "learning_rate": 0.0004677378887300407,
      "loss": 0.353,
      "step": 15810
    },
    {
      "epoch": 10.970873786407767,
      "grad_norm": 1.354388952255249,
      "learning_rate": 0.00046756749241356343,
      "loss": 0.4916,
      "step": 15820
    },
    {
      "epoch": 10.977808599167822,
      "grad_norm": 1.8708425760269165,
      "learning_rate": 0.00046739709609708616,
      "loss": 0.3192,
      "step": 15830
    },
    {
      "epoch": 10.984743411927878,
      "grad_norm": 1.396862268447876,
      "learning_rate": 0.00046722669978060895,
      "loss": 0.3822,
      "step": 15840
    },
    {
      "epoch": 10.991678224687934,
      "grad_norm": 1.5160913467407227,
      "learning_rate": 0.0004670563034641317,
      "loss": 0.292,
      "step": 15850
    },
    {
      "epoch": 10.998613037447988,
      "grad_norm": 1.164091944694519,
      "learning_rate": 0.0004668859071476544,
      "loss": 0.3567,
      "step": 15860
    },
    {
      "epoch": 11.0,
      "eval_accuracy": 0.8244883801595561,
      "eval_auc": 0.8244638152923931,
      "eval_f1": 0.8110530246452577,
      "eval_loss": 0.4516999423503876,
      "eval_mcc": 0.655558000075647,
      "eval_precision": 0.8779304769603881,
      "eval_recall": 0.7536433032616239,
      "eval_runtime": 78.8036,
      "eval_samples_per_second": 36.585,
      "eval_steps_per_second": 2.297,
      "step": 15862
    },
    {
      "epoch": 11.005547850208044,
      "grad_norm": 2.412841796875,
      "learning_rate": 0.0004667155108311772,
      "loss": 0.3796,
      "step": 15870
    },
    {
      "epoch": 11.0124826629681,
      "grad_norm": 2.7643043994903564,
      "learning_rate": 0.00046654511451469993,
      "loss": 0.2185,
      "step": 15880
    },
    {
      "epoch": 11.019417475728156,
      "grad_norm": 1.8751466274261475,
      "learning_rate": 0.00046637471819822266,
      "loss": 0.2768,
      "step": 15890
    },
    {
      "epoch": 11.02635228848821,
      "grad_norm": 3.951260566711426,
      "learning_rate": 0.0004662043218817455,
      "loss": 0.2918,
      "step": 15900
    },
    {
      "epoch": 11.033287101248266,
      "grad_norm": 1.6628645658493042,
      "learning_rate": 0.00046603392556526823,
      "loss": 0.2919,
      "step": 15910
    },
    {
      "epoch": 11.040221914008322,
      "grad_norm": 4.115045070648193,
      "learning_rate": 0.0004658635292487909,
      "loss": 0.3568,
      "step": 15920
    },
    {
      "epoch": 11.047156726768378,
      "grad_norm": 2.339653730392456,
      "learning_rate": 0.00046569313293231374,
      "loss": 0.3731,
      "step": 15930
    },
    {
      "epoch": 11.054091539528432,
      "grad_norm": 1.477208137512207,
      "learning_rate": 0.0004655227366158365,
      "loss": 0.31,
      "step": 15940
    },
    {
      "epoch": 11.061026352288488,
      "grad_norm": 0.9497664570808411,
      "learning_rate": 0.0004653523402993592,
      "loss": 0.3545,
      "step": 15950
    },
    {
      "epoch": 11.067961165048544,
      "grad_norm": 3.464906692504883,
      "learning_rate": 0.000465181943982882,
      "loss": 0.2633,
      "step": 15960
    },
    {
      "epoch": 11.0748959778086,
      "grad_norm": 3.516143798828125,
      "learning_rate": 0.0004650115476664047,
      "loss": 0.4264,
      "step": 15970
    },
    {
      "epoch": 11.081830790568654,
      "grad_norm": 1.3529927730560303,
      "learning_rate": 0.00046484115134992745,
      "loss": 0.353,
      "step": 15980
    },
    {
      "epoch": 11.08876560332871,
      "grad_norm": 1.920776605606079,
      "learning_rate": 0.00046467075503345024,
      "loss": 0.3482,
      "step": 15990
    },
    {
      "epoch": 11.095700416088766,
      "grad_norm": 1.8084849119186401,
      "learning_rate": 0.00046450035871697297,
      "loss": 0.2624,
      "step": 16000
    },
    {
      "epoch": 11.102635228848822,
      "grad_norm": 3.2720601558685303,
      "learning_rate": 0.0004643299624004957,
      "loss": 0.3587,
      "step": 16010
    },
    {
      "epoch": 11.109570041608876,
      "grad_norm": 2.425952196121216,
      "learning_rate": 0.00046415956608401854,
      "loss": 0.355,
      "step": 16020
    },
    {
      "epoch": 11.116504854368932,
      "grad_norm": 3.2542903423309326,
      "learning_rate": 0.00046398916976754127,
      "loss": 0.3978,
      "step": 16030
    },
    {
      "epoch": 11.123439667128988,
      "grad_norm": 1.382828950881958,
      "learning_rate": 0.000463818773451064,
      "loss": 0.2699,
      "step": 16040
    },
    {
      "epoch": 11.130374479889044,
      "grad_norm": 1.3917535543441772,
      "learning_rate": 0.0004636483771345868,
      "loss": 0.3467,
      "step": 16050
    },
    {
      "epoch": 11.137309292649098,
      "grad_norm": 2.7162675857543945,
      "learning_rate": 0.0004634779808181095,
      "loss": 0.3213,
      "step": 16060
    },
    {
      "epoch": 11.144244105409154,
      "grad_norm": 5.638134956359863,
      "learning_rate": 0.00046330758450163225,
      "loss": 0.3909,
      "step": 16070
    },
    {
      "epoch": 11.15117891816921,
      "grad_norm": 2.2046804428100586,
      "learning_rate": 0.00046313718818515503,
      "loss": 0.4245,
      "step": 16080
    },
    {
      "epoch": 11.158113730929266,
      "grad_norm": 1.6162751913070679,
      "learning_rate": 0.00046296679186867777,
      "loss": 0.2935,
      "step": 16090
    },
    {
      "epoch": 11.16504854368932,
      "grad_norm": 1.101334571838379,
      "learning_rate": 0.0004627963955522005,
      "loss": 0.294,
      "step": 16100
    },
    {
      "epoch": 11.171983356449376,
      "grad_norm": 2.9525792598724365,
      "learning_rate": 0.0004626259992357233,
      "loss": 0.2433,
      "step": 16110
    },
    {
      "epoch": 11.178918169209432,
      "grad_norm": 2.5161054134368896,
      "learning_rate": 0.000462455602919246,
      "loss": 0.3142,
      "step": 16120
    },
    {
      "epoch": 11.185852981969488,
      "grad_norm": 0.8474400639533997,
      "learning_rate": 0.00046228520660276874,
      "loss": 0.3136,
      "step": 16130
    },
    {
      "epoch": 11.192787794729542,
      "grad_norm": 1.470093011856079,
      "learning_rate": 0.0004621148102862916,
      "loss": 0.3701,
      "step": 16140
    },
    {
      "epoch": 11.199722607489598,
      "grad_norm": 1.5381602048873901,
      "learning_rate": 0.0004619444139698143,
      "loss": 0.3245,
      "step": 16150
    },
    {
      "epoch": 11.206657420249654,
      "grad_norm": 5.387851715087891,
      "learning_rate": 0.00046177401765333704,
      "loss": 0.3216,
      "step": 16160
    },
    {
      "epoch": 11.21359223300971,
      "grad_norm": 0.9676237106323242,
      "learning_rate": 0.00046160362133685983,
      "loss": 0.4156,
      "step": 16170
    },
    {
      "epoch": 11.220527045769764,
      "grad_norm": 2.0715394020080566,
      "learning_rate": 0.00046143322502038256,
      "loss": 0.3543,
      "step": 16180
    },
    {
      "epoch": 11.22746185852982,
      "grad_norm": 1.9941192865371704,
      "learning_rate": 0.0004612628287039053,
      "loss": 0.3295,
      "step": 16190
    },
    {
      "epoch": 11.234396671289876,
      "grad_norm": 2.822601556777954,
      "learning_rate": 0.0004610924323874281,
      "loss": 0.3282,
      "step": 16200
    },
    {
      "epoch": 11.241331484049931,
      "grad_norm": 0.7219970226287842,
      "learning_rate": 0.0004609220360709508,
      "loss": 0.2632,
      "step": 16210
    },
    {
      "epoch": 11.248266296809986,
      "grad_norm": 4.323014259338379,
      "learning_rate": 0.00046075163975447354,
      "loss": 0.3821,
      "step": 16220
    },
    {
      "epoch": 11.255201109570041,
      "grad_norm": 1.161797046661377,
      "learning_rate": 0.00046058124343799627,
      "loss": 0.3496,
      "step": 16230
    },
    {
      "epoch": 11.262135922330097,
      "grad_norm": 2.495203971862793,
      "learning_rate": 0.00046041084712151906,
      "loss": 0.3005,
      "step": 16240
    },
    {
      "epoch": 11.269070735090153,
      "grad_norm": 3.1786558628082275,
      "learning_rate": 0.0004602404508050418,
      "loss": 0.3872,
      "step": 16250
    },
    {
      "epoch": 11.276005547850207,
      "grad_norm": 1.32046639919281,
      "learning_rate": 0.0004600700544885645,
      "loss": 0.3514,
      "step": 16260
    },
    {
      "epoch": 11.282940360610263,
      "grad_norm": 1.520270824432373,
      "learning_rate": 0.00045989965817208736,
      "loss": 0.3764,
      "step": 16270
    },
    {
      "epoch": 11.28987517337032,
      "grad_norm": 2.016268491744995,
      "learning_rate": 0.0004597292618556101,
      "loss": 0.3308,
      "step": 16280
    },
    {
      "epoch": 11.296809986130375,
      "grad_norm": 2.9474778175354004,
      "learning_rate": 0.0004595588655391328,
      "loss": 0.3304,
      "step": 16290
    },
    {
      "epoch": 11.30374479889043,
      "grad_norm": 3.3713741302490234,
      "learning_rate": 0.0004593884692226556,
      "loss": 0.3659,
      "step": 16300
    },
    {
      "epoch": 11.310679611650485,
      "grad_norm": 2.4052231311798096,
      "learning_rate": 0.00045921807290617833,
      "loss": 0.3797,
      "step": 16310
    },
    {
      "epoch": 11.317614424410541,
      "grad_norm": 3.77616024017334,
      "learning_rate": 0.00045904767658970107,
      "loss": 0.3489,
      "step": 16320
    },
    {
      "epoch": 11.324549237170597,
      "grad_norm": 1.566021203994751,
      "learning_rate": 0.00045887728027322385,
      "loss": 0.4658,
      "step": 16330
    },
    {
      "epoch": 11.331484049930651,
      "grad_norm": 3.353712797164917,
      "learning_rate": 0.0004587068839567466,
      "loss": 0.303,
      "step": 16340
    },
    {
      "epoch": 11.338418862690707,
      "grad_norm": 4.0335187911987305,
      "learning_rate": 0.0004585364876402693,
      "loss": 0.2886,
      "step": 16350
    },
    {
      "epoch": 11.345353675450763,
      "grad_norm": 1.4801783561706543,
      "learning_rate": 0.00045836609132379215,
      "loss": 0.3076,
      "step": 16360
    },
    {
      "epoch": 11.352288488210819,
      "grad_norm": 1.815561056137085,
      "learning_rate": 0.0004581956950073149,
      "loss": 0.2859,
      "step": 16370
    },
    {
      "epoch": 11.359223300970873,
      "grad_norm": 2.240365743637085,
      "learning_rate": 0.00045802529869083756,
      "loss": 0.3224,
      "step": 16380
    },
    {
      "epoch": 11.366158113730929,
      "grad_norm": 1.9930447340011597,
      "learning_rate": 0.0004578549023743604,
      "loss": 0.3803,
      "step": 16390
    },
    {
      "epoch": 11.373092926490985,
      "grad_norm": 1.4485031366348267,
      "learning_rate": 0.00045768450605788313,
      "loss": 0.3479,
      "step": 16400
    },
    {
      "epoch": 11.380027739251041,
      "grad_norm": 1.4963650703430176,
      "learning_rate": 0.00045751410974140586,
      "loss": 0.2389,
      "step": 16410
    },
    {
      "epoch": 11.386962552011095,
      "grad_norm": 3.599417209625244,
      "learning_rate": 0.00045734371342492865,
      "loss": 0.2897,
      "step": 16420
    },
    {
      "epoch": 11.393897364771151,
      "grad_norm": 3.148883581161499,
      "learning_rate": 0.0004571733171084514,
      "loss": 0.2562,
      "step": 16430
    },
    {
      "epoch": 11.400832177531207,
      "grad_norm": 1.969531774520874,
      "learning_rate": 0.0004570029207919741,
      "loss": 0.3014,
      "step": 16440
    },
    {
      "epoch": 11.407766990291263,
      "grad_norm": 1.4955177307128906,
      "learning_rate": 0.0004568325244754969,
      "loss": 0.3707,
      "step": 16450
    },
    {
      "epoch": 11.414701803051317,
      "grad_norm": 1.6427695751190186,
      "learning_rate": 0.0004566621281590196,
      "loss": 0.3647,
      "step": 16460
    },
    {
      "epoch": 11.421636615811373,
      "grad_norm": 1.952633023262024,
      "learning_rate": 0.00045649173184254235,
      "loss": 0.3247,
      "step": 16470
    },
    {
      "epoch": 11.428571428571429,
      "grad_norm": 2.8350777626037598,
      "learning_rate": 0.0004563213355260652,
      "loss": 0.4561,
      "step": 16480
    },
    {
      "epoch": 11.435506241331485,
      "grad_norm": 1.2738244533538818,
      "learning_rate": 0.0004561509392095879,
      "loss": 0.2777,
      "step": 16490
    },
    {
      "epoch": 11.442441054091539,
      "grad_norm": 1.8887945413589478,
      "learning_rate": 0.00045598054289311066,
      "loss": 0.3394,
      "step": 16500
    },
    {
      "epoch": 11.449375866851595,
      "grad_norm": 4.194677352905273,
      "learning_rate": 0.00045581014657663344,
      "loss": 0.313,
      "step": 16510
    },
    {
      "epoch": 11.45631067961165,
      "grad_norm": 1.2673131227493286,
      "learning_rate": 0.00045563975026015617,
      "loss": 0.2637,
      "step": 16520
    },
    {
      "epoch": 11.463245492371707,
      "grad_norm": 1.1390630006790161,
      "learning_rate": 0.0004554693539436789,
      "loss": 0.3277,
      "step": 16530
    },
    {
      "epoch": 11.47018030513176,
      "grad_norm": 1.9049397706985474,
      "learning_rate": 0.0004552989576272017,
      "loss": 0.3692,
      "step": 16540
    },
    {
      "epoch": 11.477115117891817,
      "grad_norm": 2.381721019744873,
      "learning_rate": 0.0004551285613107244,
      "loss": 0.3021,
      "step": 16550
    },
    {
      "epoch": 11.484049930651873,
      "grad_norm": 3.9929635524749756,
      "learning_rate": 0.00045495816499424715,
      "loss": 0.4707,
      "step": 16560
    },
    {
      "epoch": 11.490984743411929,
      "grad_norm": 1.5990846157073975,
      "learning_rate": 0.00045478776867776994,
      "loss": 0.3856,
      "step": 16570
    },
    {
      "epoch": 11.497919556171983,
      "grad_norm": 1.656497836112976,
      "learning_rate": 0.00045461737236129267,
      "loss": 0.3997,
      "step": 16580
    },
    {
      "epoch": 11.504854368932039,
      "grad_norm": 2.0001461505889893,
      "learning_rate": 0.0004544469760448154,
      "loss": 0.4027,
      "step": 16590
    },
    {
      "epoch": 11.511789181692095,
      "grad_norm": 1.0463788509368896,
      "learning_rate": 0.00045427657972833824,
      "loss": 0.3422,
      "step": 16600
    },
    {
      "epoch": 11.51872399445215,
      "grad_norm": 1.4906005859375,
      "learning_rate": 0.00045410618341186097,
      "loss": 0.35,
      "step": 16610
    },
    {
      "epoch": 11.525658807212205,
      "grad_norm": 4.2393951416015625,
      "learning_rate": 0.0004539357870953837,
      "loss": 0.2831,
      "step": 16620
    },
    {
      "epoch": 11.53259361997226,
      "grad_norm": 2.378485679626465,
      "learning_rate": 0.0004537653907789065,
      "loss": 0.3168,
      "step": 16630
    },
    {
      "epoch": 11.539528432732316,
      "grad_norm": 1.4746341705322266,
      "learning_rate": 0.0004535949944624292,
      "loss": 0.3421,
      "step": 16640
    },
    {
      "epoch": 11.546463245492372,
      "grad_norm": 3.918382167816162,
      "learning_rate": 0.00045342459814595195,
      "loss": 0.295,
      "step": 16650
    },
    {
      "epoch": 11.553398058252426,
      "grad_norm": 2.2616066932678223,
      "learning_rate": 0.00045325420182947473,
      "loss": 0.3735,
      "step": 16660
    },
    {
      "epoch": 11.560332871012482,
      "grad_norm": 0.984702467918396,
      "learning_rate": 0.00045308380551299746,
      "loss": 0.2586,
      "step": 16670
    },
    {
      "epoch": 11.567267683772538,
      "grad_norm": 1.290800929069519,
      "learning_rate": 0.0004529134091965202,
      "loss": 0.3652,
      "step": 16680
    },
    {
      "epoch": 11.574202496532594,
      "grad_norm": 7.151215553283691,
      "learning_rate": 0.00045274301288004303,
      "loss": 0.2415,
      "step": 16690
    },
    {
      "epoch": 11.581137309292648,
      "grad_norm": 1.50279700756073,
      "learning_rate": 0.0004525726165635657,
      "loss": 0.443,
      "step": 16700
    },
    {
      "epoch": 11.588072122052704,
      "grad_norm": 4.846193790435791,
      "learning_rate": 0.00045240222024708844,
      "loss": 0.3874,
      "step": 16710
    },
    {
      "epoch": 11.59500693481276,
      "grad_norm": 1.7748041152954102,
      "learning_rate": 0.0004522318239306113,
      "loss": 0.3459,
      "step": 16720
    },
    {
      "epoch": 11.601941747572816,
      "grad_norm": 2.1857216358184814,
      "learning_rate": 0.000452061427614134,
      "loss": 0.2887,
      "step": 16730
    },
    {
      "epoch": 11.60887656033287,
      "grad_norm": 2.16359543800354,
      "learning_rate": 0.00045189103129765674,
      "loss": 0.3895,
      "step": 16740
    },
    {
      "epoch": 11.615811373092926,
      "grad_norm": 2.285080909729004,
      "learning_rate": 0.0004517206349811795,
      "loss": 0.3452,
      "step": 16750
    },
    {
      "epoch": 11.622746185852982,
      "grad_norm": 1.6995686292648315,
      "learning_rate": 0.00045155023866470226,
      "loss": 0.2933,
      "step": 16760
    },
    {
      "epoch": 11.629680998613038,
      "grad_norm": 3.6768860816955566,
      "learning_rate": 0.000451379842348225,
      "loss": 0.3705,
      "step": 16770
    },
    {
      "epoch": 11.636615811373092,
      "grad_norm": 2.402545213699341,
      "learning_rate": 0.0004512094460317478,
      "loss": 0.3313,
      "step": 16780
    },
    {
      "epoch": 11.643550624133148,
      "grad_norm": 2.1056323051452637,
      "learning_rate": 0.0004510390497152705,
      "loss": 0.3235,
      "step": 16790
    },
    {
      "epoch": 11.650485436893204,
      "grad_norm": 0.9546226263046265,
      "learning_rate": 0.00045086865339879324,
      "loss": 0.4254,
      "step": 16800
    },
    {
      "epoch": 11.65742024965326,
      "grad_norm": 2.646172523498535,
      "learning_rate": 0.0004506982570823161,
      "loss": 0.2986,
      "step": 16810
    },
    {
      "epoch": 11.664355062413314,
      "grad_norm": 2.1350371837615967,
      "learning_rate": 0.0004505278607658388,
      "loss": 0.4045,
      "step": 16820
    },
    {
      "epoch": 11.67128987517337,
      "grad_norm": 0.8816059231758118,
      "learning_rate": 0.00045035746444936154,
      "loss": 0.3167,
      "step": 16830
    },
    {
      "epoch": 11.678224687933426,
      "grad_norm": 1.7915414571762085,
      "learning_rate": 0.0004501870681328843,
      "loss": 0.3543,
      "step": 16840
    },
    {
      "epoch": 11.685159500693482,
      "grad_norm": 1.6184791326522827,
      "learning_rate": 0.00045001667181640705,
      "loss": 0.3761,
      "step": 16850
    },
    {
      "epoch": 11.692094313453536,
      "grad_norm": 1.5791645050048828,
      "learning_rate": 0.0004498462754999298,
      "loss": 0.3083,
      "step": 16860
    },
    {
      "epoch": 11.699029126213592,
      "grad_norm": 2.3061106204986572,
      "learning_rate": 0.00044967587918345257,
      "loss": 0.3627,
      "step": 16870
    },
    {
      "epoch": 11.705963938973648,
      "grad_norm": 1.6626578569412231,
      "learning_rate": 0.0004495054828669753,
      "loss": 0.2479,
      "step": 16880
    },
    {
      "epoch": 11.712898751733704,
      "grad_norm": 5.697100639343262,
      "learning_rate": 0.00044933508655049803,
      "loss": 0.3387,
      "step": 16890
    },
    {
      "epoch": 11.719833564493758,
      "grad_norm": 1.711822509765625,
      "learning_rate": 0.0004491646902340208,
      "loss": 0.4748,
      "step": 16900
    },
    {
      "epoch": 11.726768377253814,
      "grad_norm": 1.8673325777053833,
      "learning_rate": 0.00044899429391754355,
      "loss": 0.3172,
      "step": 16910
    },
    {
      "epoch": 11.73370319001387,
      "grad_norm": 2.3767507076263428,
      "learning_rate": 0.0004488238976010663,
      "loss": 0.3211,
      "step": 16920
    },
    {
      "epoch": 11.740638002773926,
      "grad_norm": 2.370020627975464,
      "learning_rate": 0.0004486535012845891,
      "loss": 0.344,
      "step": 16930
    },
    {
      "epoch": 11.74757281553398,
      "grad_norm": 1.9474565982818604,
      "learning_rate": 0.00044848310496811185,
      "loss": 0.3342,
      "step": 16940
    },
    {
      "epoch": 11.754507628294036,
      "grad_norm": 2.455885887145996,
      "learning_rate": 0.0004483127086516346,
      "loss": 0.3458,
      "step": 16950
    },
    {
      "epoch": 11.761442441054092,
      "grad_norm": 1.9355288743972778,
      "learning_rate": 0.00044814231233515736,
      "loss": 0.4261,
      "step": 16960
    },
    {
      "epoch": 11.768377253814148,
      "grad_norm": 1.4938308000564575,
      "learning_rate": 0.0004479719160186801,
      "loss": 0.4064,
      "step": 16970
    },
    {
      "epoch": 11.775312066574202,
      "grad_norm": 1.4783356189727783,
      "learning_rate": 0.0004478015197022028,
      "loss": 0.3263,
      "step": 16980
    },
    {
      "epoch": 11.782246879334258,
      "grad_norm": 2.9247751235961914,
      "learning_rate": 0.0004476311233857256,
      "loss": 0.3499,
      "step": 16990
    },
    {
      "epoch": 11.789181692094314,
      "grad_norm": 3.7051820755004883,
      "learning_rate": 0.00044746072706924834,
      "loss": 0.3551,
      "step": 17000
    },
    {
      "epoch": 11.79611650485437,
      "grad_norm": 2.598801374435425,
      "learning_rate": 0.0004472903307527711,
      "loss": 0.3822,
      "step": 17010
    },
    {
      "epoch": 11.803051317614424,
      "grad_norm": 1.8168364763259888,
      "learning_rate": 0.0004471199344362939,
      "loss": 0.4147,
      "step": 17020
    },
    {
      "epoch": 11.80998613037448,
      "grad_norm": 2.3171331882476807,
      "learning_rate": 0.0004469495381198166,
      "loss": 0.3851,
      "step": 17030
    },
    {
      "epoch": 11.816920943134535,
      "grad_norm": 1.6115403175354004,
      "learning_rate": 0.0004467791418033393,
      "loss": 0.3638,
      "step": 17040
    },
    {
      "epoch": 11.823855755894591,
      "grad_norm": 2.0647130012512207,
      "learning_rate": 0.00044660874548686216,
      "loss": 0.377,
      "step": 17050
    },
    {
      "epoch": 11.830790568654646,
      "grad_norm": 3.2475128173828125,
      "learning_rate": 0.0004464383491703849,
      "loss": 0.2875,
      "step": 17060
    },
    {
      "epoch": 11.837725381414701,
      "grad_norm": 4.056009292602539,
      "learning_rate": 0.0004462679528539076,
      "loss": 0.2878,
      "step": 17070
    },
    {
      "epoch": 11.844660194174757,
      "grad_norm": 2.6790354251861572,
      "learning_rate": 0.0004460975565374304,
      "loss": 0.3634,
      "step": 17080
    },
    {
      "epoch": 11.851595006934813,
      "grad_norm": 4.081317901611328,
      "learning_rate": 0.00044592716022095314,
      "loss": 0.3475,
      "step": 17090
    },
    {
      "epoch": 11.858529819694867,
      "grad_norm": 1.2444416284561157,
      "learning_rate": 0.00044575676390447587,
      "loss": 0.3506,
      "step": 17100
    },
    {
      "epoch": 11.865464632454923,
      "grad_norm": 3.1298649311065674,
      "learning_rate": 0.00044558636758799865,
      "loss": 0.2864,
      "step": 17110
    },
    {
      "epoch": 11.87239944521498,
      "grad_norm": 1.6040642261505127,
      "learning_rate": 0.0004454159712715214,
      "loss": 0.3027,
      "step": 17120
    },
    {
      "epoch": 11.879334257975035,
      "grad_norm": 4.720376968383789,
      "learning_rate": 0.0004452455749550441,
      "loss": 0.4458,
      "step": 17130
    },
    {
      "epoch": 11.88626907073509,
      "grad_norm": 1.7223200798034668,
      "learning_rate": 0.00044507517863856696,
      "loss": 0.3118,
      "step": 17140
    },
    {
      "epoch": 11.893203883495145,
      "grad_norm": 0.8895152807235718,
      "learning_rate": 0.0004449047823220897,
      "loss": 0.3569,
      "step": 17150
    },
    {
      "epoch": 11.900138696255201,
      "grad_norm": 2.068523645401001,
      "learning_rate": 0.00044473438600561236,
      "loss": 0.3333,
      "step": 17160
    },
    {
      "epoch": 11.907073509015257,
      "grad_norm": 2.8448164463043213,
      "learning_rate": 0.0004445639896891352,
      "loss": 0.3147,
      "step": 17170
    },
    {
      "epoch": 11.914008321775313,
      "grad_norm": 2.1254284381866455,
      "learning_rate": 0.00044439359337265793,
      "loss": 0.3231,
      "step": 17180
    },
    {
      "epoch": 11.920943134535367,
      "grad_norm": 2.198045492172241,
      "learning_rate": 0.00044422319705618066,
      "loss": 0.3954,
      "step": 17190
    },
    {
      "epoch": 11.927877947295423,
      "grad_norm": 4.537614822387695,
      "learning_rate": 0.00044405280073970345,
      "loss": 0.3644,
      "step": 17200
    },
    {
      "epoch": 11.934812760055479,
      "grad_norm": 1.3485662937164307,
      "learning_rate": 0.0004438824044232262,
      "loss": 0.3012,
      "step": 17210
    },
    {
      "epoch": 11.941747572815533,
      "grad_norm": 5.296013355255127,
      "learning_rate": 0.0004437120081067489,
      "loss": 0.3143,
      "step": 17220
    },
    {
      "epoch": 11.948682385575589,
      "grad_norm": 2.8471055030822754,
      "learning_rate": 0.0004435416117902717,
      "loss": 0.34,
      "step": 17230
    },
    {
      "epoch": 11.955617198335645,
      "grad_norm": 2.9448983669281006,
      "learning_rate": 0.00044337121547379443,
      "loss": 0.3406,
      "step": 17240
    },
    {
      "epoch": 11.9625520110957,
      "grad_norm": 1.0258480310440063,
      "learning_rate": 0.00044320081915731716,
      "loss": 0.3522,
      "step": 17250
    },
    {
      "epoch": 11.969486823855757,
      "grad_norm": 1.8578975200653076,
      "learning_rate": 0.00044303042284084,
      "loss": 0.3418,
      "step": 17260
    },
    {
      "epoch": 11.976421636615811,
      "grad_norm": 1.3604075908660889,
      "learning_rate": 0.00044286002652436273,
      "loss": 0.3651,
      "step": 17270
    },
    {
      "epoch": 11.983356449375867,
      "grad_norm": 2.6824982166290283,
      "learning_rate": 0.00044268963020788546,
      "loss": 0.4225,
      "step": 17280
    },
    {
      "epoch": 11.990291262135923,
      "grad_norm": 1.5706099271774292,
      "learning_rate": 0.00044251923389140824,
      "loss": 0.3479,
      "step": 17290
    },
    {
      "epoch": 11.997226074895977,
      "grad_norm": 1.1648155450820923,
      "learning_rate": 0.000442348837574931,
      "loss": 0.3263,
      "step": 17300
    },
    {
      "epoch": 12.0,
      "eval_accuracy": 0.8428720083246618,
      "eval_auc": 0.8428535816070093,
      "eval_f1": 0.8340051300842799,
      "eval_loss": 0.3993834853172302,
      "eval_mcc": 0.6896282310352807,
      "eval_precision": 0.8835403726708074,
      "eval_recall": 0.7897293546148508,
      "eval_runtime": 78.8869,
      "eval_samples_per_second": 36.546,
      "eval_steps_per_second": 2.294,
      "step": 17304
    },
    {
      "epoch": 12.004160887656033,
      "grad_norm": 1.8531782627105713,
      "learning_rate": 0.0004421784412584537,
      "loss": 0.2652,
      "step": 17310
    },
    {
      "epoch": 12.011095700416089,
      "grad_norm": 1.2399059534072876,
      "learning_rate": 0.0004420080449419765,
      "loss": 0.2753,
      "step": 17320
    },
    {
      "epoch": 12.018030513176145,
      "grad_norm": 6.964696884155273,
      "learning_rate": 0.0004418376486254992,
      "loss": 0.2911,
      "step": 17330
    },
    {
      "epoch": 12.0249653259362,
      "grad_norm": 3.6154143810272217,
      "learning_rate": 0.00044166725230902195,
      "loss": 0.2478,
      "step": 17340
    },
    {
      "epoch": 12.031900138696255,
      "grad_norm": 2.930004835128784,
      "learning_rate": 0.0004414968559925448,
      "loss": 0.2792,
      "step": 17350
    },
    {
      "epoch": 12.03883495145631,
      "grad_norm": 1.2585831880569458,
      "learning_rate": 0.00044132645967606747,
      "loss": 0.2912,
      "step": 17360
    },
    {
      "epoch": 12.045769764216367,
      "grad_norm": 2.9416379928588867,
      "learning_rate": 0.0004411560633595902,
      "loss": 0.3184,
      "step": 17370
    },
    {
      "epoch": 12.052704576976423,
      "grad_norm": 3.2681498527526855,
      "learning_rate": 0.00044098566704311304,
      "loss": 0.3644,
      "step": 17380
    },
    {
      "epoch": 12.059639389736477,
      "grad_norm": 2.189401388168335,
      "learning_rate": 0.00044081527072663577,
      "loss": 0.3056,
      "step": 17390
    },
    {
      "epoch": 12.066574202496533,
      "grad_norm": 3.3798067569732666,
      "learning_rate": 0.0004406448744101585,
      "loss": 0.3141,
      "step": 17400
    },
    {
      "epoch": 12.073509015256588,
      "grad_norm": 1.7129604816436768,
      "learning_rate": 0.0004404744780936813,
      "loss": 0.2559,
      "step": 17410
    },
    {
      "epoch": 12.080443828016644,
      "grad_norm": 2.2528975009918213,
      "learning_rate": 0.000440304081777204,
      "loss": 0.3622,
      "step": 17420
    },
    {
      "epoch": 12.087378640776699,
      "grad_norm": 1.7725013494491577,
      "learning_rate": 0.00044013368546072675,
      "loss": 0.3741,
      "step": 17430
    },
    {
      "epoch": 12.094313453536754,
      "grad_norm": 2.222749948501587,
      "learning_rate": 0.00043996328914424953,
      "loss": 0.3399,
      "step": 17440
    },
    {
      "epoch": 12.10124826629681,
      "grad_norm": 1.6492072343826294,
      "learning_rate": 0.00043979289282777227,
      "loss": 0.2933,
      "step": 17450
    },
    {
      "epoch": 12.108183079056866,
      "grad_norm": 1.6548426151275635,
      "learning_rate": 0.000439622496511295,
      "loss": 0.3307,
      "step": 17460
    },
    {
      "epoch": 12.11511789181692,
      "grad_norm": 1.6444590091705322,
      "learning_rate": 0.00043945210019481784,
      "loss": 0.2927,
      "step": 17470
    },
    {
      "epoch": 12.122052704576976,
      "grad_norm": 2.7226650714874268,
      "learning_rate": 0.00043928170387834057,
      "loss": 0.439,
      "step": 17480
    },
    {
      "epoch": 12.128987517337032,
      "grad_norm": 2.6756539344787598,
      "learning_rate": 0.00043911130756186324,
      "loss": 0.317,
      "step": 17490
    },
    {
      "epoch": 12.135922330097088,
      "grad_norm": 1.635077714920044,
      "learning_rate": 0.0004389409112453861,
      "loss": 0.345,
      "step": 17500
    },
    {
      "epoch": 12.142857142857142,
      "grad_norm": 2.956583261489868,
      "learning_rate": 0.0004387705149289088,
      "loss": 0.4038,
      "step": 17510
    },
    {
      "epoch": 12.149791955617198,
      "grad_norm": 2.5252890586853027,
      "learning_rate": 0.00043860011861243154,
      "loss": 0.2769,
      "step": 17520
    },
    {
      "epoch": 12.156726768377254,
      "grad_norm": 1.9303232431411743,
      "learning_rate": 0.00043842972229595433,
      "loss": 0.3023,
      "step": 17530
    },
    {
      "epoch": 12.16366158113731,
      "grad_norm": 3.7934534549713135,
      "learning_rate": 0.00043825932597947706,
      "loss": 0.327,
      "step": 17540
    },
    {
      "epoch": 12.170596393897364,
      "grad_norm": 1.500038504600525,
      "learning_rate": 0.0004380889296629998,
      "loss": 0.3365,
      "step": 17550
    },
    {
      "epoch": 12.17753120665742,
      "grad_norm": 2.85353946685791,
      "learning_rate": 0.0004379185333465226,
      "loss": 0.4102,
      "step": 17560
    },
    {
      "epoch": 12.184466019417476,
      "grad_norm": 2.8061304092407227,
      "learning_rate": 0.0004377481370300453,
      "loss": 0.272,
      "step": 17570
    },
    {
      "epoch": 12.191400832177532,
      "grad_norm": 4.497204303741455,
      "learning_rate": 0.00043757774071356804,
      "loss": 0.325,
      "step": 17580
    },
    {
      "epoch": 12.198335644937586,
      "grad_norm": 2.2955431938171387,
      "learning_rate": 0.0004374073443970909,
      "loss": 0.3681,
      "step": 17590
    },
    {
      "epoch": 12.205270457697642,
      "grad_norm": 2.0679290294647217,
      "learning_rate": 0.0004372369480806136,
      "loss": 0.2737,
      "step": 17600
    },
    {
      "epoch": 12.212205270457698,
      "grad_norm": 4.2779388427734375,
      "learning_rate": 0.00043706655176413634,
      "loss": 0.3469,
      "step": 17610
    },
    {
      "epoch": 12.219140083217754,
      "grad_norm": 1.376226782798767,
      "learning_rate": 0.0004368961554476591,
      "loss": 0.2691,
      "step": 17620
    },
    {
      "epoch": 12.226074895977808,
      "grad_norm": 4.340633392333984,
      "learning_rate": 0.00043672575913118186,
      "loss": 0.3598,
      "step": 17630
    },
    {
      "epoch": 12.233009708737864,
      "grad_norm": 2.215993881225586,
      "learning_rate": 0.0004365553628147046,
      "loss": 0.3021,
      "step": 17640
    },
    {
      "epoch": 12.23994452149792,
      "grad_norm": 1.1729904413223267,
      "learning_rate": 0.00043638496649822737,
      "loss": 0.2483,
      "step": 17650
    },
    {
      "epoch": 12.246879334257976,
      "grad_norm": 3.3677592277526855,
      "learning_rate": 0.0004362145701817501,
      "loss": 0.4823,
      "step": 17660
    },
    {
      "epoch": 12.25381414701803,
      "grad_norm": 1.5256946086883545,
      "learning_rate": 0.00043604417386527283,
      "loss": 0.2921,
      "step": 17670
    },
    {
      "epoch": 12.260748959778086,
      "grad_norm": 1.2473629713058472,
      "learning_rate": 0.0004358737775487957,
      "loss": 0.4134,
      "step": 17680
    },
    {
      "epoch": 12.267683772538142,
      "grad_norm": 5.1647162437438965,
      "learning_rate": 0.00043570338123231835,
      "loss": 0.3551,
      "step": 17690
    },
    {
      "epoch": 12.274618585298198,
      "grad_norm": 2.9337093830108643,
      "learning_rate": 0.0004355329849158411,
      "loss": 0.3951,
      "step": 17700
    },
    {
      "epoch": 12.281553398058252,
      "grad_norm": 2.127584934234619,
      "learning_rate": 0.0004353625885993639,
      "loss": 0.3659,
      "step": 17710
    },
    {
      "epoch": 12.288488210818308,
      "grad_norm": 1.065909743309021,
      "learning_rate": 0.00043519219228288665,
      "loss": 0.3125,
      "step": 17720
    },
    {
      "epoch": 12.295423023578364,
      "grad_norm": 1.8193936347961426,
      "learning_rate": 0.0004350217959664094,
      "loss": 0.2691,
      "step": 17730
    },
    {
      "epoch": 12.30235783633842,
      "grad_norm": 3.3656675815582275,
      "learning_rate": 0.00043485139964993217,
      "loss": 0.3015,
      "step": 17740
    },
    {
      "epoch": 12.309292649098474,
      "grad_norm": 1.0891497135162354,
      "learning_rate": 0.0004346810033334549,
      "loss": 0.3449,
      "step": 17750
    },
    {
      "epoch": 12.31622746185853,
      "grad_norm": 2.496650457382202,
      "learning_rate": 0.00043451060701697763,
      "loss": 0.3961,
      "step": 17760
    },
    {
      "epoch": 12.323162274618586,
      "grad_norm": 1.9341905117034912,
      "learning_rate": 0.0004343402107005004,
      "loss": 0.3772,
      "step": 17770
    },
    {
      "epoch": 12.330097087378642,
      "grad_norm": 2.011669874191284,
      "learning_rate": 0.00043416981438402315,
      "loss": 0.3445,
      "step": 17780
    },
    {
      "epoch": 12.337031900138696,
      "grad_norm": 1.0249770879745483,
      "learning_rate": 0.0004339994180675459,
      "loss": 0.2793,
      "step": 17790
    },
    {
      "epoch": 12.343966712898752,
      "grad_norm": 0.8047107458114624,
      "learning_rate": 0.0004338290217510687,
      "loss": 0.2382,
      "step": 17800
    },
    {
      "epoch": 12.350901525658808,
      "grad_norm": 1.878745675086975,
      "learning_rate": 0.00043365862543459145,
      "loss": 0.4129,
      "step": 17810
    },
    {
      "epoch": 12.357836338418863,
      "grad_norm": 1.8751798868179321,
      "learning_rate": 0.0004334882291181141,
      "loss": 0.4001,
      "step": 17820
    },
    {
      "epoch": 12.364771151178918,
      "grad_norm": 1.242044448852539,
      "learning_rate": 0.00043331783280163696,
      "loss": 0.3558,
      "step": 17830
    },
    {
      "epoch": 12.371705963938973,
      "grad_norm": 2.389021396636963,
      "learning_rate": 0.0004331474364851597,
      "loss": 0.2561,
      "step": 17840
    },
    {
      "epoch": 12.37864077669903,
      "grad_norm": 2.2284648418426514,
      "learning_rate": 0.0004329770401686824,
      "loss": 0.3463,
      "step": 17850
    },
    {
      "epoch": 12.385575589459085,
      "grad_norm": 2.961641788482666,
      "learning_rate": 0.0004328066438522052,
      "loss": 0.1881,
      "step": 17860
    },
    {
      "epoch": 12.39251040221914,
      "grad_norm": 3.9888100624084473,
      "learning_rate": 0.00043263624753572794,
      "loss": 0.3562,
      "step": 17870
    },
    {
      "epoch": 12.399445214979195,
      "grad_norm": 4.558578014373779,
      "learning_rate": 0.00043246585121925067,
      "loss": 0.367,
      "step": 17880
    },
    {
      "epoch": 12.406380027739251,
      "grad_norm": 2.073679208755493,
      "learning_rate": 0.00043229545490277346,
      "loss": 0.3169,
      "step": 17890
    },
    {
      "epoch": 12.413314840499307,
      "grad_norm": 1.8744324445724487,
      "learning_rate": 0.0004321250585862962,
      "loss": 0.431,
      "step": 17900
    },
    {
      "epoch": 12.420249653259361,
      "grad_norm": 1.0650238990783691,
      "learning_rate": 0.0004319546622698189,
      "loss": 0.1971,
      "step": 17910
    },
    {
      "epoch": 12.427184466019417,
      "grad_norm": 1.6094708442687988,
      "learning_rate": 0.00043178426595334176,
      "loss": 0.346,
      "step": 17920
    },
    {
      "epoch": 12.434119278779473,
      "grad_norm": 2.9246039390563965,
      "learning_rate": 0.0004316138696368645,
      "loss": 0.3949,
      "step": 17930
    },
    {
      "epoch": 12.44105409153953,
      "grad_norm": 2.904886245727539,
      "learning_rate": 0.0004314434733203872,
      "loss": 0.3856,
      "step": 17940
    },
    {
      "epoch": 12.447988904299583,
      "grad_norm": 1.8262183666229248,
      "learning_rate": 0.00043127307700391,
      "loss": 0.3216,
      "step": 17950
    },
    {
      "epoch": 12.45492371705964,
      "grad_norm": 2.085099697113037,
      "learning_rate": 0.00043110268068743274,
      "loss": 0.2791,
      "step": 17960
    },
    {
      "epoch": 12.461858529819695,
      "grad_norm": 3.664046049118042,
      "learning_rate": 0.00043093228437095547,
      "loss": 0.2931,
      "step": 17970
    },
    {
      "epoch": 12.468793342579751,
      "grad_norm": 2.783808946609497,
      "learning_rate": 0.00043076188805447825,
      "loss": 0.3302,
      "step": 17980
    },
    {
      "epoch": 12.475728155339805,
      "grad_norm": 1.9656097888946533,
      "learning_rate": 0.000430591491738001,
      "loss": 0.2649,
      "step": 17990
    },
    {
      "epoch": 12.482662968099861,
      "grad_norm": 3.3042449951171875,
      "learning_rate": 0.0004304210954215237,
      "loss": 0.271,
      "step": 18000
    },
    {
      "epoch": 12.489597780859917,
      "grad_norm": 1.7108376026153564,
      "learning_rate": 0.00043025069910504655,
      "loss": 0.3611,
      "step": 18010
    },
    {
      "epoch": 12.496532593619973,
      "grad_norm": 3.94218373298645,
      "learning_rate": 0.00043008030278856923,
      "loss": 0.41,
      "step": 18020
    },
    {
      "epoch": 12.503467406380027,
      "grad_norm": 1.4095041751861572,
      "learning_rate": 0.00042990990647209196,
      "loss": 0.3356,
      "step": 18030
    },
    {
      "epoch": 12.510402219140083,
      "grad_norm": 2.4416370391845703,
      "learning_rate": 0.0004297395101556148,
      "loss": 0.2831,
      "step": 18040
    },
    {
      "epoch": 12.517337031900139,
      "grad_norm": 1.0984737873077393,
      "learning_rate": 0.00042956911383913753,
      "loss": 0.2899,
      "step": 18050
    },
    {
      "epoch": 12.524271844660195,
      "grad_norm": 1.520275592803955,
      "learning_rate": 0.00042939871752266026,
      "loss": 0.2592,
      "step": 18060
    },
    {
      "epoch": 12.531206657420249,
      "grad_norm": 1.4078919887542725,
      "learning_rate": 0.00042922832120618305,
      "loss": 0.3864,
      "step": 18070
    },
    {
      "epoch": 12.538141470180305,
      "grad_norm": 2.290689468383789,
      "learning_rate": 0.0004290579248897058,
      "loss": 0.378,
      "step": 18080
    },
    {
      "epoch": 12.54507628294036,
      "grad_norm": 4.7494635581970215,
      "learning_rate": 0.0004288875285732285,
      "loss": 0.3501,
      "step": 18090
    },
    {
      "epoch": 12.552011095700417,
      "grad_norm": 2.1595726013183594,
      "learning_rate": 0.0004287171322567513,
      "loss": 0.353,
      "step": 18100
    },
    {
      "epoch": 12.55894590846047,
      "grad_norm": 2.4153146743774414,
      "learning_rate": 0.000428546735940274,
      "loss": 0.337,
      "step": 18110
    },
    {
      "epoch": 12.565880721220527,
      "grad_norm": 1.6141899824142456,
      "learning_rate": 0.00042837633962379676,
      "loss": 0.3223,
      "step": 18120
    },
    {
      "epoch": 12.572815533980583,
      "grad_norm": 1.83565092086792,
      "learning_rate": 0.0004282059433073196,
      "loss": 0.4327,
      "step": 18130
    },
    {
      "epoch": 12.579750346740639,
      "grad_norm": 2.0930159091949463,
      "learning_rate": 0.00042803554699084233,
      "loss": 0.3106,
      "step": 18140
    },
    {
      "epoch": 12.586685159500693,
      "grad_norm": 1.4508240222930908,
      "learning_rate": 0.000427865150674365,
      "loss": 0.2394,
      "step": 18150
    },
    {
      "epoch": 12.593619972260749,
      "grad_norm": 3.1416218280792236,
      "learning_rate": 0.00042769475435788784,
      "loss": 0.3481,
      "step": 18160
    },
    {
      "epoch": 12.600554785020805,
      "grad_norm": 3.3840949535369873,
      "learning_rate": 0.0004275243580414106,
      "loss": 0.3281,
      "step": 18170
    },
    {
      "epoch": 12.60748959778086,
      "grad_norm": 1.9369266033172607,
      "learning_rate": 0.0004273539617249333,
      "loss": 0.3163,
      "step": 18180
    },
    {
      "epoch": 12.614424410540915,
      "grad_norm": 2.881016731262207,
      "learning_rate": 0.0004271835654084561,
      "loss": 0.3368,
      "step": 18190
    },
    {
      "epoch": 12.62135922330097,
      "grad_norm": 2.735839366912842,
      "learning_rate": 0.0004270131690919788,
      "loss": 0.3175,
      "step": 18200
    },
    {
      "epoch": 12.628294036061027,
      "grad_norm": 2.658733606338501,
      "learning_rate": 0.00042684277277550155,
      "loss": 0.3548,
      "step": 18210
    },
    {
      "epoch": 12.635228848821082,
      "grad_norm": 2.4589991569519043,
      "learning_rate": 0.00042667237645902434,
      "loss": 0.3465,
      "step": 18220
    },
    {
      "epoch": 12.642163661581137,
      "grad_norm": 1.9096143245697021,
      "learning_rate": 0.00042650198014254707,
      "loss": 0.3383,
      "step": 18230
    },
    {
      "epoch": 12.649098474341192,
      "grad_norm": 1.9342602491378784,
      "learning_rate": 0.0004263315838260698,
      "loss": 0.2886,
      "step": 18240
    },
    {
      "epoch": 12.656033287101248,
      "grad_norm": 2.9886350631713867,
      "learning_rate": 0.00042616118750959264,
      "loss": 0.4861,
      "step": 18250
    },
    {
      "epoch": 12.662968099861304,
      "grad_norm": 2.0033771991729736,
      "learning_rate": 0.00042599079119311537,
      "loss": 0.3308,
      "step": 18260
    },
    {
      "epoch": 12.669902912621358,
      "grad_norm": 3.5446364879608154,
      "learning_rate": 0.0004258203948766381,
      "loss": 0.3124,
      "step": 18270
    },
    {
      "epoch": 12.676837725381414,
      "grad_norm": 2.614424705505371,
      "learning_rate": 0.0004256499985601609,
      "loss": 0.3072,
      "step": 18280
    },
    {
      "epoch": 12.68377253814147,
      "grad_norm": 1.8660334348678589,
      "learning_rate": 0.0004254796022436836,
      "loss": 0.3378,
      "step": 18290
    },
    {
      "epoch": 12.690707350901526,
      "grad_norm": 3.6773195266723633,
      "learning_rate": 0.00042530920592720635,
      "loss": 0.3129,
      "step": 18300
    },
    {
      "epoch": 12.69764216366158,
      "grad_norm": 1.9032597541809082,
      "learning_rate": 0.00042513880961072913,
      "loss": 0.3342,
      "step": 18310
    },
    {
      "epoch": 12.704576976421636,
      "grad_norm": 4.148677349090576,
      "learning_rate": 0.00042496841329425186,
      "loss": 0.2916,
      "step": 18320
    },
    {
      "epoch": 12.711511789181692,
      "grad_norm": 3.486619472503662,
      "learning_rate": 0.0004247980169777746,
      "loss": 0.3476,
      "step": 18330
    },
    {
      "epoch": 12.718446601941748,
      "grad_norm": 2.674846649169922,
      "learning_rate": 0.00042462762066129743,
      "loss": 0.3178,
      "step": 18340
    },
    {
      "epoch": 12.725381414701802,
      "grad_norm": 3.543945550918579,
      "learning_rate": 0.0004244572243448201,
      "loss": 0.3914,
      "step": 18350
    },
    {
      "epoch": 12.732316227461858,
      "grad_norm": 4.236156463623047,
      "learning_rate": 0.00042428682802834284,
      "loss": 0.3284,
      "step": 18360
    },
    {
      "epoch": 12.739251040221914,
      "grad_norm": 2.2670023441314697,
      "learning_rate": 0.0004241164317118657,
      "loss": 0.2837,
      "step": 18370
    },
    {
      "epoch": 12.74618585298197,
      "grad_norm": 5.684597969055176,
      "learning_rate": 0.0004239460353953884,
      "loss": 0.3325,
      "step": 18380
    },
    {
      "epoch": 12.753120665742024,
      "grad_norm": 5.082701206207275,
      "learning_rate": 0.00042377563907891114,
      "loss": 0.3113,
      "step": 18390
    },
    {
      "epoch": 12.76005547850208,
      "grad_norm": 1.757273554801941,
      "learning_rate": 0.00042360524276243393,
      "loss": 0.3956,
      "step": 18400
    },
    {
      "epoch": 12.766990291262136,
      "grad_norm": 1.9086883068084717,
      "learning_rate": 0.00042343484644595666,
      "loss": 0.295,
      "step": 18410
    },
    {
      "epoch": 12.773925104022192,
      "grad_norm": 3.4640581607818604,
      "learning_rate": 0.0004232644501294794,
      "loss": 0.4091,
      "step": 18420
    },
    {
      "epoch": 12.780859916782246,
      "grad_norm": 2.1587300300598145,
      "learning_rate": 0.0004230940538130022,
      "loss": 0.2206,
      "step": 18430
    },
    {
      "epoch": 12.787794729542302,
      "grad_norm": 3.1849899291992188,
      "learning_rate": 0.0004229236574965249,
      "loss": 0.3555,
      "step": 18440
    },
    {
      "epoch": 12.794729542302358,
      "grad_norm": 5.736027240753174,
      "learning_rate": 0.00042275326118004764,
      "loss": 0.2406,
      "step": 18450
    },
    {
      "epoch": 12.801664355062414,
      "grad_norm": 2.101968288421631,
      "learning_rate": 0.0004225828648635705,
      "loss": 0.2677,
      "step": 18460
    },
    {
      "epoch": 12.808599167822468,
      "grad_norm": 2.623117685317993,
      "learning_rate": 0.0004224124685470932,
      "loss": 0.3233,
      "step": 18470
    },
    {
      "epoch": 12.815533980582524,
      "grad_norm": 1.3931947946548462,
      "learning_rate": 0.0004222420722306159,
      "loss": 0.2619,
      "step": 18480
    },
    {
      "epoch": 12.82246879334258,
      "grad_norm": 1.4015517234802246,
      "learning_rate": 0.0004220716759141387,
      "loss": 0.314,
      "step": 18490
    },
    {
      "epoch": 12.829403606102636,
      "grad_norm": 2.2148704528808594,
      "learning_rate": 0.00042190127959766145,
      "loss": 0.5407,
      "step": 18500
    },
    {
      "epoch": 12.83633841886269,
      "grad_norm": 2.280965566635132,
      "learning_rate": 0.0004217308832811842,
      "loss": 0.4497,
      "step": 18510
    },
    {
      "epoch": 12.843273231622746,
      "grad_norm": 1.3052432537078857,
      "learning_rate": 0.00042156048696470697,
      "loss": 0.3019,
      "step": 18520
    },
    {
      "epoch": 12.850208044382802,
      "grad_norm": 2.038665294647217,
      "learning_rate": 0.0004213900906482297,
      "loss": 0.322,
      "step": 18530
    },
    {
      "epoch": 12.857142857142858,
      "grad_norm": 1.4931221008300781,
      "learning_rate": 0.00042121969433175243,
      "loss": 0.3673,
      "step": 18540
    },
    {
      "epoch": 12.864077669902912,
      "grad_norm": 2.065295934677124,
      "learning_rate": 0.0004210492980152752,
      "loss": 0.3213,
      "step": 18550
    },
    {
      "epoch": 12.871012482662968,
      "grad_norm": 3.8946385383605957,
      "learning_rate": 0.00042087890169879795,
      "loss": 0.4117,
      "step": 18560
    },
    {
      "epoch": 12.877947295423024,
      "grad_norm": 2.170703411102295,
      "learning_rate": 0.0004207085053823207,
      "loss": 0.3421,
      "step": 18570
    },
    {
      "epoch": 12.88488210818308,
      "grad_norm": 2.4944610595703125,
      "learning_rate": 0.0004205381090658435,
      "loss": 0.3061,
      "step": 18580
    },
    {
      "epoch": 12.891816920943135,
      "grad_norm": 1.8984402418136597,
      "learning_rate": 0.00042036771274936625,
      "loss": 0.3311,
      "step": 18590
    },
    {
      "epoch": 12.89875173370319,
      "grad_norm": 3.4816179275512695,
      "learning_rate": 0.000420197316432889,
      "loss": 0.4335,
      "step": 18600
    },
    {
      "epoch": 12.905686546463246,
      "grad_norm": 1.1281734704971313,
      "learning_rate": 0.00042002692011641177,
      "loss": 0.338,
      "step": 18610
    },
    {
      "epoch": 12.912621359223301,
      "grad_norm": 2.4560413360595703,
      "learning_rate": 0.0004198565237999345,
      "loss": 0.3496,
      "step": 18620
    },
    {
      "epoch": 12.919556171983356,
      "grad_norm": 2.4298386573791504,
      "learning_rate": 0.00041968612748345723,
      "loss": 0.2791,
      "step": 18630
    },
    {
      "epoch": 12.926490984743412,
      "grad_norm": 3.577214002609253,
      "learning_rate": 0.00041951573116698,
      "loss": 0.3552,
      "step": 18640
    },
    {
      "epoch": 12.933425797503467,
      "grad_norm": 0.8867579102516174,
      "learning_rate": 0.00041934533485050274,
      "loss": 0.3019,
      "step": 18650
    },
    {
      "epoch": 12.940360610263523,
      "grad_norm": 2.2609922885894775,
      "learning_rate": 0.0004191749385340255,
      "loss": 0.3601,
      "step": 18660
    },
    {
      "epoch": 12.94729542302358,
      "grad_norm": 1.5003336668014526,
      "learning_rate": 0.0004190045422175483,
      "loss": 0.3208,
      "step": 18670
    },
    {
      "epoch": 12.954230235783633,
      "grad_norm": 1.479023814201355,
      "learning_rate": 0.000418834145901071,
      "loss": 0.4021,
      "step": 18680
    },
    {
      "epoch": 12.96116504854369,
      "grad_norm": 2.36977219581604,
      "learning_rate": 0.0004186637495845937,
      "loss": 0.285,
      "step": 18690
    },
    {
      "epoch": 12.968099861303745,
      "grad_norm": 3.0964794158935547,
      "learning_rate": 0.00041849335326811656,
      "loss": 0.3236,
      "step": 18700
    },
    {
      "epoch": 12.9750346740638,
      "grad_norm": 4.653445243835449,
      "learning_rate": 0.0004183229569516393,
      "loss": 0.3828,
      "step": 18710
    },
    {
      "epoch": 12.981969486823855,
      "grad_norm": 1.5410584211349487,
      "learning_rate": 0.000418152560635162,
      "loss": 0.2886,
      "step": 18720
    },
    {
      "epoch": 12.988904299583911,
      "grad_norm": 2.407233715057373,
      "learning_rate": 0.0004179821643186848,
      "loss": 0.3839,
      "step": 18730
    },
    {
      "epoch": 12.995839112343967,
      "grad_norm": 1.9085636138916016,
      "learning_rate": 0.00041781176800220754,
      "loss": 0.2896,
      "step": 18740
    },
    {
      "epoch": 13.0,
      "eval_accuracy": 0.8518903919528269,
      "eval_auc": 0.8518724475702169,
      "eval_f1": 0.8437614343212586,
      "eval_loss": 0.41389602422714233,
      "eval_mcc": 0.7075604207417839,
      "eval_precision": 0.8924148606811145,
      "eval_recall": 0.8001387925052047,
      "eval_runtime": 78.9565,
      "eval_samples_per_second": 36.514,
      "eval_steps_per_second": 2.292,
      "step": 18746
    }
  ],
  "logging_steps": 10,
  "max_steps": 43260,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 30,
  "save_steps": 500,
  "total_flos": 2.6881654819499994e+17,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": {
    "_wandb": {},
    "assignments": {},
    "learning_rate": 0.0007371344650805668,
    "max_grad_norm": 1,
    "metric": "eval/loss",
    "per_device_train_batch_size": 4,
    "weight_decay": 0.2
  }
}
