{
  "best_metric": 0.8815371485166827,
  "best_model_checkpoint": "models/multi-label/v2/rand/esm2_t30_150M_UR50D_2024-12-10_15-35-47/run-cjc0tdwg/checkpoint-85300",
  "epoch": 25.0,
  "eval_steps": 500,
  "global_step": 85300,
  "is_hyper_param_search": true,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0029308323563892145,
      "grad_norm": 0.3431500494480133,
      "learning_rate": 0.0007146985368411796,
      "loss": 0.4137,
      "step": 10
    },
    {
      "epoch": 0.005861664712778429,
      "grad_norm": 0.50290846824646,
      "learning_rate": 0.0007146147405535913,
      "loss": 0.2304,
      "step": 20
    },
    {
      "epoch": 0.008792497069167644,
      "grad_norm": 0.4680326282978058,
      "learning_rate": 0.0007145309442660029,
      "loss": 0.2193,
      "step": 30
    },
    {
      "epoch": 0.011723329425556858,
      "grad_norm": 0.350372314453125,
      "learning_rate": 0.0007144471479784146,
      "loss": 0.2491,
      "step": 40
    },
    {
      "epoch": 0.014654161781946073,
      "grad_norm": 0.453129380941391,
      "learning_rate": 0.0007143633516908262,
      "loss": 0.2305,
      "step": 50
    },
    {
      "epoch": 0.017584994138335287,
      "grad_norm": 0.5196264386177063,
      "learning_rate": 0.0007142795554032378,
      "loss": 0.2371,
      "step": 60
    },
    {
      "epoch": 0.020515826494724502,
      "grad_norm": 0.31419047713279724,
      "learning_rate": 0.0007141957591156495,
      "loss": 0.2433,
      "step": 70
    },
    {
      "epoch": 0.023446658851113716,
      "grad_norm": 0.39055633544921875,
      "learning_rate": 0.0007141119628280611,
      "loss": 0.2597,
      "step": 80
    },
    {
      "epoch": 0.02637749120750293,
      "grad_norm": 0.3081972301006317,
      "learning_rate": 0.0007140281665404727,
      "loss": 0.2388,
      "step": 90
    },
    {
      "epoch": 0.029308323563892145,
      "grad_norm": 0.23380501568317413,
      "learning_rate": 0.0007139443702528843,
      "loss": 0.2161,
      "step": 100
    },
    {
      "epoch": 0.03223915592028136,
      "grad_norm": 0.21667654812335968,
      "learning_rate": 0.000713860573965296,
      "loss": 0.2185,
      "step": 110
    },
    {
      "epoch": 0.035169988276670575,
      "grad_norm": 0.347335547208786,
      "learning_rate": 0.0007137767776777077,
      "loss": 0.2308,
      "step": 120
    },
    {
      "epoch": 0.038100820633059786,
      "grad_norm": 0.49815651774406433,
      "learning_rate": 0.0007136929813901193,
      "loss": 0.1975,
      "step": 130
    },
    {
      "epoch": 0.041031652989449004,
      "grad_norm": 0.3160574436187744,
      "learning_rate": 0.0007136091851025309,
      "loss": 0.227,
      "step": 140
    },
    {
      "epoch": 0.043962485345838215,
      "grad_norm": 0.26130038499832153,
      "learning_rate": 0.0007135253888149425,
      "loss": 0.2375,
      "step": 150
    },
    {
      "epoch": 0.04689331770222743,
      "grad_norm": 0.3963715434074402,
      "learning_rate": 0.0007134415925273542,
      "loss": 0.2287,
      "step": 160
    },
    {
      "epoch": 0.049824150058616644,
      "grad_norm": 0.25844642519950867,
      "learning_rate": 0.0007133577962397658,
      "loss": 0.2151,
      "step": 170
    },
    {
      "epoch": 0.05275498241500586,
      "grad_norm": 0.2609550654888153,
      "learning_rate": 0.0007132739999521774,
      "loss": 0.2174,
      "step": 180
    },
    {
      "epoch": 0.05568581477139507,
      "grad_norm": 0.29847365617752075,
      "learning_rate": 0.000713190203664589,
      "loss": 0.2191,
      "step": 190
    },
    {
      "epoch": 0.05861664712778429,
      "grad_norm": 0.3514517545700073,
      "learning_rate": 0.0007131064073770006,
      "loss": 0.2092,
      "step": 200
    },
    {
      "epoch": 0.0615474794841735,
      "grad_norm": 0.38027387857437134,
      "learning_rate": 0.0007130226110894124,
      "loss": 0.2379,
      "step": 210
    },
    {
      "epoch": 0.06447831184056271,
      "grad_norm": 0.5385406017303467,
      "learning_rate": 0.000712938814801824,
      "loss": 0.2314,
      "step": 220
    },
    {
      "epoch": 0.06740914419695193,
      "grad_norm": 0.3304857611656189,
      "learning_rate": 0.0007128550185142356,
      "loss": 0.216,
      "step": 230
    },
    {
      "epoch": 0.07033997655334115,
      "grad_norm": 0.2234864979982376,
      "learning_rate": 0.0007127712222266472,
      "loss": 0.215,
      "step": 240
    },
    {
      "epoch": 0.07327080890973037,
      "grad_norm": 0.2928571105003357,
      "learning_rate": 0.0007126874259390588,
      "loss": 0.2161,
      "step": 250
    },
    {
      "epoch": 0.07620164126611957,
      "grad_norm": 0.3588677644729614,
      "learning_rate": 0.0007126036296514705,
      "loss": 0.227,
      "step": 260
    },
    {
      "epoch": 0.07913247362250879,
      "grad_norm": 0.3633083403110504,
      "learning_rate": 0.0007125198333638821,
      "loss": 0.2282,
      "step": 270
    },
    {
      "epoch": 0.08206330597889801,
      "grad_norm": 0.37155628204345703,
      "learning_rate": 0.0007124360370762937,
      "loss": 0.216,
      "step": 280
    },
    {
      "epoch": 0.08499413833528723,
      "grad_norm": 0.3777288496494293,
      "learning_rate": 0.0007123522407887054,
      "loss": 0.2341,
      "step": 290
    },
    {
      "epoch": 0.08792497069167643,
      "grad_norm": 0.3006057143211365,
      "learning_rate": 0.000712268444501117,
      "loss": 0.2102,
      "step": 300
    },
    {
      "epoch": 0.09085580304806565,
      "grad_norm": 0.3485446870326996,
      "learning_rate": 0.0007121846482135287,
      "loss": 0.2188,
      "step": 310
    },
    {
      "epoch": 0.09378663540445487,
      "grad_norm": 0.2598789632320404,
      "learning_rate": 0.0007121008519259403,
      "loss": 0.2148,
      "step": 320
    },
    {
      "epoch": 0.09671746776084408,
      "grad_norm": 0.3318910598754883,
      "learning_rate": 0.0007120170556383519,
      "loss": 0.2165,
      "step": 330
    },
    {
      "epoch": 0.09964830011723329,
      "grad_norm": 0.45508289337158203,
      "learning_rate": 0.0007119332593507635,
      "loss": 0.2177,
      "step": 340
    },
    {
      "epoch": 0.1025791324736225,
      "grad_norm": 0.35929328203201294,
      "learning_rate": 0.0007118494630631751,
      "loss": 0.2132,
      "step": 350
    },
    {
      "epoch": 0.10550996483001172,
      "grad_norm": 0.2550891637802124,
      "learning_rate": 0.0007117656667755868,
      "loss": 0.2211,
      "step": 360
    },
    {
      "epoch": 0.10844079718640094,
      "grad_norm": 0.2997000813484192,
      "learning_rate": 0.0007116818704879984,
      "loss": 0.2318,
      "step": 370
    },
    {
      "epoch": 0.11137162954279015,
      "grad_norm": 0.3187679946422577,
      "learning_rate": 0.0007115980742004102,
      "loss": 0.2133,
      "step": 380
    },
    {
      "epoch": 0.11430246189917936,
      "grad_norm": 0.5145382881164551,
      "learning_rate": 0.0007115142779128217,
      "loss": 0.223,
      "step": 390
    },
    {
      "epoch": 0.11723329425556858,
      "grad_norm": 0.28414270281791687,
      "learning_rate": 0.0007114304816252333,
      "loss": 0.244,
      "step": 400
    },
    {
      "epoch": 0.1201641266119578,
      "grad_norm": 0.362728089094162,
      "learning_rate": 0.000711346685337645,
      "loss": 0.2235,
      "step": 410
    },
    {
      "epoch": 0.123094958968347,
      "grad_norm": 0.42892879247665405,
      "learning_rate": 0.0007112628890500566,
      "loss": 0.2347,
      "step": 420
    },
    {
      "epoch": 0.12602579132473624,
      "grad_norm": 0.5652612447738647,
      "learning_rate": 0.0007111790927624683,
      "loss": 0.2367,
      "step": 430
    },
    {
      "epoch": 0.12895662368112543,
      "grad_norm": 0.25103265047073364,
      "learning_rate": 0.0007110952964748799,
      "loss": 0.2203,
      "step": 440
    },
    {
      "epoch": 0.13188745603751464,
      "grad_norm": 0.2661643922328949,
      "learning_rate": 0.0007110115001872915,
      "loss": 0.2286,
      "step": 450
    },
    {
      "epoch": 0.13481828839390386,
      "grad_norm": 0.3167148530483246,
      "learning_rate": 0.0007109277038997032,
      "loss": 0.2346,
      "step": 460
    },
    {
      "epoch": 0.13774912075029308,
      "grad_norm": 0.2392716407775879,
      "learning_rate": 0.0007108439076121148,
      "loss": 0.2355,
      "step": 470
    },
    {
      "epoch": 0.1406799531066823,
      "grad_norm": 0.29042014479637146,
      "learning_rate": 0.0007107601113245265,
      "loss": 0.2169,
      "step": 480
    },
    {
      "epoch": 0.14361078546307152,
      "grad_norm": 0.2828752398490906,
      "learning_rate": 0.0007106763150369381,
      "loss": 0.2343,
      "step": 490
    },
    {
      "epoch": 0.14654161781946073,
      "grad_norm": 0.3069002330303192,
      "learning_rate": 0.0007105925187493497,
      "loss": 0.2439,
      "step": 500
    },
    {
      "epoch": 0.14947245017584995,
      "grad_norm": 0.26041147112846375,
      "learning_rate": 0.0007105087224617613,
      "loss": 0.2455,
      "step": 510
    },
    {
      "epoch": 0.15240328253223914,
      "grad_norm": 0.22531139850616455,
      "learning_rate": 0.0007104249261741729,
      "loss": 0.2165,
      "step": 520
    },
    {
      "epoch": 0.15533411488862836,
      "grad_norm": 0.27336379885673523,
      "learning_rate": 0.0007103411298865846,
      "loss": 0.1951,
      "step": 530
    },
    {
      "epoch": 0.15826494724501758,
      "grad_norm": 0.2198285162448883,
      "learning_rate": 0.0007102573335989962,
      "loss": 0.2093,
      "step": 540
    },
    {
      "epoch": 0.1611957796014068,
      "grad_norm": 0.3670281767845154,
      "learning_rate": 0.0007101735373114079,
      "loss": 0.2259,
      "step": 550
    },
    {
      "epoch": 0.16412661195779601,
      "grad_norm": 0.24077896773815155,
      "learning_rate": 0.0007100897410238195,
      "loss": 0.2185,
      "step": 560
    },
    {
      "epoch": 0.16705744431418523,
      "grad_norm": 0.28311264514923096,
      "learning_rate": 0.0007100059447362311,
      "loss": 0.2007,
      "step": 570
    },
    {
      "epoch": 0.16998827667057445,
      "grad_norm": 0.33459144830703735,
      "learning_rate": 0.0007099221484486428,
      "loss": 0.213,
      "step": 580
    },
    {
      "epoch": 0.17291910902696367,
      "grad_norm": 0.3106169104576111,
      "learning_rate": 0.0007098383521610544,
      "loss": 0.225,
      "step": 590
    },
    {
      "epoch": 0.17584994138335286,
      "grad_norm": 0.30425846576690674,
      "learning_rate": 0.000709754555873466,
      "loss": 0.2269,
      "step": 600
    },
    {
      "epoch": 0.17878077373974208,
      "grad_norm": 0.3947877883911133,
      "learning_rate": 0.0007096707595858776,
      "loss": 0.2239,
      "step": 610
    },
    {
      "epoch": 0.1817116060961313,
      "grad_norm": 0.39545974135398865,
      "learning_rate": 0.0007095869632982892,
      "loss": 0.2102,
      "step": 620
    },
    {
      "epoch": 0.1846424384525205,
      "grad_norm": 0.2385302484035492,
      "learning_rate": 0.000709503167010701,
      "loss": 0.2087,
      "step": 630
    },
    {
      "epoch": 0.18757327080890973,
      "grad_norm": 0.27831047773361206,
      "learning_rate": 0.0007094193707231126,
      "loss": 0.2268,
      "step": 640
    },
    {
      "epoch": 0.19050410316529895,
      "grad_norm": 0.2994888722896576,
      "learning_rate": 0.0007093355744355242,
      "loss": 0.2194,
      "step": 650
    },
    {
      "epoch": 0.19343493552168817,
      "grad_norm": 0.4104238748550415,
      "learning_rate": 0.0007092517781479358,
      "loss": 0.2172,
      "step": 660
    },
    {
      "epoch": 0.19636576787807739,
      "grad_norm": 0.3508947789669037,
      "learning_rate": 0.0007091679818603475,
      "loss": 0.2322,
      "step": 670
    },
    {
      "epoch": 0.19929660023446658,
      "grad_norm": 0.2897152602672577,
      "learning_rate": 0.0007090841855727591,
      "loss": 0.2295,
      "step": 680
    },
    {
      "epoch": 0.2022274325908558,
      "grad_norm": 0.2679256796836853,
      "learning_rate": 0.0007090003892851707,
      "loss": 0.2269,
      "step": 690
    },
    {
      "epoch": 0.205158264947245,
      "grad_norm": 0.37026071548461914,
      "learning_rate": 0.0007089165929975823,
      "loss": 0.2167,
      "step": 700
    },
    {
      "epoch": 0.20808909730363423,
      "grad_norm": 0.2845928966999054,
      "learning_rate": 0.0007088327967099939,
      "loss": 0.2084,
      "step": 710
    },
    {
      "epoch": 0.21101992966002345,
      "grad_norm": 0.27294301986694336,
      "learning_rate": 0.0007087490004224057,
      "loss": 0.1976,
      "step": 720
    },
    {
      "epoch": 0.21395076201641267,
      "grad_norm": 0.2514020800590515,
      "learning_rate": 0.0007086652041348173,
      "loss": 0.232,
      "step": 730
    },
    {
      "epoch": 0.21688159437280188,
      "grad_norm": 0.343115895986557,
      "learning_rate": 0.0007085814078472289,
      "loss": 0.2203,
      "step": 740
    },
    {
      "epoch": 0.2198124267291911,
      "grad_norm": 0.3491676151752472,
      "learning_rate": 0.0007084976115596405,
      "loss": 0.2214,
      "step": 750
    },
    {
      "epoch": 0.2227432590855803,
      "grad_norm": 0.2953735589981079,
      "learning_rate": 0.0007084138152720521,
      "loss": 0.2227,
      "step": 760
    },
    {
      "epoch": 0.2256740914419695,
      "grad_norm": 0.24858702719211578,
      "learning_rate": 0.0007083300189844638,
      "loss": 0.2193,
      "step": 770
    },
    {
      "epoch": 0.22860492379835873,
      "grad_norm": 0.33082640171051025,
      "learning_rate": 0.0007082462226968754,
      "loss": 0.2136,
      "step": 780
    },
    {
      "epoch": 0.23153575615474795,
      "grad_norm": 0.28532907366752625,
      "learning_rate": 0.000708162426409287,
      "loss": 0.22,
      "step": 790
    },
    {
      "epoch": 0.23446658851113716,
      "grad_norm": 0.31315985321998596,
      "learning_rate": 0.0007080786301216986,
      "loss": 0.2254,
      "step": 800
    },
    {
      "epoch": 0.23739742086752638,
      "grad_norm": 0.31275084614753723,
      "learning_rate": 0.0007079948338341103,
      "loss": 0.2069,
      "step": 810
    },
    {
      "epoch": 0.2403282532239156,
      "grad_norm": 0.36475905776023865,
      "learning_rate": 0.000707911037546522,
      "loss": 0.2245,
      "step": 820
    },
    {
      "epoch": 0.24325908558030482,
      "grad_norm": 0.3105497658252716,
      "learning_rate": 0.0007078272412589336,
      "loss": 0.2197,
      "step": 830
    },
    {
      "epoch": 0.246189917936694,
      "grad_norm": 0.5561652779579163,
      "learning_rate": 0.0007077434449713453,
      "loss": 0.2117,
      "step": 840
    },
    {
      "epoch": 0.24912075029308323,
      "grad_norm": 0.3841799795627594,
      "learning_rate": 0.0007076596486837569,
      "loss": 0.2085,
      "step": 850
    },
    {
      "epoch": 0.25205158264947247,
      "grad_norm": 0.4627702534198761,
      "learning_rate": 0.0007075758523961684,
      "loss": 0.2136,
      "step": 860
    },
    {
      "epoch": 0.25498241500586166,
      "grad_norm": 0.2763863503932953,
      "learning_rate": 0.0007074920561085801,
      "loss": 0.2168,
      "step": 870
    },
    {
      "epoch": 0.25791324736225085,
      "grad_norm": 0.2315494865179062,
      "learning_rate": 0.0007074082598209917,
      "loss": 0.2161,
      "step": 880
    },
    {
      "epoch": 0.2608440797186401,
      "grad_norm": 0.6660311818122864,
      "learning_rate": 0.0007073244635334035,
      "loss": 0.2208,
      "step": 890
    },
    {
      "epoch": 0.2637749120750293,
      "grad_norm": 0.18685433268547058,
      "learning_rate": 0.0007072406672458151,
      "loss": 0.2093,
      "step": 900
    },
    {
      "epoch": 0.26670574443141853,
      "grad_norm": 0.38683658838272095,
      "learning_rate": 0.0007071568709582266,
      "loss": 0.2073,
      "step": 910
    },
    {
      "epoch": 0.2696365767878077,
      "grad_norm": 0.7506878972053528,
      "learning_rate": 0.0007070730746706383,
      "loss": 0.2203,
      "step": 920
    },
    {
      "epoch": 0.27256740914419697,
      "grad_norm": 0.41044750809669495,
      "learning_rate": 0.0007069892783830499,
      "loss": 0.2288,
      "step": 930
    },
    {
      "epoch": 0.27549824150058616,
      "grad_norm": 0.2834239900112152,
      "learning_rate": 0.0007069054820954616,
      "loss": 0.2182,
      "step": 940
    },
    {
      "epoch": 0.2784290738569754,
      "grad_norm": 0.45547395944595337,
      "learning_rate": 0.0007068216858078732,
      "loss": 0.2221,
      "step": 950
    },
    {
      "epoch": 0.2813599062133646,
      "grad_norm": 0.39195001125335693,
      "learning_rate": 0.0007067378895202848,
      "loss": 0.2203,
      "step": 960
    },
    {
      "epoch": 0.2842907385697538,
      "grad_norm": 0.31422150135040283,
      "learning_rate": 0.0007066540932326964,
      "loss": 0.211,
      "step": 970
    },
    {
      "epoch": 0.28722157092614303,
      "grad_norm": 0.25795018672943115,
      "learning_rate": 0.000706570296945108,
      "loss": 0.2298,
      "step": 980
    },
    {
      "epoch": 0.2901524032825322,
      "grad_norm": 0.3812417685985565,
      "learning_rate": 0.0007064865006575198,
      "loss": 0.2307,
      "step": 990
    },
    {
      "epoch": 0.29308323563892147,
      "grad_norm": 0.3770311772823334,
      "learning_rate": 0.0007064027043699314,
      "loss": 0.2359,
      "step": 1000
    },
    {
      "epoch": 0.29601406799531066,
      "grad_norm": 0.35967931151390076,
      "learning_rate": 0.000706318908082343,
      "loss": 0.2195,
      "step": 1010
    },
    {
      "epoch": 0.2989449003516999,
      "grad_norm": 0.3688676059246063,
      "learning_rate": 0.0007062351117947546,
      "loss": 0.2112,
      "step": 1020
    },
    {
      "epoch": 0.3018757327080891,
      "grad_norm": 0.37434694170951843,
      "learning_rate": 0.0007061513155071662,
      "loss": 0.2227,
      "step": 1030
    },
    {
      "epoch": 0.3048065650644783,
      "grad_norm": 0.32686102390289307,
      "learning_rate": 0.0007060675192195779,
      "loss": 0.2198,
      "step": 1040
    },
    {
      "epoch": 0.30773739742086753,
      "grad_norm": 0.3833833932876587,
      "learning_rate": 0.0007059837229319895,
      "loss": 0.2106,
      "step": 1050
    },
    {
      "epoch": 0.3106682297772567,
      "grad_norm": 0.312965452671051,
      "learning_rate": 0.0007058999266444011,
      "loss": 0.2247,
      "step": 1060
    },
    {
      "epoch": 0.31359906213364597,
      "grad_norm": 0.3928625285625458,
      "learning_rate": 0.0007058161303568128,
      "loss": 0.2239,
      "step": 1070
    },
    {
      "epoch": 0.31652989449003516,
      "grad_norm": 0.2921335697174072,
      "learning_rate": 0.0007057323340692244,
      "loss": 0.2024,
      "step": 1080
    },
    {
      "epoch": 0.3194607268464244,
      "grad_norm": 0.24967333674430847,
      "learning_rate": 0.0007056485377816361,
      "loss": 0.2362,
      "step": 1090
    },
    {
      "epoch": 0.3223915592028136,
      "grad_norm": 0.37919139862060547,
      "learning_rate": 0.0007055647414940477,
      "loss": 0.2165,
      "step": 1100
    },
    {
      "epoch": 0.32532239155920284,
      "grad_norm": 0.4375402331352234,
      "learning_rate": 0.0007054809452064593,
      "loss": 0.2384,
      "step": 1110
    },
    {
      "epoch": 0.32825322391559203,
      "grad_norm": 0.4561120867729187,
      "learning_rate": 0.0007053971489188709,
      "loss": 0.2201,
      "step": 1120
    },
    {
      "epoch": 0.3311840562719812,
      "grad_norm": 0.5868270993232727,
      "learning_rate": 0.0007053133526312825,
      "loss": 0.2207,
      "step": 1130
    },
    {
      "epoch": 0.33411488862837047,
      "grad_norm": 1.32292640209198,
      "learning_rate": 0.0007052295563436942,
      "loss": 0.224,
      "step": 1140
    },
    {
      "epoch": 0.33704572098475966,
      "grad_norm": 0.32161444425582886,
      "learning_rate": 0.0007051457600561058,
      "loss": 0.2227,
      "step": 1150
    },
    {
      "epoch": 0.3399765533411489,
      "grad_norm": 1.1011528968811035,
      "learning_rate": 0.0007050619637685175,
      "loss": 0.2269,
      "step": 1160
    },
    {
      "epoch": 0.3429073856975381,
      "grad_norm": 1.4109669923782349,
      "learning_rate": 0.0007049781674809291,
      "loss": 0.2066,
      "step": 1170
    },
    {
      "epoch": 0.34583821805392734,
      "grad_norm": 0.35817915201187134,
      "learning_rate": 0.0007048943711933408,
      "loss": 0.2102,
      "step": 1180
    },
    {
      "epoch": 0.34876905041031653,
      "grad_norm": 0.39486199617385864,
      "learning_rate": 0.0007048105749057524,
      "loss": 0.2207,
      "step": 1190
    },
    {
      "epoch": 0.3516998827667057,
      "grad_norm": 0.3971536457538605,
      "learning_rate": 0.000704726778618164,
      "loss": 0.2235,
      "step": 1200
    },
    {
      "epoch": 0.35463071512309496,
      "grad_norm": 0.3534165024757385,
      "learning_rate": 0.0007046429823305757,
      "loss": 0.2257,
      "step": 1210
    },
    {
      "epoch": 0.35756154747948415,
      "grad_norm": 0.2619028687477112,
      "learning_rate": 0.0007045591860429872,
      "loss": 0.2165,
      "step": 1220
    },
    {
      "epoch": 0.3604923798358734,
      "grad_norm": 0.3387294411659241,
      "learning_rate": 0.0007044753897553989,
      "loss": 0.2136,
      "step": 1230
    },
    {
      "epoch": 0.3634232121922626,
      "grad_norm": 0.25723108649253845,
      "learning_rate": 0.0007043915934678106,
      "loss": 0.2021,
      "step": 1240
    },
    {
      "epoch": 0.36635404454865184,
      "grad_norm": 0.3781186640262604,
      "learning_rate": 0.0007043077971802222,
      "loss": 0.2095,
      "step": 1250
    },
    {
      "epoch": 0.369284876905041,
      "grad_norm": 0.3095160126686096,
      "learning_rate": 0.0007042240008926339,
      "loss": 0.2006,
      "step": 1260
    },
    {
      "epoch": 0.37221570926143027,
      "grad_norm": 0.20266899466514587,
      "learning_rate": 0.0007041402046050454,
      "loss": 0.2044,
      "step": 1270
    },
    {
      "epoch": 0.37514654161781946,
      "grad_norm": 0.3391405940055847,
      "learning_rate": 0.0007040564083174571,
      "loss": 0.2004,
      "step": 1280
    },
    {
      "epoch": 0.37807737397420865,
      "grad_norm": 0.2841906249523163,
      "learning_rate": 0.0007039726120298687,
      "loss": 0.2097,
      "step": 1290
    },
    {
      "epoch": 0.3810082063305979,
      "grad_norm": 0.253935843706131,
      "learning_rate": 0.0007038888157422803,
      "loss": 0.2038,
      "step": 1300
    },
    {
      "epoch": 0.3839390386869871,
      "grad_norm": 0.22739912569522858,
      "learning_rate": 0.000703805019454692,
      "loss": 0.2085,
      "step": 1310
    },
    {
      "epoch": 0.38686987104337633,
      "grad_norm": 0.2692492604255676,
      "learning_rate": 0.0007037212231671036,
      "loss": 0.2272,
      "step": 1320
    },
    {
      "epoch": 0.3898007033997655,
      "grad_norm": 0.20697027444839478,
      "learning_rate": 0.0007036374268795153,
      "loss": 0.2037,
      "step": 1330
    },
    {
      "epoch": 0.39273153575615477,
      "grad_norm": 0.35048574209213257,
      "learning_rate": 0.0007035536305919269,
      "loss": 0.2269,
      "step": 1340
    },
    {
      "epoch": 0.39566236811254396,
      "grad_norm": 0.39265039563179016,
      "learning_rate": 0.0007034698343043386,
      "loss": 0.2024,
      "step": 1350
    },
    {
      "epoch": 0.39859320046893315,
      "grad_norm": 0.8352916240692139,
      "learning_rate": 0.0007033860380167502,
      "loss": 0.2326,
      "step": 1360
    },
    {
      "epoch": 0.4015240328253224,
      "grad_norm": 0.4826146364212036,
      "learning_rate": 0.0007033022417291618,
      "loss": 0.2436,
      "step": 1370
    },
    {
      "epoch": 0.4044548651817116,
      "grad_norm": 0.5080786347389221,
      "learning_rate": 0.0007032184454415734,
      "loss": 0.2184,
      "step": 1380
    },
    {
      "epoch": 0.40738569753810083,
      "grad_norm": 0.2981506884098053,
      "learning_rate": 0.000703134649153985,
      "loss": 0.2073,
      "step": 1390
    },
    {
      "epoch": 0.41031652989449,
      "grad_norm": 0.263689249753952,
      "learning_rate": 0.0007030508528663967,
      "loss": 0.2179,
      "step": 1400
    },
    {
      "epoch": 0.41324736225087927,
      "grad_norm": 0.37030723690986633,
      "learning_rate": 0.0007029670565788084,
      "loss": 0.2154,
      "step": 1410
    },
    {
      "epoch": 0.41617819460726846,
      "grad_norm": 0.35392501950263977,
      "learning_rate": 0.00070288326029122,
      "loss": 0.2365,
      "step": 1420
    },
    {
      "epoch": 0.4191090269636577,
      "grad_norm": 0.26922091841697693,
      "learning_rate": 0.0007027994640036316,
      "loss": 0.2125,
      "step": 1430
    },
    {
      "epoch": 0.4220398593200469,
      "grad_norm": 0.30995696783065796,
      "learning_rate": 0.0007027156677160432,
      "loss": 0.1939,
      "step": 1440
    },
    {
      "epoch": 0.4249706916764361,
      "grad_norm": 0.45001471042633057,
      "learning_rate": 0.0007026318714284549,
      "loss": 0.2203,
      "step": 1450
    },
    {
      "epoch": 0.42790152403282533,
      "grad_norm": 0.5226913094520569,
      "learning_rate": 0.0007025480751408665,
      "loss": 0.206,
      "step": 1460
    },
    {
      "epoch": 0.4308323563892145,
      "grad_norm": 0.4992619454860687,
      "learning_rate": 0.0007024642788532781,
      "loss": 0.2134,
      "step": 1470
    },
    {
      "epoch": 0.43376318874560377,
      "grad_norm": 0.35760632157325745,
      "learning_rate": 0.0007023804825656897,
      "loss": 0.2172,
      "step": 1480
    },
    {
      "epoch": 0.43669402110199296,
      "grad_norm": 0.3058198392391205,
      "learning_rate": 0.0007022966862781013,
      "loss": 0.2192,
      "step": 1490
    },
    {
      "epoch": 0.4396248534583822,
      "grad_norm": 0.2940808832645416,
      "learning_rate": 0.0007022128899905131,
      "loss": 0.2239,
      "step": 1500
    },
    {
      "epoch": 0.4425556858147714,
      "grad_norm": 0.3916027545928955,
      "learning_rate": 0.0007021290937029247,
      "loss": 0.2305,
      "step": 1510
    },
    {
      "epoch": 0.4454865181711606,
      "grad_norm": 0.4567375183105469,
      "learning_rate": 0.0007020452974153363,
      "loss": 0.2263,
      "step": 1520
    },
    {
      "epoch": 0.44841735052754983,
      "grad_norm": 0.35591378808021545,
      "learning_rate": 0.0007019615011277479,
      "loss": 0.204,
      "step": 1530
    },
    {
      "epoch": 0.451348182883939,
      "grad_norm": 0.28315314650535583,
      "learning_rate": 0.0007018777048401595,
      "loss": 0.2188,
      "step": 1540
    },
    {
      "epoch": 0.45427901524032827,
      "grad_norm": 0.3493272364139557,
      "learning_rate": 0.0007017939085525712,
      "loss": 0.2167,
      "step": 1550
    },
    {
      "epoch": 0.45720984759671746,
      "grad_norm": 0.5956444144248962,
      "learning_rate": 0.0007017101122649828,
      "loss": 0.2113,
      "step": 1560
    },
    {
      "epoch": 0.4601406799531067,
      "grad_norm": 0.23853179812431335,
      "learning_rate": 0.0007016263159773944,
      "loss": 0.2116,
      "step": 1570
    },
    {
      "epoch": 0.4630715123094959,
      "grad_norm": 0.5844146609306335,
      "learning_rate": 0.000701542519689806,
      "loss": 0.1889,
      "step": 1580
    },
    {
      "epoch": 0.46600234466588514,
      "grad_norm": 0.2894713878631592,
      "learning_rate": 0.0007014587234022176,
      "loss": 0.2198,
      "step": 1590
    },
    {
      "epoch": 0.46893317702227433,
      "grad_norm": 0.6409784555435181,
      "learning_rate": 0.0007013749271146294,
      "loss": 0.2105,
      "step": 1600
    },
    {
      "epoch": 0.4718640093786635,
      "grad_norm": 0.6933387517929077,
      "learning_rate": 0.000701291130827041,
      "loss": 0.1997,
      "step": 1610
    },
    {
      "epoch": 0.47479484173505276,
      "grad_norm": 0.5898562073707581,
      "learning_rate": 0.0007012073345394527,
      "loss": 0.2145,
      "step": 1620
    },
    {
      "epoch": 0.47772567409144195,
      "grad_norm": 0.39995768666267395,
      "learning_rate": 0.0007011235382518642,
      "loss": 0.2017,
      "step": 1630
    },
    {
      "epoch": 0.4806565064478312,
      "grad_norm": 0.4596414566040039,
      "learning_rate": 0.0007010397419642758,
      "loss": 0.1899,
      "step": 1640
    },
    {
      "epoch": 0.4835873388042204,
      "grad_norm": 0.6788212060928345,
      "learning_rate": 0.0007009559456766875,
      "loss": 0.2275,
      "step": 1650
    },
    {
      "epoch": 0.48651817116060964,
      "grad_norm": 0.5013369917869568,
      "learning_rate": 0.0007008721493890991,
      "loss": 0.2254,
      "step": 1660
    },
    {
      "epoch": 0.4894490035169988,
      "grad_norm": 0.6822735667228699,
      "learning_rate": 0.0007007883531015109,
      "loss": 0.2126,
      "step": 1670
    },
    {
      "epoch": 0.492379835873388,
      "grad_norm": 0.3089332580566406,
      "learning_rate": 0.0007007045568139224,
      "loss": 0.2407,
      "step": 1680
    },
    {
      "epoch": 0.49531066822977726,
      "grad_norm": 0.4580439627170563,
      "learning_rate": 0.0007006207605263341,
      "loss": 0.2181,
      "step": 1690
    },
    {
      "epoch": 0.49824150058616645,
      "grad_norm": 0.30615419149398804,
      "learning_rate": 0.0007005369642387457,
      "loss": 0.2202,
      "step": 1700
    },
    {
      "epoch": 0.5011723329425557,
      "grad_norm": 0.4106939136981964,
      "learning_rate": 0.0007004531679511573,
      "loss": 0.2185,
      "step": 1710
    },
    {
      "epoch": 0.5041031652989449,
      "grad_norm": 0.4670943319797516,
      "learning_rate": 0.000700369371663569,
      "loss": 0.2078,
      "step": 1720
    },
    {
      "epoch": 0.5070339976553341,
      "grad_norm": 0.35059067606925964,
      "learning_rate": 0.0007002855753759806,
      "loss": 0.2149,
      "step": 1730
    },
    {
      "epoch": 0.5099648300117233,
      "grad_norm": 0.46209126710891724,
      "learning_rate": 0.0007002017790883922,
      "loss": 0.2147,
      "step": 1740
    },
    {
      "epoch": 0.5128956623681126,
      "grad_norm": 0.473591148853302,
      "learning_rate": 0.0007001179828008038,
      "loss": 0.217,
      "step": 1750
    },
    {
      "epoch": 0.5158264947245017,
      "grad_norm": 0.34667620062828064,
      "learning_rate": 0.0007000341865132154,
      "loss": 0.2061,
      "step": 1760
    },
    {
      "epoch": 0.518757327080891,
      "grad_norm": 0.5607213377952576,
      "learning_rate": 0.0006999503902256272,
      "loss": 0.208,
      "step": 1770
    },
    {
      "epoch": 0.5216881594372802,
      "grad_norm": 0.42735612392425537,
      "learning_rate": 0.0006998665939380388,
      "loss": 0.2155,
      "step": 1780
    },
    {
      "epoch": 0.5246189917936694,
      "grad_norm": 0.6885902881622314,
      "learning_rate": 0.0006997827976504504,
      "loss": 0.2057,
      "step": 1790
    },
    {
      "epoch": 0.5275498241500586,
      "grad_norm": 0.46150028705596924,
      "learning_rate": 0.000699699001362862,
      "loss": 0.2265,
      "step": 1800
    },
    {
      "epoch": 0.5304806565064478,
      "grad_norm": 0.29136037826538086,
      "learning_rate": 0.0006996152050752736,
      "loss": 0.2057,
      "step": 1810
    },
    {
      "epoch": 0.5334114888628371,
      "grad_norm": 0.3566972315311432,
      "learning_rate": 0.0006995314087876853,
      "loss": 0.214,
      "step": 1820
    },
    {
      "epoch": 0.5363423212192263,
      "grad_norm": 0.30712246894836426,
      "learning_rate": 0.0006994476125000969,
      "loss": 0.2235,
      "step": 1830
    },
    {
      "epoch": 0.5392731535756154,
      "grad_norm": 0.2984785735607147,
      "learning_rate": 0.0006993638162125085,
      "loss": 0.2103,
      "step": 1840
    },
    {
      "epoch": 0.5422039859320047,
      "grad_norm": 0.35043197870254517,
      "learning_rate": 0.0006992800199249202,
      "loss": 0.2208,
      "step": 1850
    },
    {
      "epoch": 0.5451348182883939,
      "grad_norm": 0.3874368667602539,
      "learning_rate": 0.0006991962236373318,
      "loss": 0.2202,
      "step": 1860
    },
    {
      "epoch": 0.5480656506447831,
      "grad_norm": 0.299444317817688,
      "learning_rate": 0.0006991124273497435,
      "loss": 0.2235,
      "step": 1870
    },
    {
      "epoch": 0.5509964830011723,
      "grad_norm": 0.30864575505256653,
      "learning_rate": 0.0006990286310621551,
      "loss": 0.2109,
      "step": 1880
    },
    {
      "epoch": 0.5539273153575616,
      "grad_norm": 0.21435272693634033,
      "learning_rate": 0.0006989448347745667,
      "loss": 0.2095,
      "step": 1890
    },
    {
      "epoch": 0.5568581477139508,
      "grad_norm": 0.3870876729488373,
      "learning_rate": 0.0006988610384869783,
      "loss": 0.2269,
      "step": 1900
    },
    {
      "epoch": 0.55978898007034,
      "grad_norm": 0.37343886494636536,
      "learning_rate": 0.00069877724219939,
      "loss": 0.2027,
      "step": 1910
    },
    {
      "epoch": 0.5627198124267292,
      "grad_norm": 0.3702000081539154,
      "learning_rate": 0.0006986934459118016,
      "loss": 0.2301,
      "step": 1920
    },
    {
      "epoch": 0.5656506447831184,
      "grad_norm": 0.4867165684700012,
      "learning_rate": 0.0006986096496242132,
      "loss": 0.2301,
      "step": 1930
    },
    {
      "epoch": 0.5685814771395076,
      "grad_norm": 0.37962836027145386,
      "learning_rate": 0.0006985258533366249,
      "loss": 0.2055,
      "step": 1940
    },
    {
      "epoch": 0.5715123094958968,
      "grad_norm": 0.3003216087818146,
      "learning_rate": 0.0006984420570490365,
      "loss": 0.2005,
      "step": 1950
    },
    {
      "epoch": 0.5744431418522861,
      "grad_norm": 0.33616507053375244,
      "learning_rate": 0.0006983582607614482,
      "loss": 0.2073,
      "step": 1960
    },
    {
      "epoch": 0.5773739742086753,
      "grad_norm": 0.28688716888427734,
      "learning_rate": 0.0006982744644738598,
      "loss": 0.1979,
      "step": 1970
    },
    {
      "epoch": 0.5803048065650644,
      "grad_norm": 0.3392135202884674,
      "learning_rate": 0.0006981906681862714,
      "loss": 0.2208,
      "step": 1980
    },
    {
      "epoch": 0.5832356389214537,
      "grad_norm": 0.37554001808166504,
      "learning_rate": 0.000698106871898683,
      "loss": 0.2199,
      "step": 1990
    },
    {
      "epoch": 0.5861664712778429,
      "grad_norm": 0.26051145792007446,
      "learning_rate": 0.0006980230756110946,
      "loss": 0.2041,
      "step": 2000
    },
    {
      "epoch": 0.5890973036342321,
      "grad_norm": 0.3256095349788666,
      "learning_rate": 0.0006979392793235063,
      "loss": 0.2154,
      "step": 2010
    },
    {
      "epoch": 0.5920281359906213,
      "grad_norm": 0.6084034442901611,
      "learning_rate": 0.000697855483035918,
      "loss": 0.2239,
      "step": 2020
    },
    {
      "epoch": 0.5949589683470106,
      "grad_norm": 0.6843586564064026,
      "learning_rate": 0.0006977716867483296,
      "loss": 0.2293,
      "step": 2030
    },
    {
      "epoch": 0.5978898007033998,
      "grad_norm": 0.2538241147994995,
      "learning_rate": 0.0006976878904607412,
      "loss": 0.2048,
      "step": 2040
    },
    {
      "epoch": 0.6008206330597889,
      "grad_norm": 0.26934006810188293,
      "learning_rate": 0.0006976040941731528,
      "loss": 0.1977,
      "step": 2050
    },
    {
      "epoch": 0.6037514654161782,
      "grad_norm": 0.2844369113445282,
      "learning_rate": 0.0006975202978855645,
      "loss": 0.2202,
      "step": 2060
    },
    {
      "epoch": 0.6066822977725674,
      "grad_norm": 0.28739410638809204,
      "learning_rate": 0.0006974365015979761,
      "loss": 0.2116,
      "step": 2070
    },
    {
      "epoch": 0.6096131301289566,
      "grad_norm": 0.24383409321308136,
      "learning_rate": 0.0006973527053103878,
      "loss": 0.2111,
      "step": 2080
    },
    {
      "epoch": 0.6125439624853458,
      "grad_norm": 0.4181525409221649,
      "learning_rate": 0.0006972689090227994,
      "loss": 0.201,
      "step": 2090
    },
    {
      "epoch": 0.6154747948417351,
      "grad_norm": 0.34379321336746216,
      "learning_rate": 0.0006971851127352109,
      "loss": 0.2126,
      "step": 2100
    },
    {
      "epoch": 0.6184056271981243,
      "grad_norm": 0.6232630014419556,
      "learning_rate": 0.0006971013164476227,
      "loss": 0.2084,
      "step": 2110
    },
    {
      "epoch": 0.6213364595545134,
      "grad_norm": 0.1969255805015564,
      "learning_rate": 0.0006970175201600343,
      "loss": 0.198,
      "step": 2120
    },
    {
      "epoch": 0.6242672919109027,
      "grad_norm": 0.485113263130188,
      "learning_rate": 0.000696933723872446,
      "loss": 0.2081,
      "step": 2130
    },
    {
      "epoch": 0.6271981242672919,
      "grad_norm": 0.3211025297641754,
      "learning_rate": 0.0006968499275848576,
      "loss": 0.2046,
      "step": 2140
    },
    {
      "epoch": 0.6301289566236812,
      "grad_norm": 0.3510150611400604,
      "learning_rate": 0.0006967661312972691,
      "loss": 0.1911,
      "step": 2150
    },
    {
      "epoch": 0.6330597889800703,
      "grad_norm": 0.35577335953712463,
      "learning_rate": 0.0006966823350096808,
      "loss": 0.2083,
      "step": 2160
    },
    {
      "epoch": 0.6359906213364596,
      "grad_norm": 0.49120673537254333,
      "learning_rate": 0.0006965985387220924,
      "loss": 0.2062,
      "step": 2170
    },
    {
      "epoch": 0.6389214536928488,
      "grad_norm": 0.361187607049942,
      "learning_rate": 0.0006965147424345041,
      "loss": 0.2049,
      "step": 2180
    },
    {
      "epoch": 0.6418522860492379,
      "grad_norm": 0.4686076045036316,
      "learning_rate": 0.0006964309461469157,
      "loss": 0.2091,
      "step": 2190
    },
    {
      "epoch": 0.6447831184056272,
      "grad_norm": 0.30856379866600037,
      "learning_rate": 0.0006963471498593274,
      "loss": 0.193,
      "step": 2200
    },
    {
      "epoch": 0.6477139507620164,
      "grad_norm": 0.47809547185897827,
      "learning_rate": 0.000696263353571739,
      "loss": 0.2144,
      "step": 2210
    },
    {
      "epoch": 0.6506447831184057,
      "grad_norm": 0.4172581434249878,
      "learning_rate": 0.0006961795572841506,
      "loss": 0.1988,
      "step": 2220
    },
    {
      "epoch": 0.6535756154747948,
      "grad_norm": 0.38631579279899597,
      "learning_rate": 0.0006960957609965623,
      "loss": 0.1883,
      "step": 2230
    },
    {
      "epoch": 0.6565064478311841,
      "grad_norm": 0.4112786650657654,
      "learning_rate": 0.0006960119647089739,
      "loss": 0.2098,
      "step": 2240
    },
    {
      "epoch": 0.6594372801875733,
      "grad_norm": 0.29405033588409424,
      "learning_rate": 0.0006959281684213855,
      "loss": 0.2013,
      "step": 2250
    },
    {
      "epoch": 0.6623681125439624,
      "grad_norm": 0.5281264185905457,
      "learning_rate": 0.0006958443721337971,
      "loss": 0.2236,
      "step": 2260
    },
    {
      "epoch": 0.6652989449003517,
      "grad_norm": 0.28028327226638794,
      "learning_rate": 0.0006957605758462087,
      "loss": 0.2096,
      "step": 2270
    },
    {
      "epoch": 0.6682297772567409,
      "grad_norm": 0.43062520027160645,
      "learning_rate": 0.0006956767795586205,
      "loss": 0.1901,
      "step": 2280
    },
    {
      "epoch": 0.6711606096131302,
      "grad_norm": 0.6390825510025024,
      "learning_rate": 0.0006955929832710321,
      "loss": 0.2056,
      "step": 2290
    },
    {
      "epoch": 0.6740914419695193,
      "grad_norm": 0.7629620432853699,
      "learning_rate": 0.0006955091869834437,
      "loss": 0.1988,
      "step": 2300
    },
    {
      "epoch": 0.6770222743259086,
      "grad_norm": 0.34738728404045105,
      "learning_rate": 0.0006954253906958553,
      "loss": 0.1991,
      "step": 2310
    },
    {
      "epoch": 0.6799531066822978,
      "grad_norm": 0.4457070231437683,
      "learning_rate": 0.0006953415944082669,
      "loss": 0.2127,
      "step": 2320
    },
    {
      "epoch": 0.6828839390386869,
      "grad_norm": 0.5099700689315796,
      "learning_rate": 0.0006952577981206786,
      "loss": 0.2141,
      "step": 2330
    },
    {
      "epoch": 0.6858147713950762,
      "grad_norm": 0.5644245743751526,
      "learning_rate": 0.0006951740018330902,
      "loss": 0.2109,
      "step": 2340
    },
    {
      "epoch": 0.6887456037514654,
      "grad_norm": 0.3945325016975403,
      "learning_rate": 0.0006950902055455018,
      "loss": 0.1991,
      "step": 2350
    },
    {
      "epoch": 0.6916764361078547,
      "grad_norm": 0.5936745405197144,
      "learning_rate": 0.0006950064092579134,
      "loss": 0.1935,
      "step": 2360
    },
    {
      "epoch": 0.6946072684642438,
      "grad_norm": 0.3358243405818939,
      "learning_rate": 0.000694922612970325,
      "loss": 0.2196,
      "step": 2370
    },
    {
      "epoch": 0.6975381008206331,
      "grad_norm": 0.38823845982551575,
      "learning_rate": 0.0006948388166827368,
      "loss": 0.2055,
      "step": 2380
    },
    {
      "epoch": 0.7004689331770223,
      "grad_norm": 0.39557668566703796,
      "learning_rate": 0.0006947550203951484,
      "loss": 0.2283,
      "step": 2390
    },
    {
      "epoch": 0.7033997655334114,
      "grad_norm": 0.3313976526260376,
      "learning_rate": 0.00069467122410756,
      "loss": 0.1986,
      "step": 2400
    },
    {
      "epoch": 0.7063305978898007,
      "grad_norm": 0.47552135586738586,
      "learning_rate": 0.0006945874278199716,
      "loss": 0.2079,
      "step": 2410
    },
    {
      "epoch": 0.7092614302461899,
      "grad_norm": 0.5660049915313721,
      "learning_rate": 0.0006945036315323833,
      "loss": 0.2172,
      "step": 2420
    },
    {
      "epoch": 0.7121922626025792,
      "grad_norm": 0.4750288128852844,
      "learning_rate": 0.0006944198352447949,
      "loss": 0.2084,
      "step": 2430
    },
    {
      "epoch": 0.7151230949589683,
      "grad_norm": 0.39009398221969604,
      "learning_rate": 0.0006943360389572065,
      "loss": 0.2083,
      "step": 2440
    },
    {
      "epoch": 0.7180539273153576,
      "grad_norm": 0.37594085931777954,
      "learning_rate": 0.0006942522426696181,
      "loss": 0.2134,
      "step": 2450
    },
    {
      "epoch": 0.7209847596717468,
      "grad_norm": 0.35952362418174744,
      "learning_rate": 0.0006941684463820298,
      "loss": 0.2004,
      "step": 2460
    },
    {
      "epoch": 0.723915592028136,
      "grad_norm": 0.298353374004364,
      "learning_rate": 0.0006940846500944415,
      "loss": 0.203,
      "step": 2470
    },
    {
      "epoch": 0.7268464243845252,
      "grad_norm": 0.43250399827957153,
      "learning_rate": 0.0006940008538068531,
      "loss": 0.2032,
      "step": 2480
    },
    {
      "epoch": 0.7297772567409144,
      "grad_norm": 0.48076680302619934,
      "learning_rate": 0.0006939170575192647,
      "loss": 0.2108,
      "step": 2490
    },
    {
      "epoch": 0.7327080890973037,
      "grad_norm": 0.294706255197525,
      "learning_rate": 0.0006938332612316764,
      "loss": 0.1992,
      "step": 2500
    },
    {
      "epoch": 0.7356389214536928,
      "grad_norm": 0.5174208283424377,
      "learning_rate": 0.0006937494649440879,
      "loss": 0.1992,
      "step": 2510
    },
    {
      "epoch": 0.738569753810082,
      "grad_norm": 0.45973968505859375,
      "learning_rate": 0.0006936656686564996,
      "loss": 0.2091,
      "step": 2520
    },
    {
      "epoch": 0.7415005861664713,
      "grad_norm": 0.39055347442626953,
      "learning_rate": 0.0006935818723689112,
      "loss": 0.211,
      "step": 2530
    },
    {
      "epoch": 0.7444314185228605,
      "grad_norm": 1.025766134262085,
      "learning_rate": 0.0006934980760813228,
      "loss": 0.2207,
      "step": 2540
    },
    {
      "epoch": 0.7473622508792497,
      "grad_norm": 0.502131998538971,
      "learning_rate": 0.0006934142797937346,
      "loss": 0.2008,
      "step": 2550
    },
    {
      "epoch": 0.7502930832356389,
      "grad_norm": 0.40998899936676025,
      "learning_rate": 0.0006933304835061461,
      "loss": 0.1959,
      "step": 2560
    },
    {
      "epoch": 0.7532239155920282,
      "grad_norm": 0.37210237979888916,
      "learning_rate": 0.0006932466872185578,
      "loss": 0.2242,
      "step": 2570
    },
    {
      "epoch": 0.7561547479484173,
      "grad_norm": 0.5177266597747803,
      "learning_rate": 0.0006931628909309694,
      "loss": 0.2085,
      "step": 2580
    },
    {
      "epoch": 0.7590855803048066,
      "grad_norm": 0.4441749155521393,
      "learning_rate": 0.0006930790946433811,
      "loss": 0.2043,
      "step": 2590
    },
    {
      "epoch": 0.7620164126611958,
      "grad_norm": 1.618105173110962,
      "learning_rate": 0.0006929952983557927,
      "loss": 0.2062,
      "step": 2600
    },
    {
      "epoch": 0.764947245017585,
      "grad_norm": 0.5329073071479797,
      "learning_rate": 0.0006929115020682043,
      "loss": 0.2068,
      "step": 2610
    },
    {
      "epoch": 0.7678780773739742,
      "grad_norm": 0.333680123090744,
      "learning_rate": 0.0006928277057806159,
      "loss": 0.2112,
      "step": 2620
    },
    {
      "epoch": 0.7708089097303634,
      "grad_norm": 0.7738091349601746,
      "learning_rate": 0.0006927439094930276,
      "loss": 0.2132,
      "step": 2630
    },
    {
      "epoch": 0.7737397420867527,
      "grad_norm": 0.2822437286376953,
      "learning_rate": 0.0006926601132054393,
      "loss": 0.217,
      "step": 2640
    },
    {
      "epoch": 0.7766705744431418,
      "grad_norm": 0.3591287136077881,
      "learning_rate": 0.0006925763169178509,
      "loss": 0.2127,
      "step": 2650
    },
    {
      "epoch": 0.779601406799531,
      "grad_norm": 0.4692360758781433,
      "learning_rate": 0.0006924925206302625,
      "loss": 0.2052,
      "step": 2660
    },
    {
      "epoch": 0.7825322391559203,
      "grad_norm": 0.3491719663143158,
      "learning_rate": 0.0006924087243426741,
      "loss": 0.2049,
      "step": 2670
    },
    {
      "epoch": 0.7854630715123095,
      "grad_norm": 0.4899257719516754,
      "learning_rate": 0.0006923249280550857,
      "loss": 0.2136,
      "step": 2680
    },
    {
      "epoch": 0.7883939038686987,
      "grad_norm": 0.37259259819984436,
      "learning_rate": 0.0006922411317674974,
      "loss": 0.2042,
      "step": 2690
    },
    {
      "epoch": 0.7913247362250879,
      "grad_norm": 0.5215005874633789,
      "learning_rate": 0.000692157335479909,
      "loss": 0.2012,
      "step": 2700
    },
    {
      "epoch": 0.7942555685814772,
      "grad_norm": 0.4063393175601959,
      "learning_rate": 0.0006920735391923206,
      "loss": 0.218,
      "step": 2710
    },
    {
      "epoch": 0.7971864009378663,
      "grad_norm": 0.34150007367134094,
      "learning_rate": 0.0006919897429047323,
      "loss": 0.2279,
      "step": 2720
    },
    {
      "epoch": 0.8001172332942555,
      "grad_norm": 0.3627108633518219,
      "learning_rate": 0.0006919059466171439,
      "loss": 0.1982,
      "step": 2730
    },
    {
      "epoch": 0.8030480656506448,
      "grad_norm": 0.28289929032325745,
      "learning_rate": 0.0006918221503295556,
      "loss": 0.2144,
      "step": 2740
    },
    {
      "epoch": 0.805978898007034,
      "grad_norm": 0.7150212526321411,
      "learning_rate": 0.0006917383540419672,
      "loss": 0.1811,
      "step": 2750
    },
    {
      "epoch": 0.8089097303634232,
      "grad_norm": 0.6927107572555542,
      "learning_rate": 0.0006916545577543788,
      "loss": 0.2105,
      "step": 2760
    },
    {
      "epoch": 0.8118405627198124,
      "grad_norm": 1.8804781436920166,
      "learning_rate": 0.0006915707614667904,
      "loss": 0.2029,
      "step": 2770
    },
    {
      "epoch": 0.8147713950762017,
      "grad_norm": 0.4114798903465271,
      "learning_rate": 0.000691486965179202,
      "loss": 0.2223,
      "step": 2780
    },
    {
      "epoch": 0.8177022274325909,
      "grad_norm": 0.27066099643707275,
      "learning_rate": 0.0006914031688916137,
      "loss": 0.1923,
      "step": 2790
    },
    {
      "epoch": 0.82063305978898,
      "grad_norm": 0.3187044560909271,
      "learning_rate": 0.0006913193726040254,
      "loss": 0.2003,
      "step": 2800
    },
    {
      "epoch": 0.8235638921453693,
      "grad_norm": 0.5271864533424377,
      "learning_rate": 0.000691235576316437,
      "loss": 0.1941,
      "step": 2810
    },
    {
      "epoch": 0.8264947245017585,
      "grad_norm": 0.3200379014015198,
      "learning_rate": 0.0006911517800288486,
      "loss": 0.2178,
      "step": 2820
    },
    {
      "epoch": 0.8294255568581477,
      "grad_norm": 0.3724834620952606,
      "learning_rate": 0.0006910679837412602,
      "loss": 0.2089,
      "step": 2830
    },
    {
      "epoch": 0.8323563892145369,
      "grad_norm": 0.3673563301563263,
      "learning_rate": 0.0006909841874536719,
      "loss": 0.2034,
      "step": 2840
    },
    {
      "epoch": 0.8352872215709262,
      "grad_norm": 0.39872032403945923,
      "learning_rate": 0.0006909003911660835,
      "loss": 0.2183,
      "step": 2850
    },
    {
      "epoch": 0.8382180539273154,
      "grad_norm": 0.44200995564460754,
      "learning_rate": 0.0006908165948784952,
      "loss": 0.1974,
      "step": 2860
    },
    {
      "epoch": 0.8411488862837045,
      "grad_norm": 0.5264356136322021,
      "learning_rate": 0.0006907327985909067,
      "loss": 0.2025,
      "step": 2870
    },
    {
      "epoch": 0.8440797186400938,
      "grad_norm": 0.3856295943260193,
      "learning_rate": 0.0006906490023033183,
      "loss": 0.189,
      "step": 2880
    },
    {
      "epoch": 0.847010550996483,
      "grad_norm": 0.5082962512969971,
      "learning_rate": 0.0006905652060157301,
      "loss": 0.2011,
      "step": 2890
    },
    {
      "epoch": 0.8499413833528722,
      "grad_norm": 0.6257930994033813,
      "learning_rate": 0.0006904814097281417,
      "loss": 0.1762,
      "step": 2900
    },
    {
      "epoch": 0.8528722157092614,
      "grad_norm": 2.917433261871338,
      "learning_rate": 0.0006903976134405534,
      "loss": 0.2073,
      "step": 2910
    },
    {
      "epoch": 0.8558030480656507,
      "grad_norm": 0.3500216007232666,
      "learning_rate": 0.0006903138171529649,
      "loss": 0.218,
      "step": 2920
    },
    {
      "epoch": 0.8587338804220399,
      "grad_norm": 0.2601329982280731,
      "learning_rate": 0.0006902300208653765,
      "loss": 0.2345,
      "step": 2930
    },
    {
      "epoch": 0.861664712778429,
      "grad_norm": 0.34921881556510925,
      "learning_rate": 0.0006901462245777882,
      "loss": 0.207,
      "step": 2940
    },
    {
      "epoch": 0.8645955451348183,
      "grad_norm": 0.3593926727771759,
      "learning_rate": 0.0006900624282901998,
      "loss": 0.1843,
      "step": 2950
    },
    {
      "epoch": 0.8675263774912075,
      "grad_norm": 0.5014644265174866,
      "learning_rate": 0.0006899786320026115,
      "loss": 0.1992,
      "step": 2960
    },
    {
      "epoch": 0.8704572098475967,
      "grad_norm": 0.7133957147598267,
      "learning_rate": 0.0006898948357150231,
      "loss": 0.1919,
      "step": 2970
    },
    {
      "epoch": 0.8733880422039859,
      "grad_norm": 0.32851549983024597,
      "learning_rate": 0.0006898110394274348,
      "loss": 0.1961,
      "step": 2980
    },
    {
      "epoch": 0.8763188745603752,
      "grad_norm": 0.5338538885116577,
      "learning_rate": 0.0006897272431398464,
      "loss": 0.1901,
      "step": 2990
    },
    {
      "epoch": 0.8792497069167644,
      "grad_norm": 0.25205370783805847,
      "learning_rate": 0.000689643446852258,
      "loss": 0.1924,
      "step": 3000
    },
    {
      "epoch": 0.8821805392731535,
      "grad_norm": 0.39902523159980774,
      "learning_rate": 0.0006895596505646697,
      "loss": 0.2098,
      "step": 3010
    },
    {
      "epoch": 0.8851113716295428,
      "grad_norm": 0.43004703521728516,
      "learning_rate": 0.0006894758542770813,
      "loss": 0.1852,
      "step": 3020
    },
    {
      "epoch": 0.888042203985932,
      "grad_norm": 0.5561000108718872,
      "learning_rate": 0.0006893920579894929,
      "loss": 0.2002,
      "step": 3030
    },
    {
      "epoch": 0.8909730363423212,
      "grad_norm": 0.5158193707466125,
      "learning_rate": 0.0006893082617019045,
      "loss": 0.22,
      "step": 3040
    },
    {
      "epoch": 0.8939038686987104,
      "grad_norm": 0.48004311323165894,
      "learning_rate": 0.0006892244654143161,
      "loss": 0.2204,
      "step": 3050
    },
    {
      "epoch": 0.8968347010550997,
      "grad_norm": 0.2980142831802368,
      "learning_rate": 0.0006891406691267279,
      "loss": 0.1868,
      "step": 3060
    },
    {
      "epoch": 0.8997655334114889,
      "grad_norm": 0.49269598722457886,
      "learning_rate": 0.0006890568728391395,
      "loss": 0.1943,
      "step": 3070
    },
    {
      "epoch": 0.902696365767878,
      "grad_norm": 0.36847493052482605,
      "learning_rate": 0.0006889730765515511,
      "loss": 0.1865,
      "step": 3080
    },
    {
      "epoch": 0.9056271981242673,
      "grad_norm": 0.4741630256175995,
      "learning_rate": 0.0006888892802639627,
      "loss": 0.206,
      "step": 3090
    },
    {
      "epoch": 0.9085580304806565,
      "grad_norm": 0.4284859299659729,
      "learning_rate": 0.0006888054839763743,
      "loss": 0.2157,
      "step": 3100
    },
    {
      "epoch": 0.9114888628370457,
      "grad_norm": 0.30119091272354126,
      "learning_rate": 0.000688721687688786,
      "loss": 0.1863,
      "step": 3110
    },
    {
      "epoch": 0.9144196951934349,
      "grad_norm": 0.5072365999221802,
      "learning_rate": 0.0006886378914011976,
      "loss": 0.1995,
      "step": 3120
    },
    {
      "epoch": 0.9173505275498242,
      "grad_norm": 0.38899001479148865,
      "learning_rate": 0.0006885540951136092,
      "loss": 0.1843,
      "step": 3130
    },
    {
      "epoch": 0.9202813599062134,
      "grad_norm": 0.4976820945739746,
      "learning_rate": 0.0006884702988260208,
      "loss": 0.1905,
      "step": 3140
    },
    {
      "epoch": 0.9232121922626025,
      "grad_norm": 0.9179698824882507,
      "learning_rate": 0.0006883865025384326,
      "loss": 0.208,
      "step": 3150
    },
    {
      "epoch": 0.9261430246189918,
      "grad_norm": 0.514853298664093,
      "learning_rate": 0.0006883027062508442,
      "loss": 0.2059,
      "step": 3160
    },
    {
      "epoch": 0.929073856975381,
      "grad_norm": 0.33035823702812195,
      "learning_rate": 0.0006882189099632558,
      "loss": 0.1998,
      "step": 3170
    },
    {
      "epoch": 0.9320046893317703,
      "grad_norm": 0.5211396813392639,
      "learning_rate": 0.0006881351136756674,
      "loss": 0.1886,
      "step": 3180
    },
    {
      "epoch": 0.9349355216881594,
      "grad_norm": 0.32195207476615906,
      "learning_rate": 0.000688051317388079,
      "loss": 0.1931,
      "step": 3190
    },
    {
      "epoch": 0.9378663540445487,
      "grad_norm": 0.9848979115486145,
      "learning_rate": 0.0006879675211004907,
      "loss": 0.2153,
      "step": 3200
    },
    {
      "epoch": 0.9407971864009379,
      "grad_norm": 0.5909284353256226,
      "learning_rate": 0.0006878837248129023,
      "loss": 0.1948,
      "step": 3210
    },
    {
      "epoch": 0.943728018757327,
      "grad_norm": 0.4317075312137604,
      "learning_rate": 0.0006877999285253139,
      "loss": 0.181,
      "step": 3220
    },
    {
      "epoch": 0.9466588511137163,
      "grad_norm": 0.34791994094848633,
      "learning_rate": 0.0006877161322377255,
      "loss": 0.2149,
      "step": 3230
    },
    {
      "epoch": 0.9495896834701055,
      "grad_norm": 0.6339690089225769,
      "learning_rate": 0.0006876323359501372,
      "loss": 0.1901,
      "step": 3240
    },
    {
      "epoch": 0.9525205158264948,
      "grad_norm": 0.41809654235839844,
      "learning_rate": 0.0006875485396625489,
      "loss": 0.1802,
      "step": 3250
    },
    {
      "epoch": 0.9554513481828839,
      "grad_norm": 0.5400978922843933,
      "learning_rate": 0.0006874647433749605,
      "loss": 0.1967,
      "step": 3260
    },
    {
      "epoch": 0.9583821805392732,
      "grad_norm": 0.37287747859954834,
      "learning_rate": 0.0006873809470873721,
      "loss": 0.1948,
      "step": 3270
    },
    {
      "epoch": 0.9613130128956624,
      "grad_norm": 0.316036194562912,
      "learning_rate": 0.0006872971507997837,
      "loss": 0.2022,
      "step": 3280
    },
    {
      "epoch": 0.9642438452520515,
      "grad_norm": 0.5104859471321106,
      "learning_rate": 0.0006872133545121953,
      "loss": 0.2092,
      "step": 3290
    },
    {
      "epoch": 0.9671746776084408,
      "grad_norm": 0.5475963354110718,
      "learning_rate": 0.000687129558224607,
      "loss": 0.2067,
      "step": 3300
    },
    {
      "epoch": 0.97010550996483,
      "grad_norm": 0.3676128089427948,
      "learning_rate": 0.0006870457619370186,
      "loss": 0.1998,
      "step": 3310
    },
    {
      "epoch": 0.9730363423212193,
      "grad_norm": 0.5722980499267578,
      "learning_rate": 0.0006869619656494304,
      "loss": 0.196,
      "step": 3320
    },
    {
      "epoch": 0.9759671746776084,
      "grad_norm": 0.5081174373626709,
      "learning_rate": 0.0006868781693618419,
      "loss": 0.1966,
      "step": 3330
    },
    {
      "epoch": 0.9788980070339977,
      "grad_norm": 0.2285604029893875,
      "learning_rate": 0.0006867943730742535,
      "loss": 0.2098,
      "step": 3340
    },
    {
      "epoch": 0.9818288393903869,
      "grad_norm": 0.41495853662490845,
      "learning_rate": 0.0006867105767866652,
      "loss": 0.2007,
      "step": 3350
    },
    {
      "epoch": 0.984759671746776,
      "grad_norm": 0.4081357717514038,
      "learning_rate": 0.0006866267804990768,
      "loss": 0.205,
      "step": 3360
    },
    {
      "epoch": 0.9876905041031653,
      "grad_norm": 0.4759844243526459,
      "learning_rate": 0.0006865429842114885,
      "loss": 0.1903,
      "step": 3370
    },
    {
      "epoch": 0.9906213364595545,
      "grad_norm": 0.4497276842594147,
      "learning_rate": 0.0006864591879239001,
      "loss": 0.2003,
      "step": 3380
    },
    {
      "epoch": 0.9935521688159438,
      "grad_norm": 0.31883662939071655,
      "learning_rate": 0.0006863753916363116,
      "loss": 0.1772,
      "step": 3390
    },
    {
      "epoch": 0.9964830011723329,
      "grad_norm": 0.3692326247692108,
      "learning_rate": 0.0006862915953487233,
      "loss": 0.2256,
      "step": 3400
    },
    {
      "epoch": 0.9994138335287222,
      "grad_norm": 0.41255396604537964,
      "learning_rate": 0.000686207799061135,
      "loss": 0.19,
      "step": 3410
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.08638311902406857,
      "eval_f1_macro": 0.05619027744770116,
      "eval_f1_micro": 0.15971515768056968,
      "eval_f1_weighted": 0.1442375808972778,
      "eval_loss": 0.19564712047576904,
      "eval_roc_auc": 0.5433655242005758,
      "eval_runtime": 162.9569,
      "eval_samples_per_second": 18.612,
      "eval_steps_per_second": 2.332,
      "step": 3412
    },
    {
      "epoch": 1.0023446658851114,
      "grad_norm": 0.4505973756313324,
      "learning_rate": 0.0006861240027735467,
      "loss": 0.1657,
      "step": 3420
    },
    {
      "epoch": 1.0052754982415006,
      "grad_norm": 0.48871803283691406,
      "learning_rate": 0.0006860402064859583,
      "loss": 0.1956,
      "step": 3430
    },
    {
      "epoch": 1.0082063305978899,
      "grad_norm": 1.0089876651763916,
      "learning_rate": 0.0006859564101983699,
      "loss": 0.1959,
      "step": 3440
    },
    {
      "epoch": 1.011137162954279,
      "grad_norm": 0.356033056974411,
      "learning_rate": 0.0006858726139107815,
      "loss": 0.226,
      "step": 3450
    },
    {
      "epoch": 1.0140679953106682,
      "grad_norm": 0.4597281813621521,
      "learning_rate": 0.0006857888176231931,
      "loss": 0.2226,
      "step": 3460
    },
    {
      "epoch": 1.0169988276670574,
      "grad_norm": 0.3625481426715851,
      "learning_rate": 0.0006857050213356048,
      "loss": 0.2115,
      "step": 3470
    },
    {
      "epoch": 1.0199296600234466,
      "grad_norm": 0.4130008816719055,
      "learning_rate": 0.0006856212250480164,
      "loss": 0.1912,
      "step": 3480
    },
    {
      "epoch": 1.022860492379836,
      "grad_norm": 0.4490097761154175,
      "learning_rate": 0.000685537428760428,
      "loss": 0.202,
      "step": 3490
    },
    {
      "epoch": 1.0257913247362251,
      "grad_norm": 0.437945693731308,
      "learning_rate": 0.0006854536324728397,
      "loss": 0.1919,
      "step": 3500
    },
    {
      "epoch": 1.0287221570926144,
      "grad_norm": 0.3402969539165497,
      "learning_rate": 0.0006853698361852513,
      "loss": 0.21,
      "step": 3510
    },
    {
      "epoch": 1.0316529894490034,
      "grad_norm": 0.4167284667491913,
      "learning_rate": 0.000685286039897663,
      "loss": 0.1862,
      "step": 3520
    },
    {
      "epoch": 1.0345838218053927,
      "grad_norm": 0.6581047177314758,
      "learning_rate": 0.0006852022436100746,
      "loss": 0.1928,
      "step": 3530
    },
    {
      "epoch": 1.037514654161782,
      "grad_norm": 0.7264534831047058,
      "learning_rate": 0.0006851184473224862,
      "loss": 0.2053,
      "step": 3540
    },
    {
      "epoch": 1.0404454865181711,
      "grad_norm": 0.5534377098083496,
      "learning_rate": 0.0006850346510348978,
      "loss": 0.1977,
      "step": 3550
    },
    {
      "epoch": 1.0433763188745604,
      "grad_norm": 0.4757896661758423,
      "learning_rate": 0.0006849508547473094,
      "loss": 0.1972,
      "step": 3560
    },
    {
      "epoch": 1.0463071512309496,
      "grad_norm": 0.30146563053131104,
      "learning_rate": 0.0006848670584597211,
      "loss": 0.19,
      "step": 3570
    },
    {
      "epoch": 1.0492379835873389,
      "grad_norm": 0.3399186134338379,
      "learning_rate": 0.0006847832621721328,
      "loss": 0.185,
      "step": 3580
    },
    {
      "epoch": 1.0521688159437281,
      "grad_norm": 0.5793264508247375,
      "learning_rate": 0.0006846994658845444,
      "loss": 0.2266,
      "step": 3590
    },
    {
      "epoch": 1.0550996483001172,
      "grad_norm": 0.36906203627586365,
      "learning_rate": 0.000684615669596956,
      "loss": 0.1908,
      "step": 3600
    },
    {
      "epoch": 1.0580304806565064,
      "grad_norm": 0.8497394919395447,
      "learning_rate": 0.0006845318733093676,
      "loss": 0.1906,
      "step": 3610
    },
    {
      "epoch": 1.0609613130128956,
      "grad_norm": 0.5139603018760681,
      "learning_rate": 0.0006844480770217793,
      "loss": 0.1869,
      "step": 3620
    },
    {
      "epoch": 1.063892145369285,
      "grad_norm": 0.3381076157093048,
      "learning_rate": 0.0006843642807341909,
      "loss": 0.2155,
      "step": 3630
    },
    {
      "epoch": 1.0668229777256741,
      "grad_norm": 0.328944593667984,
      "learning_rate": 0.0006842804844466025,
      "loss": 0.2068,
      "step": 3640
    },
    {
      "epoch": 1.0697538100820634,
      "grad_norm": 0.4368436336517334,
      "learning_rate": 0.0006841966881590141,
      "loss": 0.1864,
      "step": 3650
    },
    {
      "epoch": 1.0726846424384524,
      "grad_norm": 0.40662288665771484,
      "learning_rate": 0.0006841128918714258,
      "loss": 0.2011,
      "step": 3660
    },
    {
      "epoch": 1.0756154747948417,
      "grad_norm": 0.5529335141181946,
      "learning_rate": 0.0006840290955838375,
      "loss": 0.1834,
      "step": 3670
    },
    {
      "epoch": 1.078546307151231,
      "grad_norm": 0.5021382570266724,
      "learning_rate": 0.0006839452992962491,
      "loss": 0.1883,
      "step": 3680
    },
    {
      "epoch": 1.0814771395076201,
      "grad_norm": 0.48955410718917847,
      "learning_rate": 0.0006838615030086607,
      "loss": 0.179,
      "step": 3690
    },
    {
      "epoch": 1.0844079718640094,
      "grad_norm": 0.826737105846405,
      "learning_rate": 0.0006837777067210723,
      "loss": 0.1815,
      "step": 3700
    },
    {
      "epoch": 1.0873388042203986,
      "grad_norm": 0.4710840880870819,
      "learning_rate": 0.000683693910433484,
      "loss": 0.2096,
      "step": 3710
    },
    {
      "epoch": 1.0902696365767879,
      "grad_norm": 0.5194830894470215,
      "learning_rate": 0.0006836101141458956,
      "loss": 0.1962,
      "step": 3720
    },
    {
      "epoch": 1.0932004689331771,
      "grad_norm": 0.4628157615661621,
      "learning_rate": 0.0006835263178583072,
      "loss": 0.2046,
      "step": 3730
    },
    {
      "epoch": 1.0961313012895662,
      "grad_norm": 0.6172699928283691,
      "learning_rate": 0.0006834425215707189,
      "loss": 0.2047,
      "step": 3740
    },
    {
      "epoch": 1.0990621336459554,
      "grad_norm": 0.37693074345588684,
      "learning_rate": 0.0006833587252831304,
      "loss": 0.1981,
      "step": 3750
    },
    {
      "epoch": 1.1019929660023446,
      "grad_norm": 0.4445815980434418,
      "learning_rate": 0.0006832749289955422,
      "loss": 0.1986,
      "step": 3760
    },
    {
      "epoch": 1.1049237983587339,
      "grad_norm": 2.285773515701294,
      "learning_rate": 0.0006831911327079538,
      "loss": 0.1986,
      "step": 3770
    },
    {
      "epoch": 1.1078546307151231,
      "grad_norm": 0.4814300239086151,
      "learning_rate": 0.0006831073364203654,
      "loss": 0.1847,
      "step": 3780
    },
    {
      "epoch": 1.1107854630715124,
      "grad_norm": 0.39269396662712097,
      "learning_rate": 0.0006830235401327771,
      "loss": 0.1918,
      "step": 3790
    },
    {
      "epoch": 1.1137162954279016,
      "grad_norm": 0.32762980461120605,
      "learning_rate": 0.0006829397438451886,
      "loss": 0.1809,
      "step": 3800
    },
    {
      "epoch": 1.1166471277842906,
      "grad_norm": 0.6976489424705505,
      "learning_rate": 0.0006828559475576003,
      "loss": 0.2135,
      "step": 3810
    },
    {
      "epoch": 1.11957796014068,
      "grad_norm": 0.37386006116867065,
      "learning_rate": 0.0006827721512700119,
      "loss": 0.1955,
      "step": 3820
    },
    {
      "epoch": 1.1225087924970691,
      "grad_norm": 0.2653023302555084,
      "learning_rate": 0.0006826883549824236,
      "loss": 0.2123,
      "step": 3830
    },
    {
      "epoch": 1.1254396248534584,
      "grad_norm": 0.38994714617729187,
      "learning_rate": 0.0006826045586948353,
      "loss": 0.2161,
      "step": 3840
    },
    {
      "epoch": 1.1283704572098476,
      "grad_norm": 0.3817770183086395,
      "learning_rate": 0.0006825207624072469,
      "loss": 0.1792,
      "step": 3850
    },
    {
      "epoch": 1.1313012895662369,
      "grad_norm": 0.385083943605423,
      "learning_rate": 0.0006824369661196585,
      "loss": 0.1927,
      "step": 3860
    },
    {
      "epoch": 1.1342321219226261,
      "grad_norm": 0.5368137955665588,
      "learning_rate": 0.0006823531698320701,
      "loss": 0.1839,
      "step": 3870
    },
    {
      "epoch": 1.1371629542790151,
      "grad_norm": 0.38120755553245544,
      "learning_rate": 0.0006822693735444818,
      "loss": 0.2137,
      "step": 3880
    },
    {
      "epoch": 1.1400937866354044,
      "grad_norm": 0.8428572416305542,
      "learning_rate": 0.0006821855772568934,
      "loss": 0.2011,
      "step": 3890
    },
    {
      "epoch": 1.1430246189917936,
      "grad_norm": 0.48802635073661804,
      "learning_rate": 0.000682101780969305,
      "loss": 0.2134,
      "step": 3900
    },
    {
      "epoch": 1.1459554513481829,
      "grad_norm": 0.45093992352485657,
      "learning_rate": 0.0006820179846817166,
      "loss": 0.2028,
      "step": 3910
    },
    {
      "epoch": 1.1488862837045721,
      "grad_norm": 0.2465531975030899,
      "learning_rate": 0.0006819341883941282,
      "loss": 0.2072,
      "step": 3920
    },
    {
      "epoch": 1.1518171160609614,
      "grad_norm": 0.4441339075565338,
      "learning_rate": 0.00068185039210654,
      "loss": 0.1836,
      "step": 3930
    },
    {
      "epoch": 1.1547479484173506,
      "grad_norm": 0.7831472158432007,
      "learning_rate": 0.0006817665958189516,
      "loss": 0.2161,
      "step": 3940
    },
    {
      "epoch": 1.1576787807737396,
      "grad_norm": 0.4826456308364868,
      "learning_rate": 0.0006816827995313632,
      "loss": 0.1674,
      "step": 3950
    },
    {
      "epoch": 1.160609613130129,
      "grad_norm": 0.3320751488208771,
      "learning_rate": 0.0006815990032437748,
      "loss": 0.1766,
      "step": 3960
    },
    {
      "epoch": 1.1635404454865181,
      "grad_norm": 0.3534000813961029,
      "learning_rate": 0.0006815152069561864,
      "loss": 0.1957,
      "step": 3970
    },
    {
      "epoch": 1.1664712778429074,
      "grad_norm": 0.47693708539009094,
      "learning_rate": 0.0006814314106685981,
      "loss": 0.1907,
      "step": 3980
    },
    {
      "epoch": 1.1694021101992966,
      "grad_norm": 0.6700259447097778,
      "learning_rate": 0.0006813476143810097,
      "loss": 0.1749,
      "step": 3990
    },
    {
      "epoch": 1.1723329425556859,
      "grad_norm": 0.49928340315818787,
      "learning_rate": 0.0006812638180934213,
      "loss": 0.187,
      "step": 4000
    },
    {
      "epoch": 1.1752637749120751,
      "grad_norm": 0.4319155812263489,
      "learning_rate": 0.0006811800218058329,
      "loss": 0.194,
      "step": 4010
    },
    {
      "epoch": 1.1781946072684644,
      "grad_norm": 0.3284686207771301,
      "learning_rate": 0.0006810962255182446,
      "loss": 0.1914,
      "step": 4020
    },
    {
      "epoch": 1.1811254396248534,
      "grad_norm": 0.43729543685913086,
      "learning_rate": 0.0006810124292306563,
      "loss": 0.183,
      "step": 4030
    },
    {
      "epoch": 1.1840562719812426,
      "grad_norm": 0.5701565742492676,
      "learning_rate": 0.0006809286329430679,
      "loss": 0.2235,
      "step": 4040
    },
    {
      "epoch": 1.1869871043376319,
      "grad_norm": 0.801277220249176,
      "learning_rate": 0.0006808448366554795,
      "loss": 0.2011,
      "step": 4050
    },
    {
      "epoch": 1.1899179366940211,
      "grad_norm": 0.39733773469924927,
      "learning_rate": 0.0006807610403678911,
      "loss": 0.1892,
      "step": 4060
    },
    {
      "epoch": 1.1928487690504104,
      "grad_norm": 0.3965820074081421,
      "learning_rate": 0.0006806772440803027,
      "loss": 0.1884,
      "step": 4070
    },
    {
      "epoch": 1.1957796014067996,
      "grad_norm": 0.4057735502719879,
      "learning_rate": 0.0006805934477927144,
      "loss": 0.1911,
      "step": 4080
    },
    {
      "epoch": 1.1987104337631886,
      "grad_norm": 0.4483124315738678,
      "learning_rate": 0.000680509651505126,
      "loss": 0.2068,
      "step": 4090
    },
    {
      "epoch": 1.2016412661195779,
      "grad_norm": 0.5234428644180298,
      "learning_rate": 0.0006804258552175376,
      "loss": 0.1978,
      "step": 4100
    },
    {
      "epoch": 1.2045720984759671,
      "grad_norm": 0.5163419246673584,
      "learning_rate": 0.0006803420589299493,
      "loss": 0.2032,
      "step": 4110
    },
    {
      "epoch": 1.2075029308323564,
      "grad_norm": 0.35664328932762146,
      "learning_rate": 0.0006802582626423609,
      "loss": 0.1917,
      "step": 4120
    },
    {
      "epoch": 1.2104337631887456,
      "grad_norm": 0.45350444316864014,
      "learning_rate": 0.0006801744663547726,
      "loss": 0.1682,
      "step": 4130
    },
    {
      "epoch": 1.2133645955451349,
      "grad_norm": 0.37204402685165405,
      "learning_rate": 0.0006800906700671842,
      "loss": 0.1921,
      "step": 4140
    },
    {
      "epoch": 1.2162954279015241,
      "grad_norm": 0.5537919998168945,
      "learning_rate": 0.0006800068737795959,
      "loss": 0.2121,
      "step": 4150
    },
    {
      "epoch": 1.2192262602579134,
      "grad_norm": 1.5750492811203003,
      "learning_rate": 0.0006799230774920074,
      "loss": 0.1875,
      "step": 4160
    },
    {
      "epoch": 1.2221570926143024,
      "grad_norm": 0.4016132056713104,
      "learning_rate": 0.000679839281204419,
      "loss": 0.1729,
      "step": 4170
    },
    {
      "epoch": 1.2250879249706916,
      "grad_norm": 0.9167094230651855,
      "learning_rate": 0.0006797554849168307,
      "loss": 0.2034,
      "step": 4180
    },
    {
      "epoch": 1.2280187573270809,
      "grad_norm": 0.24321886897087097,
      "learning_rate": 0.0006796716886292424,
      "loss": 0.1813,
      "step": 4190
    },
    {
      "epoch": 1.2309495896834701,
      "grad_norm": 0.7783734202384949,
      "learning_rate": 0.0006795878923416541,
      "loss": 0.1975,
      "step": 4200
    },
    {
      "epoch": 1.2338804220398594,
      "grad_norm": 0.7015627026557922,
      "learning_rate": 0.0006795040960540657,
      "loss": 0.1918,
      "step": 4210
    },
    {
      "epoch": 1.2368112543962486,
      "grad_norm": 0.8975042700767517,
      "learning_rate": 0.0006794202997664773,
      "loss": 0.1965,
      "step": 4220
    },
    {
      "epoch": 1.2397420867526376,
      "grad_norm": 1.237105131149292,
      "learning_rate": 0.0006793365034788889,
      "loss": 0.1861,
      "step": 4230
    },
    {
      "epoch": 1.2426729191090269,
      "grad_norm": 0.5999436974525452,
      "learning_rate": 0.0006792527071913005,
      "loss": 0.187,
      "step": 4240
    },
    {
      "epoch": 1.2456037514654161,
      "grad_norm": 0.41444793343544006,
      "learning_rate": 0.0006791689109037122,
      "loss": 0.2046,
      "step": 4250
    },
    {
      "epoch": 1.2485345838218054,
      "grad_norm": 0.6234647631645203,
      "learning_rate": 0.0006790851146161238,
      "loss": 0.1968,
      "step": 4260
    },
    {
      "epoch": 1.2514654161781946,
      "grad_norm": 0.4115156829357147,
      "learning_rate": 0.0006790013183285354,
      "loss": 0.2039,
      "step": 4270
    },
    {
      "epoch": 1.2543962485345839,
      "grad_norm": 0.36532288789749146,
      "learning_rate": 0.0006789175220409471,
      "loss": 0.18,
      "step": 4280
    },
    {
      "epoch": 1.2573270808909731,
      "grad_norm": 0.5390464663505554,
      "learning_rate": 0.0006788337257533587,
      "loss": 0.1913,
      "step": 4290
    },
    {
      "epoch": 1.2602579132473624,
      "grad_norm": 0.5112124085426331,
      "learning_rate": 0.0006787499294657704,
      "loss": 0.2071,
      "step": 4300
    },
    {
      "epoch": 1.2631887456037514,
      "grad_norm": 0.3619367182254791,
      "learning_rate": 0.000678666133178182,
      "loss": 0.2084,
      "step": 4310
    },
    {
      "epoch": 1.2661195779601406,
      "grad_norm": 0.7639949321746826,
      "learning_rate": 0.0006785823368905936,
      "loss": 0.1939,
      "step": 4320
    },
    {
      "epoch": 1.2690504103165299,
      "grad_norm": 0.38875582814216614,
      "learning_rate": 0.0006784985406030052,
      "loss": 0.1973,
      "step": 4330
    },
    {
      "epoch": 1.2719812426729191,
      "grad_norm": 1.074072241783142,
      "learning_rate": 0.0006784147443154168,
      "loss": 0.1947,
      "step": 4340
    },
    {
      "epoch": 1.2749120750293084,
      "grad_norm": 0.7054648399353027,
      "learning_rate": 0.0006783309480278285,
      "loss": 0.2018,
      "step": 4350
    },
    {
      "epoch": 1.2778429073856976,
      "grad_norm": 0.686152458190918,
      "learning_rate": 0.0006782471517402402,
      "loss": 0.1972,
      "step": 4360
    },
    {
      "epoch": 1.2807737397420866,
      "grad_norm": 0.48137012124061584,
      "learning_rate": 0.0006781633554526518,
      "loss": 0.1928,
      "step": 4370
    },
    {
      "epoch": 1.2837045720984759,
      "grad_norm": 0.47073835134506226,
      "learning_rate": 0.0006780795591650634,
      "loss": 0.2145,
      "step": 4380
    },
    {
      "epoch": 1.2866354044548651,
      "grad_norm": 0.3417048752307892,
      "learning_rate": 0.0006779957628774751,
      "loss": 0.1815,
      "step": 4390
    },
    {
      "epoch": 1.2895662368112544,
      "grad_norm": 0.3493683338165283,
      "learning_rate": 0.0006779119665898867,
      "loss": 0.1782,
      "step": 4400
    },
    {
      "epoch": 1.2924970691676436,
      "grad_norm": 0.29942551255226135,
      "learning_rate": 0.0006778281703022983,
      "loss": 0.1846,
      "step": 4410
    },
    {
      "epoch": 1.2954279015240329,
      "grad_norm": 0.44002974033355713,
      "learning_rate": 0.0006777443740147099,
      "loss": 0.1809,
      "step": 4420
    },
    {
      "epoch": 1.2983587338804221,
      "grad_norm": 0.4236498773097992,
      "learning_rate": 0.0006776605777271215,
      "loss": 0.1991,
      "step": 4430
    },
    {
      "epoch": 1.3012895662368114,
      "grad_norm": 0.827945351600647,
      "learning_rate": 0.0006775767814395332,
      "loss": 0.2073,
      "step": 4440
    },
    {
      "epoch": 1.3042203985932006,
      "grad_norm": 0.5374637246131897,
      "learning_rate": 0.0006774929851519449,
      "loss": 0.187,
      "step": 4450
    },
    {
      "epoch": 1.3071512309495896,
      "grad_norm": 0.5065147280693054,
      "learning_rate": 0.0006774091888643565,
      "loss": 0.2002,
      "step": 4460
    },
    {
      "epoch": 1.3100820633059789,
      "grad_norm": 0.8198986649513245,
      "learning_rate": 0.0006773253925767681,
      "loss": 0.1881,
      "step": 4470
    },
    {
      "epoch": 1.3130128956623681,
      "grad_norm": 1.0239912271499634,
      "learning_rate": 0.0006772415962891797,
      "loss": 0.1831,
      "step": 4480
    },
    {
      "epoch": 1.3159437280187574,
      "grad_norm": 0.6930854916572571,
      "learning_rate": 0.0006771578000015914,
      "loss": 0.1834,
      "step": 4490
    },
    {
      "epoch": 1.3188745603751466,
      "grad_norm": 0.7067488431930542,
      "learning_rate": 0.000677074003714003,
      "loss": 0.1939,
      "step": 4500
    },
    {
      "epoch": 1.3218053927315356,
      "grad_norm": 0.5202122330665588,
      "learning_rate": 0.0006769902074264146,
      "loss": 0.1745,
      "step": 4510
    },
    {
      "epoch": 1.3247362250879249,
      "grad_norm": 0.4039599597454071,
      "learning_rate": 0.0006769064111388262,
      "loss": 0.2037,
      "step": 4520
    },
    {
      "epoch": 1.3276670574443141,
      "grad_norm": 0.39969074726104736,
      "learning_rate": 0.0006768226148512378,
      "loss": 0.1867,
      "step": 4530
    },
    {
      "epoch": 1.3305978898007034,
      "grad_norm": 0.3178532123565674,
      "learning_rate": 0.0006767388185636496,
      "loss": 0.1936,
      "step": 4540
    },
    {
      "epoch": 1.3335287221570926,
      "grad_norm": 0.3268692195415497,
      "learning_rate": 0.0006766550222760612,
      "loss": 0.2044,
      "step": 4550
    },
    {
      "epoch": 1.3364595545134819,
      "grad_norm": 0.458767831325531,
      "learning_rate": 0.0006765712259884729,
      "loss": 0.1843,
      "step": 4560
    },
    {
      "epoch": 1.339390386869871,
      "grad_norm": 0.31649062037467957,
      "learning_rate": 0.0006764874297008844,
      "loss": 0.2086,
      "step": 4570
    },
    {
      "epoch": 1.3423212192262604,
      "grad_norm": 0.4281989634037018,
      "learning_rate": 0.000676403633413296,
      "loss": 0.1951,
      "step": 4580
    },
    {
      "epoch": 1.3452520515826496,
      "grad_norm": 0.6592671275138855,
      "learning_rate": 0.0006763198371257077,
      "loss": 0.197,
      "step": 4590
    },
    {
      "epoch": 1.3481828839390386,
      "grad_norm": 0.44703152775764465,
      "learning_rate": 0.0006762360408381193,
      "loss": 0.2109,
      "step": 4600
    },
    {
      "epoch": 1.3511137162954279,
      "grad_norm": 0.35592296719551086,
      "learning_rate": 0.000676152244550531,
      "loss": 0.1925,
      "step": 4610
    },
    {
      "epoch": 1.3540445486518171,
      "grad_norm": 0.6864845752716064,
      "learning_rate": 0.0006760684482629427,
      "loss": 0.1839,
      "step": 4620
    },
    {
      "epoch": 1.3569753810082064,
      "grad_norm": 0.4391559660434723,
      "learning_rate": 0.0006759846519753542,
      "loss": 0.1958,
      "step": 4630
    },
    {
      "epoch": 1.3599062133645956,
      "grad_norm": 0.44367843866348267,
      "learning_rate": 0.0006759008556877659,
      "loss": 0.2085,
      "step": 4640
    },
    {
      "epoch": 1.3628370457209846,
      "grad_norm": 0.4660152494907379,
      "learning_rate": 0.0006758170594001775,
      "loss": 0.1827,
      "step": 4650
    },
    {
      "epoch": 1.3657678780773739,
      "grad_norm": 0.41864028573036194,
      "learning_rate": 0.0006757332631125892,
      "loss": 0.1936,
      "step": 4660
    },
    {
      "epoch": 1.3686987104337631,
      "grad_norm": 0.37886160612106323,
      "learning_rate": 0.0006756494668250008,
      "loss": 0.1835,
      "step": 4670
    },
    {
      "epoch": 1.3716295427901524,
      "grad_norm": 0.33206313848495483,
      "learning_rate": 0.0006755656705374123,
      "loss": 0.1852,
      "step": 4680
    },
    {
      "epoch": 1.3745603751465416,
      "grad_norm": 0.37161847949028015,
      "learning_rate": 0.000675481874249824,
      "loss": 0.1875,
      "step": 4690
    },
    {
      "epoch": 1.3774912075029309,
      "grad_norm": 0.7925884127616882,
      "learning_rate": 0.0006753980779622356,
      "loss": 0.1783,
      "step": 4700
    },
    {
      "epoch": 1.38042203985932,
      "grad_norm": 0.5614631175994873,
      "learning_rate": 0.0006753142816746474,
      "loss": 0.2165,
      "step": 4710
    },
    {
      "epoch": 1.3833528722157094,
      "grad_norm": 0.39924225211143494,
      "learning_rate": 0.000675230485387059,
      "loss": 0.2134,
      "step": 4720
    },
    {
      "epoch": 1.3862837045720986,
      "grad_norm": 0.43238455057144165,
      "learning_rate": 0.0006751466890994706,
      "loss": 0.2018,
      "step": 4730
    },
    {
      "epoch": 1.3892145369284876,
      "grad_norm": 0.6767054796218872,
      "learning_rate": 0.0006750628928118822,
      "loss": 0.1809,
      "step": 4740
    },
    {
      "epoch": 1.3921453692848769,
      "grad_norm": 0.42892616987228394,
      "learning_rate": 0.0006749790965242938,
      "loss": 0.1774,
      "step": 4750
    },
    {
      "epoch": 1.3950762016412661,
      "grad_norm": 0.6294037103652954,
      "learning_rate": 0.0006748953002367055,
      "loss": 0.1925,
      "step": 4760
    },
    {
      "epoch": 1.3980070339976554,
      "grad_norm": 0.6185023784637451,
      "learning_rate": 0.0006748115039491171,
      "loss": 0.167,
      "step": 4770
    },
    {
      "epoch": 1.4009378663540446,
      "grad_norm": 0.36163219809532166,
      "learning_rate": 0.0006747277076615287,
      "loss": 0.1858,
      "step": 4780
    },
    {
      "epoch": 1.4038686987104336,
      "grad_norm": 0.8393384218215942,
      "learning_rate": 0.0006746439113739403,
      "loss": 0.1931,
      "step": 4790
    },
    {
      "epoch": 1.4067995310668229,
      "grad_norm": 0.9717970490455627,
      "learning_rate": 0.000674560115086352,
      "loss": 0.193,
      "step": 4800
    },
    {
      "epoch": 1.4097303634232121,
      "grad_norm": 0.38289523124694824,
      "learning_rate": 0.0006744763187987637,
      "loss": 0.176,
      "step": 4810
    },
    {
      "epoch": 1.4126611957796014,
      "grad_norm": 0.548057496547699,
      "learning_rate": 0.0006743925225111753,
      "loss": 0.1711,
      "step": 4820
    },
    {
      "epoch": 1.4155920281359906,
      "grad_norm": 0.9232014417648315,
      "learning_rate": 0.0006743087262235869,
      "loss": 0.1862,
      "step": 4830
    },
    {
      "epoch": 1.4185228604923799,
      "grad_norm": 0.31581687927246094,
      "learning_rate": 0.0006742249299359985,
      "loss": 0.1749,
      "step": 4840
    },
    {
      "epoch": 1.421453692848769,
      "grad_norm": 0.4527137577533722,
      "learning_rate": 0.0006741411336484101,
      "loss": 0.203,
      "step": 4850
    },
    {
      "epoch": 1.4243845252051583,
      "grad_norm": 0.4974783957004547,
      "learning_rate": 0.0006740573373608218,
      "loss": 0.1893,
      "step": 4860
    },
    {
      "epoch": 1.4273153575615476,
      "grad_norm": 0.6068015694618225,
      "learning_rate": 0.0006739735410732334,
      "loss": 0.1915,
      "step": 4870
    },
    {
      "epoch": 1.4302461899179366,
      "grad_norm": 0.44971051812171936,
      "learning_rate": 0.000673889744785645,
      "loss": 0.2043,
      "step": 4880
    },
    {
      "epoch": 1.4331770222743259,
      "grad_norm": 0.45342209935188293,
      "learning_rate": 0.0006738059484980567,
      "loss": 0.1936,
      "step": 4890
    },
    {
      "epoch": 1.436107854630715,
      "grad_norm": 0.4320896565914154,
      "learning_rate": 0.0006737221522104684,
      "loss": 0.1905,
      "step": 4900
    },
    {
      "epoch": 1.4390386869871044,
      "grad_norm": 0.4155865013599396,
      "learning_rate": 0.00067363835592288,
      "loss": 0.2101,
      "step": 4910
    },
    {
      "epoch": 1.4419695193434936,
      "grad_norm": 0.563140869140625,
      "learning_rate": 0.0006735545596352916,
      "loss": 0.1861,
      "step": 4920
    },
    {
      "epoch": 1.4449003516998828,
      "grad_norm": 0.4603794813156128,
      "learning_rate": 0.0006734707633477032,
      "loss": 0.2017,
      "step": 4930
    },
    {
      "epoch": 1.4478311840562719,
      "grad_norm": 0.40930911898612976,
      "learning_rate": 0.0006733869670601148,
      "loss": 0.2039,
      "step": 4940
    },
    {
      "epoch": 1.4507620164126611,
      "grad_norm": 0.40089088678359985,
      "learning_rate": 0.0006733031707725265,
      "loss": 0.1817,
      "step": 4950
    },
    {
      "epoch": 1.4536928487690504,
      "grad_norm": 0.3434719443321228,
      "learning_rate": 0.0006732193744849381,
      "loss": 0.1813,
      "step": 4960
    },
    {
      "epoch": 1.4566236811254396,
      "grad_norm": 0.7647321224212646,
      "learning_rate": 0.0006731355781973498,
      "loss": 0.1742,
      "step": 4970
    },
    {
      "epoch": 1.4595545134818289,
      "grad_norm": 0.723455548286438,
      "learning_rate": 0.0006730517819097615,
      "loss": 0.1863,
      "step": 4980
    },
    {
      "epoch": 1.462485345838218,
      "grad_norm": 0.5630262494087219,
      "learning_rate": 0.000672967985622173,
      "loss": 0.192,
      "step": 4990
    },
    {
      "epoch": 1.4654161781946073,
      "grad_norm": 0.6677998304367065,
      "learning_rate": 0.0006728841893345847,
      "loss": 0.1964,
      "step": 5000
    },
    {
      "epoch": 1.4683470105509966,
      "grad_norm": 0.5285955667495728,
      "learning_rate": 0.0006728003930469963,
      "loss": 0.2175,
      "step": 5010
    },
    {
      "epoch": 1.4712778429073858,
      "grad_norm": 0.7055652141571045,
      "learning_rate": 0.0006727165967594079,
      "loss": 0.2016,
      "step": 5020
    },
    {
      "epoch": 1.4742086752637749,
      "grad_norm": 0.6748186945915222,
      "learning_rate": 0.0006726328004718196,
      "loss": 0.2185,
      "step": 5030
    },
    {
      "epoch": 1.477139507620164,
      "grad_norm": 0.6023404598236084,
      "learning_rate": 0.0006725490041842311,
      "loss": 0.2019,
      "step": 5040
    },
    {
      "epoch": 1.4800703399765534,
      "grad_norm": 0.3575468957424164,
      "learning_rate": 0.0006724652078966428,
      "loss": 0.2015,
      "step": 5050
    },
    {
      "epoch": 1.4830011723329426,
      "grad_norm": 0.7064626812934875,
      "learning_rate": 0.0006723814116090545,
      "loss": 0.1941,
      "step": 5060
    },
    {
      "epoch": 1.4859320046893318,
      "grad_norm": 0.5655660629272461,
      "learning_rate": 0.0006722976153214662,
      "loss": 0.1997,
      "step": 5070
    },
    {
      "epoch": 1.4888628370457209,
      "grad_norm": 0.48271945118904114,
      "learning_rate": 0.0006722138190338778,
      "loss": 0.203,
      "step": 5080
    },
    {
      "epoch": 1.4917936694021101,
      "grad_norm": 0.4549765884876251,
      "learning_rate": 0.0006721300227462894,
      "loss": 0.1965,
      "step": 5090
    },
    {
      "epoch": 1.4947245017584994,
      "grad_norm": 0.6093425750732422,
      "learning_rate": 0.000672046226458701,
      "loss": 0.1969,
      "step": 5100
    },
    {
      "epoch": 1.4976553341148886,
      "grad_norm": 0.8624505996704102,
      "learning_rate": 0.0006719624301711126,
      "loss": 0.2143,
      "step": 5110
    },
    {
      "epoch": 1.5005861664712778,
      "grad_norm": 0.41119953989982605,
      "learning_rate": 0.0006718786338835243,
      "loss": 0.1929,
      "step": 5120
    },
    {
      "epoch": 1.503516998827667,
      "grad_norm": 1.1252158880233765,
      "learning_rate": 0.0006717948375959359,
      "loss": 0.1857,
      "step": 5130
    },
    {
      "epoch": 1.5064478311840563,
      "grad_norm": 0.41639405488967896,
      "learning_rate": 0.0006717110413083476,
      "loss": 0.1978,
      "step": 5140
    },
    {
      "epoch": 1.5093786635404456,
      "grad_norm": 0.5215347409248352,
      "learning_rate": 0.0006716272450207592,
      "loss": 0.2107,
      "step": 5150
    },
    {
      "epoch": 1.5123094958968348,
      "grad_norm": 0.35451820492744446,
      "learning_rate": 0.0006715434487331708,
      "loss": 0.1846,
      "step": 5160
    },
    {
      "epoch": 1.515240328253224,
      "grad_norm": 0.6454969644546509,
      "learning_rate": 0.0006714596524455825,
      "loss": 0.2094,
      "step": 5170
    },
    {
      "epoch": 1.518171160609613,
      "grad_norm": 0.4603482782840729,
      "learning_rate": 0.0006713758561579941,
      "loss": 0.2052,
      "step": 5180
    },
    {
      "epoch": 1.5211019929660023,
      "grad_norm": 0.40913626551628113,
      "learning_rate": 0.0006712920598704057,
      "loss": 0.1877,
      "step": 5190
    },
    {
      "epoch": 1.5240328253223916,
      "grad_norm": 0.533105731010437,
      "learning_rate": 0.0006712082635828173,
      "loss": 0.1803,
      "step": 5200
    },
    {
      "epoch": 1.5269636576787806,
      "grad_norm": 0.384036660194397,
      "learning_rate": 0.0006711244672952289,
      "loss": 0.1868,
      "step": 5210
    },
    {
      "epoch": 1.5298944900351699,
      "grad_norm": 0.40697920322418213,
      "learning_rate": 0.0006710406710076406,
      "loss": 0.1752,
      "step": 5220
    },
    {
      "epoch": 1.532825322391559,
      "grad_norm": 0.6226237416267395,
      "learning_rate": 0.0006709568747200523,
      "loss": 0.1818,
      "step": 5230
    },
    {
      "epoch": 1.5357561547479484,
      "grad_norm": 0.5550580024719238,
      "learning_rate": 0.0006708730784324639,
      "loss": 0.1827,
      "step": 5240
    },
    {
      "epoch": 1.5386869871043376,
      "grad_norm": 0.6120147705078125,
      "learning_rate": 0.0006707892821448755,
      "loss": 0.1715,
      "step": 5250
    },
    {
      "epoch": 1.5416178194607268,
      "grad_norm": 0.5044808983802795,
      "learning_rate": 0.0006707054858572871,
      "loss": 0.1916,
      "step": 5260
    },
    {
      "epoch": 1.544548651817116,
      "grad_norm": 0.3662976026535034,
      "learning_rate": 0.0006706216895696988,
      "loss": 0.1904,
      "step": 5270
    },
    {
      "epoch": 1.5474794841735053,
      "grad_norm": 0.39139172434806824,
      "learning_rate": 0.0006705378932821104,
      "loss": 0.199,
      "step": 5280
    },
    {
      "epoch": 1.5504103165298946,
      "grad_norm": 0.3249717056751251,
      "learning_rate": 0.000670454096994522,
      "loss": 0.1843,
      "step": 5290
    },
    {
      "epoch": 1.5533411488862838,
      "grad_norm": 0.3531765639781952,
      "learning_rate": 0.0006703703007069336,
      "loss": 0.1776,
      "step": 5300
    },
    {
      "epoch": 1.556271981242673,
      "grad_norm": 0.6090799570083618,
      "learning_rate": 0.0006702865044193452,
      "loss": 0.2048,
      "step": 5310
    },
    {
      "epoch": 1.559202813599062,
      "grad_norm": 0.469409704208374,
      "learning_rate": 0.000670202708131757,
      "loss": 0.1697,
      "step": 5320
    },
    {
      "epoch": 1.5621336459554513,
      "grad_norm": 0.6481700539588928,
      "learning_rate": 0.0006701189118441686,
      "loss": 0.1834,
      "step": 5330
    },
    {
      "epoch": 1.5650644783118406,
      "grad_norm": 0.4184474050998688,
      "learning_rate": 0.0006700351155565802,
      "loss": 0.1881,
      "step": 5340
    },
    {
      "epoch": 1.5679953106682296,
      "grad_norm": 0.7951863408088684,
      "learning_rate": 0.0006699513192689918,
      "loss": 0.1788,
      "step": 5350
    },
    {
      "epoch": 1.5709261430246189,
      "grad_norm": 0.9031449556350708,
      "learning_rate": 0.0006698675229814034,
      "loss": 0.1978,
      "step": 5360
    },
    {
      "epoch": 1.573856975381008,
      "grad_norm": 0.4148399233818054,
      "learning_rate": 0.0006697837266938151,
      "loss": 0.1781,
      "step": 5370
    },
    {
      "epoch": 1.5767878077373974,
      "grad_norm": 0.6381756663322449,
      "learning_rate": 0.0006696999304062267,
      "loss": 0.1804,
      "step": 5380
    },
    {
      "epoch": 1.5797186400937866,
      "grad_norm": 0.8162452578544617,
      "learning_rate": 0.0006696161341186384,
      "loss": 0.1623,
      "step": 5390
    },
    {
      "epoch": 1.5826494724501758,
      "grad_norm": 0.6717135906219482,
      "learning_rate": 0.0006695323378310499,
      "loss": 0.2161,
      "step": 5400
    },
    {
      "epoch": 1.585580304806565,
      "grad_norm": 0.4712221026420593,
      "learning_rate": 0.0006694485415434616,
      "loss": 0.1975,
      "step": 5410
    },
    {
      "epoch": 1.5885111371629543,
      "grad_norm": 0.6019139885902405,
      "learning_rate": 0.0006693647452558733,
      "loss": 0.192,
      "step": 5420
    },
    {
      "epoch": 1.5914419695193436,
      "grad_norm": 0.5354062914848328,
      "learning_rate": 0.0006692809489682849,
      "loss": 0.1924,
      "step": 5430
    },
    {
      "epoch": 1.5943728018757328,
      "grad_norm": 0.601179301738739,
      "learning_rate": 0.0006691971526806966,
      "loss": 0.1705,
      "step": 5440
    },
    {
      "epoch": 1.597303634232122,
      "grad_norm": 0.4421226680278778,
      "learning_rate": 0.0006691133563931081,
      "loss": 0.1868,
      "step": 5450
    },
    {
      "epoch": 1.600234466588511,
      "grad_norm": 0.8614981770515442,
      "learning_rate": 0.0006690295601055198,
      "loss": 0.1789,
      "step": 5460
    },
    {
      "epoch": 1.6031652989449003,
      "grad_norm": 0.5258734822273254,
      "learning_rate": 0.0006689457638179314,
      "loss": 0.187,
      "step": 5470
    },
    {
      "epoch": 1.6060961313012896,
      "grad_norm": 0.7131326198577881,
      "learning_rate": 0.000668861967530343,
      "loss": 0.1838,
      "step": 5480
    },
    {
      "epoch": 1.6090269636576788,
      "grad_norm": 0.550012469291687,
      "learning_rate": 0.0006687781712427548,
      "loss": 0.1928,
      "step": 5490
    },
    {
      "epoch": 1.6119577960140679,
      "grad_norm": 0.6146078705787659,
      "learning_rate": 0.0006686943749551664,
      "loss": 0.183,
      "step": 5500
    },
    {
      "epoch": 1.614888628370457,
      "grad_norm": 0.6169481873512268,
      "learning_rate": 0.000668610578667578,
      "loss": 0.1837,
      "step": 5510
    },
    {
      "epoch": 1.6178194607268463,
      "grad_norm": 0.4600955843925476,
      "learning_rate": 0.0006685267823799896,
      "loss": 0.1953,
      "step": 5520
    },
    {
      "epoch": 1.6207502930832356,
      "grad_norm": 0.5796840786933899,
      "learning_rate": 0.0006684429860924012,
      "loss": 0.1816,
      "step": 5530
    },
    {
      "epoch": 1.6236811254396248,
      "grad_norm": 0.3734830319881439,
      "learning_rate": 0.0006683591898048129,
      "loss": 0.2011,
      "step": 5540
    },
    {
      "epoch": 1.626611957796014,
      "grad_norm": 0.41197118163108826,
      "learning_rate": 0.0006682753935172245,
      "loss": 0.197,
      "step": 5550
    },
    {
      "epoch": 1.6295427901524033,
      "grad_norm": 0.3349093198776245,
      "learning_rate": 0.0006681915972296361,
      "loss": 0.1961,
      "step": 5560
    },
    {
      "epoch": 1.6324736225087926,
      "grad_norm": 0.3121652901172638,
      "learning_rate": 0.0006681078009420477,
      "loss": 0.1867,
      "step": 5570
    },
    {
      "epoch": 1.6354044548651818,
      "grad_norm": 0.4584363102912903,
      "learning_rate": 0.0006680240046544594,
      "loss": 0.1676,
      "step": 5580
    },
    {
      "epoch": 1.638335287221571,
      "grad_norm": 0.686059832572937,
      "learning_rate": 0.0006679402083668711,
      "loss": 0.1987,
      "step": 5590
    },
    {
      "epoch": 1.64126611957796,
      "grad_norm": 0.5681869387626648,
      "learning_rate": 0.0006678564120792827,
      "loss": 0.1826,
      "step": 5600
    },
    {
      "epoch": 1.6441969519343493,
      "grad_norm": 0.7274885177612305,
      "learning_rate": 0.0006677726157916943,
      "loss": 0.2007,
      "step": 5610
    },
    {
      "epoch": 1.6471277842907386,
      "grad_norm": 0.5618794560432434,
      "learning_rate": 0.0006676888195041059,
      "loss": 0.2062,
      "step": 5620
    },
    {
      "epoch": 1.6500586166471278,
      "grad_norm": 1.052126169204712,
      "learning_rate": 0.0006676050232165176,
      "loss": 0.1853,
      "step": 5630
    },
    {
      "epoch": 1.6529894490035169,
      "grad_norm": 0.6445032954216003,
      "learning_rate": 0.0006675212269289292,
      "loss": 0.2029,
      "step": 5640
    },
    {
      "epoch": 1.655920281359906,
      "grad_norm": 0.7601693868637085,
      "learning_rate": 0.0006674374306413408,
      "loss": 0.1744,
      "step": 5650
    },
    {
      "epoch": 1.6588511137162953,
      "grad_norm": 0.4094017446041107,
      "learning_rate": 0.0006673536343537524,
      "loss": 0.1897,
      "step": 5660
    },
    {
      "epoch": 1.6617819460726846,
      "grad_norm": 0.8057734370231628,
      "learning_rate": 0.0006672698380661641,
      "loss": 0.2104,
      "step": 5670
    },
    {
      "epoch": 1.6647127784290738,
      "grad_norm": 0.44080594182014465,
      "learning_rate": 0.0006671860417785758,
      "loss": 0.204,
      "step": 5680
    },
    {
      "epoch": 1.667643610785463,
      "grad_norm": 0.4511506259441376,
      "learning_rate": 0.0006671022454909874,
      "loss": 0.1935,
      "step": 5690
    },
    {
      "epoch": 1.6705744431418523,
      "grad_norm": 0.659976601600647,
      "learning_rate": 0.000667018449203399,
      "loss": 0.1798,
      "step": 5700
    },
    {
      "epoch": 1.6735052754982416,
      "grad_norm": 0.36476409435272217,
      "learning_rate": 0.0006669346529158106,
      "loss": 0.1924,
      "step": 5710
    },
    {
      "epoch": 1.6764361078546308,
      "grad_norm": 0.3396187424659729,
      "learning_rate": 0.0006668508566282222,
      "loss": 0.1978,
      "step": 5720
    },
    {
      "epoch": 1.67936694021102,
      "grad_norm": 0.43948984146118164,
      "learning_rate": 0.0006667670603406339,
      "loss": 0.1726,
      "step": 5730
    },
    {
      "epoch": 1.682297772567409,
      "grad_norm": 0.6320687532424927,
      "learning_rate": 0.0006666832640530455,
      "loss": 0.1847,
      "step": 5740
    },
    {
      "epoch": 1.6852286049237983,
      "grad_norm": 0.6322684288024902,
      "learning_rate": 0.0006665994677654572,
      "loss": 0.1623,
      "step": 5750
    },
    {
      "epoch": 1.6881594372801876,
      "grad_norm": 0.7751505374908447,
      "learning_rate": 0.0006665156714778688,
      "loss": 0.1758,
      "step": 5760
    },
    {
      "epoch": 1.6910902696365768,
      "grad_norm": 0.6921502351760864,
      "learning_rate": 0.0006664318751902804,
      "loss": 0.2322,
      "step": 5770
    },
    {
      "epoch": 1.6940211019929658,
      "grad_norm": 0.5732599496841431,
      "learning_rate": 0.0006663480789026921,
      "loss": 0.177,
      "step": 5780
    },
    {
      "epoch": 1.696951934349355,
      "grad_norm": 0.43449801206588745,
      "learning_rate": 0.0006662642826151037,
      "loss": 0.1754,
      "step": 5790
    },
    {
      "epoch": 1.6998827667057443,
      "grad_norm": 0.7087424397468567,
      "learning_rate": 0.0006661804863275154,
      "loss": 0.1691,
      "step": 5800
    },
    {
      "epoch": 1.7028135990621336,
      "grad_norm": 0.3842886984348297,
      "learning_rate": 0.0006660966900399269,
      "loss": 0.1734,
      "step": 5810
    },
    {
      "epoch": 1.7057444314185228,
      "grad_norm": 0.3012372553348541,
      "learning_rate": 0.0006660128937523385,
      "loss": 0.1924,
      "step": 5820
    },
    {
      "epoch": 1.708675263774912,
      "grad_norm": 0.4293745458126068,
      "learning_rate": 0.0006659290974647502,
      "loss": 0.2022,
      "step": 5830
    },
    {
      "epoch": 1.7116060961313013,
      "grad_norm": 0.4002905488014221,
      "learning_rate": 0.0006658453011771619,
      "loss": 0.1739,
      "step": 5840
    },
    {
      "epoch": 1.7145369284876906,
      "grad_norm": 0.5524121522903442,
      "learning_rate": 0.0006657615048895736,
      "loss": 0.171,
      "step": 5850
    },
    {
      "epoch": 1.7174677608440798,
      "grad_norm": 0.5867640972137451,
      "learning_rate": 0.0006656777086019852,
      "loss": 0.2313,
      "step": 5860
    },
    {
      "epoch": 1.720398593200469,
      "grad_norm": 0.4535168409347534,
      "learning_rate": 0.0006655939123143967,
      "loss": 0.1867,
      "step": 5870
    },
    {
      "epoch": 1.7233294255568583,
      "grad_norm": 0.6277126669883728,
      "learning_rate": 0.0006655101160268084,
      "loss": 0.1689,
      "step": 5880
    },
    {
      "epoch": 1.7262602579132473,
      "grad_norm": 0.36532142758369446,
      "learning_rate": 0.00066542631973922,
      "loss": 0.198,
      "step": 5890
    },
    {
      "epoch": 1.7291910902696366,
      "grad_norm": 0.4850155711174011,
      "learning_rate": 0.0006653425234516317,
      "loss": 0.1844,
      "step": 5900
    },
    {
      "epoch": 1.7321219226260258,
      "grad_norm": 0.5667616128921509,
      "learning_rate": 0.0006652587271640433,
      "loss": 0.1814,
      "step": 5910
    },
    {
      "epoch": 1.7350527549824148,
      "grad_norm": 0.6749927997589111,
      "learning_rate": 0.0006651749308764548,
      "loss": 0.2126,
      "step": 5920
    },
    {
      "epoch": 1.737983587338804,
      "grad_norm": 0.39253103733062744,
      "learning_rate": 0.0006650911345888666,
      "loss": 0.1852,
      "step": 5930
    },
    {
      "epoch": 1.7409144196951933,
      "grad_norm": 0.3103346526622772,
      "learning_rate": 0.0006650073383012782,
      "loss": 0.1896,
      "step": 5940
    },
    {
      "epoch": 1.7438452520515826,
      "grad_norm": 0.4018707573413849,
      "learning_rate": 0.0006649235420136899,
      "loss": 0.166,
      "step": 5950
    },
    {
      "epoch": 1.7467760844079718,
      "grad_norm": 0.5290033221244812,
      "learning_rate": 0.0006648397457261015,
      "loss": 0.1596,
      "step": 5960
    },
    {
      "epoch": 1.749706916764361,
      "grad_norm": 0.6684449911117554,
      "learning_rate": 0.0006647559494385131,
      "loss": 0.1916,
      "step": 5970
    },
    {
      "epoch": 1.7526377491207503,
      "grad_norm": 0.6955764293670654,
      "learning_rate": 0.0006646721531509247,
      "loss": 0.1836,
      "step": 5980
    },
    {
      "epoch": 1.7555685814771396,
      "grad_norm": 0.5400288701057434,
      "learning_rate": 0.0006645883568633363,
      "loss": 0.1973,
      "step": 5990
    },
    {
      "epoch": 1.7584994138335288,
      "grad_norm": 0.43422362208366394,
      "learning_rate": 0.000664504560575748,
      "loss": 0.1946,
      "step": 6000
    },
    {
      "epoch": 1.761430246189918,
      "grad_norm": 0.7679637670516968,
      "learning_rate": 0.0006644207642881597,
      "loss": 0.1801,
      "step": 6010
    },
    {
      "epoch": 1.7643610785463073,
      "grad_norm": 0.410607248544693,
      "learning_rate": 0.0006643369680005713,
      "loss": 0.1839,
      "step": 6020
    },
    {
      "epoch": 1.7672919109026963,
      "grad_norm": 0.7569833993911743,
      "learning_rate": 0.0006642531717129829,
      "loss": 0.1887,
      "step": 6030
    },
    {
      "epoch": 1.7702227432590856,
      "grad_norm": 1.036086916923523,
      "learning_rate": 0.0006641693754253945,
      "loss": 0.1924,
      "step": 6040
    },
    {
      "epoch": 1.7731535756154748,
      "grad_norm": 0.5131836533546448,
      "learning_rate": 0.0006640855791378062,
      "loss": 0.1935,
      "step": 6050
    },
    {
      "epoch": 1.7760844079718638,
      "grad_norm": 0.5267009735107422,
      "learning_rate": 0.0006640017828502178,
      "loss": 0.1859,
      "step": 6060
    },
    {
      "epoch": 1.779015240328253,
      "grad_norm": 0.5410290956497192,
      "learning_rate": 0.0006639179865626294,
      "loss": 0.164,
      "step": 6070
    },
    {
      "epoch": 1.7819460726846423,
      "grad_norm": 1.0113961696624756,
      "learning_rate": 0.000663834190275041,
      "loss": 0.1767,
      "step": 6080
    },
    {
      "epoch": 1.7848769050410316,
      "grad_norm": 0.45361092686653137,
      "learning_rate": 0.0006637503939874526,
      "loss": 0.1881,
      "step": 6090
    },
    {
      "epoch": 1.7878077373974208,
      "grad_norm": 0.63942551612854,
      "learning_rate": 0.0006636665976998644,
      "loss": 0.1785,
      "step": 6100
    },
    {
      "epoch": 1.79073856975381,
      "grad_norm": 0.3948519825935364,
      "learning_rate": 0.000663582801412276,
      "loss": 0.1879,
      "step": 6110
    },
    {
      "epoch": 1.7936694021101993,
      "grad_norm": 0.5904169678688049,
      "learning_rate": 0.0006634990051246876,
      "loss": 0.1855,
      "step": 6120
    },
    {
      "epoch": 1.7966002344665886,
      "grad_norm": 0.4866385757923126,
      "learning_rate": 0.0006634152088370992,
      "loss": 0.1697,
      "step": 6130
    },
    {
      "epoch": 1.7995310668229778,
      "grad_norm": 0.6051859259605408,
      "learning_rate": 0.0006633314125495109,
      "loss": 0.184,
      "step": 6140
    },
    {
      "epoch": 1.802461899179367,
      "grad_norm": 0.7505449652671814,
      "learning_rate": 0.0006632476162619225,
      "loss": 0.1873,
      "step": 6150
    },
    {
      "epoch": 1.8053927315357563,
      "grad_norm": 1.0936474800109863,
      "learning_rate": 0.0006631638199743341,
      "loss": 0.2039,
      "step": 6160
    },
    {
      "epoch": 1.8083235638921453,
      "grad_norm": 0.5155618786811829,
      "learning_rate": 0.0006630800236867457,
      "loss": 0.1937,
      "step": 6170
    },
    {
      "epoch": 1.8112543962485346,
      "grad_norm": 0.4091019034385681,
      "learning_rate": 0.0006629962273991573,
      "loss": 0.1809,
      "step": 6180
    },
    {
      "epoch": 1.8141852286049238,
      "grad_norm": 0.3883344531059265,
      "learning_rate": 0.0006629124311115691,
      "loss": 0.1904,
      "step": 6190
    },
    {
      "epoch": 1.817116060961313,
      "grad_norm": 0.324056476354599,
      "learning_rate": 0.0006628286348239807,
      "loss": 0.1608,
      "step": 6200
    },
    {
      "epoch": 1.820046893317702,
      "grad_norm": 0.6046651601791382,
      "learning_rate": 0.0006627448385363923,
      "loss": 0.2003,
      "step": 6210
    },
    {
      "epoch": 1.8229777256740913,
      "grad_norm": 0.9134698510169983,
      "learning_rate": 0.0006626610422488039,
      "loss": 0.1684,
      "step": 6220
    },
    {
      "epoch": 1.8259085580304806,
      "grad_norm": 0.7288062572479248,
      "learning_rate": 0.0006625772459612155,
      "loss": 0.1765,
      "step": 6230
    },
    {
      "epoch": 1.8288393903868698,
      "grad_norm": 0.38812336325645447,
      "learning_rate": 0.0006624934496736272,
      "loss": 0.1876,
      "step": 6240
    },
    {
      "epoch": 1.831770222743259,
      "grad_norm": 0.33589115738868713,
      "learning_rate": 0.0006624096533860388,
      "loss": 0.1601,
      "step": 6250
    },
    {
      "epoch": 1.8347010550996483,
      "grad_norm": 0.6883012652397156,
      "learning_rate": 0.0006623258570984504,
      "loss": 0.1873,
      "step": 6260
    },
    {
      "epoch": 1.8376318874560376,
      "grad_norm": 0.4494796097278595,
      "learning_rate": 0.0006622420608108622,
      "loss": 0.1764,
      "step": 6270
    },
    {
      "epoch": 1.8405627198124268,
      "grad_norm": 0.39577868580818176,
      "learning_rate": 0.0006621582645232737,
      "loss": 0.1994,
      "step": 6280
    },
    {
      "epoch": 1.843493552168816,
      "grad_norm": 0.5470613837242126,
      "learning_rate": 0.0006620744682356854,
      "loss": 0.2004,
      "step": 6290
    },
    {
      "epoch": 1.8464243845252053,
      "grad_norm": 0.2703152894973755,
      "learning_rate": 0.000661990671948097,
      "loss": 0.1701,
      "step": 6300
    },
    {
      "epoch": 1.8493552168815943,
      "grad_norm": 0.3965056538581848,
      "learning_rate": 0.0006619068756605087,
      "loss": 0.1709,
      "step": 6310
    },
    {
      "epoch": 1.8522860492379836,
      "grad_norm": 0.4570316672325134,
      "learning_rate": 0.0006618230793729203,
      "loss": 0.19,
      "step": 6320
    },
    {
      "epoch": 1.8552168815943728,
      "grad_norm": 0.6068870425224304,
      "learning_rate": 0.0006617392830853318,
      "loss": 0.1843,
      "step": 6330
    },
    {
      "epoch": 1.858147713950762,
      "grad_norm": 0.5718895196914673,
      "learning_rate": 0.0006616554867977435,
      "loss": 0.1764,
      "step": 6340
    },
    {
      "epoch": 1.861078546307151,
      "grad_norm": 0.601071834564209,
      "learning_rate": 0.0006615716905101551,
      "loss": 0.1856,
      "step": 6350
    },
    {
      "epoch": 1.8640093786635403,
      "grad_norm": 0.5097955465316772,
      "learning_rate": 0.0006614878942225669,
      "loss": 0.1592,
      "step": 6360
    },
    {
      "epoch": 1.8669402110199296,
      "grad_norm": 0.7145910859107971,
      "learning_rate": 0.0006614040979349785,
      "loss": 0.1787,
      "step": 6370
    },
    {
      "epoch": 1.8698710433763188,
      "grad_norm": 0.4671053886413574,
      "learning_rate": 0.0006613203016473901,
      "loss": 0.1964,
      "step": 6380
    },
    {
      "epoch": 1.872801875732708,
      "grad_norm": 0.6549659371376038,
      "learning_rate": 0.0006612365053598017,
      "loss": 0.1773,
      "step": 6390
    },
    {
      "epoch": 1.8757327080890973,
      "grad_norm": 0.5954188704490662,
      "learning_rate": 0.0006611527090722133,
      "loss": 0.1851,
      "step": 6400
    },
    {
      "epoch": 1.8786635404454866,
      "grad_norm": 0.4980485439300537,
      "learning_rate": 0.000661068912784625,
      "loss": 0.1714,
      "step": 6410
    },
    {
      "epoch": 1.8815943728018758,
      "grad_norm": 0.4704665243625641,
      "learning_rate": 0.0006609851164970366,
      "loss": 0.1886,
      "step": 6420
    },
    {
      "epoch": 1.884525205158265,
      "grad_norm": 0.6071121096611023,
      "learning_rate": 0.0006609013202094482,
      "loss": 0.1758,
      "step": 6430
    },
    {
      "epoch": 1.8874560375146543,
      "grad_norm": 0.9532005190849304,
      "learning_rate": 0.0006608175239218598,
      "loss": 0.1773,
      "step": 6440
    },
    {
      "epoch": 1.8903868698710435,
      "grad_norm": 0.5975298881530762,
      "learning_rate": 0.0006607337276342715,
      "loss": 0.1942,
      "step": 6450
    },
    {
      "epoch": 1.8933177022274326,
      "grad_norm": 0.45339658856391907,
      "learning_rate": 0.0006606499313466832,
      "loss": 0.1775,
      "step": 6460
    },
    {
      "epoch": 1.8962485345838218,
      "grad_norm": 0.4864000380039215,
      "learning_rate": 0.0006605661350590948,
      "loss": 0.18,
      "step": 6470
    },
    {
      "epoch": 1.899179366940211,
      "grad_norm": 1.0952513217926025,
      "learning_rate": 0.0006604823387715064,
      "loss": 0.172,
      "step": 6480
    },
    {
      "epoch": 1.9021101992966,
      "grad_norm": 0.7395535111427307,
      "learning_rate": 0.000660398542483918,
      "loss": 0.1771,
      "step": 6490
    },
    {
      "epoch": 1.9050410316529893,
      "grad_norm": 0.5991368293762207,
      "learning_rate": 0.0006603147461963296,
      "loss": 0.189,
      "step": 6500
    },
    {
      "epoch": 1.9079718640093786,
      "grad_norm": 0.6626979112625122,
      "learning_rate": 0.0006602309499087413,
      "loss": 0.187,
      "step": 6510
    },
    {
      "epoch": 1.9109026963657678,
      "grad_norm": 0.4136454164981842,
      "learning_rate": 0.0006601471536211529,
      "loss": 0.1734,
      "step": 6520
    },
    {
      "epoch": 1.913833528722157,
      "grad_norm": 0.606174647808075,
      "learning_rate": 0.0006600633573335646,
      "loss": 0.1759,
      "step": 6530
    },
    {
      "epoch": 1.9167643610785463,
      "grad_norm": 0.498939573764801,
      "learning_rate": 0.0006599795610459762,
      "loss": 0.1848,
      "step": 6540
    },
    {
      "epoch": 1.9196951934349356,
      "grad_norm": 0.5568070411682129,
      "learning_rate": 0.0006598957647583878,
      "loss": 0.1997,
      "step": 6550
    },
    {
      "epoch": 1.9226260257913248,
      "grad_norm": 0.6304788589477539,
      "learning_rate": 0.0006598119684707995,
      "loss": 0.1756,
      "step": 6560
    },
    {
      "epoch": 1.925556858147714,
      "grad_norm": 0.717888593673706,
      "learning_rate": 0.0006597281721832111,
      "loss": 0.1793,
      "step": 6570
    },
    {
      "epoch": 1.9284876905041033,
      "grad_norm": 0.3441299796104431,
      "learning_rate": 0.0006596443758956227,
      "loss": 0.1965,
      "step": 6580
    },
    {
      "epoch": 1.9314185228604925,
      "grad_norm": 0.4651332497596741,
      "learning_rate": 0.0006595605796080343,
      "loss": 0.1648,
      "step": 6590
    },
    {
      "epoch": 1.9343493552168816,
      "grad_norm": 0.6432451009750366,
      "learning_rate": 0.0006594767833204459,
      "loss": 0.1947,
      "step": 6600
    },
    {
      "epoch": 1.9372801875732708,
      "grad_norm": 0.44602495431900024,
      "learning_rate": 0.0006593929870328576,
      "loss": 0.1696,
      "step": 6610
    },
    {
      "epoch": 1.94021101992966,
      "grad_norm": 0.5137454867362976,
      "learning_rate": 0.0006593091907452693,
      "loss": 0.1774,
      "step": 6620
    },
    {
      "epoch": 1.943141852286049,
      "grad_norm": 0.4211450517177582,
      "learning_rate": 0.000659225394457681,
      "loss": 0.1799,
      "step": 6630
    },
    {
      "epoch": 1.9460726846424383,
      "grad_norm": 0.3919197916984558,
      "learning_rate": 0.0006591415981700925,
      "loss": 0.1532,
      "step": 6640
    },
    {
      "epoch": 1.9490035169988276,
      "grad_norm": 0.5390710830688477,
      "learning_rate": 0.0006590578018825041,
      "loss": 0.1527,
      "step": 6650
    },
    {
      "epoch": 1.9519343493552168,
      "grad_norm": 0.4617311954498291,
      "learning_rate": 0.0006589740055949158,
      "loss": 0.182,
      "step": 6660
    },
    {
      "epoch": 1.954865181711606,
      "grad_norm": 0.6955477595329285,
      "learning_rate": 0.0006588902093073274,
      "loss": 0.1515,
      "step": 6670
    },
    {
      "epoch": 1.9577960140679953,
      "grad_norm": 0.3636013865470886,
      "learning_rate": 0.0006588064130197391,
      "loss": 0.1842,
      "step": 6680
    },
    {
      "epoch": 1.9607268464243846,
      "grad_norm": 0.6650410294532776,
      "learning_rate": 0.0006587226167321506,
      "loss": 0.193,
      "step": 6690
    },
    {
      "epoch": 1.9636576787807738,
      "grad_norm": 0.519415557384491,
      "learning_rate": 0.0006586388204445624,
      "loss": 0.1915,
      "step": 6700
    },
    {
      "epoch": 1.966588511137163,
      "grad_norm": 0.34001755714416504,
      "learning_rate": 0.000658555024156974,
      "loss": 0.1902,
      "step": 6710
    },
    {
      "epoch": 1.9695193434935523,
      "grad_norm": 0.7032938599586487,
      "learning_rate": 0.0006584712278693856,
      "loss": 0.1582,
      "step": 6720
    },
    {
      "epoch": 1.9724501758499415,
      "grad_norm": 0.4772844910621643,
      "learning_rate": 0.0006583874315817973,
      "loss": 0.1905,
      "step": 6730
    },
    {
      "epoch": 1.9753810082063306,
      "grad_norm": 1.0819593667984009,
      "learning_rate": 0.0006583036352942089,
      "loss": 0.191,
      "step": 6740
    },
    {
      "epoch": 1.9783118405627198,
      "grad_norm": 0.3561711013317108,
      "learning_rate": 0.0006582198390066205,
      "loss": 0.1859,
      "step": 6750
    },
    {
      "epoch": 1.981242672919109,
      "grad_norm": 0.6444904208183289,
      "learning_rate": 0.0006581360427190321,
      "loss": 0.1845,
      "step": 6760
    },
    {
      "epoch": 1.9841735052754983,
      "grad_norm": 0.6345682144165039,
      "learning_rate": 0.0006580522464314437,
      "loss": 0.1383,
      "step": 6770
    },
    {
      "epoch": 1.9871043376318873,
      "grad_norm": 0.6555349826812744,
      "learning_rate": 0.0006579684501438554,
      "loss": 0.1854,
      "step": 6780
    },
    {
      "epoch": 1.9900351699882766,
      "grad_norm": 0.6048805713653564,
      "learning_rate": 0.0006578846538562671,
      "loss": 0.17,
      "step": 6790
    },
    {
      "epoch": 1.9929660023446658,
      "grad_norm": 1.6064473390579224,
      "learning_rate": 0.0006578008575686787,
      "loss": 0.1931,
      "step": 6800
    },
    {
      "epoch": 1.995896834701055,
      "grad_norm": 0.45037201046943665,
      "learning_rate": 0.0006577170612810903,
      "loss": 0.1828,
      "step": 6810
    },
    {
      "epoch": 1.9988276670574443,
      "grad_norm": 0.6232679486274719,
      "learning_rate": 0.0006576332649935019,
      "loss": 0.1685,
      "step": 6820
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.3306956808440488,
      "eval_f1_macro": 0.18800786946918496,
      "eval_f1_micro": 0.44499713028505833,
      "eval_f1_weighted": 0.3685185019478498,
      "eval_loss": 0.17590869963169098,
      "eval_roc_auc": 0.6614856948754665,
      "eval_runtime": 163.1674,
      "eval_samples_per_second": 18.588,
      "eval_steps_per_second": 2.329,
      "step": 6824
    },
    {
      "epoch": 2.0017584994138335,
      "grad_norm": 0.4565509557723999,
      "learning_rate": 0.0006575494687059136,
      "loss": 0.1956,
      "step": 6830
    },
    {
      "epoch": 2.004689331770223,
      "grad_norm": 0.6718484163284302,
      "learning_rate": 0.0006574656724183252,
      "loss": 0.1707,
      "step": 6840
    },
    {
      "epoch": 2.007620164126612,
      "grad_norm": 0.8844833970069885,
      "learning_rate": 0.0006573818761307368,
      "loss": 0.1667,
      "step": 6850
    },
    {
      "epoch": 2.0105509964830013,
      "grad_norm": 0.7228973507881165,
      "learning_rate": 0.0006572980798431484,
      "loss": 0.1887,
      "step": 6860
    },
    {
      "epoch": 2.0134818288393905,
      "grad_norm": 0.7414658665657043,
      "learning_rate": 0.0006572142835555601,
      "loss": 0.1939,
      "step": 6870
    },
    {
      "epoch": 2.0164126611957798,
      "grad_norm": 0.6226022839546204,
      "learning_rate": 0.0006571304872679718,
      "loss": 0.208,
      "step": 6880
    },
    {
      "epoch": 2.019343493552169,
      "grad_norm": 0.5557547211647034,
      "learning_rate": 0.0006570466909803834,
      "loss": 0.1819,
      "step": 6890
    },
    {
      "epoch": 2.022274325908558,
      "grad_norm": 0.69775390625,
      "learning_rate": 0.000656962894692795,
      "loss": 0.1616,
      "step": 6900
    },
    {
      "epoch": 2.025205158264947,
      "grad_norm": 0.6860307455062866,
      "learning_rate": 0.0006568790984052066,
      "loss": 0.1784,
      "step": 6910
    },
    {
      "epoch": 2.0281359906213363,
      "grad_norm": 1.046659231185913,
      "learning_rate": 0.0006567953021176183,
      "loss": 0.187,
      "step": 6920
    },
    {
      "epoch": 2.0310668229777256,
      "grad_norm": 0.42223691940307617,
      "learning_rate": 0.0006567115058300299,
      "loss": 0.1656,
      "step": 6930
    },
    {
      "epoch": 2.033997655334115,
      "grad_norm": 0.4789469838142395,
      "learning_rate": 0.0006566277095424415,
      "loss": 0.1881,
      "step": 6940
    },
    {
      "epoch": 2.036928487690504,
      "grad_norm": 0.3185060918331146,
      "learning_rate": 0.0006565439132548531,
      "loss": 0.17,
      "step": 6950
    },
    {
      "epoch": 2.0398593200468933,
      "grad_norm": 0.44561266899108887,
      "learning_rate": 0.0006564601169672647,
      "loss": 0.1725,
      "step": 6960
    },
    {
      "epoch": 2.0427901524032825,
      "grad_norm": 0.7581659555435181,
      "learning_rate": 0.0006563763206796765,
      "loss": 0.187,
      "step": 6970
    },
    {
      "epoch": 2.045720984759672,
      "grad_norm": 1.0881346464157104,
      "learning_rate": 0.0006562925243920881,
      "loss": 0.1788,
      "step": 6980
    },
    {
      "epoch": 2.048651817116061,
      "grad_norm": 0.5351889729499817,
      "learning_rate": 0.0006562087281044997,
      "loss": 0.181,
      "step": 6990
    },
    {
      "epoch": 2.0515826494724503,
      "grad_norm": 0.33219122886657715,
      "learning_rate": 0.0006561249318169113,
      "loss": 0.1479,
      "step": 7000
    },
    {
      "epoch": 2.0545134818288395,
      "grad_norm": 0.506685733795166,
      "learning_rate": 0.0006560411355293229,
      "loss": 0.1566,
      "step": 7010
    },
    {
      "epoch": 2.0574443141852288,
      "grad_norm": 0.5348899364471436,
      "learning_rate": 0.0006559573392417346,
      "loss": 0.1874,
      "step": 7020
    },
    {
      "epoch": 2.060375146541618,
      "grad_norm": 0.4476706087589264,
      "learning_rate": 0.0006558735429541462,
      "loss": 0.1685,
      "step": 7030
    },
    {
      "epoch": 2.063305978898007,
      "grad_norm": 0.5711273550987244,
      "learning_rate": 0.000655789746666558,
      "loss": 0.1735,
      "step": 7040
    },
    {
      "epoch": 2.066236811254396,
      "grad_norm": 0.561558723449707,
      "learning_rate": 0.0006557059503789694,
      "loss": 0.1913,
      "step": 7050
    },
    {
      "epoch": 2.0691676436107853,
      "grad_norm": 0.9569294452667236,
      "learning_rate": 0.0006556221540913811,
      "loss": 0.1746,
      "step": 7060
    },
    {
      "epoch": 2.0720984759671746,
      "grad_norm": 0.9315235614776611,
      "learning_rate": 0.0006555383578037928,
      "loss": 0.1845,
      "step": 7070
    },
    {
      "epoch": 2.075029308323564,
      "grad_norm": 0.5257319808006287,
      "learning_rate": 0.0006554545615162044,
      "loss": 0.1757,
      "step": 7080
    },
    {
      "epoch": 2.077960140679953,
      "grad_norm": 0.6280490159988403,
      "learning_rate": 0.0006553707652286161,
      "loss": 0.1829,
      "step": 7090
    },
    {
      "epoch": 2.0808909730363423,
      "grad_norm": 0.8073405027389526,
      "learning_rate": 0.0006552869689410276,
      "loss": 0.1773,
      "step": 7100
    },
    {
      "epoch": 2.0838218053927315,
      "grad_norm": 0.736516535282135,
      "learning_rate": 0.0006552031726534392,
      "loss": 0.1898,
      "step": 7110
    },
    {
      "epoch": 2.086752637749121,
      "grad_norm": 0.6531160473823547,
      "learning_rate": 0.0006551193763658509,
      "loss": 0.1807,
      "step": 7120
    },
    {
      "epoch": 2.08968347010551,
      "grad_norm": 0.5337594151496887,
      "learning_rate": 0.0006550355800782625,
      "loss": 0.2011,
      "step": 7130
    },
    {
      "epoch": 2.0926143024618993,
      "grad_norm": 0.6719411611557007,
      "learning_rate": 0.0006549517837906743,
      "loss": 0.1767,
      "step": 7140
    },
    {
      "epoch": 2.0955451348182885,
      "grad_norm": 0.8887331485748291,
      "learning_rate": 0.0006548679875030859,
      "loss": 0.1568,
      "step": 7150
    },
    {
      "epoch": 2.0984759671746778,
      "grad_norm": 0.5062684416770935,
      "learning_rate": 0.0006547841912154974,
      "loss": 0.1864,
      "step": 7160
    },
    {
      "epoch": 2.101406799531067,
      "grad_norm": 0.6950578689575195,
      "learning_rate": 0.0006547003949279091,
      "loss": 0.1941,
      "step": 7170
    },
    {
      "epoch": 2.1043376318874563,
      "grad_norm": 0.9967973828315735,
      "learning_rate": 0.0006546165986403207,
      "loss": 0.1566,
      "step": 7180
    },
    {
      "epoch": 2.107268464243845,
      "grad_norm": 0.5585135221481323,
      "learning_rate": 0.0006545328023527324,
      "loss": 0.1592,
      "step": 7190
    },
    {
      "epoch": 2.1101992966002343,
      "grad_norm": 1.1022286415100098,
      "learning_rate": 0.000654449006065144,
      "loss": 0.182,
      "step": 7200
    },
    {
      "epoch": 2.1131301289566236,
      "grad_norm": 0.9930090308189392,
      "learning_rate": 0.0006543652097775556,
      "loss": 0.208,
      "step": 7210
    },
    {
      "epoch": 2.116060961313013,
      "grad_norm": 0.5302219390869141,
      "learning_rate": 0.0006542814134899672,
      "loss": 0.158,
      "step": 7220
    },
    {
      "epoch": 2.118991793669402,
      "grad_norm": 0.3629416525363922,
      "learning_rate": 0.0006541976172023789,
      "loss": 0.1615,
      "step": 7230
    },
    {
      "epoch": 2.1219226260257913,
      "grad_norm": 0.5655179619789124,
      "learning_rate": 0.0006541138209147906,
      "loss": 0.1879,
      "step": 7240
    },
    {
      "epoch": 2.1248534583821805,
      "grad_norm": 0.9368987083435059,
      "learning_rate": 0.0006540300246272022,
      "loss": 0.1782,
      "step": 7250
    },
    {
      "epoch": 2.12778429073857,
      "grad_norm": 0.8422134518623352,
      "learning_rate": 0.0006539462283396138,
      "loss": 0.1757,
      "step": 7260
    },
    {
      "epoch": 2.130715123094959,
      "grad_norm": 0.5937405824661255,
      "learning_rate": 0.0006538624320520254,
      "loss": 0.1696,
      "step": 7270
    },
    {
      "epoch": 2.1336459554513483,
      "grad_norm": 0.7122732996940613,
      "learning_rate": 0.000653778635764437,
      "loss": 0.1783,
      "step": 7280
    },
    {
      "epoch": 2.1365767878077375,
      "grad_norm": 0.653502345085144,
      "learning_rate": 0.0006536948394768487,
      "loss": 0.1691,
      "step": 7290
    },
    {
      "epoch": 2.1395076201641268,
      "grad_norm": 0.43362611532211304,
      "learning_rate": 0.0006536110431892603,
      "loss": 0.186,
      "step": 7300
    },
    {
      "epoch": 2.142438452520516,
      "grad_norm": 0.5874306559562683,
      "learning_rate": 0.000653527246901672,
      "loss": 0.1667,
      "step": 7310
    },
    {
      "epoch": 2.145369284876905,
      "grad_norm": 0.5526377558708191,
      "learning_rate": 0.0006534434506140836,
      "loss": 0.1925,
      "step": 7320
    },
    {
      "epoch": 2.148300117233294,
      "grad_norm": 0.4445514976978302,
      "learning_rate": 0.0006533596543264952,
      "loss": 0.19,
      "step": 7330
    },
    {
      "epoch": 2.1512309495896833,
      "grad_norm": 0.51381516456604,
      "learning_rate": 0.0006532758580389069,
      "loss": 0.1736,
      "step": 7340
    },
    {
      "epoch": 2.1541617819460726,
      "grad_norm": 0.7926880121231079,
      "learning_rate": 0.0006531920617513185,
      "loss": 0.1712,
      "step": 7350
    },
    {
      "epoch": 2.157092614302462,
      "grad_norm": 0.739208996295929,
      "learning_rate": 0.0006531082654637301,
      "loss": 0.1909,
      "step": 7360
    },
    {
      "epoch": 2.160023446658851,
      "grad_norm": 0.6537209153175354,
      "learning_rate": 0.0006530244691761417,
      "loss": 0.1968,
      "step": 7370
    },
    {
      "epoch": 2.1629542790152403,
      "grad_norm": 0.44596290588378906,
      "learning_rate": 0.0006529406728885534,
      "loss": 0.1946,
      "step": 7380
    },
    {
      "epoch": 2.1658851113716295,
      "grad_norm": 0.4980657398700714,
      "learning_rate": 0.000652856876600965,
      "loss": 0.1701,
      "step": 7390
    },
    {
      "epoch": 2.168815943728019,
      "grad_norm": 0.4631592035293579,
      "learning_rate": 0.0006527730803133767,
      "loss": 0.1683,
      "step": 7400
    },
    {
      "epoch": 2.171746776084408,
      "grad_norm": 1.0716160535812378,
      "learning_rate": 0.0006526892840257883,
      "loss": 0.1608,
      "step": 7410
    },
    {
      "epoch": 2.1746776084407973,
      "grad_norm": 0.5133041739463806,
      "learning_rate": 0.0006526054877381999,
      "loss": 0.1876,
      "step": 7420
    },
    {
      "epoch": 2.1776084407971865,
      "grad_norm": 1.0243830680847168,
      "learning_rate": 0.0006525216914506116,
      "loss": 0.1525,
      "step": 7430
    },
    {
      "epoch": 2.1805392731535758,
      "grad_norm": 0.5836502909660339,
      "learning_rate": 0.0006524378951630232,
      "loss": 0.1539,
      "step": 7440
    },
    {
      "epoch": 2.183470105509965,
      "grad_norm": 0.6685124039649963,
      "learning_rate": 0.0006523540988754348,
      "loss": 0.17,
      "step": 7450
    },
    {
      "epoch": 2.1864009378663543,
      "grad_norm": 0.8522244095802307,
      "learning_rate": 0.0006522703025878464,
      "loss": 0.1579,
      "step": 7460
    },
    {
      "epoch": 2.189331770222743,
      "grad_norm": 0.7827406525611877,
      "learning_rate": 0.000652186506300258,
      "loss": 0.1577,
      "step": 7470
    },
    {
      "epoch": 2.1922626025791323,
      "grad_norm": 0.5101587176322937,
      "learning_rate": 0.0006521027100126698,
      "loss": 0.1663,
      "step": 7480
    },
    {
      "epoch": 2.1951934349355215,
      "grad_norm": 0.6222384572029114,
      "learning_rate": 0.0006520189137250814,
      "loss": 0.1667,
      "step": 7490
    },
    {
      "epoch": 2.198124267291911,
      "grad_norm": 0.35854628682136536,
      "learning_rate": 0.000651935117437493,
      "loss": 0.1914,
      "step": 7500
    },
    {
      "epoch": 2.2010550996483,
      "grad_norm": 0.6098361015319824,
      "learning_rate": 0.0006518513211499047,
      "loss": 0.181,
      "step": 7510
    },
    {
      "epoch": 2.2039859320046893,
      "grad_norm": 0.517519474029541,
      "learning_rate": 0.0006517675248623162,
      "loss": 0.1551,
      "step": 7520
    },
    {
      "epoch": 2.2069167643610785,
      "grad_norm": 0.979112982749939,
      "learning_rate": 0.0006516837285747279,
      "loss": 0.1863,
      "step": 7530
    },
    {
      "epoch": 2.2098475967174678,
      "grad_norm": 0.7538898587226868,
      "learning_rate": 0.0006515999322871395,
      "loss": 0.1932,
      "step": 7540
    },
    {
      "epoch": 2.212778429073857,
      "grad_norm": 0.8652201890945435,
      "learning_rate": 0.0006515161359995511,
      "loss": 0.1712,
      "step": 7550
    },
    {
      "epoch": 2.2157092614302463,
      "grad_norm": 0.5386930108070374,
      "learning_rate": 0.0006514323397119628,
      "loss": 0.1898,
      "step": 7560
    },
    {
      "epoch": 2.2186400937866355,
      "grad_norm": 0.5362184047698975,
      "learning_rate": 0.0006513485434243743,
      "loss": 0.175,
      "step": 7570
    },
    {
      "epoch": 2.2215709261430248,
      "grad_norm": 0.5755069851875305,
      "learning_rate": 0.0006512647471367861,
      "loss": 0.1935,
      "step": 7580
    },
    {
      "epoch": 2.224501758499414,
      "grad_norm": 0.5809360146522522,
      "learning_rate": 0.0006511809508491977,
      "loss": 0.1699,
      "step": 7590
    },
    {
      "epoch": 2.2274325908558033,
      "grad_norm": 0.6462223529815674,
      "learning_rate": 0.0006510971545616094,
      "loss": 0.1662,
      "step": 7600
    },
    {
      "epoch": 2.230363423212192,
      "grad_norm": 1.038874864578247,
      "learning_rate": 0.000651013358274021,
      "loss": 0.1834,
      "step": 7610
    },
    {
      "epoch": 2.2332942555685813,
      "grad_norm": 0.6567023992538452,
      "learning_rate": 0.0006509295619864326,
      "loss": 0.1636,
      "step": 7620
    },
    {
      "epoch": 2.2362250879249705,
      "grad_norm": 0.8338069319725037,
      "learning_rate": 0.0006508457656988442,
      "loss": 0.1848,
      "step": 7630
    },
    {
      "epoch": 2.23915592028136,
      "grad_norm": 0.5478644967079163,
      "learning_rate": 0.0006507619694112558,
      "loss": 0.1632,
      "step": 7640
    },
    {
      "epoch": 2.242086752637749,
      "grad_norm": 0.503264307975769,
      "learning_rate": 0.0006506781731236675,
      "loss": 0.1648,
      "step": 7650
    },
    {
      "epoch": 2.2450175849941383,
      "grad_norm": 0.5598105192184448,
      "learning_rate": 0.0006505943768360792,
      "loss": 0.1357,
      "step": 7660
    },
    {
      "epoch": 2.2479484173505275,
      "grad_norm": 0.6427900195121765,
      "learning_rate": 0.0006505105805484908,
      "loss": 0.1677,
      "step": 7670
    },
    {
      "epoch": 2.2508792497069168,
      "grad_norm": 0.7290948629379272,
      "learning_rate": 0.0006504267842609024,
      "loss": 0.1827,
      "step": 7680
    },
    {
      "epoch": 2.253810082063306,
      "grad_norm": 1.4398705959320068,
      "learning_rate": 0.000650342987973314,
      "loss": 0.1646,
      "step": 7690
    },
    {
      "epoch": 2.2567409144196953,
      "grad_norm": 0.710793673992157,
      "learning_rate": 0.0006502591916857257,
      "loss": 0.172,
      "step": 7700
    },
    {
      "epoch": 2.2596717467760845,
      "grad_norm": 0.9646854400634766,
      "learning_rate": 0.0006501753953981373,
      "loss": 0.1583,
      "step": 7710
    },
    {
      "epoch": 2.2626025791324738,
      "grad_norm": 0.43771040439605713,
      "learning_rate": 0.0006500915991105489,
      "loss": 0.1756,
      "step": 7720
    },
    {
      "epoch": 2.265533411488863,
      "grad_norm": 0.5993973612785339,
      "learning_rate": 0.0006500078028229605,
      "loss": 0.1667,
      "step": 7730
    },
    {
      "epoch": 2.2684642438452522,
      "grad_norm": 0.5941229462623596,
      "learning_rate": 0.0006499240065353721,
      "loss": 0.1857,
      "step": 7740
    },
    {
      "epoch": 2.2713950762016415,
      "grad_norm": 0.692956268787384,
      "learning_rate": 0.0006498402102477839,
      "loss": 0.1833,
      "step": 7750
    },
    {
      "epoch": 2.2743259085580303,
      "grad_norm": 0.469999760389328,
      "learning_rate": 0.0006497564139601955,
      "loss": 0.1668,
      "step": 7760
    },
    {
      "epoch": 2.2772567409144195,
      "grad_norm": 0.9823307394981384,
      "learning_rate": 0.0006496726176726071,
      "loss": 0.1638,
      "step": 7770
    },
    {
      "epoch": 2.280187573270809,
      "grad_norm": 0.656642496585846,
      "learning_rate": 0.0006495888213850187,
      "loss": 0.147,
      "step": 7780
    },
    {
      "epoch": 2.283118405627198,
      "grad_norm": 0.41870951652526855,
      "learning_rate": 0.0006495050250974303,
      "loss": 0.1708,
      "step": 7790
    },
    {
      "epoch": 2.2860492379835873,
      "grad_norm": 0.9379134774208069,
      "learning_rate": 0.000649421228809842,
      "loss": 0.1622,
      "step": 7800
    },
    {
      "epoch": 2.2889800703399765,
      "grad_norm": 0.5501497387886047,
      "learning_rate": 0.0006493374325222536,
      "loss": 0.1645,
      "step": 7810
    },
    {
      "epoch": 2.2919109026963658,
      "grad_norm": 0.6470403671264648,
      "learning_rate": 0.0006492536362346652,
      "loss": 0.1641,
      "step": 7820
    },
    {
      "epoch": 2.294841735052755,
      "grad_norm": 0.8762192726135254,
      "learning_rate": 0.0006491698399470768,
      "loss": 0.1706,
      "step": 7830
    },
    {
      "epoch": 2.2977725674091443,
      "grad_norm": 0.4883936643600464,
      "learning_rate": 0.0006490860436594885,
      "loss": 0.1624,
      "step": 7840
    },
    {
      "epoch": 2.3007033997655335,
      "grad_norm": 0.4476325511932373,
      "learning_rate": 0.0006490022473719002,
      "loss": 0.1633,
      "step": 7850
    },
    {
      "epoch": 2.3036342321219228,
      "grad_norm": 0.6121617555618286,
      "learning_rate": 0.0006489184510843118,
      "loss": 0.187,
      "step": 7860
    },
    {
      "epoch": 2.306565064478312,
      "grad_norm": 0.755191445350647,
      "learning_rate": 0.0006488346547967234,
      "loss": 0.1855,
      "step": 7870
    },
    {
      "epoch": 2.3094958968347012,
      "grad_norm": 0.8103970885276794,
      "learning_rate": 0.000648750858509135,
      "loss": 0.1694,
      "step": 7880
    },
    {
      "epoch": 2.31242672919109,
      "grad_norm": 0.7910616993904114,
      "learning_rate": 0.0006486670622215466,
      "loss": 0.1718,
      "step": 7890
    },
    {
      "epoch": 2.3153575615474793,
      "grad_norm": 1.2581636905670166,
      "learning_rate": 0.0006485832659339583,
      "loss": 0.1847,
      "step": 7900
    },
    {
      "epoch": 2.3182883939038685,
      "grad_norm": 0.4043489396572113,
      "learning_rate": 0.0006484994696463699,
      "loss": 0.1681,
      "step": 7910
    },
    {
      "epoch": 2.321219226260258,
      "grad_norm": 0.5340336561203003,
      "learning_rate": 0.0006484156733587817,
      "loss": 0.1729,
      "step": 7920
    },
    {
      "epoch": 2.324150058616647,
      "grad_norm": 0.6776106953620911,
      "learning_rate": 0.0006483318770711932,
      "loss": 0.1683,
      "step": 7930
    },
    {
      "epoch": 2.3270808909730363,
      "grad_norm": 0.7552731037139893,
      "learning_rate": 0.0006482480807836049,
      "loss": 0.1807,
      "step": 7940
    },
    {
      "epoch": 2.3300117233294255,
      "grad_norm": 0.6180658936500549,
      "learning_rate": 0.0006481642844960165,
      "loss": 0.1762,
      "step": 7950
    },
    {
      "epoch": 2.3329425556858148,
      "grad_norm": 0.6467523574829102,
      "learning_rate": 0.0006480804882084281,
      "loss": 0.1729,
      "step": 7960
    },
    {
      "epoch": 2.335873388042204,
      "grad_norm": 0.6622480154037476,
      "learning_rate": 0.0006479966919208398,
      "loss": 0.1556,
      "step": 7970
    },
    {
      "epoch": 2.3388042203985933,
      "grad_norm": 0.4762193262577057,
      "learning_rate": 0.0006479128956332514,
      "loss": 0.159,
      "step": 7980
    },
    {
      "epoch": 2.3417350527549825,
      "grad_norm": 0.6968675255775452,
      "learning_rate": 0.000647829099345663,
      "loss": 0.1847,
      "step": 7990
    },
    {
      "epoch": 2.3446658851113718,
      "grad_norm": 0.9108985066413879,
      "learning_rate": 0.0006477453030580746,
      "loss": 0.1742,
      "step": 8000
    },
    {
      "epoch": 2.347596717467761,
      "grad_norm": 0.5138872861862183,
      "learning_rate": 0.0006476615067704863,
      "loss": 0.2059,
      "step": 8010
    },
    {
      "epoch": 2.3505275498241502,
      "grad_norm": 0.4282732605934143,
      "learning_rate": 0.000647577710482898,
      "loss": 0.1842,
      "step": 8020
    },
    {
      "epoch": 2.3534583821805395,
      "grad_norm": 0.6658418774604797,
      "learning_rate": 0.0006474939141953096,
      "loss": 0.1762,
      "step": 8030
    },
    {
      "epoch": 2.3563892145369287,
      "grad_norm": 0.4991570711135864,
      "learning_rate": 0.0006474101179077212,
      "loss": 0.1716,
      "step": 8040
    },
    {
      "epoch": 2.3593200468933175,
      "grad_norm": 0.33108294010162354,
      "learning_rate": 0.0006473263216201328,
      "loss": 0.1629,
      "step": 8050
    },
    {
      "epoch": 2.362250879249707,
      "grad_norm": 0.7270640134811401,
      "learning_rate": 0.0006472425253325444,
      "loss": 0.1741,
      "step": 8060
    },
    {
      "epoch": 2.365181711606096,
      "grad_norm": 0.4034178853034973,
      "learning_rate": 0.0006471587290449561,
      "loss": 0.181,
      "step": 8070
    },
    {
      "epoch": 2.3681125439624853,
      "grad_norm": 0.48532578349113464,
      "learning_rate": 0.0006470749327573677,
      "loss": 0.1633,
      "step": 8080
    },
    {
      "epoch": 2.3710433763188745,
      "grad_norm": 0.5803252458572388,
      "learning_rate": 0.0006469911364697794,
      "loss": 0.1441,
      "step": 8090
    },
    {
      "epoch": 2.3739742086752638,
      "grad_norm": 0.46166449785232544,
      "learning_rate": 0.000646907340182191,
      "loss": 0.1465,
      "step": 8100
    },
    {
      "epoch": 2.376905041031653,
      "grad_norm": 1.3138792514801025,
      "learning_rate": 0.0006468235438946027,
      "loss": 0.1723,
      "step": 8110
    },
    {
      "epoch": 2.3798358733880423,
      "grad_norm": 0.6175723075866699,
      "learning_rate": 0.0006467397476070143,
      "loss": 0.1503,
      "step": 8120
    },
    {
      "epoch": 2.3827667057444315,
      "grad_norm": 0.6461417078971863,
      "learning_rate": 0.0006466559513194259,
      "loss": 0.1466,
      "step": 8130
    },
    {
      "epoch": 2.3856975381008207,
      "grad_norm": 0.860291063785553,
      "learning_rate": 0.0006465721550318375,
      "loss": 0.1749,
      "step": 8140
    },
    {
      "epoch": 2.38862837045721,
      "grad_norm": 0.8481177091598511,
      "learning_rate": 0.0006464883587442491,
      "loss": 0.2181,
      "step": 8150
    },
    {
      "epoch": 2.3915592028135992,
      "grad_norm": 0.5058659315109253,
      "learning_rate": 0.0006464045624566608,
      "loss": 0.1701,
      "step": 8160
    },
    {
      "epoch": 2.394490035169988,
      "grad_norm": 1.5879453420639038,
      "learning_rate": 0.0006463207661690724,
      "loss": 0.2002,
      "step": 8170
    },
    {
      "epoch": 2.3974208675263773,
      "grad_norm": 0.5950260162353516,
      "learning_rate": 0.0006462369698814841,
      "loss": 0.1693,
      "step": 8180
    },
    {
      "epoch": 2.4003516998827665,
      "grad_norm": 0.44298356771469116,
      "learning_rate": 0.0006461531735938957,
      "loss": 0.1686,
      "step": 8190
    },
    {
      "epoch": 2.4032825322391558,
      "grad_norm": 0.5327070355415344,
      "learning_rate": 0.0006460693773063073,
      "loss": 0.1721,
      "step": 8200
    },
    {
      "epoch": 2.406213364595545,
      "grad_norm": 0.6293365359306335,
      "learning_rate": 0.000645985581018719,
      "loss": 0.1566,
      "step": 8210
    },
    {
      "epoch": 2.4091441969519343,
      "grad_norm": 0.715808093547821,
      "learning_rate": 0.0006459017847311306,
      "loss": 0.1663,
      "step": 8220
    },
    {
      "epoch": 2.4120750293083235,
      "grad_norm": 0.7227391004562378,
      "learning_rate": 0.0006458179884435422,
      "loss": 0.1685,
      "step": 8230
    },
    {
      "epoch": 2.4150058616647128,
      "grad_norm": 1.2601937055587769,
      "learning_rate": 0.0006457341921559538,
      "loss": 0.1778,
      "step": 8240
    },
    {
      "epoch": 2.417936694021102,
      "grad_norm": 0.7091661691665649,
      "learning_rate": 0.0006456503958683654,
      "loss": 0.1919,
      "step": 8250
    },
    {
      "epoch": 2.4208675263774913,
      "grad_norm": 0.6188592314720154,
      "learning_rate": 0.0006455665995807771,
      "loss": 0.1681,
      "step": 8260
    },
    {
      "epoch": 2.4237983587338805,
      "grad_norm": 0.41672685742378235,
      "learning_rate": 0.0006454828032931888,
      "loss": 0.1724,
      "step": 8270
    },
    {
      "epoch": 2.4267291910902697,
      "grad_norm": 0.8194456100463867,
      "learning_rate": 0.0006453990070056005,
      "loss": 0.169,
      "step": 8280
    },
    {
      "epoch": 2.429660023446659,
      "grad_norm": 0.9471624493598938,
      "learning_rate": 0.000645315210718012,
      "loss": 0.1716,
      "step": 8290
    },
    {
      "epoch": 2.4325908558030482,
      "grad_norm": 0.39432162046432495,
      "learning_rate": 0.0006452314144304236,
      "loss": 0.1782,
      "step": 8300
    },
    {
      "epoch": 2.4355216881594375,
      "grad_norm": 0.32213813066482544,
      "learning_rate": 0.0006451476181428353,
      "loss": 0.1624,
      "step": 8310
    },
    {
      "epoch": 2.4384525205158267,
      "grad_norm": 0.6108971834182739,
      "learning_rate": 0.0006450638218552469,
      "loss": 0.1925,
      "step": 8320
    },
    {
      "epoch": 2.4413833528722155,
      "grad_norm": 0.3701747953891754,
      "learning_rate": 0.0006449800255676586,
      "loss": 0.1694,
      "step": 8330
    },
    {
      "epoch": 2.4443141852286048,
      "grad_norm": 0.6812824606895447,
      "learning_rate": 0.0006448962292800701,
      "loss": 0.1755,
      "step": 8340
    },
    {
      "epoch": 2.447245017584994,
      "grad_norm": 1.031118392944336,
      "learning_rate": 0.0006448124329924817,
      "loss": 0.1515,
      "step": 8350
    },
    {
      "epoch": 2.4501758499413833,
      "grad_norm": 0.7660183310508728,
      "learning_rate": 0.0006447286367048935,
      "loss": 0.1769,
      "step": 8360
    },
    {
      "epoch": 2.4531066822977725,
      "grad_norm": 0.596778154373169,
      "learning_rate": 0.0006446448404173051,
      "loss": 0.163,
      "step": 8370
    },
    {
      "epoch": 2.4560375146541618,
      "grad_norm": 0.45577502250671387,
      "learning_rate": 0.0006445610441297168,
      "loss": 0.1744,
      "step": 8380
    },
    {
      "epoch": 2.458968347010551,
      "grad_norm": 0.6454020142555237,
      "learning_rate": 0.0006444772478421284,
      "loss": 0.1412,
      "step": 8390
    },
    {
      "epoch": 2.4618991793669402,
      "grad_norm": 0.9185082912445068,
      "learning_rate": 0.0006443934515545399,
      "loss": 0.1568,
      "step": 8400
    },
    {
      "epoch": 2.4648300117233295,
      "grad_norm": 0.5971356630325317,
      "learning_rate": 0.0006443096552669516,
      "loss": 0.1886,
      "step": 8410
    },
    {
      "epoch": 2.4677608440797187,
      "grad_norm": 0.4558323919773102,
      "learning_rate": 0.0006442258589793632,
      "loss": 0.1845,
      "step": 8420
    },
    {
      "epoch": 2.470691676436108,
      "grad_norm": 0.533230721950531,
      "learning_rate": 0.000644142062691775,
      "loss": 0.1944,
      "step": 8430
    },
    {
      "epoch": 2.4736225087924972,
      "grad_norm": 0.5798358917236328,
      "learning_rate": 0.0006440582664041866,
      "loss": 0.1643,
      "step": 8440
    },
    {
      "epoch": 2.4765533411488865,
      "grad_norm": 0.6408767700195312,
      "learning_rate": 0.0006439744701165982,
      "loss": 0.1436,
      "step": 8450
    },
    {
      "epoch": 2.4794841735052753,
      "grad_norm": 0.7049710154533386,
      "learning_rate": 0.0006438906738290098,
      "loss": 0.1585,
      "step": 8460
    },
    {
      "epoch": 2.4824150058616645,
      "grad_norm": 1.3639066219329834,
      "learning_rate": 0.0006438068775414214,
      "loss": 0.1761,
      "step": 8470
    },
    {
      "epoch": 2.4853458382180538,
      "grad_norm": 0.7140143513679504,
      "learning_rate": 0.0006437230812538331,
      "loss": 0.1773,
      "step": 8480
    },
    {
      "epoch": 2.488276670574443,
      "grad_norm": 0.6327720880508423,
      "learning_rate": 0.0006436392849662447,
      "loss": 0.1773,
      "step": 8490
    },
    {
      "epoch": 2.4912075029308323,
      "grad_norm": 1.6077935695648193,
      "learning_rate": 0.0006435554886786563,
      "loss": 0.1688,
      "step": 8500
    },
    {
      "epoch": 2.4941383352872215,
      "grad_norm": 1.46786630153656,
      "learning_rate": 0.0006434716923910679,
      "loss": 0.1647,
      "step": 8510
    },
    {
      "epoch": 2.4970691676436108,
      "grad_norm": 0.660934567451477,
      "learning_rate": 0.0006433878961034795,
      "loss": 0.1722,
      "step": 8520
    },
    {
      "epoch": 2.5,
      "grad_norm": 0.7599868774414062,
      "learning_rate": 0.0006433040998158913,
      "loss": 0.1978,
      "step": 8530
    },
    {
      "epoch": 2.5029308323563892,
      "grad_norm": 0.5162473320960999,
      "learning_rate": 0.0006432203035283029,
      "loss": 0.1923,
      "step": 8540
    },
    {
      "epoch": 2.5058616647127785,
      "grad_norm": 0.5572067499160767,
      "learning_rate": 0.0006431365072407145,
      "loss": 0.1749,
      "step": 8550
    },
    {
      "epoch": 2.5087924970691677,
      "grad_norm": 0.997381329536438,
      "learning_rate": 0.0006430527109531261,
      "loss": 0.1763,
      "step": 8560
    },
    {
      "epoch": 2.511723329425557,
      "grad_norm": 1.4355767965316772,
      "learning_rate": 0.0006429689146655377,
      "loss": 0.1778,
      "step": 8570
    },
    {
      "epoch": 2.5146541617819462,
      "grad_norm": 0.9767903685569763,
      "learning_rate": 0.0006428851183779494,
      "loss": 0.1407,
      "step": 8580
    },
    {
      "epoch": 2.5175849941383355,
      "grad_norm": 0.908747136592865,
      "learning_rate": 0.000642801322090361,
      "loss": 0.1939,
      "step": 8590
    },
    {
      "epoch": 2.5205158264947247,
      "grad_norm": 0.7788431644439697,
      "learning_rate": 0.0006427175258027726,
      "loss": 0.1663,
      "step": 8600
    },
    {
      "epoch": 2.523446658851114,
      "grad_norm": 0.5043449997901917,
      "learning_rate": 0.0006426337295151842,
      "loss": 0.1912,
      "step": 8610
    },
    {
      "epoch": 2.5263774912075028,
      "grad_norm": 0.4995315670967102,
      "learning_rate": 0.000642549933227596,
      "loss": 0.1795,
      "step": 8620
    },
    {
      "epoch": 2.529308323563892,
      "grad_norm": 1.1297439336776733,
      "learning_rate": 0.0006424661369400076,
      "loss": 0.1839,
      "step": 8630
    },
    {
      "epoch": 2.5322391559202813,
      "grad_norm": 0.6849915385246277,
      "learning_rate": 0.0006423823406524192,
      "loss": 0.1719,
      "step": 8640
    },
    {
      "epoch": 2.5351699882766705,
      "grad_norm": 0.8991413712501526,
      "learning_rate": 0.0006422985443648308,
      "loss": 0.1527,
      "step": 8650
    },
    {
      "epoch": 2.5381008206330598,
      "grad_norm": 0.810631275177002,
      "learning_rate": 0.0006422147480772424,
      "loss": 0.1433,
      "step": 8660
    },
    {
      "epoch": 2.541031652989449,
      "grad_norm": 0.42195814847946167,
      "learning_rate": 0.0006421309517896541,
      "loss": 0.1591,
      "step": 8670
    },
    {
      "epoch": 2.5439624853458382,
      "grad_norm": 0.8396697044372559,
      "learning_rate": 0.0006420471555020657,
      "loss": 0.1649,
      "step": 8680
    },
    {
      "epoch": 2.5468933177022275,
      "grad_norm": 0.5025221109390259,
      "learning_rate": 0.0006419633592144773,
      "loss": 0.166,
      "step": 8690
    },
    {
      "epoch": 2.5498241500586167,
      "grad_norm": 1.1529077291488647,
      "learning_rate": 0.000641879562926889,
      "loss": 0.166,
      "step": 8700
    },
    {
      "epoch": 2.552754982415006,
      "grad_norm": 0.958031415939331,
      "learning_rate": 0.0006417957666393006,
      "loss": 0.1712,
      "step": 8710
    },
    {
      "epoch": 2.5556858147713952,
      "grad_norm": 0.4458763599395752,
      "learning_rate": 0.0006417119703517123,
      "loss": 0.1487,
      "step": 8720
    },
    {
      "epoch": 2.558616647127784,
      "grad_norm": 1.3503464460372925,
      "learning_rate": 0.0006416281740641239,
      "loss": 0.1669,
      "step": 8730
    },
    {
      "epoch": 2.5615474794841733,
      "grad_norm": 1.13065505027771,
      "learning_rate": 0.0006415443777765355,
      "loss": 0.1628,
      "step": 8740
    },
    {
      "epoch": 2.5644783118405625,
      "grad_norm": 0.3949418067932129,
      "learning_rate": 0.0006414605814889471,
      "loss": 0.173,
      "step": 8750
    },
    {
      "epoch": 2.5674091441969518,
      "grad_norm": 2.087578535079956,
      "learning_rate": 0.0006413767852013587,
      "loss": 0.2116,
      "step": 8760
    },
    {
      "epoch": 2.570339976553341,
      "grad_norm": 0.8890023827552795,
      "learning_rate": 0.0006412929889137704,
      "loss": 0.1849,
      "step": 8770
    },
    {
      "epoch": 2.5732708089097303,
      "grad_norm": 0.43407803773880005,
      "learning_rate": 0.000641209192626182,
      "loss": 0.1867,
      "step": 8780
    },
    {
      "epoch": 2.5762016412661195,
      "grad_norm": 0.6053786873817444,
      "learning_rate": 0.0006411253963385937,
      "loss": 0.148,
      "step": 8790
    },
    {
      "epoch": 2.5791324736225087,
      "grad_norm": 0.8748763799667358,
      "learning_rate": 0.0006410416000510054,
      "loss": 0.188,
      "step": 8800
    },
    {
      "epoch": 2.582063305978898,
      "grad_norm": 0.541413426399231,
      "learning_rate": 0.0006409578037634169,
      "loss": 0.169,
      "step": 8810
    },
    {
      "epoch": 2.5849941383352872,
      "grad_norm": 0.6218447685241699,
      "learning_rate": 0.0006408740074758286,
      "loss": 0.1821,
      "step": 8820
    },
    {
      "epoch": 2.5879249706916765,
      "grad_norm": 0.5815736055374146,
      "learning_rate": 0.0006407902111882402,
      "loss": 0.1709,
      "step": 8830
    },
    {
      "epoch": 2.5908558030480657,
      "grad_norm": 0.8025656342506409,
      "learning_rate": 0.0006407064149006519,
      "loss": 0.1471,
      "step": 8840
    },
    {
      "epoch": 2.593786635404455,
      "grad_norm": 0.3021416664123535,
      "learning_rate": 0.0006406226186130635,
      "loss": 0.1877,
      "step": 8850
    },
    {
      "epoch": 2.5967174677608442,
      "grad_norm": 0.46785691380500793,
      "learning_rate": 0.0006405388223254751,
      "loss": 0.1525,
      "step": 8860
    },
    {
      "epoch": 2.5996483001172335,
      "grad_norm": 0.6661226749420166,
      "learning_rate": 0.0006404550260378868,
      "loss": 0.1519,
      "step": 8870
    },
    {
      "epoch": 2.6025791324736227,
      "grad_norm": 0.86155104637146,
      "learning_rate": 0.0006403712297502984,
      "loss": 0.1619,
      "step": 8880
    },
    {
      "epoch": 2.605509964830012,
      "grad_norm": 0.8398460745811462,
      "learning_rate": 0.0006402874334627101,
      "loss": 0.178,
      "step": 8890
    },
    {
      "epoch": 2.608440797186401,
      "grad_norm": 0.8612363338470459,
      "learning_rate": 0.0006402036371751217,
      "loss": 0.181,
      "step": 8900
    },
    {
      "epoch": 2.61137162954279,
      "grad_norm": 0.5725764632225037,
      "learning_rate": 0.0006401198408875333,
      "loss": 0.1759,
      "step": 8910
    },
    {
      "epoch": 2.6143024618991793,
      "grad_norm": 0.5123998522758484,
      "learning_rate": 0.0006400360445999449,
      "loss": 0.1777,
      "step": 8920
    },
    {
      "epoch": 2.6172332942555685,
      "grad_norm": 0.45451661944389343,
      "learning_rate": 0.0006399522483123565,
      "loss": 0.191,
      "step": 8930
    },
    {
      "epoch": 2.6201641266119577,
      "grad_norm": 0.7827260494232178,
      "learning_rate": 0.0006398684520247682,
      "loss": 0.1401,
      "step": 8940
    },
    {
      "epoch": 2.623094958968347,
      "grad_norm": 1.0332307815551758,
      "learning_rate": 0.0006397846557371798,
      "loss": 0.1744,
      "step": 8950
    },
    {
      "epoch": 2.6260257913247362,
      "grad_norm": 0.5947915315628052,
      "learning_rate": 0.0006397008594495915,
      "loss": 0.1259,
      "step": 8960
    },
    {
      "epoch": 2.6289566236811255,
      "grad_norm": 0.8526666760444641,
      "learning_rate": 0.0006396170631620031,
      "loss": 0.1778,
      "step": 8970
    },
    {
      "epoch": 2.6318874560375147,
      "grad_norm": 0.6005951762199402,
      "learning_rate": 0.0006395332668744147,
      "loss": 0.1596,
      "step": 8980
    },
    {
      "epoch": 2.634818288393904,
      "grad_norm": 0.5710488557815552,
      "learning_rate": 0.0006394494705868264,
      "loss": 0.1465,
      "step": 8990
    },
    {
      "epoch": 2.637749120750293,
      "grad_norm": 0.6244637370109558,
      "learning_rate": 0.000639365674299238,
      "loss": 0.1546,
      "step": 9000
    },
    {
      "epoch": 2.640679953106682,
      "grad_norm": 0.659504234790802,
      "learning_rate": 0.0006392818780116496,
      "loss": 0.1546,
      "step": 9010
    },
    {
      "epoch": 2.6436107854630713,
      "grad_norm": 0.5735707879066467,
      "learning_rate": 0.0006391980817240612,
      "loss": 0.1632,
      "step": 9020
    },
    {
      "epoch": 2.6465416178194605,
      "grad_norm": 0.6581693887710571,
      "learning_rate": 0.0006391142854364728,
      "loss": 0.1634,
      "step": 9030
    },
    {
      "epoch": 2.6494724501758498,
      "grad_norm": 1.292943000793457,
      "learning_rate": 0.0006390304891488845,
      "loss": 0.1795,
      "step": 9040
    },
    {
      "epoch": 2.652403282532239,
      "grad_norm": 0.5379953980445862,
      "learning_rate": 0.0006389466928612962,
      "loss": 0.1596,
      "step": 9050
    },
    {
      "epoch": 2.6553341148886282,
      "grad_norm": 0.8609451651573181,
      "learning_rate": 0.0006388628965737078,
      "loss": 0.1648,
      "step": 9060
    },
    {
      "epoch": 2.6582649472450175,
      "grad_norm": 0.5047585964202881,
      "learning_rate": 0.0006387791002861194,
      "loss": 0.1762,
      "step": 9070
    },
    {
      "epoch": 2.6611957796014067,
      "grad_norm": 0.47177648544311523,
      "learning_rate": 0.000638695303998531,
      "loss": 0.1535,
      "step": 9080
    },
    {
      "epoch": 2.664126611957796,
      "grad_norm": 0.8074469566345215,
      "learning_rate": 0.0006386115077109427,
      "loss": 0.1822,
      "step": 9090
    },
    {
      "epoch": 2.6670574443141852,
      "grad_norm": 0.5555473566055298,
      "learning_rate": 0.0006385277114233543,
      "loss": 0.1675,
      "step": 9100
    },
    {
      "epoch": 2.6699882766705745,
      "grad_norm": 0.5830333828926086,
      "learning_rate": 0.0006384439151357659,
      "loss": 0.1623,
      "step": 9110
    },
    {
      "epoch": 2.6729191090269637,
      "grad_norm": 0.6281741857528687,
      "learning_rate": 0.0006383601188481775,
      "loss": 0.1448,
      "step": 9120
    },
    {
      "epoch": 2.675849941383353,
      "grad_norm": 1.1161762475967407,
      "learning_rate": 0.0006382763225605891,
      "loss": 0.1724,
      "step": 9130
    },
    {
      "epoch": 2.678780773739742,
      "grad_norm": 0.3567960262298584,
      "learning_rate": 0.0006381925262730009,
      "loss": 0.1705,
      "step": 9140
    },
    {
      "epoch": 2.6817116060961315,
      "grad_norm": 0.5034151673316956,
      "learning_rate": 0.0006381087299854125,
      "loss": 0.1487,
      "step": 9150
    },
    {
      "epoch": 2.6846424384525207,
      "grad_norm": 0.6495622992515564,
      "learning_rate": 0.0006380249336978242,
      "loss": 0.1557,
      "step": 9160
    },
    {
      "epoch": 2.68757327080891,
      "grad_norm": 0.5080502033233643,
      "learning_rate": 0.0006379411374102357,
      "loss": 0.1566,
      "step": 9170
    },
    {
      "epoch": 2.690504103165299,
      "grad_norm": 0.6998418569564819,
      "learning_rate": 0.0006378573411226474,
      "loss": 0.1538,
      "step": 9180
    },
    {
      "epoch": 2.693434935521688,
      "grad_norm": 0.38761577010154724,
      "learning_rate": 0.000637773544835059,
      "loss": 0.1515,
      "step": 9190
    },
    {
      "epoch": 2.6963657678780772,
      "grad_norm": 0.3782553970813751,
      "learning_rate": 0.0006376897485474706,
      "loss": 0.1835,
      "step": 9200
    },
    {
      "epoch": 2.6992966002344665,
      "grad_norm": 0.6601791381835938,
      "learning_rate": 0.0006376059522598823,
      "loss": 0.1716,
      "step": 9210
    },
    {
      "epoch": 2.7022274325908557,
      "grad_norm": 0.6275686621665955,
      "learning_rate": 0.0006375221559722938,
      "loss": 0.1548,
      "step": 9220
    },
    {
      "epoch": 2.705158264947245,
      "grad_norm": 0.6561318039894104,
      "learning_rate": 0.0006374383596847056,
      "loss": 0.1504,
      "step": 9230
    },
    {
      "epoch": 2.7080890973036342,
      "grad_norm": 0.8674641847610474,
      "learning_rate": 0.0006373545633971172,
      "loss": 0.1762,
      "step": 9240
    },
    {
      "epoch": 2.7110199296600235,
      "grad_norm": 0.810512125492096,
      "learning_rate": 0.0006372707671095288,
      "loss": 0.1931,
      "step": 9250
    },
    {
      "epoch": 2.7139507620164127,
      "grad_norm": 1.1659226417541504,
      "learning_rate": 0.0006371869708219405,
      "loss": 0.1339,
      "step": 9260
    },
    {
      "epoch": 2.716881594372802,
      "grad_norm": 0.7039133906364441,
      "learning_rate": 0.0006371031745343521,
      "loss": 0.1576,
      "step": 9270
    },
    {
      "epoch": 2.719812426729191,
      "grad_norm": 0.732305109500885,
      "learning_rate": 0.0006370193782467637,
      "loss": 0.1699,
      "step": 9280
    },
    {
      "epoch": 2.7227432590855805,
      "grad_norm": 1.0061208009719849,
      "learning_rate": 0.0006369355819591753,
      "loss": 0.1542,
      "step": 9290
    },
    {
      "epoch": 2.7256740914419693,
      "grad_norm": 0.42532041668891907,
      "learning_rate": 0.0006368517856715869,
      "loss": 0.1471,
      "step": 9300
    },
    {
      "epoch": 2.7286049237983585,
      "grad_norm": 0.3969126045703888,
      "learning_rate": 0.0006367679893839987,
      "loss": 0.1702,
      "step": 9310
    },
    {
      "epoch": 2.7315357561547478,
      "grad_norm": 0.6803697347640991,
      "learning_rate": 0.0006366841930964103,
      "loss": 0.1445,
      "step": 9320
    },
    {
      "epoch": 2.734466588511137,
      "grad_norm": 0.8805614113807678,
      "learning_rate": 0.0006366003968088219,
      "loss": 0.1771,
      "step": 9330
    },
    {
      "epoch": 2.7373974208675262,
      "grad_norm": 0.8555045127868652,
      "learning_rate": 0.0006365166005212335,
      "loss": 0.1653,
      "step": 9340
    },
    {
      "epoch": 2.7403282532239155,
      "grad_norm": 0.6016002893447876,
      "learning_rate": 0.0006364328042336452,
      "loss": 0.1627,
      "step": 9350
    },
    {
      "epoch": 2.7432590855803047,
      "grad_norm": 0.5701836943626404,
      "learning_rate": 0.0006363490079460568,
      "loss": 0.2029,
      "step": 9360
    },
    {
      "epoch": 2.746189917936694,
      "grad_norm": 0.7391039729118347,
      "learning_rate": 0.0006362652116584684,
      "loss": 0.1882,
      "step": 9370
    },
    {
      "epoch": 2.7491207502930832,
      "grad_norm": 0.7418883442878723,
      "learning_rate": 0.00063618141537088,
      "loss": 0.1693,
      "step": 9380
    },
    {
      "epoch": 2.7520515826494725,
      "grad_norm": 0.5861601829528809,
      "learning_rate": 0.0006360976190832916,
      "loss": 0.1555,
      "step": 9390
    },
    {
      "epoch": 2.7549824150058617,
      "grad_norm": 0.5272443890571594,
      "learning_rate": 0.0006360138227957034,
      "loss": 0.1541,
      "step": 9400
    },
    {
      "epoch": 2.757913247362251,
      "grad_norm": 0.5137810111045837,
      "learning_rate": 0.000635930026508115,
      "loss": 0.1704,
      "step": 9410
    },
    {
      "epoch": 2.76084407971864,
      "grad_norm": 0.7031596302986145,
      "learning_rate": 0.0006358462302205266,
      "loss": 0.1748,
      "step": 9420
    },
    {
      "epoch": 2.7637749120750295,
      "grad_norm": 0.6299744844436646,
      "learning_rate": 0.0006357624339329382,
      "loss": 0.1943,
      "step": 9430
    },
    {
      "epoch": 2.7667057444314187,
      "grad_norm": 0.38144731521606445,
      "learning_rate": 0.0006356786376453498,
      "loss": 0.1374,
      "step": 9440
    },
    {
      "epoch": 2.769636576787808,
      "grad_norm": 0.39410391449928284,
      "learning_rate": 0.0006355948413577615,
      "loss": 0.1467,
      "step": 9450
    },
    {
      "epoch": 2.772567409144197,
      "grad_norm": 0.43262091279029846,
      "learning_rate": 0.0006355110450701731,
      "loss": 0.1613,
      "step": 9460
    },
    {
      "epoch": 2.7754982415005864,
      "grad_norm": 1.195364236831665,
      "learning_rate": 0.0006354272487825847,
      "loss": 0.1635,
      "step": 9470
    },
    {
      "epoch": 2.7784290738569752,
      "grad_norm": 0.38379064202308655,
      "learning_rate": 0.0006353434524949964,
      "loss": 0.1662,
      "step": 9480
    },
    {
      "epoch": 2.7813599062133645,
      "grad_norm": 0.67038494348526,
      "learning_rate": 0.000635259656207408,
      "loss": 0.1782,
      "step": 9490
    },
    {
      "epoch": 2.7842907385697537,
      "grad_norm": 0.5593022108078003,
      "learning_rate": 0.0006351758599198197,
      "loss": 0.159,
      "step": 9500
    },
    {
      "epoch": 2.787221570926143,
      "grad_norm": 0.4964085817337036,
      "learning_rate": 0.0006350920636322313,
      "loss": 0.1647,
      "step": 9510
    },
    {
      "epoch": 2.7901524032825322,
      "grad_norm": 0.8937264084815979,
      "learning_rate": 0.0006350082673446429,
      "loss": 0.1591,
      "step": 9520
    },
    {
      "epoch": 2.7930832356389215,
      "grad_norm": 0.5755484700202942,
      "learning_rate": 0.0006349244710570545,
      "loss": 0.1286,
      "step": 9530
    },
    {
      "epoch": 2.7960140679953107,
      "grad_norm": 0.9009122848510742,
      "learning_rate": 0.0006348406747694661,
      "loss": 0.1548,
      "step": 9540
    },
    {
      "epoch": 2.7989449003517,
      "grad_norm": 0.7025287747383118,
      "learning_rate": 0.0006347568784818778,
      "loss": 0.1766,
      "step": 9550
    },
    {
      "epoch": 2.801875732708089,
      "grad_norm": 1.2697983980178833,
      "learning_rate": 0.0006346730821942894,
      "loss": 0.1523,
      "step": 9560
    },
    {
      "epoch": 2.8048065650644785,
      "grad_norm": 1.437759280204773,
      "learning_rate": 0.0006345892859067012,
      "loss": 0.1494,
      "step": 9570
    },
    {
      "epoch": 2.8077373974208673,
      "grad_norm": 1.0688871145248413,
      "learning_rate": 0.0006345054896191127,
      "loss": 0.1797,
      "step": 9580
    },
    {
      "epoch": 2.8106682297772565,
      "grad_norm": 0.5910476446151733,
      "learning_rate": 0.0006344216933315243,
      "loss": 0.1512,
      "step": 9590
    },
    {
      "epoch": 2.8135990621336457,
      "grad_norm": 0.6781176924705505,
      "learning_rate": 0.000634337897043936,
      "loss": 0.1739,
      "step": 9600
    },
    {
      "epoch": 2.816529894490035,
      "grad_norm": 0.7651438117027283,
      "learning_rate": 0.0006342541007563476,
      "loss": 0.1713,
      "step": 9610
    },
    {
      "epoch": 2.8194607268464242,
      "grad_norm": 0.38881123065948486,
      "learning_rate": 0.0006341703044687593,
      "loss": 0.1565,
      "step": 9620
    },
    {
      "epoch": 2.8223915592028135,
      "grad_norm": 0.5017493367195129,
      "learning_rate": 0.0006340865081811709,
      "loss": 0.1567,
      "step": 9630
    },
    {
      "epoch": 2.8253223915592027,
      "grad_norm": 1.1043624877929688,
      "learning_rate": 0.0006340027118935824,
      "loss": 0.1744,
      "step": 9640
    },
    {
      "epoch": 2.828253223915592,
      "grad_norm": 0.4351906180381775,
      "learning_rate": 0.0006339189156059942,
      "loss": 0.1511,
      "step": 9650
    },
    {
      "epoch": 2.831184056271981,
      "grad_norm": 0.4859020411968231,
      "learning_rate": 0.0006338351193184058,
      "loss": 0.1682,
      "step": 9660
    },
    {
      "epoch": 2.8341148886283705,
      "grad_norm": 0.9926716685295105,
      "learning_rate": 0.0006337513230308175,
      "loss": 0.1438,
      "step": 9670
    },
    {
      "epoch": 2.8370457209847597,
      "grad_norm": 0.7518337368965149,
      "learning_rate": 0.0006336675267432291,
      "loss": 0.1699,
      "step": 9680
    },
    {
      "epoch": 2.839976553341149,
      "grad_norm": 0.9334236979484558,
      "learning_rate": 0.0006335837304556407,
      "loss": 0.1786,
      "step": 9690
    },
    {
      "epoch": 2.842907385697538,
      "grad_norm": 1.146850347518921,
      "learning_rate": 0.0006334999341680523,
      "loss": 0.163,
      "step": 9700
    },
    {
      "epoch": 2.8458382180539274,
      "grad_norm": 0.46427249908447266,
      "learning_rate": 0.0006334161378804639,
      "loss": 0.172,
      "step": 9710
    },
    {
      "epoch": 2.8487690504103167,
      "grad_norm": 0.4826280474662781,
      "learning_rate": 0.0006333323415928756,
      "loss": 0.1561,
      "step": 9720
    },
    {
      "epoch": 2.851699882766706,
      "grad_norm": 0.7689763307571411,
      "learning_rate": 0.0006332485453052872,
      "loss": 0.1799,
      "step": 9730
    },
    {
      "epoch": 2.854630715123095,
      "grad_norm": 0.7936307191848755,
      "learning_rate": 0.0006331647490176989,
      "loss": 0.1717,
      "step": 9740
    },
    {
      "epoch": 2.8575615474794844,
      "grad_norm": 0.8040868043899536,
      "learning_rate": 0.0006330809527301105,
      "loss": 0.1363,
      "step": 9750
    },
    {
      "epoch": 2.8604923798358732,
      "grad_norm": 0.4272244870662689,
      "learning_rate": 0.0006329971564425221,
      "loss": 0.181,
      "step": 9760
    },
    {
      "epoch": 2.8634232121922625,
      "grad_norm": 0.521085798740387,
      "learning_rate": 0.0006329133601549338,
      "loss": 0.1532,
      "step": 9770
    },
    {
      "epoch": 2.8663540445486517,
      "grad_norm": 0.5136265754699707,
      "learning_rate": 0.0006328295638673454,
      "loss": 0.1783,
      "step": 9780
    },
    {
      "epoch": 2.869284876905041,
      "grad_norm": 0.5880075097084045,
      "learning_rate": 0.000632745767579757,
      "loss": 0.184,
      "step": 9790
    },
    {
      "epoch": 2.87221570926143,
      "grad_norm": 0.8425337672233582,
      "learning_rate": 0.0006326619712921686,
      "loss": 0.1595,
      "step": 9800
    },
    {
      "epoch": 2.8751465416178195,
      "grad_norm": 1.0733826160430908,
      "learning_rate": 0.0006325781750045802,
      "loss": 0.1658,
      "step": 9810
    },
    {
      "epoch": 2.8780773739742087,
      "grad_norm": 0.9477130770683289,
      "learning_rate": 0.000632494378716992,
      "loss": 0.1826,
      "step": 9820
    },
    {
      "epoch": 2.881008206330598,
      "grad_norm": 0.6077185273170471,
      "learning_rate": 0.0006324105824294036,
      "loss": 0.1462,
      "step": 9830
    },
    {
      "epoch": 2.883939038686987,
      "grad_norm": 1.4347800016403198,
      "learning_rate": 0.0006323267861418152,
      "loss": 0.1592,
      "step": 9840
    },
    {
      "epoch": 2.8868698710433764,
      "grad_norm": 0.6375239491462708,
      "learning_rate": 0.0006322429898542268,
      "loss": 0.1539,
      "step": 9850
    },
    {
      "epoch": 2.8898007033997657,
      "grad_norm": 0.40855830907821655,
      "learning_rate": 0.0006321591935666384,
      "loss": 0.1705,
      "step": 9860
    },
    {
      "epoch": 2.8927315357561545,
      "grad_norm": 0.5522786974906921,
      "learning_rate": 0.0006320753972790501,
      "loss": 0.1748,
      "step": 9870
    },
    {
      "epoch": 2.8956623681125437,
      "grad_norm": 0.4315309226512909,
      "learning_rate": 0.0006319916009914617,
      "loss": 0.1605,
      "step": 9880
    },
    {
      "epoch": 2.898593200468933,
      "grad_norm": 0.4995769262313843,
      "learning_rate": 0.0006319078047038733,
      "loss": 0.1493,
      "step": 9890
    },
    {
      "epoch": 2.9015240328253222,
      "grad_norm": 0.48382118344306946,
      "learning_rate": 0.0006318240084162849,
      "loss": 0.151,
      "step": 9900
    },
    {
      "epoch": 2.9044548651817115,
      "grad_norm": 0.6112707257270813,
      "learning_rate": 0.0006317402121286967,
      "loss": 0.1583,
      "step": 9910
    },
    {
      "epoch": 2.9073856975381007,
      "grad_norm": 0.7096498608589172,
      "learning_rate": 0.0006316564158411083,
      "loss": 0.1387,
      "step": 9920
    },
    {
      "epoch": 2.91031652989449,
      "grad_norm": 0.6283459663391113,
      "learning_rate": 0.0006315726195535199,
      "loss": 0.1569,
      "step": 9930
    },
    {
      "epoch": 2.913247362250879,
      "grad_norm": 0.4757820665836334,
      "learning_rate": 0.0006314888232659315,
      "loss": 0.1803,
      "step": 9940
    },
    {
      "epoch": 2.9161781946072685,
      "grad_norm": 0.9721315503120422,
      "learning_rate": 0.0006314050269783431,
      "loss": 0.1749,
      "step": 9950
    },
    {
      "epoch": 2.9191090269636577,
      "grad_norm": 0.3012491762638092,
      "learning_rate": 0.0006313212306907548,
      "loss": 0.1693,
      "step": 9960
    },
    {
      "epoch": 2.922039859320047,
      "grad_norm": 0.6479233503341675,
      "learning_rate": 0.0006312374344031664,
      "loss": 0.1298,
      "step": 9970
    },
    {
      "epoch": 2.924970691676436,
      "grad_norm": 0.5652623772621155,
      "learning_rate": 0.000631153638115578,
      "loss": 0.1452,
      "step": 9980
    },
    {
      "epoch": 2.9279015240328254,
      "grad_norm": 0.7532525658607483,
      "learning_rate": 0.0006310698418279896,
      "loss": 0.1525,
      "step": 9990
    },
    {
      "epoch": 2.9308323563892147,
      "grad_norm": 0.7285540699958801,
      "learning_rate": 0.0006309860455404012,
      "loss": 0.1631,
      "step": 10000
    },
    {
      "epoch": 2.933763188745604,
      "grad_norm": 0.5982478857040405,
      "learning_rate": 0.000630902249252813,
      "loss": 0.1557,
      "step": 10010
    },
    {
      "epoch": 2.936694021101993,
      "grad_norm": 1.1357828378677368,
      "learning_rate": 0.0006308184529652246,
      "loss": 0.1689,
      "step": 10020
    },
    {
      "epoch": 2.9396248534583824,
      "grad_norm": 0.7409547567367554,
      "learning_rate": 0.0006307346566776362,
      "loss": 0.1487,
      "step": 10030
    },
    {
      "epoch": 2.9425556858147717,
      "grad_norm": 0.4728175401687622,
      "learning_rate": 0.0006306508603900479,
      "loss": 0.1673,
      "step": 10040
    },
    {
      "epoch": 2.9454865181711605,
      "grad_norm": 0.9725975394248962,
      "learning_rate": 0.0006305670641024594,
      "loss": 0.1427,
      "step": 10050
    },
    {
      "epoch": 2.9484173505275497,
      "grad_norm": 0.39787107706069946,
      "learning_rate": 0.0006304832678148711,
      "loss": 0.1417,
      "step": 10060
    },
    {
      "epoch": 2.951348182883939,
      "grad_norm": 0.7649297714233398,
      "learning_rate": 0.0006303994715272827,
      "loss": 0.1756,
      "step": 10070
    },
    {
      "epoch": 2.954279015240328,
      "grad_norm": 0.41387346386909485,
      "learning_rate": 0.0006303156752396945,
      "loss": 0.1334,
      "step": 10080
    },
    {
      "epoch": 2.9572098475967175,
      "grad_norm": 0.6958008408546448,
      "learning_rate": 0.0006302318789521061,
      "loss": 0.1419,
      "step": 10090
    },
    {
      "epoch": 2.9601406799531067,
      "grad_norm": 0.6954452991485596,
      "learning_rate": 0.0006301480826645176,
      "loss": 0.1679,
      "step": 10100
    },
    {
      "epoch": 2.963071512309496,
      "grad_norm": 1.2097247838974,
      "learning_rate": 0.0006300642863769293,
      "loss": 0.1626,
      "step": 10110
    },
    {
      "epoch": 2.966002344665885,
      "grad_norm": 0.40562278032302856,
      "learning_rate": 0.0006299804900893409,
      "loss": 0.1527,
      "step": 10120
    },
    {
      "epoch": 2.9689331770222744,
      "grad_norm": 0.4802412688732147,
      "learning_rate": 0.0006298966938017526,
      "loss": 0.165,
      "step": 10130
    },
    {
      "epoch": 2.9718640093786637,
      "grad_norm": 0.5938863754272461,
      "learning_rate": 0.0006298128975141642,
      "loss": 0.1549,
      "step": 10140
    },
    {
      "epoch": 2.9747948417350525,
      "grad_norm": 0.4367009103298187,
      "learning_rate": 0.0006297291012265758,
      "loss": 0.1879,
      "step": 10150
    },
    {
      "epoch": 2.9777256740914417,
      "grad_norm": 0.6083993911743164,
      "learning_rate": 0.0006296453049389874,
      "loss": 0.16,
      "step": 10160
    },
    {
      "epoch": 2.980656506447831,
      "grad_norm": 0.8441765308380127,
      "learning_rate": 0.000629561508651399,
      "loss": 0.1692,
      "step": 10170
    },
    {
      "epoch": 2.9835873388042202,
      "grad_norm": 0.669230043888092,
      "learning_rate": 0.0006294777123638108,
      "loss": 0.1805,
      "step": 10180
    },
    {
      "epoch": 2.9865181711606095,
      "grad_norm": 0.5360379219055176,
      "learning_rate": 0.0006293939160762224,
      "loss": 0.1649,
      "step": 10190
    },
    {
      "epoch": 2.9894490035169987,
      "grad_norm": 0.44919395446777344,
      "learning_rate": 0.000629310119788634,
      "loss": 0.1569,
      "step": 10200
    },
    {
      "epoch": 2.992379835873388,
      "grad_norm": 1.517201542854309,
      "learning_rate": 0.0006292263235010456,
      "loss": 0.1571,
      "step": 10210
    },
    {
      "epoch": 2.995310668229777,
      "grad_norm": 1.0424736738204956,
      "learning_rate": 0.0006291425272134572,
      "loss": 0.1662,
      "step": 10220
    },
    {
      "epoch": 2.9982415005861665,
      "grad_norm": 0.7386482357978821,
      "learning_rate": 0.0006290587309258689,
      "loss": 0.1757,
      "step": 10230
    },
    {
      "epoch": 3.0,
      "eval_accuracy": 0.33201450708869107,
      "eval_f1_macro": 0.2534307164427659,
      "eval_f1_micro": 0.47352526676062007,
      "eval_f1_weighted": 0.4366690980316827,
      "eval_loss": 0.15475942194461823,
      "eval_roc_auc": 0.6668787956094165,
      "eval_runtime": 162.8082,
      "eval_samples_per_second": 18.629,
      "eval_steps_per_second": 2.334,
      "step": 10236
    },
    {
      "epoch": 3.0011723329425557,
      "grad_norm": 0.6979667544364929,
      "learning_rate": 0.0006289749346382805,
      "loss": 0.162,
      "step": 10240
    },
    {
      "epoch": 3.004103165298945,
      "grad_norm": 0.5018287897109985,
      "learning_rate": 0.0006288911383506921,
      "loss": 0.1527,
      "step": 10250
    },
    {
      "epoch": 3.007033997655334,
      "grad_norm": 0.49907752871513367,
      "learning_rate": 0.0006288073420631038,
      "loss": 0.1566,
      "step": 10260
    },
    {
      "epoch": 3.0099648300117234,
      "grad_norm": 0.8014822602272034,
      "learning_rate": 0.0006287235457755154,
      "loss": 0.1637,
      "step": 10270
    },
    {
      "epoch": 3.0128956623681127,
      "grad_norm": 0.866736114025116,
      "learning_rate": 0.0006286397494879271,
      "loss": 0.1716,
      "step": 10280
    },
    {
      "epoch": 3.015826494724502,
      "grad_norm": 1.0850249528884888,
      "learning_rate": 0.0006285559532003387,
      "loss": 0.1784,
      "step": 10290
    },
    {
      "epoch": 3.018757327080891,
      "grad_norm": 0.8122854828834534,
      "learning_rate": 0.0006284721569127503,
      "loss": 0.1817,
      "step": 10300
    },
    {
      "epoch": 3.02168815943728,
      "grad_norm": 0.60346919298172,
      "learning_rate": 0.0006283883606251619,
      "loss": 0.1569,
      "step": 10310
    },
    {
      "epoch": 3.024618991793669,
      "grad_norm": 0.5637441277503967,
      "learning_rate": 0.0006283045643375735,
      "loss": 0.1691,
      "step": 10320
    },
    {
      "epoch": 3.0275498241500585,
      "grad_norm": 0.4734494388103485,
      "learning_rate": 0.0006282207680499852,
      "loss": 0.1887,
      "step": 10330
    },
    {
      "epoch": 3.0304806565064477,
      "grad_norm": 0.8151260018348694,
      "learning_rate": 0.0006281369717623968,
      "loss": 0.1443,
      "step": 10340
    },
    {
      "epoch": 3.033411488862837,
      "grad_norm": 0.9729391932487488,
      "learning_rate": 0.0006280531754748085,
      "loss": 0.1427,
      "step": 10350
    },
    {
      "epoch": 3.036342321219226,
      "grad_norm": 0.6427983045578003,
      "learning_rate": 0.0006279693791872201,
      "loss": 0.1562,
      "step": 10360
    },
    {
      "epoch": 3.0392731535756154,
      "grad_norm": 0.5704420804977417,
      "learning_rate": 0.0006278855828996317,
      "loss": 0.1527,
      "step": 10370
    },
    {
      "epoch": 3.0422039859320047,
      "grad_norm": 1.045617699623108,
      "learning_rate": 0.0006278017866120434,
      "loss": 0.1635,
      "step": 10380
    },
    {
      "epoch": 3.045134818288394,
      "grad_norm": 0.620718240737915,
      "learning_rate": 0.000627717990324455,
      "loss": 0.1465,
      "step": 10390
    },
    {
      "epoch": 3.048065650644783,
      "grad_norm": 0.6577665209770203,
      "learning_rate": 0.0006276341940368667,
      "loss": 0.1407,
      "step": 10400
    },
    {
      "epoch": 3.0509964830011724,
      "grad_norm": 0.6792163848876953,
      "learning_rate": 0.0006275503977492782,
      "loss": 0.1605,
      "step": 10410
    },
    {
      "epoch": 3.0539273153575617,
      "grad_norm": 0.4645596146583557,
      "learning_rate": 0.0006274666014616899,
      "loss": 0.1737,
      "step": 10420
    },
    {
      "epoch": 3.056858147713951,
      "grad_norm": 1.4444116353988647,
      "learning_rate": 0.0006273828051741016,
      "loss": 0.1694,
      "step": 10430
    },
    {
      "epoch": 3.05978898007034,
      "grad_norm": 0.5906309485435486,
      "learning_rate": 0.0006272990088865132,
      "loss": 0.1589,
      "step": 10440
    },
    {
      "epoch": 3.062719812426729,
      "grad_norm": 0.5785675644874573,
      "learning_rate": 0.0006272152125989249,
      "loss": 0.1435,
      "step": 10450
    },
    {
      "epoch": 3.065650644783118,
      "grad_norm": 1.0533055067062378,
      "learning_rate": 0.0006271314163113364,
      "loss": 0.1776,
      "step": 10460
    },
    {
      "epoch": 3.0685814771395075,
      "grad_norm": 0.6900242567062378,
      "learning_rate": 0.0006270476200237481,
      "loss": 0.1614,
      "step": 10470
    },
    {
      "epoch": 3.0715123094958967,
      "grad_norm": 0.9465684294700623,
      "learning_rate": 0.0006269638237361597,
      "loss": 0.1661,
      "step": 10480
    },
    {
      "epoch": 3.074443141852286,
      "grad_norm": 0.6475703120231628,
      "learning_rate": 0.0006268800274485713,
      "loss": 0.1565,
      "step": 10490
    },
    {
      "epoch": 3.077373974208675,
      "grad_norm": 0.6137199997901917,
      "learning_rate": 0.000626796231160983,
      "loss": 0.1629,
      "step": 10500
    },
    {
      "epoch": 3.0803048065650644,
      "grad_norm": 0.606418788433075,
      "learning_rate": 0.0006267124348733946,
      "loss": 0.1386,
      "step": 10510
    },
    {
      "epoch": 3.0832356389214537,
      "grad_norm": 0.5591362118721008,
      "learning_rate": 0.0006266286385858063,
      "loss": 0.1439,
      "step": 10520
    },
    {
      "epoch": 3.086166471277843,
      "grad_norm": 0.472275972366333,
      "learning_rate": 0.0006265448422982179,
      "loss": 0.161,
      "step": 10530
    },
    {
      "epoch": 3.089097303634232,
      "grad_norm": 0.5308976173400879,
      "learning_rate": 0.0006264610460106295,
      "loss": 0.1589,
      "step": 10540
    },
    {
      "epoch": 3.0920281359906214,
      "grad_norm": 0.6490008234977722,
      "learning_rate": 0.0006263772497230412,
      "loss": 0.161,
      "step": 10550
    },
    {
      "epoch": 3.0949589683470107,
      "grad_norm": 1.0271050930023193,
      "learning_rate": 0.0006262934534354528,
      "loss": 0.1578,
      "step": 10560
    },
    {
      "epoch": 3.0978898007034,
      "grad_norm": 0.7289991974830627,
      "learning_rate": 0.0006262096571478644,
      "loss": 0.1505,
      "step": 10570
    },
    {
      "epoch": 3.100820633059789,
      "grad_norm": 0.5729020237922668,
      "learning_rate": 0.000626125860860276,
      "loss": 0.1672,
      "step": 10580
    },
    {
      "epoch": 3.1037514654161784,
      "grad_norm": 0.4202406108379364,
      "learning_rate": 0.0006260420645726877,
      "loss": 0.1421,
      "step": 10590
    },
    {
      "epoch": 3.106682297772567,
      "grad_norm": 0.552513837814331,
      "learning_rate": 0.0006259582682850993,
      "loss": 0.1667,
      "step": 10600
    },
    {
      "epoch": 3.1096131301289565,
      "grad_norm": 0.46170899271965027,
      "learning_rate": 0.000625874471997511,
      "loss": 0.1645,
      "step": 10610
    },
    {
      "epoch": 3.1125439624853457,
      "grad_norm": 0.5356215238571167,
      "learning_rate": 0.0006257906757099226,
      "loss": 0.1595,
      "step": 10620
    },
    {
      "epoch": 3.115474794841735,
      "grad_norm": 2.043339490890503,
      "learning_rate": 0.0006257068794223342,
      "loss": 0.1604,
      "step": 10630
    },
    {
      "epoch": 3.118405627198124,
      "grad_norm": 0.3839629590511322,
      "learning_rate": 0.0006256230831347459,
      "loss": 0.1611,
      "step": 10640
    },
    {
      "epoch": 3.1213364595545134,
      "grad_norm": 0.9386957287788391,
      "learning_rate": 0.0006255392868471575,
      "loss": 0.1744,
      "step": 10650
    },
    {
      "epoch": 3.1242672919109027,
      "grad_norm": 0.5517517924308777,
      "learning_rate": 0.0006254554905595691,
      "loss": 0.1853,
      "step": 10660
    },
    {
      "epoch": 3.127198124267292,
      "grad_norm": 0.6332966089248657,
      "learning_rate": 0.0006253716942719807,
      "loss": 0.1571,
      "step": 10670
    },
    {
      "epoch": 3.130128956623681,
      "grad_norm": 0.8929604887962341,
      "learning_rate": 0.0006252878979843923,
      "loss": 0.1546,
      "step": 10680
    },
    {
      "epoch": 3.1330597889800704,
      "grad_norm": 0.546899676322937,
      "learning_rate": 0.000625204101696804,
      "loss": 0.1517,
      "step": 10690
    },
    {
      "epoch": 3.1359906213364597,
      "grad_norm": 0.6947184205055237,
      "learning_rate": 0.0006251203054092157,
      "loss": 0.1553,
      "step": 10700
    },
    {
      "epoch": 3.138921453692849,
      "grad_norm": 0.6877213716506958,
      "learning_rate": 0.0006250365091216273,
      "loss": 0.1541,
      "step": 10710
    },
    {
      "epoch": 3.141852286049238,
      "grad_norm": 0.43242713809013367,
      "learning_rate": 0.0006249527128340389,
      "loss": 0.1625,
      "step": 10720
    },
    {
      "epoch": 3.144783118405627,
      "grad_norm": 0.43729662895202637,
      "learning_rate": 0.0006248689165464505,
      "loss": 0.1417,
      "step": 10730
    },
    {
      "epoch": 3.147713950762016,
      "grad_norm": 0.6503291130065918,
      "learning_rate": 0.0006247851202588622,
      "loss": 0.1544,
      "step": 10740
    },
    {
      "epoch": 3.1506447831184055,
      "grad_norm": 0.9868684411048889,
      "learning_rate": 0.0006247013239712738,
      "loss": 0.152,
      "step": 10750
    },
    {
      "epoch": 3.1535756154747947,
      "grad_norm": 0.5957306623458862,
      "learning_rate": 0.0006246175276836854,
      "loss": 0.1622,
      "step": 10760
    },
    {
      "epoch": 3.156506447831184,
      "grad_norm": 0.4894132614135742,
      "learning_rate": 0.000624533731396097,
      "loss": 0.1663,
      "step": 10770
    },
    {
      "epoch": 3.159437280187573,
      "grad_norm": 0.8374919891357422,
      "learning_rate": 0.0006244499351085086,
      "loss": 0.1552,
      "step": 10780
    },
    {
      "epoch": 3.1623681125439624,
      "grad_norm": 0.5982324481010437,
      "learning_rate": 0.0006243661388209204,
      "loss": 0.1685,
      "step": 10790
    },
    {
      "epoch": 3.1652989449003517,
      "grad_norm": 0.6325609683990479,
      "learning_rate": 0.000624282342533332,
      "loss": 0.164,
      "step": 10800
    },
    {
      "epoch": 3.168229777256741,
      "grad_norm": 0.610142171382904,
      "learning_rate": 0.0006241985462457437,
      "loss": 0.1458,
      "step": 10810
    },
    {
      "epoch": 3.17116060961313,
      "grad_norm": 0.5599451661109924,
      "learning_rate": 0.0006241147499581552,
      "loss": 0.1511,
      "step": 10820
    },
    {
      "epoch": 3.1740914419695194,
      "grad_norm": 1.1002414226531982,
      "learning_rate": 0.0006240309536705668,
      "loss": 0.1818,
      "step": 10830
    },
    {
      "epoch": 3.1770222743259087,
      "grad_norm": 0.5855389833450317,
      "learning_rate": 0.0006239471573829785,
      "loss": 0.1389,
      "step": 10840
    },
    {
      "epoch": 3.179953106682298,
      "grad_norm": 1.9781423807144165,
      "learning_rate": 0.0006238633610953901,
      "loss": 0.1698,
      "step": 10850
    },
    {
      "epoch": 3.182883939038687,
      "grad_norm": 0.5067430734634399,
      "learning_rate": 0.0006237795648078019,
      "loss": 0.1464,
      "step": 10860
    },
    {
      "epoch": 3.1858147713950764,
      "grad_norm": 0.6899082064628601,
      "learning_rate": 0.0006236957685202134,
      "loss": 0.1593,
      "step": 10870
    },
    {
      "epoch": 3.1887456037514657,
      "grad_norm": 0.6335270404815674,
      "learning_rate": 0.000623611972232625,
      "loss": 0.1302,
      "step": 10880
    },
    {
      "epoch": 3.1916764361078545,
      "grad_norm": 1.2901087999343872,
      "learning_rate": 0.0006235281759450367,
      "loss": 0.1308,
      "step": 10890
    },
    {
      "epoch": 3.1946072684642437,
      "grad_norm": 0.6399994492530823,
      "learning_rate": 0.0006234443796574483,
      "loss": 0.1454,
      "step": 10900
    },
    {
      "epoch": 3.197538100820633,
      "grad_norm": 1.5994752645492554,
      "learning_rate": 0.00062336058336986,
      "loss": 0.1562,
      "step": 10910
    },
    {
      "epoch": 3.200468933177022,
      "grad_norm": 0.7241935133934021,
      "learning_rate": 0.0006232767870822716,
      "loss": 0.1456,
      "step": 10920
    },
    {
      "epoch": 3.2033997655334114,
      "grad_norm": 0.5440154671669006,
      "learning_rate": 0.0006231929907946832,
      "loss": 0.1646,
      "step": 10930
    },
    {
      "epoch": 3.2063305978898007,
      "grad_norm": 0.29228195548057556,
      "learning_rate": 0.0006231091945070948,
      "loss": 0.1689,
      "step": 10940
    },
    {
      "epoch": 3.20926143024619,
      "grad_norm": 0.8576306104660034,
      "learning_rate": 0.0006230253982195064,
      "loss": 0.1783,
      "step": 10950
    },
    {
      "epoch": 3.212192262602579,
      "grad_norm": 0.6326671242713928,
      "learning_rate": 0.0006229416019319182,
      "loss": 0.1289,
      "step": 10960
    },
    {
      "epoch": 3.2151230949589684,
      "grad_norm": 1.4327863454818726,
      "learning_rate": 0.0006228578056443298,
      "loss": 0.1328,
      "step": 10970
    },
    {
      "epoch": 3.2180539273153577,
      "grad_norm": 0.8150014281272888,
      "learning_rate": 0.0006227740093567414,
      "loss": 0.1575,
      "step": 10980
    },
    {
      "epoch": 3.220984759671747,
      "grad_norm": 0.7450911998748779,
      "learning_rate": 0.000622690213069153,
      "loss": 0.1377,
      "step": 10990
    },
    {
      "epoch": 3.223915592028136,
      "grad_norm": 0.789098858833313,
      "learning_rate": 0.0006226064167815646,
      "loss": 0.1467,
      "step": 11000
    },
    {
      "epoch": 3.2268464243845254,
      "grad_norm": 0.6875211000442505,
      "learning_rate": 0.0006225226204939763,
      "loss": 0.1661,
      "step": 11010
    },
    {
      "epoch": 3.229777256740914,
      "grad_norm": 1.0224117040634155,
      "learning_rate": 0.0006224388242063879,
      "loss": 0.1807,
      "step": 11020
    },
    {
      "epoch": 3.2327080890973034,
      "grad_norm": 0.41722825169563293,
      "learning_rate": 0.0006223550279187995,
      "loss": 0.1654,
      "step": 11030
    },
    {
      "epoch": 3.2356389214536927,
      "grad_norm": 0.6571640968322754,
      "learning_rate": 0.0006222712316312112,
      "loss": 0.1359,
      "step": 11040
    },
    {
      "epoch": 3.238569753810082,
      "grad_norm": 0.7229109406471252,
      "learning_rate": 0.0006221874353436228,
      "loss": 0.1711,
      "step": 11050
    },
    {
      "epoch": 3.241500586166471,
      "grad_norm": 0.6704310774803162,
      "learning_rate": 0.0006221036390560345,
      "loss": 0.1676,
      "step": 11060
    },
    {
      "epoch": 3.2444314185228604,
      "grad_norm": 0.5766848921775818,
      "learning_rate": 0.0006220198427684461,
      "loss": 0.166,
      "step": 11070
    },
    {
      "epoch": 3.2473622508792497,
      "grad_norm": 0.5280086398124695,
      "learning_rate": 0.0006219360464808577,
      "loss": 0.1705,
      "step": 11080
    },
    {
      "epoch": 3.250293083235639,
      "grad_norm": 0.57764732837677,
      "learning_rate": 0.0006218522501932693,
      "loss": 0.1705,
      "step": 11090
    },
    {
      "epoch": 3.253223915592028,
      "grad_norm": 0.498605340719223,
      "learning_rate": 0.0006217684539056809,
      "loss": 0.1286,
      "step": 11100
    },
    {
      "epoch": 3.2561547479484174,
      "grad_norm": 0.5910439491271973,
      "learning_rate": 0.0006216846576180926,
      "loss": 0.1647,
      "step": 11110
    },
    {
      "epoch": 3.2590855803048067,
      "grad_norm": 0.6527032852172852,
      "learning_rate": 0.0006216008613305042,
      "loss": 0.1519,
      "step": 11120
    },
    {
      "epoch": 3.262016412661196,
      "grad_norm": 0.697461724281311,
      "learning_rate": 0.0006215170650429159,
      "loss": 0.1274,
      "step": 11130
    },
    {
      "epoch": 3.264947245017585,
      "grad_norm": 0.6370290517807007,
      "learning_rate": 0.0006214332687553275,
      "loss": 0.1656,
      "step": 11140
    },
    {
      "epoch": 3.2678780773739744,
      "grad_norm": 1.083776831626892,
      "learning_rate": 0.0006213494724677392,
      "loss": 0.1567,
      "step": 11150
    },
    {
      "epoch": 3.2708089097303636,
      "grad_norm": 0.6386713981628418,
      "learning_rate": 0.0006212656761801508,
      "loss": 0.1387,
      "step": 11160
    },
    {
      "epoch": 3.2737397420867524,
      "grad_norm": 0.5155457258224487,
      "learning_rate": 0.0006211818798925624,
      "loss": 0.1951,
      "step": 11170
    },
    {
      "epoch": 3.2766705744431417,
      "grad_norm": 1.1051394939422607,
      "learning_rate": 0.000621098083604974,
      "loss": 0.1536,
      "step": 11180
    },
    {
      "epoch": 3.279601406799531,
      "grad_norm": 0.6747174859046936,
      "learning_rate": 0.0006210142873173856,
      "loss": 0.1517,
      "step": 11190
    },
    {
      "epoch": 3.28253223915592,
      "grad_norm": 0.8724287152290344,
      "learning_rate": 0.0006209304910297973,
      "loss": 0.1666,
      "step": 11200
    },
    {
      "epoch": 3.2854630715123094,
      "grad_norm": 0.8514455556869507,
      "learning_rate": 0.000620846694742209,
      "loss": 0.1428,
      "step": 11210
    },
    {
      "epoch": 3.2883939038686987,
      "grad_norm": 0.5483033657073975,
      "learning_rate": 0.0006207628984546206,
      "loss": 0.1557,
      "step": 11220
    },
    {
      "epoch": 3.291324736225088,
      "grad_norm": 0.46631699800491333,
      "learning_rate": 0.0006206791021670322,
      "loss": 0.1626,
      "step": 11230
    },
    {
      "epoch": 3.294255568581477,
      "grad_norm": 0.516017735004425,
      "learning_rate": 0.0006205953058794438,
      "loss": 0.1422,
      "step": 11240
    },
    {
      "epoch": 3.2971864009378664,
      "grad_norm": 0.5950846076011658,
      "learning_rate": 0.0006205115095918555,
      "loss": 0.1562,
      "step": 11250
    },
    {
      "epoch": 3.3001172332942557,
      "grad_norm": 0.9997163414955139,
      "learning_rate": 0.0006204277133042671,
      "loss": 0.1648,
      "step": 11260
    },
    {
      "epoch": 3.303048065650645,
      "grad_norm": 0.6368048787117004,
      "learning_rate": 0.0006203439170166787,
      "loss": 0.1448,
      "step": 11270
    },
    {
      "epoch": 3.305978898007034,
      "grad_norm": 1.0022528171539307,
      "learning_rate": 0.0006202601207290904,
      "loss": 0.1536,
      "step": 11280
    },
    {
      "epoch": 3.3089097303634234,
      "grad_norm": 1.116983413696289,
      "learning_rate": 0.0006201763244415019,
      "loss": 0.147,
      "step": 11290
    },
    {
      "epoch": 3.311840562719812,
      "grad_norm": 0.5418190360069275,
      "learning_rate": 0.0006200925281539137,
      "loss": 0.1851,
      "step": 11300
    },
    {
      "epoch": 3.3147713950762014,
      "grad_norm": 0.4790792763233185,
      "learning_rate": 0.0006200087318663253,
      "loss": 0.1608,
      "step": 11310
    },
    {
      "epoch": 3.3177022274325907,
      "grad_norm": 0.4892513155937195,
      "learning_rate": 0.000619924935578737,
      "loss": 0.1557,
      "step": 11320
    },
    {
      "epoch": 3.32063305978898,
      "grad_norm": 0.7241116762161255,
      "learning_rate": 0.0006198411392911486,
      "loss": 0.1407,
      "step": 11330
    },
    {
      "epoch": 3.323563892145369,
      "grad_norm": 0.9626215696334839,
      "learning_rate": 0.0006197573430035601,
      "loss": 0.1699,
      "step": 11340
    },
    {
      "epoch": 3.3264947245017584,
      "grad_norm": 0.7567112445831299,
      "learning_rate": 0.0006196735467159718,
      "loss": 0.1866,
      "step": 11350
    },
    {
      "epoch": 3.3294255568581477,
      "grad_norm": 0.6882949471473694,
      "learning_rate": 0.0006195897504283834,
      "loss": 0.151,
      "step": 11360
    },
    {
      "epoch": 3.332356389214537,
      "grad_norm": 0.8236778974533081,
      "learning_rate": 0.0006195059541407951,
      "loss": 0.1624,
      "step": 11370
    },
    {
      "epoch": 3.335287221570926,
      "grad_norm": 0.6722796559333801,
      "learning_rate": 0.0006194221578532067,
      "loss": 0.1555,
      "step": 11380
    },
    {
      "epoch": 3.3382180539273154,
      "grad_norm": 0.5580237507820129,
      "learning_rate": 0.0006193383615656184,
      "loss": 0.168,
      "step": 11390
    },
    {
      "epoch": 3.3411488862837047,
      "grad_norm": 0.6475955843925476,
      "learning_rate": 0.00061925456527803,
      "loss": 0.1693,
      "step": 11400
    },
    {
      "epoch": 3.344079718640094,
      "grad_norm": 0.8397572636604309,
      "learning_rate": 0.0006191707689904416,
      "loss": 0.1857,
      "step": 11410
    },
    {
      "epoch": 3.347010550996483,
      "grad_norm": 0.6403765082359314,
      "learning_rate": 0.0006190869727028533,
      "loss": 0.1464,
      "step": 11420
    },
    {
      "epoch": 3.3499413833528724,
      "grad_norm": 1.4015129804611206,
      "learning_rate": 0.0006190031764152649,
      "loss": 0.1594,
      "step": 11430
    },
    {
      "epoch": 3.3528722157092616,
      "grad_norm": 0.4335048794746399,
      "learning_rate": 0.0006189193801276765,
      "loss": 0.1474,
      "step": 11440
    },
    {
      "epoch": 3.355803048065651,
      "grad_norm": 0.7784940004348755,
      "learning_rate": 0.0006188355838400881,
      "loss": 0.1793,
      "step": 11450
    },
    {
      "epoch": 3.3587338804220397,
      "grad_norm": 0.45334064960479736,
      "learning_rate": 0.0006187517875524997,
      "loss": 0.1423,
      "step": 11460
    },
    {
      "epoch": 3.361664712778429,
      "grad_norm": 1.6356418132781982,
      "learning_rate": 0.0006186679912649115,
      "loss": 0.1429,
      "step": 11470
    },
    {
      "epoch": 3.364595545134818,
      "grad_norm": 1.6750766038894653,
      "learning_rate": 0.0006185841949773231,
      "loss": 0.1666,
      "step": 11480
    },
    {
      "epoch": 3.3675263774912074,
      "grad_norm": 0.753884494304657,
      "learning_rate": 0.0006185003986897347,
      "loss": 0.1668,
      "step": 11490
    },
    {
      "epoch": 3.3704572098475967,
      "grad_norm": 2.1573824882507324,
      "learning_rate": 0.0006184166024021463,
      "loss": 0.1445,
      "step": 11500
    },
    {
      "epoch": 3.373388042203986,
      "grad_norm": 1.1348609924316406,
      "learning_rate": 0.0006183328061145579,
      "loss": 0.1583,
      "step": 11510
    },
    {
      "epoch": 3.376318874560375,
      "grad_norm": 0.47449713945388794,
      "learning_rate": 0.0006182490098269696,
      "loss": 0.1571,
      "step": 11520
    },
    {
      "epoch": 3.3792497069167644,
      "grad_norm": 0.6707467436790466,
      "learning_rate": 0.0006181652135393812,
      "loss": 0.1484,
      "step": 11530
    },
    {
      "epoch": 3.3821805392731537,
      "grad_norm": 0.3597204387187958,
      "learning_rate": 0.0006180814172517928,
      "loss": 0.1316,
      "step": 11540
    },
    {
      "epoch": 3.385111371629543,
      "grad_norm": 1.251466155052185,
      "learning_rate": 0.0006179976209642044,
      "loss": 0.1317,
      "step": 11550
    },
    {
      "epoch": 3.388042203985932,
      "grad_norm": 0.8386398553848267,
      "learning_rate": 0.000617913824676616,
      "loss": 0.1637,
      "step": 11560
    },
    {
      "epoch": 3.3909730363423214,
      "grad_norm": 0.9162918329238892,
      "learning_rate": 0.0006178300283890278,
      "loss": 0.1592,
      "step": 11570
    },
    {
      "epoch": 3.39390386869871,
      "grad_norm": 1.480491042137146,
      "learning_rate": 0.0006177462321014394,
      "loss": 0.1415,
      "step": 11580
    },
    {
      "epoch": 3.3968347010550994,
      "grad_norm": 0.5094533562660217,
      "learning_rate": 0.000617662435813851,
      "loss": 0.1742,
      "step": 11590
    },
    {
      "epoch": 3.3997655334114887,
      "grad_norm": 1.476556658744812,
      "learning_rate": 0.0006175786395262626,
      "loss": 0.1739,
      "step": 11600
    },
    {
      "epoch": 3.402696365767878,
      "grad_norm": 0.7696077823638916,
      "learning_rate": 0.0006174948432386742,
      "loss": 0.1539,
      "step": 11610
    },
    {
      "epoch": 3.405627198124267,
      "grad_norm": 0.6941834092140198,
      "learning_rate": 0.0006174110469510859,
      "loss": 0.1434,
      "step": 11620
    },
    {
      "epoch": 3.4085580304806564,
      "grad_norm": 1.1463475227355957,
      "learning_rate": 0.0006173272506634975,
      "loss": 0.1605,
      "step": 11630
    },
    {
      "epoch": 3.4114888628370457,
      "grad_norm": 1.013229250907898,
      "learning_rate": 0.0006172434543759091,
      "loss": 0.1304,
      "step": 11640
    },
    {
      "epoch": 3.414419695193435,
      "grad_norm": 1.015641450881958,
      "learning_rate": 0.0006171596580883208,
      "loss": 0.1313,
      "step": 11650
    },
    {
      "epoch": 3.417350527549824,
      "grad_norm": 2.080869436264038,
      "learning_rate": 0.0006170758618007325,
      "loss": 0.1683,
      "step": 11660
    },
    {
      "epoch": 3.4202813599062134,
      "grad_norm": 0.6029918193817139,
      "learning_rate": 0.0006169920655131441,
      "loss": 0.1592,
      "step": 11670
    },
    {
      "epoch": 3.4232121922626026,
      "grad_norm": 0.5216482877731323,
      "learning_rate": 0.0006169082692255557,
      "loss": 0.1536,
      "step": 11680
    },
    {
      "epoch": 3.426143024618992,
      "grad_norm": 0.43458759784698486,
      "learning_rate": 0.0006168244729379674,
      "loss": 0.1273,
      "step": 11690
    },
    {
      "epoch": 3.429073856975381,
      "grad_norm": 0.7027468681335449,
      "learning_rate": 0.0006167406766503789,
      "loss": 0.1472,
      "step": 11700
    },
    {
      "epoch": 3.4320046893317704,
      "grad_norm": 0.731717586517334,
      "learning_rate": 0.0006166568803627906,
      "loss": 0.1402,
      "step": 11710
    },
    {
      "epoch": 3.4349355216881596,
      "grad_norm": 2.6742377281188965,
      "learning_rate": 0.0006165730840752022,
      "loss": 0.1435,
      "step": 11720
    },
    {
      "epoch": 3.437866354044549,
      "grad_norm": 0.8151509165763855,
      "learning_rate": 0.0006164892877876138,
      "loss": 0.1675,
      "step": 11730
    },
    {
      "epoch": 3.4407971864009377,
      "grad_norm": 2.789790391921997,
      "learning_rate": 0.0006164054915000256,
      "loss": 0.1578,
      "step": 11740
    },
    {
      "epoch": 3.443728018757327,
      "grad_norm": 0.700695276260376,
      "learning_rate": 0.0006163216952124371,
      "loss": 0.1604,
      "step": 11750
    },
    {
      "epoch": 3.446658851113716,
      "grad_norm": 0.9141601324081421,
      "learning_rate": 0.0006162378989248488,
      "loss": 0.189,
      "step": 11760
    },
    {
      "epoch": 3.4495896834701054,
      "grad_norm": 1.205532431602478,
      "learning_rate": 0.0006161541026372604,
      "loss": 0.1517,
      "step": 11770
    },
    {
      "epoch": 3.4525205158264947,
      "grad_norm": 0.6039425730705261,
      "learning_rate": 0.000616070306349672,
      "loss": 0.173,
      "step": 11780
    },
    {
      "epoch": 3.455451348182884,
      "grad_norm": 0.6330525875091553,
      "learning_rate": 0.0006159865100620837,
      "loss": 0.1347,
      "step": 11790
    },
    {
      "epoch": 3.458382180539273,
      "grad_norm": 0.8183841109275818,
      "learning_rate": 0.0006159027137744953,
      "loss": 0.1674,
      "step": 11800
    },
    {
      "epoch": 3.4613130128956624,
      "grad_norm": 1.0163629055023193,
      "learning_rate": 0.0006158189174869069,
      "loss": 0.1537,
      "step": 11810
    },
    {
      "epoch": 3.4642438452520516,
      "grad_norm": 1.2126710414886475,
      "learning_rate": 0.0006157351211993186,
      "loss": 0.1613,
      "step": 11820
    },
    {
      "epoch": 3.467174677608441,
      "grad_norm": 0.6790556311607361,
      "learning_rate": 0.0006156513249117303,
      "loss": 0.147,
      "step": 11830
    },
    {
      "epoch": 3.47010550996483,
      "grad_norm": 0.48634225130081177,
      "learning_rate": 0.0006155675286241419,
      "loss": 0.1458,
      "step": 11840
    },
    {
      "epoch": 3.4730363423212194,
      "grad_norm": 1.1549880504608154,
      "learning_rate": 0.0006154837323365535,
      "loss": 0.1535,
      "step": 11850
    },
    {
      "epoch": 3.4759671746776086,
      "grad_norm": 0.6809279918670654,
      "learning_rate": 0.0006153999360489651,
      "loss": 0.1414,
      "step": 11860
    },
    {
      "epoch": 3.4788980070339974,
      "grad_norm": 0.4961569905281067,
      "learning_rate": 0.0006153161397613767,
      "loss": 0.1625,
      "step": 11870
    },
    {
      "epoch": 3.4818288393903867,
      "grad_norm": 0.7716221809387207,
      "learning_rate": 0.0006152323434737884,
      "loss": 0.1523,
      "step": 11880
    },
    {
      "epoch": 3.484759671746776,
      "grad_norm": 0.5691015720367432,
      "learning_rate": 0.0006151485471862,
      "loss": 0.1525,
      "step": 11890
    },
    {
      "epoch": 3.487690504103165,
      "grad_norm": 1.0000922679901123,
      "learning_rate": 0.0006150647508986116,
      "loss": 0.1539,
      "step": 11900
    },
    {
      "epoch": 3.4906213364595544,
      "grad_norm": 0.9649940133094788,
      "learning_rate": 0.0006149809546110233,
      "loss": 0.165,
      "step": 11910
    },
    {
      "epoch": 3.4935521688159437,
      "grad_norm": 1.0083296298980713,
      "learning_rate": 0.0006148971583234349,
      "loss": 0.1542,
      "step": 11920
    },
    {
      "epoch": 3.496483001172333,
      "grad_norm": 0.5576121211051941,
      "learning_rate": 0.0006148133620358466,
      "loss": 0.1524,
      "step": 11930
    },
    {
      "epoch": 3.499413833528722,
      "grad_norm": 0.4655028283596039,
      "learning_rate": 0.0006147295657482582,
      "loss": 0.1286,
      "step": 11940
    },
    {
      "epoch": 3.5023446658851114,
      "grad_norm": 0.6620337963104248,
      "learning_rate": 0.0006146457694606698,
      "loss": 0.1324,
      "step": 11950
    },
    {
      "epoch": 3.5052754982415006,
      "grad_norm": 0.7855849862098694,
      "learning_rate": 0.0006145619731730814,
      "loss": 0.1497,
      "step": 11960
    },
    {
      "epoch": 3.50820633059789,
      "grad_norm": 0.5732852220535278,
      "learning_rate": 0.000614478176885493,
      "loss": 0.1191,
      "step": 11970
    },
    {
      "epoch": 3.511137162954279,
      "grad_norm": 0.9986535310745239,
      "learning_rate": 0.0006143943805979047,
      "loss": 0.1698,
      "step": 11980
    },
    {
      "epoch": 3.5140679953106684,
      "grad_norm": 0.6441960334777832,
      "learning_rate": 0.0006143105843103164,
      "loss": 0.1116,
      "step": 11990
    },
    {
      "epoch": 3.5169988276670576,
      "grad_norm": 0.6380882859230042,
      "learning_rate": 0.000614226788022728,
      "loss": 0.1515,
      "step": 12000
    },
    {
      "epoch": 3.519929660023447,
      "grad_norm": 0.4607087969779968,
      "learning_rate": 0.0006141429917351396,
      "loss": 0.1432,
      "step": 12010
    },
    {
      "epoch": 3.522860492379836,
      "grad_norm": 0.5711358785629272,
      "learning_rate": 0.0006140591954475512,
      "loss": 0.1454,
      "step": 12020
    },
    {
      "epoch": 3.5257913247362254,
      "grad_norm": 0.7763604521751404,
      "learning_rate": 0.0006139753991599629,
      "loss": 0.1469,
      "step": 12030
    },
    {
      "epoch": 3.528722157092614,
      "grad_norm": 0.8411471247673035,
      "learning_rate": 0.0006138916028723745,
      "loss": 0.178,
      "step": 12040
    },
    {
      "epoch": 3.5316529894490034,
      "grad_norm": 1.05898916721344,
      "learning_rate": 0.0006138078065847862,
      "loss": 0.1639,
      "step": 12050
    },
    {
      "epoch": 3.5345838218053927,
      "grad_norm": 0.7377519011497498,
      "learning_rate": 0.0006137240102971977,
      "loss": 0.1387,
      "step": 12060
    },
    {
      "epoch": 3.537514654161782,
      "grad_norm": 1.2913439273834229,
      "learning_rate": 0.0006136402140096093,
      "loss": 0.1421,
      "step": 12070
    },
    {
      "epoch": 3.540445486518171,
      "grad_norm": 0.7173754572868347,
      "learning_rate": 0.0006135564177220211,
      "loss": 0.1396,
      "step": 12080
    },
    {
      "epoch": 3.5433763188745604,
      "grad_norm": 0.49176180362701416,
      "learning_rate": 0.0006134726214344327,
      "loss": 0.1561,
      "step": 12090
    },
    {
      "epoch": 3.5463071512309496,
      "grad_norm": 0.8487268686294556,
      "learning_rate": 0.0006133888251468444,
      "loss": 0.1556,
      "step": 12100
    },
    {
      "epoch": 3.549237983587339,
      "grad_norm": 1.4065483808517456,
      "learning_rate": 0.0006133050288592559,
      "loss": 0.1691,
      "step": 12110
    },
    {
      "epoch": 3.552168815943728,
      "grad_norm": 0.537325918674469,
      "learning_rate": 0.0006132212325716675,
      "loss": 0.1454,
      "step": 12120
    },
    {
      "epoch": 3.5550996483001174,
      "grad_norm": 0.7024717926979065,
      "learning_rate": 0.0006131374362840792,
      "loss": 0.1455,
      "step": 12130
    },
    {
      "epoch": 3.558030480656506,
      "grad_norm": 0.6716973781585693,
      "learning_rate": 0.0006130536399964908,
      "loss": 0.1348,
      "step": 12140
    },
    {
      "epoch": 3.5609613130128954,
      "grad_norm": 0.7213371992111206,
      "learning_rate": 0.0006129698437089025,
      "loss": 0.1547,
      "step": 12150
    },
    {
      "epoch": 3.5638921453692847,
      "grad_norm": 0.6633314490318298,
      "learning_rate": 0.0006128860474213141,
      "loss": 0.1474,
      "step": 12160
    },
    {
      "epoch": 3.566822977725674,
      "grad_norm": 0.7721573114395142,
      "learning_rate": 0.0006128022511337257,
      "loss": 0.1594,
      "step": 12170
    },
    {
      "epoch": 3.569753810082063,
      "grad_norm": 0.776767373085022,
      "learning_rate": 0.0006127184548461374,
      "loss": 0.1655,
      "step": 12180
    },
    {
      "epoch": 3.5726846424384524,
      "grad_norm": 0.6167913675308228,
      "learning_rate": 0.000612634658558549,
      "loss": 0.1506,
      "step": 12190
    },
    {
      "epoch": 3.5756154747948417,
      "grad_norm": 0.546144962310791,
      "learning_rate": 0.0006125508622709607,
      "loss": 0.1441,
      "step": 12200
    },
    {
      "epoch": 3.578546307151231,
      "grad_norm": 0.5717650055885315,
      "learning_rate": 0.0006124670659833723,
      "loss": 0.1602,
      "step": 12210
    },
    {
      "epoch": 3.58147713950762,
      "grad_norm": 0.7027503848075867,
      "learning_rate": 0.0006123832696957839,
      "loss": 0.1479,
      "step": 12220
    },
    {
      "epoch": 3.5844079718640094,
      "grad_norm": 0.5696045160293579,
      "learning_rate": 0.0006122994734081955,
      "loss": 0.14,
      "step": 12230
    },
    {
      "epoch": 3.5873388042203986,
      "grad_norm": 0.9549161195755005,
      "learning_rate": 0.0006122156771206071,
      "loss": 0.1417,
      "step": 12240
    },
    {
      "epoch": 3.590269636576788,
      "grad_norm": 0.5608143210411072,
      "learning_rate": 0.0006121318808330189,
      "loss": 0.1533,
      "step": 12250
    },
    {
      "epoch": 3.593200468933177,
      "grad_norm": 0.8020023107528687,
      "learning_rate": 0.0006120480845454305,
      "loss": 0.151,
      "step": 12260
    },
    {
      "epoch": 3.5961313012895664,
      "grad_norm": 0.694480299949646,
      "learning_rate": 0.0006119642882578421,
      "loss": 0.1424,
      "step": 12270
    },
    {
      "epoch": 3.5990621336459556,
      "grad_norm": 0.4053484797477722,
      "learning_rate": 0.0006118804919702537,
      "loss": 0.149,
      "step": 12280
    },
    {
      "epoch": 3.601992966002345,
      "grad_norm": 0.5908850431442261,
      "learning_rate": 0.0006117966956826653,
      "loss": 0.1369,
      "step": 12290
    },
    {
      "epoch": 3.604923798358734,
      "grad_norm": 0.658709704875946,
      "learning_rate": 0.000611712899395077,
      "loss": 0.1508,
      "step": 12300
    },
    {
      "epoch": 3.6078546307151234,
      "grad_norm": 0.5126379132270813,
      "learning_rate": 0.0006116291031074886,
      "loss": 0.1407,
      "step": 12310
    },
    {
      "epoch": 3.610785463071512,
      "grad_norm": 0.7356109619140625,
      "learning_rate": 0.0006115453068199002,
      "loss": 0.1803,
      "step": 12320
    },
    {
      "epoch": 3.6137162954279014,
      "grad_norm": 0.7053236365318298,
      "learning_rate": 0.0006114615105323118,
      "loss": 0.1479,
      "step": 12330
    },
    {
      "epoch": 3.6166471277842906,
      "grad_norm": 0.5909850597381592,
      "learning_rate": 0.0006113777142447234,
      "loss": 0.1467,
      "step": 12340
    },
    {
      "epoch": 3.61957796014068,
      "grad_norm": 0.9900311231613159,
      "learning_rate": 0.0006112939179571352,
      "loss": 0.1754,
      "step": 12350
    },
    {
      "epoch": 3.622508792497069,
      "grad_norm": 1.0759270191192627,
      "learning_rate": 0.0006112101216695468,
      "loss": 0.1668,
      "step": 12360
    },
    {
      "epoch": 3.6254396248534584,
      "grad_norm": 0.8771843910217285,
      "learning_rate": 0.0006111263253819584,
      "loss": 0.1382,
      "step": 12370
    },
    {
      "epoch": 3.6283704572098476,
      "grad_norm": 0.6186023950576782,
      "learning_rate": 0.00061104252909437,
      "loss": 0.1547,
      "step": 12380
    },
    {
      "epoch": 3.631301289566237,
      "grad_norm": 0.9342401027679443,
      "learning_rate": 0.0006109587328067817,
      "loss": 0.1722,
      "step": 12390
    },
    {
      "epoch": 3.634232121922626,
      "grad_norm": 0.7107071876525879,
      "learning_rate": 0.0006108749365191933,
      "loss": 0.1717,
      "step": 12400
    },
    {
      "epoch": 3.6371629542790154,
      "grad_norm": 0.7749771475791931,
      "learning_rate": 0.0006107911402316049,
      "loss": 0.1427,
      "step": 12410
    },
    {
      "epoch": 3.6400937866354046,
      "grad_norm": 0.8476743698120117,
      "learning_rate": 0.0006107073439440165,
      "loss": 0.1544,
      "step": 12420
    },
    {
      "epoch": 3.6430246189917934,
      "grad_norm": 0.5486248135566711,
      "learning_rate": 0.0006106235476564282,
      "loss": 0.1472,
      "step": 12430
    },
    {
      "epoch": 3.6459554513481827,
      "grad_norm": 1.5727992057800293,
      "learning_rate": 0.0006105397513688399,
      "loss": 0.1442,
      "step": 12440
    },
    {
      "epoch": 3.648886283704572,
      "grad_norm": 0.7317689061164856,
      "learning_rate": 0.0006104559550812515,
      "loss": 0.1473,
      "step": 12450
    },
    {
      "epoch": 3.651817116060961,
      "grad_norm": 0.6170098781585693,
      "learning_rate": 0.0006103721587936631,
      "loss": 0.1631,
      "step": 12460
    },
    {
      "epoch": 3.6547479484173504,
      "grad_norm": 0.5650933384895325,
      "learning_rate": 0.0006102883625060747,
      "loss": 0.167,
      "step": 12470
    },
    {
      "epoch": 3.6576787807737396,
      "grad_norm": 0.9829862713813782,
      "learning_rate": 0.0006102045662184863,
      "loss": 0.1804,
      "step": 12480
    },
    {
      "epoch": 3.660609613130129,
      "grad_norm": 0.4446065127849579,
      "learning_rate": 0.000610120769930898,
      "loss": 0.1269,
      "step": 12490
    },
    {
      "epoch": 3.663540445486518,
      "grad_norm": 0.7345079779624939,
      "learning_rate": 0.0006100369736433096,
      "loss": 0.1267,
      "step": 12500
    },
    {
      "epoch": 3.6664712778429074,
      "grad_norm": 1.262314796447754,
      "learning_rate": 0.0006099531773557212,
      "loss": 0.1413,
      "step": 12510
    },
    {
      "epoch": 3.6694021101992966,
      "grad_norm": 0.9464831948280334,
      "learning_rate": 0.0006098693810681329,
      "loss": 0.1896,
      "step": 12520
    },
    {
      "epoch": 3.672332942555686,
      "grad_norm": 0.7986266016960144,
      "learning_rate": 0.0006097855847805445,
      "loss": 0.1436,
      "step": 12530
    },
    {
      "epoch": 3.675263774912075,
      "grad_norm": 0.9474548101425171,
      "learning_rate": 0.0006097017884929562,
      "loss": 0.1554,
      "step": 12540
    },
    {
      "epoch": 3.6781946072684644,
      "grad_norm": 0.475253701210022,
      "learning_rate": 0.0006096179922053678,
      "loss": 0.1372,
      "step": 12550
    },
    {
      "epoch": 3.6811254396248536,
      "grad_norm": 0.41533610224723816,
      "learning_rate": 0.0006095341959177795,
      "loss": 0.1172,
      "step": 12560
    },
    {
      "epoch": 3.684056271981243,
      "grad_norm": 0.814399242401123,
      "learning_rate": 0.0006094503996301911,
      "loss": 0.1496,
      "step": 12570
    },
    {
      "epoch": 3.686987104337632,
      "grad_norm": 1.0920714139938354,
      "learning_rate": 0.0006093666033426026,
      "loss": 0.1596,
      "step": 12580
    },
    {
      "epoch": 3.6899179366940213,
      "grad_norm": 0.8856592774391174,
      "learning_rate": 0.0006092828070550143,
      "loss": 0.1886,
      "step": 12590
    },
    {
      "epoch": 3.69284876905041,
      "grad_norm": 0.6233738660812378,
      "learning_rate": 0.000609199010767426,
      "loss": 0.1695,
      "step": 12600
    },
    {
      "epoch": 3.6957796014067994,
      "grad_norm": 0.37940463423728943,
      "learning_rate": 0.0006091152144798377,
      "loss": 0.1413,
      "step": 12610
    },
    {
      "epoch": 3.6987104337631886,
      "grad_norm": 0.7419617772102356,
      "learning_rate": 0.0006090314181922493,
      "loss": 0.1808,
      "step": 12620
    },
    {
      "epoch": 3.701641266119578,
      "grad_norm": 0.6469874382019043,
      "learning_rate": 0.0006089476219046609,
      "loss": 0.1474,
      "step": 12630
    },
    {
      "epoch": 3.704572098475967,
      "grad_norm": 0.6193299889564514,
      "learning_rate": 0.0006088638256170725,
      "loss": 0.1634,
      "step": 12640
    },
    {
      "epoch": 3.7075029308323564,
      "grad_norm": 0.7074589729309082,
      "learning_rate": 0.0006087800293294841,
      "loss": 0.1424,
      "step": 12650
    },
    {
      "epoch": 3.7104337631887456,
      "grad_norm": 1.1901066303253174,
      "learning_rate": 0.0006086962330418958,
      "loss": 0.1492,
      "step": 12660
    },
    {
      "epoch": 3.713364595545135,
      "grad_norm": 0.6688225865364075,
      "learning_rate": 0.0006086124367543074,
      "loss": 0.1585,
      "step": 12670
    },
    {
      "epoch": 3.716295427901524,
      "grad_norm": 0.7751538753509521,
      "learning_rate": 0.000608528640466719,
      "loss": 0.13,
      "step": 12680
    },
    {
      "epoch": 3.7192262602579134,
      "grad_norm": 1.412506341934204,
      "learning_rate": 0.0006084448441791307,
      "loss": 0.1286,
      "step": 12690
    },
    {
      "epoch": 3.7221570926143026,
      "grad_norm": 0.6902099847793579,
      "learning_rate": 0.0006083610478915423,
      "loss": 0.144,
      "step": 12700
    },
    {
      "epoch": 3.7250879249706914,
      "grad_norm": 1.2651588916778564,
      "learning_rate": 0.000608277251603954,
      "loss": 0.1664,
      "step": 12710
    },
    {
      "epoch": 3.7280187573270807,
      "grad_norm": 1.7053266763687134,
      "learning_rate": 0.0006081934553163656,
      "loss": 0.1399,
      "step": 12720
    },
    {
      "epoch": 3.73094958968347,
      "grad_norm": 0.9649506211280823,
      "learning_rate": 0.0006081096590287772,
      "loss": 0.1505,
      "step": 12730
    },
    {
      "epoch": 3.733880422039859,
      "grad_norm": 0.8179786801338196,
      "learning_rate": 0.0006080258627411888,
      "loss": 0.1561,
      "step": 12740
    },
    {
      "epoch": 3.7368112543962484,
      "grad_norm": 0.8543479442596436,
      "learning_rate": 0.0006079420664536004,
      "loss": 0.1491,
      "step": 12750
    },
    {
      "epoch": 3.7397420867526376,
      "grad_norm": 0.49203214049339294,
      "learning_rate": 0.0006078582701660121,
      "loss": 0.1375,
      "step": 12760
    },
    {
      "epoch": 3.742672919109027,
      "grad_norm": 0.9351682066917419,
      "learning_rate": 0.0006077744738784238,
      "loss": 0.1679,
      "step": 12770
    },
    {
      "epoch": 3.745603751465416,
      "grad_norm": 0.6791542768478394,
      "learning_rate": 0.0006076906775908354,
      "loss": 0.1564,
      "step": 12780
    },
    {
      "epoch": 3.7485345838218054,
      "grad_norm": 1.028908610343933,
      "learning_rate": 0.000607606881303247,
      "loss": 0.1364,
      "step": 12790
    },
    {
      "epoch": 3.7514654161781946,
      "grad_norm": 0.7823746204376221,
      "learning_rate": 0.0006075230850156586,
      "loss": 0.1396,
      "step": 12800
    },
    {
      "epoch": 3.754396248534584,
      "grad_norm": 0.9392743110656738,
      "learning_rate": 0.0006074392887280703,
      "loss": 0.1385,
      "step": 12810
    },
    {
      "epoch": 3.757327080890973,
      "grad_norm": 0.567105233669281,
      "learning_rate": 0.0006073554924404819,
      "loss": 0.1601,
      "step": 12820
    },
    {
      "epoch": 3.7602579132473624,
      "grad_norm": 0.6835628151893616,
      "learning_rate": 0.0006072716961528935,
      "loss": 0.1492,
      "step": 12830
    },
    {
      "epoch": 3.7631887456037516,
      "grad_norm": 0.5412255525588989,
      "learning_rate": 0.0006071878998653051,
      "loss": 0.1528,
      "step": 12840
    },
    {
      "epoch": 3.766119577960141,
      "grad_norm": 0.5790740847587585,
      "learning_rate": 0.0006071041035777167,
      "loss": 0.1421,
      "step": 12850
    },
    {
      "epoch": 3.76905041031653,
      "grad_norm": 0.7969921827316284,
      "learning_rate": 0.0006070203072901285,
      "loss": 0.1619,
      "step": 12860
    },
    {
      "epoch": 3.7719812426729193,
      "grad_norm": 0.7307740449905396,
      "learning_rate": 0.0006069365110025401,
      "loss": 0.1503,
      "step": 12870
    },
    {
      "epoch": 3.7749120750293086,
      "grad_norm": 0.9875209331512451,
      "learning_rate": 0.0006068527147149517,
      "loss": 0.1429,
      "step": 12880
    },
    {
      "epoch": 3.7778429073856974,
      "grad_norm": 0.42172491550445557,
      "learning_rate": 0.0006067689184273633,
      "loss": 0.1598,
      "step": 12890
    },
    {
      "epoch": 3.7807737397420866,
      "grad_norm": 1.2271732091903687,
      "learning_rate": 0.000606685122139775,
      "loss": 0.1469,
      "step": 12900
    },
    {
      "epoch": 3.783704572098476,
      "grad_norm": 0.854591429233551,
      "learning_rate": 0.0006066013258521866,
      "loss": 0.128,
      "step": 12910
    },
    {
      "epoch": 3.786635404454865,
      "grad_norm": 1.1358448266983032,
      "learning_rate": 0.0006065175295645982,
      "loss": 0.1359,
      "step": 12920
    },
    {
      "epoch": 3.7895662368112544,
      "grad_norm": 1.2198385000228882,
      "learning_rate": 0.0006064337332770099,
      "loss": 0.1642,
      "step": 12930
    },
    {
      "epoch": 3.7924970691676436,
      "grad_norm": 0.500312089920044,
      "learning_rate": 0.0006063499369894214,
      "loss": 0.1329,
      "step": 12940
    },
    {
      "epoch": 3.795427901524033,
      "grad_norm": 0.84815514087677,
      "learning_rate": 0.0006062661407018332,
      "loss": 0.1511,
      "step": 12950
    },
    {
      "epoch": 3.798358733880422,
      "grad_norm": 0.5338693261146545,
      "learning_rate": 0.0006061823444142448,
      "loss": 0.1421,
      "step": 12960
    },
    {
      "epoch": 3.8012895662368114,
      "grad_norm": 0.987982988357544,
      "learning_rate": 0.0006060985481266564,
      "loss": 0.1345,
      "step": 12970
    },
    {
      "epoch": 3.8042203985932006,
      "grad_norm": 0.8606627583503723,
      "learning_rate": 0.0006060147518390681,
      "loss": 0.1502,
      "step": 12980
    },
    {
      "epoch": 3.80715123094959,
      "grad_norm": 0.38493287563323975,
      "learning_rate": 0.0006059309555514796,
      "loss": 0.1661,
      "step": 12990
    },
    {
      "epoch": 3.8100820633059787,
      "grad_norm": 1.0023832321166992,
      "learning_rate": 0.0006058471592638913,
      "loss": 0.1457,
      "step": 13000
    },
    {
      "epoch": 3.813012895662368,
      "grad_norm": 0.6610286235809326,
      "learning_rate": 0.0006057633629763029,
      "loss": 0.1763,
      "step": 13010
    },
    {
      "epoch": 3.815943728018757,
      "grad_norm": 0.7431963086128235,
      "learning_rate": 0.0006056795666887145,
      "loss": 0.1348,
      "step": 13020
    },
    {
      "epoch": 3.8188745603751464,
      "grad_norm": 0.6204264760017395,
      "learning_rate": 0.0006055957704011263,
      "loss": 0.1458,
      "step": 13030
    },
    {
      "epoch": 3.8218053927315356,
      "grad_norm": 0.743090808391571,
      "learning_rate": 0.0006055119741135379,
      "loss": 0.1721,
      "step": 13040
    },
    {
      "epoch": 3.824736225087925,
      "grad_norm": 1.1391799449920654,
      "learning_rate": 0.0006054281778259495,
      "loss": 0.1298,
      "step": 13050
    },
    {
      "epoch": 3.827667057444314,
      "grad_norm": 0.789202868938446,
      "learning_rate": 0.0006053443815383611,
      "loss": 0.1668,
      "step": 13060
    },
    {
      "epoch": 3.8305978898007034,
      "grad_norm": 1.1101338863372803,
      "learning_rate": 0.0006052605852507728,
      "loss": 0.135,
      "step": 13070
    },
    {
      "epoch": 3.8335287221570926,
      "grad_norm": 0.7664517164230347,
      "learning_rate": 0.0006051767889631844,
      "loss": 0.1322,
      "step": 13080
    },
    {
      "epoch": 3.836459554513482,
      "grad_norm": 0.7740065455436707,
      "learning_rate": 0.000605092992675596,
      "loss": 0.1288,
      "step": 13090
    },
    {
      "epoch": 3.839390386869871,
      "grad_norm": 0.8268626928329468,
      "learning_rate": 0.0006050091963880076,
      "loss": 0.1394,
      "step": 13100
    },
    {
      "epoch": 3.8423212192262604,
      "grad_norm": 1.3552501201629639,
      "learning_rate": 0.0006049254001004192,
      "loss": 0.1432,
      "step": 13110
    },
    {
      "epoch": 3.8452520515826496,
      "grad_norm": 1.9207146167755127,
      "learning_rate": 0.000604841603812831,
      "loss": 0.1507,
      "step": 13120
    },
    {
      "epoch": 3.848182883939039,
      "grad_norm": 0.2962343990802765,
      "learning_rate": 0.0006047578075252426,
      "loss": 0.1431,
      "step": 13130
    },
    {
      "epoch": 3.851113716295428,
      "grad_norm": 1.4363473653793335,
      "learning_rate": 0.0006046740112376542,
      "loss": 0.167,
      "step": 13140
    },
    {
      "epoch": 3.8540445486518173,
      "grad_norm": 1.0444824695587158,
      "learning_rate": 0.0006045902149500658,
      "loss": 0.1558,
      "step": 13150
    },
    {
      "epoch": 3.8569753810082066,
      "grad_norm": 0.9591439962387085,
      "learning_rate": 0.0006045064186624774,
      "loss": 0.1528,
      "step": 13160
    },
    {
      "epoch": 3.8599062133645954,
      "grad_norm": 0.5321890711784363,
      "learning_rate": 0.0006044226223748891,
      "loss": 0.1326,
      "step": 13170
    },
    {
      "epoch": 3.8628370457209846,
      "grad_norm": 0.6690525412559509,
      "learning_rate": 0.0006043388260873007,
      "loss": 0.1581,
      "step": 13180
    },
    {
      "epoch": 3.865767878077374,
      "grad_norm": 1.926810622215271,
      "learning_rate": 0.0006042550297997123,
      "loss": 0.1457,
      "step": 13190
    },
    {
      "epoch": 3.868698710433763,
      "grad_norm": 0.8105840086936951,
      "learning_rate": 0.0006041712335121239,
      "loss": 0.1656,
      "step": 13200
    },
    {
      "epoch": 3.8716295427901524,
      "grad_norm": 0.9932574033737183,
      "learning_rate": 0.0006040874372245356,
      "loss": 0.1407,
      "step": 13210
    },
    {
      "epoch": 3.8745603751465416,
      "grad_norm": 1.2811132669448853,
      "learning_rate": 0.0006040036409369473,
      "loss": 0.1577,
      "step": 13220
    },
    {
      "epoch": 3.877491207502931,
      "grad_norm": 0.6199752688407898,
      "learning_rate": 0.0006039198446493589,
      "loss": 0.1477,
      "step": 13230
    },
    {
      "epoch": 3.88042203985932,
      "grad_norm": 0.6027279496192932,
      "learning_rate": 0.0006038360483617705,
      "loss": 0.1526,
      "step": 13240
    },
    {
      "epoch": 3.8833528722157094,
      "grad_norm": 0.5309075117111206,
      "learning_rate": 0.0006037522520741821,
      "loss": 0.1495,
      "step": 13250
    },
    {
      "epoch": 3.8862837045720986,
      "grad_norm": 1.0621979236602783,
      "learning_rate": 0.0006036684557865937,
      "loss": 0.1757,
      "step": 13260
    },
    {
      "epoch": 3.889214536928488,
      "grad_norm": 1.2577489614486694,
      "learning_rate": 0.0006035846594990054,
      "loss": 0.1558,
      "step": 13270
    },
    {
      "epoch": 3.8921453692848766,
      "grad_norm": 0.5026295185089111,
      "learning_rate": 0.000603500863211417,
      "loss": 0.1406,
      "step": 13280
    },
    {
      "epoch": 3.895076201641266,
      "grad_norm": 1.321382999420166,
      "learning_rate": 0.0006034170669238286,
      "loss": 0.1649,
      "step": 13290
    },
    {
      "epoch": 3.898007033997655,
      "grad_norm": 0.8863664865493774,
      "learning_rate": 0.0006033332706362403,
      "loss": 0.1473,
      "step": 13300
    },
    {
      "epoch": 3.9009378663540444,
      "grad_norm": 0.8901286721229553,
      "learning_rate": 0.0006032494743486519,
      "loss": 0.1332,
      "step": 13310
    },
    {
      "epoch": 3.9038686987104336,
      "grad_norm": 0.5530668497085571,
      "learning_rate": 0.0006031656780610636,
      "loss": 0.1328,
      "step": 13320
    },
    {
      "epoch": 3.906799531066823,
      "grad_norm": 1.1436837911605835,
      "learning_rate": 0.0006030818817734752,
      "loss": 0.151,
      "step": 13330
    },
    {
      "epoch": 3.909730363423212,
      "grad_norm": 0.7936254143714905,
      "learning_rate": 0.0006029980854858869,
      "loss": 0.1586,
      "step": 13340
    },
    {
      "epoch": 3.9126611957796014,
      "grad_norm": 0.7115600109100342,
      "learning_rate": 0.0006029142891982984,
      "loss": 0.1424,
      "step": 13350
    },
    {
      "epoch": 3.9155920281359906,
      "grad_norm": 0.8847544193267822,
      "learning_rate": 0.00060283049291071,
      "loss": 0.1158,
      "step": 13360
    },
    {
      "epoch": 3.91852286049238,
      "grad_norm": 0.5250861048698425,
      "learning_rate": 0.0006027466966231217,
      "loss": 0.1428,
      "step": 13370
    },
    {
      "epoch": 3.921453692848769,
      "grad_norm": 0.5760290622711182,
      "learning_rate": 0.0006026629003355334,
      "loss": 0.1219,
      "step": 13380
    },
    {
      "epoch": 3.9243845252051583,
      "grad_norm": 0.7669649124145508,
      "learning_rate": 0.0006025791040479451,
      "loss": 0.1684,
      "step": 13390
    },
    {
      "epoch": 3.9273153575615476,
      "grad_norm": 1.2305772304534912,
      "learning_rate": 0.0006024953077603567,
      "loss": 0.1366,
      "step": 13400
    },
    {
      "epoch": 3.930246189917937,
      "grad_norm": 0.8853024840354919,
      "learning_rate": 0.0006024115114727682,
      "loss": 0.1379,
      "step": 13410
    },
    {
      "epoch": 3.933177022274326,
      "grad_norm": 1.2593430280685425,
      "learning_rate": 0.0006023277151851799,
      "loss": 0.1251,
      "step": 13420
    },
    {
      "epoch": 3.9361078546307153,
      "grad_norm": 1.1236064434051514,
      "learning_rate": 0.0006022439188975915,
      "loss": 0.1478,
      "step": 13430
    },
    {
      "epoch": 3.9390386869871046,
      "grad_norm": 1.0591999292373657,
      "learning_rate": 0.0006021601226100032,
      "loss": 0.1397,
      "step": 13440
    },
    {
      "epoch": 3.941969519343494,
      "grad_norm": 0.967583179473877,
      "learning_rate": 0.0006020763263224148,
      "loss": 0.1438,
      "step": 13450
    },
    {
      "epoch": 3.9449003516998826,
      "grad_norm": 0.7314274907112122,
      "learning_rate": 0.0006019925300348264,
      "loss": 0.142,
      "step": 13460
    },
    {
      "epoch": 3.947831184056272,
      "grad_norm": 0.8431068062782288,
      "learning_rate": 0.0006019087337472381,
      "loss": 0.1268,
      "step": 13470
    },
    {
      "epoch": 3.950762016412661,
      "grad_norm": 1.27033531665802,
      "learning_rate": 0.0006018249374596497,
      "loss": 0.1513,
      "step": 13480
    },
    {
      "epoch": 3.9536928487690504,
      "grad_norm": 0.7553119659423828,
      "learning_rate": 0.0006017411411720614,
      "loss": 0.1565,
      "step": 13490
    },
    {
      "epoch": 3.9566236811254396,
      "grad_norm": 0.6196548938751221,
      "learning_rate": 0.000601657344884473,
      "loss": 0.13,
      "step": 13500
    },
    {
      "epoch": 3.959554513481829,
      "grad_norm": 0.6620920896530151,
      "learning_rate": 0.0006015735485968846,
      "loss": 0.1629,
      "step": 13510
    },
    {
      "epoch": 3.962485345838218,
      "grad_norm": 1.8907972574234009,
      "learning_rate": 0.0006014897523092962,
      "loss": 0.127,
      "step": 13520
    },
    {
      "epoch": 3.9654161781946073,
      "grad_norm": 1.0806708335876465,
      "learning_rate": 0.0006014059560217078,
      "loss": 0.1869,
      "step": 13530
    },
    {
      "epoch": 3.9683470105509966,
      "grad_norm": 0.693017303943634,
      "learning_rate": 0.0006013221597341195,
      "loss": 0.1781,
      "step": 13540
    },
    {
      "epoch": 3.971277842907386,
      "grad_norm": 1.7975410223007202,
      "learning_rate": 0.0006012383634465312,
      "loss": 0.1369,
      "step": 13550
    },
    {
      "epoch": 3.9742086752637746,
      "grad_norm": 0.875984787940979,
      "learning_rate": 0.0006011545671589428,
      "loss": 0.1693,
      "step": 13560
    },
    {
      "epoch": 3.977139507620164,
      "grad_norm": 0.3234899342060089,
      "learning_rate": 0.0006010707708713544,
      "loss": 0.1329,
      "step": 13570
    },
    {
      "epoch": 3.980070339976553,
      "grad_norm": 0.8538922071456909,
      "learning_rate": 0.000600986974583766,
      "loss": 0.1624,
      "step": 13580
    },
    {
      "epoch": 3.9830011723329424,
      "grad_norm": 0.5828285217285156,
      "learning_rate": 0.0006009031782961777,
      "loss": 0.1452,
      "step": 13590
    },
    {
      "epoch": 3.9859320046893316,
      "grad_norm": 0.659111738204956,
      "learning_rate": 0.0006008193820085893,
      "loss": 0.128,
      "step": 13600
    },
    {
      "epoch": 3.988862837045721,
      "grad_norm": 0.9551500082015991,
      "learning_rate": 0.0006007355857210009,
      "loss": 0.1369,
      "step": 13610
    },
    {
      "epoch": 3.99179366940211,
      "grad_norm": 1.1804800033569336,
      "learning_rate": 0.0006006517894334125,
      "loss": 0.1354,
      "step": 13620
    },
    {
      "epoch": 3.9947245017584994,
      "grad_norm": 0.6547003984451294,
      "learning_rate": 0.0006005679931458242,
      "loss": 0.1291,
      "step": 13630
    },
    {
      "epoch": 3.9976553341148886,
      "grad_norm": 0.5018662810325623,
      "learning_rate": 0.0006004841968582359,
      "loss": 0.1179,
      "step": 13640
    },
    {
      "epoch": 4.0,
      "eval_accuracy": 0.48928453676228156,
      "eval_f1_macro": 0.31972846128270255,
      "eval_f1_micro": 0.603240261978628,
      "eval_f1_weighted": 0.5454686816938654,
      "eval_loss": 0.1435587853193283,
      "eval_roc_auc": 0.7472578434299177,
      "eval_runtime": 162.7158,
      "eval_samples_per_second": 18.64,
      "eval_steps_per_second": 2.335,
      "step": 13648
    },
    {
      "epoch": 4.000586166471278,
      "grad_norm": 0.40549418330192566,
      "learning_rate": 0.0006004004005706475,
      "loss": 0.1395,
      "step": 13650
    },
    {
      "epoch": 4.003516998827667,
      "grad_norm": 0.7203044295310974,
      "learning_rate": 0.0006003166042830591,
      "loss": 0.1318,
      "step": 13660
    },
    {
      "epoch": 4.006447831184056,
      "grad_norm": 0.4517873525619507,
      "learning_rate": 0.0006002328079954707,
      "loss": 0.1458,
      "step": 13670
    },
    {
      "epoch": 4.009378663540446,
      "grad_norm": 0.730290412902832,
      "learning_rate": 0.0006001490117078824,
      "loss": 0.1616,
      "step": 13680
    },
    {
      "epoch": 4.012309495896835,
      "grad_norm": 0.7025696039199829,
      "learning_rate": 0.000600065215420294,
      "loss": 0.1447,
      "step": 13690
    },
    {
      "epoch": 4.015240328253224,
      "grad_norm": 0.38210758566856384,
      "learning_rate": 0.0005999814191327056,
      "loss": 0.1546,
      "step": 13700
    },
    {
      "epoch": 4.018171160609613,
      "grad_norm": 1.2378592491149902,
      "learning_rate": 0.0005998976228451172,
      "loss": 0.141,
      "step": 13710
    },
    {
      "epoch": 4.021101992966003,
      "grad_norm": 0.6006885766983032,
      "learning_rate": 0.0005998138265575288,
      "loss": 0.1277,
      "step": 13720
    },
    {
      "epoch": 4.024032825322392,
      "grad_norm": 0.6245412826538086,
      "learning_rate": 0.0005997300302699406,
      "loss": 0.1289,
      "step": 13730
    },
    {
      "epoch": 4.026963657678781,
      "grad_norm": 0.7901458740234375,
      "learning_rate": 0.0005996462339823522,
      "loss": 0.1454,
      "step": 13740
    },
    {
      "epoch": 4.02989449003517,
      "grad_norm": 0.9713027477264404,
      "learning_rate": 0.0005995624376947638,
      "loss": 0.1428,
      "step": 13750
    },
    {
      "epoch": 4.0328253223915596,
      "grad_norm": 0.9641466736793518,
      "learning_rate": 0.0005994786414071754,
      "loss": 0.1365,
      "step": 13760
    },
    {
      "epoch": 4.035756154747949,
      "grad_norm": 0.48665139079093933,
      "learning_rate": 0.000599394845119587,
      "loss": 0.1573,
      "step": 13770
    },
    {
      "epoch": 4.038686987104338,
      "grad_norm": 0.7896592020988464,
      "learning_rate": 0.0005993110488319987,
      "loss": 0.1402,
      "step": 13780
    },
    {
      "epoch": 4.041617819460727,
      "grad_norm": 0.6551585793495178,
      "learning_rate": 0.0005992272525444103,
      "loss": 0.1541,
      "step": 13790
    },
    {
      "epoch": 4.044548651817116,
      "grad_norm": 0.7726640105247498,
      "learning_rate": 0.000599143456256822,
      "loss": 0.1452,
      "step": 13800
    },
    {
      "epoch": 4.047479484173505,
      "grad_norm": 0.5344569087028503,
      "learning_rate": 0.0005990596599692337,
      "loss": 0.1383,
      "step": 13810
    },
    {
      "epoch": 4.050410316529894,
      "grad_norm": 0.6098874807357788,
      "learning_rate": 0.0005989758636816452,
      "loss": 0.1712,
      "step": 13820
    },
    {
      "epoch": 4.053341148886283,
      "grad_norm": 0.9485077261924744,
      "learning_rate": 0.0005988920673940569,
      "loss": 0.1614,
      "step": 13830
    },
    {
      "epoch": 4.056271981242673,
      "grad_norm": 1.2506366968154907,
      "learning_rate": 0.0005988082711064685,
      "loss": 0.1408,
      "step": 13840
    },
    {
      "epoch": 4.059202813599062,
      "grad_norm": 0.77836674451828,
      "learning_rate": 0.0005987244748188802,
      "loss": 0.1375,
      "step": 13850
    },
    {
      "epoch": 4.062133645955451,
      "grad_norm": 0.6822171807289124,
      "learning_rate": 0.0005986406785312918,
      "loss": 0.1499,
      "step": 13860
    },
    {
      "epoch": 4.06506447831184,
      "grad_norm": 1.552470326423645,
      "learning_rate": 0.0005985568822437033,
      "loss": 0.1503,
      "step": 13870
    },
    {
      "epoch": 4.06799531066823,
      "grad_norm": 0.640288233757019,
      "learning_rate": 0.000598473085956115,
      "loss": 0.1564,
      "step": 13880
    },
    {
      "epoch": 4.070926143024619,
      "grad_norm": 0.5989663004875183,
      "learning_rate": 0.0005983892896685266,
      "loss": 0.1522,
      "step": 13890
    },
    {
      "epoch": 4.073856975381008,
      "grad_norm": 0.6683259010314941,
      "learning_rate": 0.0005983054933809384,
      "loss": 0.133,
      "step": 13900
    },
    {
      "epoch": 4.076787807737397,
      "grad_norm": 1.1714116334915161,
      "learning_rate": 0.00059822169709335,
      "loss": 0.1389,
      "step": 13910
    },
    {
      "epoch": 4.079718640093787,
      "grad_norm": 0.8319524526596069,
      "learning_rate": 0.0005981379008057616,
      "loss": 0.1668,
      "step": 13920
    },
    {
      "epoch": 4.082649472450176,
      "grad_norm": 1.3494696617126465,
      "learning_rate": 0.0005980541045181732,
      "loss": 0.1629,
      "step": 13930
    },
    {
      "epoch": 4.085580304806565,
      "grad_norm": 0.605328381061554,
      "learning_rate": 0.0005979703082305848,
      "loss": 0.1427,
      "step": 13940
    },
    {
      "epoch": 4.088511137162954,
      "grad_norm": 0.694206178188324,
      "learning_rate": 0.0005978865119429965,
      "loss": 0.1467,
      "step": 13950
    },
    {
      "epoch": 4.091441969519344,
      "grad_norm": 1.0356215238571167,
      "learning_rate": 0.0005978027156554081,
      "loss": 0.1451,
      "step": 13960
    },
    {
      "epoch": 4.094372801875733,
      "grad_norm": 0.9064589142799377,
      "learning_rate": 0.0005977189193678197,
      "loss": 0.1232,
      "step": 13970
    },
    {
      "epoch": 4.097303634232122,
      "grad_norm": 0.7937430739402771,
      "learning_rate": 0.0005976351230802313,
      "loss": 0.1292,
      "step": 13980
    },
    {
      "epoch": 4.100234466588511,
      "grad_norm": 1.3631913661956787,
      "learning_rate": 0.000597551326792643,
      "loss": 0.1551,
      "step": 13990
    },
    {
      "epoch": 4.103165298944901,
      "grad_norm": 0.5499006509780884,
      "learning_rate": 0.0005974675305050547,
      "loss": 0.1225,
      "step": 14000
    },
    {
      "epoch": 4.10609613130129,
      "grad_norm": 1.6446775197982788,
      "learning_rate": 0.0005973837342174663,
      "loss": 0.1236,
      "step": 14010
    },
    {
      "epoch": 4.109026963657679,
      "grad_norm": 0.8113130331039429,
      "learning_rate": 0.0005972999379298779,
      "loss": 0.1241,
      "step": 14020
    },
    {
      "epoch": 4.111957796014068,
      "grad_norm": 0.9195014238357544,
      "learning_rate": 0.0005972161416422895,
      "loss": 0.1267,
      "step": 14030
    },
    {
      "epoch": 4.1148886283704575,
      "grad_norm": 0.6796783208847046,
      "learning_rate": 0.0005971323453547011,
      "loss": 0.1303,
      "step": 14040
    },
    {
      "epoch": 4.117819460726847,
      "grad_norm": 0.4791697561740875,
      "learning_rate": 0.0005970485490671128,
      "loss": 0.1334,
      "step": 14050
    },
    {
      "epoch": 4.120750293083236,
      "grad_norm": 0.8057950139045715,
      "learning_rate": 0.0005969647527795244,
      "loss": 0.129,
      "step": 14060
    },
    {
      "epoch": 4.123681125439625,
      "grad_norm": 0.5475622415542603,
      "learning_rate": 0.000596880956491936,
      "loss": 0.1512,
      "step": 14070
    },
    {
      "epoch": 4.126611957796014,
      "grad_norm": 0.49991220235824585,
      "learning_rate": 0.0005967971602043477,
      "loss": 0.1688,
      "step": 14080
    },
    {
      "epoch": 4.129542790152403,
      "grad_norm": 0.5976384878158569,
      "learning_rate": 0.0005967133639167593,
      "loss": 0.1434,
      "step": 14090
    },
    {
      "epoch": 4.132473622508792,
      "grad_norm": 1.0441540479660034,
      "learning_rate": 0.000596629567629171,
      "loss": 0.1404,
      "step": 14100
    },
    {
      "epoch": 4.135404454865181,
      "grad_norm": 0.8297351598739624,
      "learning_rate": 0.0005965457713415826,
      "loss": 0.1395,
      "step": 14110
    },
    {
      "epoch": 4.138335287221571,
      "grad_norm": 0.8724652528762817,
      "learning_rate": 0.0005964619750539942,
      "loss": 0.1434,
      "step": 14120
    },
    {
      "epoch": 4.14126611957796,
      "grad_norm": 0.6282877326011658,
      "learning_rate": 0.0005963781787664058,
      "loss": 0.1205,
      "step": 14130
    },
    {
      "epoch": 4.144196951934349,
      "grad_norm": 0.4532471001148224,
      "learning_rate": 0.0005962943824788175,
      "loss": 0.1442,
      "step": 14140
    },
    {
      "epoch": 4.147127784290738,
      "grad_norm": 0.6664571166038513,
      "learning_rate": 0.0005962105861912291,
      "loss": 0.1474,
      "step": 14150
    },
    {
      "epoch": 4.150058616647128,
      "grad_norm": 0.6370375752449036,
      "learning_rate": 0.0005961267899036408,
      "loss": 0.1559,
      "step": 14160
    },
    {
      "epoch": 4.152989449003517,
      "grad_norm": 0.5044883489608765,
      "learning_rate": 0.0005960429936160525,
      "loss": 0.183,
      "step": 14170
    },
    {
      "epoch": 4.155920281359906,
      "grad_norm": 0.7552399635314941,
      "learning_rate": 0.000595959197328464,
      "loss": 0.1263,
      "step": 14180
    },
    {
      "epoch": 4.158851113716295,
      "grad_norm": 1.1361631155014038,
      "learning_rate": 0.0005958754010408757,
      "loss": 0.1345,
      "step": 14190
    },
    {
      "epoch": 4.161781946072685,
      "grad_norm": 0.692027747631073,
      "learning_rate": 0.0005957916047532873,
      "loss": 0.1387,
      "step": 14200
    },
    {
      "epoch": 4.164712778429074,
      "grad_norm": 0.7785083055496216,
      "learning_rate": 0.0005957078084656989,
      "loss": 0.1244,
      "step": 14210
    },
    {
      "epoch": 4.167643610785463,
      "grad_norm": 0.8250572085380554,
      "learning_rate": 0.0005956240121781106,
      "loss": 0.1753,
      "step": 14220
    },
    {
      "epoch": 4.170574443141852,
      "grad_norm": 0.7214244604110718,
      "learning_rate": 0.0005955402158905221,
      "loss": 0.1517,
      "step": 14230
    },
    {
      "epoch": 4.173505275498242,
      "grad_norm": 0.5953429341316223,
      "learning_rate": 0.0005954564196029338,
      "loss": 0.1322,
      "step": 14240
    },
    {
      "epoch": 4.176436107854631,
      "grad_norm": 0.6218621134757996,
      "learning_rate": 0.0005953726233153455,
      "loss": 0.1336,
      "step": 14250
    },
    {
      "epoch": 4.17936694021102,
      "grad_norm": 1.0888605117797852,
      "learning_rate": 0.0005952888270277571,
      "loss": 0.1315,
      "step": 14260
    },
    {
      "epoch": 4.182297772567409,
      "grad_norm": 0.8851839303970337,
      "learning_rate": 0.0005952050307401688,
      "loss": 0.1406,
      "step": 14270
    },
    {
      "epoch": 4.185228604923799,
      "grad_norm": 0.8702051639556885,
      "learning_rate": 0.0005951212344525804,
      "loss": 0.1286,
      "step": 14280
    },
    {
      "epoch": 4.188159437280188,
      "grad_norm": 0.4055732488632202,
      "learning_rate": 0.000595037438164992,
      "loss": 0.1236,
      "step": 14290
    },
    {
      "epoch": 4.191090269636577,
      "grad_norm": 0.7942829728126526,
      "learning_rate": 0.0005949536418774036,
      "loss": 0.1359,
      "step": 14300
    },
    {
      "epoch": 4.194021101992966,
      "grad_norm": 1.0896592140197754,
      "learning_rate": 0.0005948698455898153,
      "loss": 0.1408,
      "step": 14310
    },
    {
      "epoch": 4.1969519343493555,
      "grad_norm": 0.93650883436203,
      "learning_rate": 0.0005947860493022269,
      "loss": 0.1566,
      "step": 14320
    },
    {
      "epoch": 4.199882766705745,
      "grad_norm": 0.9907748699188232,
      "learning_rate": 0.0005947022530146386,
      "loss": 0.1545,
      "step": 14330
    },
    {
      "epoch": 4.202813599062134,
      "grad_norm": 0.8785440921783447,
      "learning_rate": 0.0005946184567270502,
      "loss": 0.1474,
      "step": 14340
    },
    {
      "epoch": 4.205744431418523,
      "grad_norm": 0.48802050948143005,
      "learning_rate": 0.0005945346604394618,
      "loss": 0.1712,
      "step": 14350
    },
    {
      "epoch": 4.2086752637749125,
      "grad_norm": 1.5780513286590576,
      "learning_rate": 0.0005944508641518735,
      "loss": 0.136,
      "step": 14360
    },
    {
      "epoch": 4.211606096131301,
      "grad_norm": 0.7339774370193481,
      "learning_rate": 0.0005943670678642851,
      "loss": 0.123,
      "step": 14370
    },
    {
      "epoch": 4.21453692848769,
      "grad_norm": 0.6482327580451965,
      "learning_rate": 0.0005942832715766967,
      "loss": 0.1204,
      "step": 14380
    },
    {
      "epoch": 4.217467760844079,
      "grad_norm": 1.16739821434021,
      "learning_rate": 0.0005941994752891083,
      "loss": 0.1561,
      "step": 14390
    },
    {
      "epoch": 4.220398593200469,
      "grad_norm": 0.5718225836753845,
      "learning_rate": 0.0005941156790015199,
      "loss": 0.1332,
      "step": 14400
    },
    {
      "epoch": 4.223329425556858,
      "grad_norm": 0.5689556002616882,
      "learning_rate": 0.0005940318827139316,
      "loss": 0.1541,
      "step": 14410
    },
    {
      "epoch": 4.226260257913247,
      "grad_norm": 1.2688270807266235,
      "learning_rate": 0.0005939480864263433,
      "loss": 0.1658,
      "step": 14420
    },
    {
      "epoch": 4.229191090269636,
      "grad_norm": 0.7497832775115967,
      "learning_rate": 0.0005938642901387549,
      "loss": 0.121,
      "step": 14430
    },
    {
      "epoch": 4.232121922626026,
      "grad_norm": 0.8713165521621704,
      "learning_rate": 0.0005937804938511665,
      "loss": 0.1477,
      "step": 14440
    },
    {
      "epoch": 4.235052754982415,
      "grad_norm": 0.8860273957252502,
      "learning_rate": 0.0005936966975635781,
      "loss": 0.1497,
      "step": 14450
    },
    {
      "epoch": 4.237983587338804,
      "grad_norm": 1.011001706123352,
      "learning_rate": 0.0005936129012759898,
      "loss": 0.125,
      "step": 14460
    },
    {
      "epoch": 4.240914419695193,
      "grad_norm": 1.1227521896362305,
      "learning_rate": 0.0005935291049884014,
      "loss": 0.1519,
      "step": 14470
    },
    {
      "epoch": 4.243845252051583,
      "grad_norm": 0.6537960171699524,
      "learning_rate": 0.000593445308700813,
      "loss": 0.1611,
      "step": 14480
    },
    {
      "epoch": 4.246776084407972,
      "grad_norm": 0.9533454179763794,
      "learning_rate": 0.0005933615124132246,
      "loss": 0.1634,
      "step": 14490
    },
    {
      "epoch": 4.249706916764361,
      "grad_norm": 1.0287549495697021,
      "learning_rate": 0.0005932777161256362,
      "loss": 0.1429,
      "step": 14500
    },
    {
      "epoch": 4.25263774912075,
      "grad_norm": 0.5382091403007507,
      "learning_rate": 0.000593193919838048,
      "loss": 0.1692,
      "step": 14510
    },
    {
      "epoch": 4.25556858147714,
      "grad_norm": 0.7163447141647339,
      "learning_rate": 0.0005931101235504596,
      "loss": 0.1198,
      "step": 14520
    },
    {
      "epoch": 4.258499413833529,
      "grad_norm": 0.8603950142860413,
      "learning_rate": 0.0005930263272628712,
      "loss": 0.1669,
      "step": 14530
    },
    {
      "epoch": 4.261430246189918,
      "grad_norm": 0.7437140345573425,
      "learning_rate": 0.0005929425309752828,
      "loss": 0.1414,
      "step": 14540
    },
    {
      "epoch": 4.264361078546307,
      "grad_norm": 0.8967456221580505,
      "learning_rate": 0.0005928587346876944,
      "loss": 0.1366,
      "step": 14550
    },
    {
      "epoch": 4.2672919109026966,
      "grad_norm": 0.9733757972717285,
      "learning_rate": 0.0005927749384001061,
      "loss": 0.1398,
      "step": 14560
    },
    {
      "epoch": 4.270222743259086,
      "grad_norm": 0.8330843448638916,
      "learning_rate": 0.0005926911421125177,
      "loss": 0.1384,
      "step": 14570
    },
    {
      "epoch": 4.273153575615475,
      "grad_norm": 1.7235368490219116,
      "learning_rate": 0.0005926073458249294,
      "loss": 0.126,
      "step": 14580
    },
    {
      "epoch": 4.276084407971864,
      "grad_norm": 1.6661081314086914,
      "learning_rate": 0.0005925235495373409,
      "loss": 0.1183,
      "step": 14590
    },
    {
      "epoch": 4.2790152403282535,
      "grad_norm": 0.9413307905197144,
      "learning_rate": 0.0005924397532497526,
      "loss": 0.1623,
      "step": 14600
    },
    {
      "epoch": 4.281946072684643,
      "grad_norm": 0.6193726658821106,
      "learning_rate": 0.0005923559569621643,
      "loss": 0.1296,
      "step": 14610
    },
    {
      "epoch": 4.284876905041032,
      "grad_norm": 0.7007548809051514,
      "learning_rate": 0.0005922721606745759,
      "loss": 0.1164,
      "step": 14620
    },
    {
      "epoch": 4.287807737397421,
      "grad_norm": 0.981306254863739,
      "learning_rate": 0.0005921883643869876,
      "loss": 0.1526,
      "step": 14630
    },
    {
      "epoch": 4.29073856975381,
      "grad_norm": 0.908105194568634,
      "learning_rate": 0.0005921045680993991,
      "loss": 0.1642,
      "step": 14640
    },
    {
      "epoch": 4.293669402110199,
      "grad_norm": 0.5797716379165649,
      "learning_rate": 0.0005920207718118107,
      "loss": 0.1337,
      "step": 14650
    },
    {
      "epoch": 4.296600234466588,
      "grad_norm": 0.5877443552017212,
      "learning_rate": 0.0005919369755242224,
      "loss": 0.1499,
      "step": 14660
    },
    {
      "epoch": 4.299531066822977,
      "grad_norm": 0.7629129886627197,
      "learning_rate": 0.000591853179236634,
      "loss": 0.1353,
      "step": 14670
    },
    {
      "epoch": 4.302461899179367,
      "grad_norm": 0.7024255990982056,
      "learning_rate": 0.0005917693829490458,
      "loss": 0.1251,
      "step": 14680
    },
    {
      "epoch": 4.305392731535756,
      "grad_norm": 1.001130223274231,
      "learning_rate": 0.0005916855866614574,
      "loss": 0.165,
      "step": 14690
    },
    {
      "epoch": 4.308323563892145,
      "grad_norm": 0.5303611755371094,
      "learning_rate": 0.000591601790373869,
      "loss": 0.1178,
      "step": 14700
    },
    {
      "epoch": 4.311254396248534,
      "grad_norm": 1.4006541967391968,
      "learning_rate": 0.0005915179940862806,
      "loss": 0.1229,
      "step": 14710
    },
    {
      "epoch": 4.314185228604924,
      "grad_norm": 0.8262465596199036,
      "learning_rate": 0.0005914341977986922,
      "loss": 0.1535,
      "step": 14720
    },
    {
      "epoch": 4.317116060961313,
      "grad_norm": 0.6358445286750793,
      "learning_rate": 0.0005913504015111039,
      "loss": 0.1482,
      "step": 14730
    },
    {
      "epoch": 4.320046893317702,
      "grad_norm": 1.7866785526275635,
      "learning_rate": 0.0005912666052235155,
      "loss": 0.1386,
      "step": 14740
    },
    {
      "epoch": 4.322977725674091,
      "grad_norm": 1.225763201713562,
      "learning_rate": 0.0005911828089359271,
      "loss": 0.1351,
      "step": 14750
    },
    {
      "epoch": 4.325908558030481,
      "grad_norm": 0.4733535945415497,
      "learning_rate": 0.0005910990126483387,
      "loss": 0.135,
      "step": 14760
    },
    {
      "epoch": 4.32883939038687,
      "grad_norm": 0.6211035847663879,
      "learning_rate": 0.0005910152163607504,
      "loss": 0.1659,
      "step": 14770
    },
    {
      "epoch": 4.331770222743259,
      "grad_norm": 0.6278489828109741,
      "learning_rate": 0.0005909314200731621,
      "loss": 0.1527,
      "step": 14780
    },
    {
      "epoch": 4.334701055099648,
      "grad_norm": 0.623757004737854,
      "learning_rate": 0.0005908476237855737,
      "loss": 0.1158,
      "step": 14790
    },
    {
      "epoch": 4.337631887456038,
      "grad_norm": 0.7586120367050171,
      "learning_rate": 0.0005907638274979853,
      "loss": 0.1427,
      "step": 14800
    },
    {
      "epoch": 4.340562719812427,
      "grad_norm": 1.2446140050888062,
      "learning_rate": 0.0005906800312103969,
      "loss": 0.1192,
      "step": 14810
    },
    {
      "epoch": 4.343493552168816,
      "grad_norm": 0.6570416688919067,
      "learning_rate": 0.0005905962349228085,
      "loss": 0.1439,
      "step": 14820
    },
    {
      "epoch": 4.346424384525205,
      "grad_norm": 1.3183009624481201,
      "learning_rate": 0.0005905124386352202,
      "loss": 0.1357,
      "step": 14830
    },
    {
      "epoch": 4.3493552168815945,
      "grad_norm": 0.9115792512893677,
      "learning_rate": 0.0005904286423476318,
      "loss": 0.1359,
      "step": 14840
    },
    {
      "epoch": 4.352286049237984,
      "grad_norm": 0.8795499205589294,
      "learning_rate": 0.0005903448460600434,
      "loss": 0.1554,
      "step": 14850
    },
    {
      "epoch": 4.355216881594373,
      "grad_norm": 1.19828462600708,
      "learning_rate": 0.0005902610497724551,
      "loss": 0.1373,
      "step": 14860
    },
    {
      "epoch": 4.358147713950762,
      "grad_norm": 0.9634402394294739,
      "learning_rate": 0.0005901772534848668,
      "loss": 0.1464,
      "step": 14870
    },
    {
      "epoch": 4.3610785463071515,
      "grad_norm": 0.9811565279960632,
      "learning_rate": 0.0005900934571972784,
      "loss": 0.1344,
      "step": 14880
    },
    {
      "epoch": 4.364009378663541,
      "grad_norm": 0.8372403383255005,
      "learning_rate": 0.00059000966090969,
      "loss": 0.1685,
      "step": 14890
    },
    {
      "epoch": 4.36694021101993,
      "grad_norm": 1.0318679809570312,
      "learning_rate": 0.0005899258646221016,
      "loss": 0.1503,
      "step": 14900
    },
    {
      "epoch": 4.369871043376319,
      "grad_norm": 0.5616901516914368,
      "learning_rate": 0.0005898420683345132,
      "loss": 0.1493,
      "step": 14910
    },
    {
      "epoch": 4.3728018757327085,
      "grad_norm": 0.49967941641807556,
      "learning_rate": 0.0005897582720469249,
      "loss": 0.1544,
      "step": 14920
    },
    {
      "epoch": 4.375732708089098,
      "grad_norm": 0.3993307054042816,
      "learning_rate": 0.0005896744757593365,
      "loss": 0.101,
      "step": 14930
    },
    {
      "epoch": 4.378663540445486,
      "grad_norm": 1.6157575845718384,
      "learning_rate": 0.0005895906794717482,
      "loss": 0.1482,
      "step": 14940
    },
    {
      "epoch": 4.381594372801875,
      "grad_norm": 0.5433279871940613,
      "learning_rate": 0.0005895068831841598,
      "loss": 0.1299,
      "step": 14950
    },
    {
      "epoch": 4.384525205158265,
      "grad_norm": 0.7059415578842163,
      "learning_rate": 0.0005894230868965714,
      "loss": 0.159,
      "step": 14960
    },
    {
      "epoch": 4.387456037514654,
      "grad_norm": 0.5485718846321106,
      "learning_rate": 0.0005893392906089831,
      "loss": 0.1697,
      "step": 14970
    },
    {
      "epoch": 4.390386869871043,
      "grad_norm": 0.7200719118118286,
      "learning_rate": 0.0005892554943213947,
      "loss": 0.1477,
      "step": 14980
    },
    {
      "epoch": 4.393317702227432,
      "grad_norm": 0.7071026563644409,
      "learning_rate": 0.0005891716980338063,
      "loss": 0.1713,
      "step": 14990
    },
    {
      "epoch": 4.396248534583822,
      "grad_norm": 0.897402286529541,
      "learning_rate": 0.0005890879017462179,
      "loss": 0.0989,
      "step": 15000
    },
    {
      "epoch": 4.399179366940211,
      "grad_norm": 0.567346453666687,
      "learning_rate": 0.0005890041054586295,
      "loss": 0.1553,
      "step": 15010
    },
    {
      "epoch": 4.4021101992966,
      "grad_norm": 0.5593740344047546,
      "learning_rate": 0.0005889203091710412,
      "loss": 0.1394,
      "step": 15020
    },
    {
      "epoch": 4.405041031652989,
      "grad_norm": 0.5986030101776123,
      "learning_rate": 0.0005888365128834529,
      "loss": 0.1511,
      "step": 15030
    },
    {
      "epoch": 4.407971864009379,
      "grad_norm": 0.7311133742332458,
      "learning_rate": 0.0005887527165958646,
      "loss": 0.155,
      "step": 15040
    },
    {
      "epoch": 4.410902696365768,
      "grad_norm": 0.6367095708847046,
      "learning_rate": 0.0005886689203082762,
      "loss": 0.1237,
      "step": 15050
    },
    {
      "epoch": 4.413833528722157,
      "grad_norm": 0.9640929102897644,
      "learning_rate": 0.0005885851240206877,
      "loss": 0.127,
      "step": 15060
    },
    {
      "epoch": 4.416764361078546,
      "grad_norm": 0.7728173136711121,
      "learning_rate": 0.0005885013277330994,
      "loss": 0.134,
      "step": 15070
    },
    {
      "epoch": 4.4196951934349356,
      "grad_norm": 0.4717300534248352,
      "learning_rate": 0.000588417531445511,
      "loss": 0.1451,
      "step": 15080
    },
    {
      "epoch": 4.422626025791325,
      "grad_norm": 0.8215091228485107,
      "learning_rate": 0.0005883337351579227,
      "loss": 0.1075,
      "step": 15090
    },
    {
      "epoch": 4.425556858147714,
      "grad_norm": 0.9642970561981201,
      "learning_rate": 0.0005882499388703343,
      "loss": 0.1286,
      "step": 15100
    },
    {
      "epoch": 4.428487690504103,
      "grad_norm": 0.9643721580505371,
      "learning_rate": 0.0005881661425827458,
      "loss": 0.1658,
      "step": 15110
    },
    {
      "epoch": 4.4314185228604925,
      "grad_norm": 0.36998894810676575,
      "learning_rate": 0.0005880823462951576,
      "loss": 0.1432,
      "step": 15120
    },
    {
      "epoch": 4.434349355216882,
      "grad_norm": 0.6940594911575317,
      "learning_rate": 0.0005879985500075692,
      "loss": 0.1422,
      "step": 15130
    },
    {
      "epoch": 4.437280187573271,
      "grad_norm": 0.6245132684707642,
      "learning_rate": 0.0005879147537199809,
      "loss": 0.1455,
      "step": 15140
    },
    {
      "epoch": 4.44021101992966,
      "grad_norm": 0.8225823640823364,
      "learning_rate": 0.0005878309574323925,
      "loss": 0.1642,
      "step": 15150
    },
    {
      "epoch": 4.4431418522860495,
      "grad_norm": 0.9654716849327087,
      "learning_rate": 0.0005877471611448041,
      "loss": 0.1436,
      "step": 15160
    },
    {
      "epoch": 4.446072684642439,
      "grad_norm": 0.844520092010498,
      "learning_rate": 0.0005876633648572157,
      "loss": 0.1723,
      "step": 15170
    },
    {
      "epoch": 4.449003516998828,
      "grad_norm": 0.7072188854217529,
      "learning_rate": 0.0005875795685696273,
      "loss": 0.1178,
      "step": 15180
    },
    {
      "epoch": 4.451934349355217,
      "grad_norm": 1.4427969455718994,
      "learning_rate": 0.000587495772282039,
      "loss": 0.1373,
      "step": 15190
    },
    {
      "epoch": 4.4548651817116065,
      "grad_norm": 0.695218563079834,
      "learning_rate": 0.0005874119759944507,
      "loss": 0.1382,
      "step": 15200
    },
    {
      "epoch": 4.457796014067995,
      "grad_norm": 0.8323598504066467,
      "learning_rate": 0.0005873281797068623,
      "loss": 0.1644,
      "step": 15210
    },
    {
      "epoch": 4.460726846424384,
      "grad_norm": 0.9897052645683289,
      "learning_rate": 0.0005872443834192739,
      "loss": 0.1496,
      "step": 15220
    },
    {
      "epoch": 4.463657678780773,
      "grad_norm": 0.619324803352356,
      "learning_rate": 0.0005871605871316855,
      "loss": 0.127,
      "step": 15230
    },
    {
      "epoch": 4.466588511137163,
      "grad_norm": 0.8319365978240967,
      "learning_rate": 0.0005870767908440972,
      "loss": 0.1431,
      "step": 15240
    },
    {
      "epoch": 4.469519343493552,
      "grad_norm": 0.6143879890441895,
      "learning_rate": 0.0005869929945565088,
      "loss": 0.1378,
      "step": 15250
    },
    {
      "epoch": 4.472450175849941,
      "grad_norm": 0.8643827438354492,
      "learning_rate": 0.0005869091982689204,
      "loss": 0.1366,
      "step": 15260
    },
    {
      "epoch": 4.47538100820633,
      "grad_norm": 0.4531683623790741,
      "learning_rate": 0.000586825401981332,
      "loss": 0.1154,
      "step": 15270
    },
    {
      "epoch": 4.47831184056272,
      "grad_norm": 0.5724119544029236,
      "learning_rate": 0.0005867416056937436,
      "loss": 0.1629,
      "step": 15280
    },
    {
      "epoch": 4.481242672919109,
      "grad_norm": 1.0636099576950073,
      "learning_rate": 0.0005866578094061554,
      "loss": 0.1513,
      "step": 15290
    },
    {
      "epoch": 4.484173505275498,
      "grad_norm": 0.5693020224571228,
      "learning_rate": 0.000586574013118567,
      "loss": 0.1703,
      "step": 15300
    },
    {
      "epoch": 4.487104337631887,
      "grad_norm": 0.6805675625801086,
      "learning_rate": 0.0005864902168309786,
      "loss": 0.1403,
      "step": 15310
    },
    {
      "epoch": 4.490035169988277,
      "grad_norm": 0.5615730881690979,
      "learning_rate": 0.0005864064205433902,
      "loss": 0.1052,
      "step": 15320
    },
    {
      "epoch": 4.492966002344666,
      "grad_norm": 1.6697444915771484,
      "learning_rate": 0.0005863226242558018,
      "loss": 0.1614,
      "step": 15330
    },
    {
      "epoch": 4.495896834701055,
      "grad_norm": 0.7049486041069031,
      "learning_rate": 0.0005862388279682135,
      "loss": 0.16,
      "step": 15340
    },
    {
      "epoch": 4.498827667057444,
      "grad_norm": 0.5753429532051086,
      "learning_rate": 0.0005861550316806251,
      "loss": 0.1163,
      "step": 15350
    },
    {
      "epoch": 4.5017584994138335,
      "grad_norm": 1.9016191959381104,
      "learning_rate": 0.0005860712353930367,
      "loss": 0.1317,
      "step": 15360
    },
    {
      "epoch": 4.504689331770223,
      "grad_norm": 1.1664786338806152,
      "learning_rate": 0.0005859874391054483,
      "loss": 0.1142,
      "step": 15370
    },
    {
      "epoch": 4.507620164126612,
      "grad_norm": 0.5246368646621704,
      "learning_rate": 0.0005859036428178601,
      "loss": 0.1475,
      "step": 15380
    },
    {
      "epoch": 4.510550996483001,
      "grad_norm": 0.7865419983863831,
      "learning_rate": 0.0005858198465302717,
      "loss": 0.1512,
      "step": 15390
    },
    {
      "epoch": 4.5134818288393905,
      "grad_norm": 0.49821871519088745,
      "learning_rate": 0.0005857360502426833,
      "loss": 0.1599,
      "step": 15400
    },
    {
      "epoch": 4.51641266119578,
      "grad_norm": 0.38891491293907166,
      "learning_rate": 0.0005856522539550949,
      "loss": 0.1308,
      "step": 15410
    },
    {
      "epoch": 4.519343493552169,
      "grad_norm": 0.5465299487113953,
      "learning_rate": 0.0005855684576675065,
      "loss": 0.1399,
      "step": 15420
    },
    {
      "epoch": 4.522274325908558,
      "grad_norm": 1.0368115901947021,
      "learning_rate": 0.0005854846613799182,
      "loss": 0.1567,
      "step": 15430
    },
    {
      "epoch": 4.5252051582649475,
      "grad_norm": 0.8837866187095642,
      "learning_rate": 0.0005854008650923298,
      "loss": 0.1341,
      "step": 15440
    },
    {
      "epoch": 4.528135990621337,
      "grad_norm": 1.3301920890808105,
      "learning_rate": 0.0005853170688047414,
      "loss": 0.1438,
      "step": 15450
    },
    {
      "epoch": 4.531066822977726,
      "grad_norm": 0.49263185262680054,
      "learning_rate": 0.0005852332725171532,
      "loss": 0.1355,
      "step": 15460
    },
    {
      "epoch": 4.533997655334115,
      "grad_norm": 0.6697394251823425,
      "learning_rate": 0.0005851494762295647,
      "loss": 0.1258,
      "step": 15470
    },
    {
      "epoch": 4.5369284876905045,
      "grad_norm": 0.8358619809150696,
      "learning_rate": 0.0005850656799419764,
      "loss": 0.1458,
      "step": 15480
    },
    {
      "epoch": 4.539859320046894,
      "grad_norm": 0.5331081748008728,
      "learning_rate": 0.000584981883654388,
      "loss": 0.1113,
      "step": 15490
    },
    {
      "epoch": 4.542790152403283,
      "grad_norm": 0.7818489670753479,
      "learning_rate": 0.0005848980873667996,
      "loss": 0.1698,
      "step": 15500
    },
    {
      "epoch": 4.545720984759672,
      "grad_norm": 1.4684504270553589,
      "learning_rate": 0.0005848142910792113,
      "loss": 0.1487,
      "step": 15510
    },
    {
      "epoch": 4.548651817116061,
      "grad_norm": 1.8291635513305664,
      "learning_rate": 0.0005847304947916228,
      "loss": 0.1534,
      "step": 15520
    },
    {
      "epoch": 4.55158264947245,
      "grad_norm": 0.652036190032959,
      "learning_rate": 0.0005846466985040345,
      "loss": 0.1168,
      "step": 15530
    },
    {
      "epoch": 4.554513481828839,
      "grad_norm": 0.41517114639282227,
      "learning_rate": 0.0005845629022164461,
      "loss": 0.1508,
      "step": 15540
    },
    {
      "epoch": 4.557444314185228,
      "grad_norm": 0.6091077923774719,
      "learning_rate": 0.0005844791059288579,
      "loss": 0.1288,
      "step": 15550
    },
    {
      "epoch": 4.560375146541618,
      "grad_norm": 1.272902250289917,
      "learning_rate": 0.0005843953096412695,
      "loss": 0.171,
      "step": 15560
    },
    {
      "epoch": 4.563305978898007,
      "grad_norm": 0.9370753169059753,
      "learning_rate": 0.0005843115133536811,
      "loss": 0.146,
      "step": 15570
    },
    {
      "epoch": 4.566236811254396,
      "grad_norm": 0.4392181932926178,
      "learning_rate": 0.0005842277170660927,
      "loss": 0.1472,
      "step": 15580
    },
    {
      "epoch": 4.569167643610785,
      "grad_norm": 0.8282386660575867,
      "learning_rate": 0.0005841439207785043,
      "loss": 0.1337,
      "step": 15590
    },
    {
      "epoch": 4.572098475967175,
      "grad_norm": 0.7860442996025085,
      "learning_rate": 0.000584060124490916,
      "loss": 0.1296,
      "step": 15600
    },
    {
      "epoch": 4.575029308323564,
      "grad_norm": 0.6131934523582458,
      "learning_rate": 0.0005839763282033276,
      "loss": 0.132,
      "step": 15610
    },
    {
      "epoch": 4.577960140679953,
      "grad_norm": 0.5041589140892029,
      "learning_rate": 0.0005838925319157392,
      "loss": 0.1409,
      "step": 15620
    },
    {
      "epoch": 4.580890973036342,
      "grad_norm": 2.044394016265869,
      "learning_rate": 0.0005838087356281508,
      "loss": 0.121,
      "step": 15630
    },
    {
      "epoch": 4.5838218053927315,
      "grad_norm": 1.0993238687515259,
      "learning_rate": 0.0005837249393405625,
      "loss": 0.1479,
      "step": 15640
    },
    {
      "epoch": 4.586752637749121,
      "grad_norm": 1.1304731369018555,
      "learning_rate": 0.0005836411430529742,
      "loss": 0.1454,
      "step": 15650
    },
    {
      "epoch": 4.58968347010551,
      "grad_norm": 1.2785743474960327,
      "learning_rate": 0.0005835573467653858,
      "loss": 0.1094,
      "step": 15660
    },
    {
      "epoch": 4.592614302461899,
      "grad_norm": 0.8986889719963074,
      "learning_rate": 0.0005834735504777974,
      "loss": 0.1581,
      "step": 15670
    },
    {
      "epoch": 4.5955451348182885,
      "grad_norm": 1.1427831649780273,
      "learning_rate": 0.000583389754190209,
      "loss": 0.1552,
      "step": 15680
    },
    {
      "epoch": 4.598475967174678,
      "grad_norm": 0.6138004064559937,
      "learning_rate": 0.0005833059579026206,
      "loss": 0.13,
      "step": 15690
    },
    {
      "epoch": 4.601406799531067,
      "grad_norm": 0.6333678364753723,
      "learning_rate": 0.0005832221616150323,
      "loss": 0.1521,
      "step": 15700
    },
    {
      "epoch": 4.604337631887456,
      "grad_norm": 1.234392523765564,
      "learning_rate": 0.0005831383653274439,
      "loss": 0.1177,
      "step": 15710
    },
    {
      "epoch": 4.6072684642438455,
      "grad_norm": 1.0325229167938232,
      "learning_rate": 0.0005830545690398556,
      "loss": 0.1349,
      "step": 15720
    },
    {
      "epoch": 4.610199296600235,
      "grad_norm": 1.5436921119689941,
      "learning_rate": 0.0005829707727522672,
      "loss": 0.1666,
      "step": 15730
    },
    {
      "epoch": 4.613130128956624,
      "grad_norm": 2.0245444774627686,
      "learning_rate": 0.0005828869764646788,
      "loss": 0.127,
      "step": 15740
    },
    {
      "epoch": 4.616060961313013,
      "grad_norm": 0.7698756456375122,
      "learning_rate": 0.0005828031801770905,
      "loss": 0.1393,
      "step": 15750
    },
    {
      "epoch": 4.6189917936694025,
      "grad_norm": 0.786220133304596,
      "learning_rate": 0.0005827193838895021,
      "loss": 0.1585,
      "step": 15760
    },
    {
      "epoch": 4.621922626025791,
      "grad_norm": 0.637841522693634,
      "learning_rate": 0.0005826355876019137,
      "loss": 0.1313,
      "step": 15770
    },
    {
      "epoch": 4.62485345838218,
      "grad_norm": 1.1017407178878784,
      "learning_rate": 0.0005825517913143253,
      "loss": 0.1366,
      "step": 15780
    },
    {
      "epoch": 4.627784290738569,
      "grad_norm": 0.8216884732246399,
      "learning_rate": 0.0005824679950267369,
      "loss": 0.1468,
      "step": 15790
    },
    {
      "epoch": 4.630715123094959,
      "grad_norm": 1.144429087638855,
      "learning_rate": 0.0005823841987391486,
      "loss": 0.1109,
      "step": 15800
    },
    {
      "epoch": 4.633645955451348,
      "grad_norm": 1.3799678087234497,
      "learning_rate": 0.0005823004024515603,
      "loss": 0.1154,
      "step": 15810
    },
    {
      "epoch": 4.636576787807737,
      "grad_norm": 1.7241405248641968,
      "learning_rate": 0.000582216606163972,
      "loss": 0.1396,
      "step": 15820
    },
    {
      "epoch": 4.639507620164126,
      "grad_norm": 0.6540200114250183,
      "learning_rate": 0.0005821328098763835,
      "loss": 0.1586,
      "step": 15830
    },
    {
      "epoch": 4.642438452520516,
      "grad_norm": 0.6234694123268127,
      "learning_rate": 0.0005820490135887951,
      "loss": 0.1265,
      "step": 15840
    },
    {
      "epoch": 4.645369284876905,
      "grad_norm": 0.6511813998222351,
      "learning_rate": 0.0005819652173012068,
      "loss": 0.1382,
      "step": 15850
    },
    {
      "epoch": 4.648300117233294,
      "grad_norm": 0.9960664510726929,
      "learning_rate": 0.0005818814210136184,
      "loss": 0.1391,
      "step": 15860
    },
    {
      "epoch": 4.651230949589683,
      "grad_norm": 0.8374837636947632,
      "learning_rate": 0.0005817976247260301,
      "loss": 0.139,
      "step": 15870
    },
    {
      "epoch": 4.6541617819460726,
      "grad_norm": 1.5400043725967407,
      "learning_rate": 0.0005817138284384416,
      "loss": 0.1399,
      "step": 15880
    },
    {
      "epoch": 4.657092614302462,
      "grad_norm": 0.7478352785110474,
      "learning_rate": 0.0005816300321508532,
      "loss": 0.1298,
      "step": 15890
    },
    {
      "epoch": 4.660023446658851,
      "grad_norm": 0.7254859805107117,
      "learning_rate": 0.000581546235863265,
      "loss": 0.1352,
      "step": 15900
    },
    {
      "epoch": 4.66295427901524,
      "grad_norm": 0.9487764239311218,
      "learning_rate": 0.0005814624395756766,
      "loss": 0.1263,
      "step": 15910
    },
    {
      "epoch": 4.6658851113716295,
      "grad_norm": 0.983902633190155,
      "learning_rate": 0.0005813786432880883,
      "loss": 0.134,
      "step": 15920
    },
    {
      "epoch": 4.668815943728019,
      "grad_norm": 0.6125565767288208,
      "learning_rate": 0.0005812948470004999,
      "loss": 0.1368,
      "step": 15930
    },
    {
      "epoch": 4.671746776084408,
      "grad_norm": 1.0572669506072998,
      "learning_rate": 0.0005812110507129115,
      "loss": 0.0969,
      "step": 15940
    },
    {
      "epoch": 4.674677608440797,
      "grad_norm": 0.7936329245567322,
      "learning_rate": 0.0005811272544253231,
      "loss": 0.1303,
      "step": 15950
    },
    {
      "epoch": 4.6776084407971865,
      "grad_norm": 0.8767409920692444,
      "learning_rate": 0.0005810434581377347,
      "loss": 0.1667,
      "step": 15960
    },
    {
      "epoch": 4.680539273153576,
      "grad_norm": 0.5020151138305664,
      "learning_rate": 0.0005809596618501464,
      "loss": 0.137,
      "step": 15970
    },
    {
      "epoch": 4.683470105509965,
      "grad_norm": 1.4177252054214478,
      "learning_rate": 0.000580875865562558,
      "loss": 0.1193,
      "step": 15980
    },
    {
      "epoch": 4.686400937866354,
      "grad_norm": 0.9507002234458923,
      "learning_rate": 0.0005807920692749697,
      "loss": 0.1712,
      "step": 15990
    },
    {
      "epoch": 4.6893317702227435,
      "grad_norm": 1.2633923292160034,
      "learning_rate": 0.0005807082729873813,
      "loss": 0.1113,
      "step": 16000
    },
    {
      "epoch": 4.692262602579133,
      "grad_norm": 0.8217214941978455,
      "learning_rate": 0.0005806244766997929,
      "loss": 0.1175,
      "step": 16010
    },
    {
      "epoch": 4.695193434935522,
      "grad_norm": 0.68715900182724,
      "learning_rate": 0.0005805406804122046,
      "loss": 0.1186,
      "step": 16020
    },
    {
      "epoch": 4.698124267291911,
      "grad_norm": 0.45129063725471497,
      "learning_rate": 0.0005804568841246162,
      "loss": 0.147,
      "step": 16030
    },
    {
      "epoch": 4.7010550996483005,
      "grad_norm": 0.8066044449806213,
      "learning_rate": 0.0005803730878370278,
      "loss": 0.1444,
      "step": 16040
    },
    {
      "epoch": 4.70398593200469,
      "grad_norm": 1.0617684125900269,
      "learning_rate": 0.0005802892915494394,
      "loss": 0.1144,
      "step": 16050
    },
    {
      "epoch": 4.706916764361079,
      "grad_norm": 1.0296233892440796,
      "learning_rate": 0.000580205495261851,
      "loss": 0.1368,
      "step": 16060
    },
    {
      "epoch": 4.709847596717468,
      "grad_norm": 0.9099271893501282,
      "learning_rate": 0.0005801216989742628,
      "loss": 0.1143,
      "step": 16070
    },
    {
      "epoch": 4.7127784290738575,
      "grad_norm": 1.7347443103790283,
      "learning_rate": 0.0005800379026866744,
      "loss": 0.1615,
      "step": 16080
    },
    {
      "epoch": 4.715709261430246,
      "grad_norm": 0.8438497185707092,
      "learning_rate": 0.000579954106399086,
      "loss": 0.1464,
      "step": 16090
    },
    {
      "epoch": 4.718640093786635,
      "grad_norm": 0.5272221565246582,
      "learning_rate": 0.0005798703101114976,
      "loss": 0.1343,
      "step": 16100
    },
    {
      "epoch": 4.721570926143024,
      "grad_norm": 0.39862897992134094,
      "learning_rate": 0.0005797865138239093,
      "loss": 0.134,
      "step": 16110
    },
    {
      "epoch": 4.724501758499414,
      "grad_norm": 0.9818441867828369,
      "learning_rate": 0.0005797027175363209,
      "loss": 0.1685,
      "step": 16120
    },
    {
      "epoch": 4.727432590855803,
      "grad_norm": 1.8487275838851929,
      "learning_rate": 0.0005796189212487325,
      "loss": 0.162,
      "step": 16130
    },
    {
      "epoch": 4.730363423212192,
      "grad_norm": 0.7770216464996338,
      "learning_rate": 0.0005795351249611441,
      "loss": 0.1547,
      "step": 16140
    },
    {
      "epoch": 4.733294255568581,
      "grad_norm": 0.9228224754333496,
      "learning_rate": 0.0005794513286735557,
      "loss": 0.1629,
      "step": 16150
    },
    {
      "epoch": 4.7362250879249705,
      "grad_norm": 0.5434608459472656,
      "learning_rate": 0.0005793675323859675,
      "loss": 0.1174,
      "step": 16160
    },
    {
      "epoch": 4.73915592028136,
      "grad_norm": 0.6398814916610718,
      "learning_rate": 0.0005792837360983791,
      "loss": 0.1259,
      "step": 16170
    },
    {
      "epoch": 4.742086752637749,
      "grad_norm": 0.779413104057312,
      "learning_rate": 0.0005791999398107907,
      "loss": 0.1319,
      "step": 16180
    },
    {
      "epoch": 4.745017584994138,
      "grad_norm": 0.6714782118797302,
      "learning_rate": 0.0005791161435232023,
      "loss": 0.1319,
      "step": 16190
    },
    {
      "epoch": 4.7479484173505275,
      "grad_norm": 0.5318485498428345,
      "learning_rate": 0.0005790323472356139,
      "loss": 0.1561,
      "step": 16200
    },
    {
      "epoch": 4.750879249706917,
      "grad_norm": 0.3653278946876526,
      "learning_rate": 0.0005789485509480256,
      "loss": 0.1327,
      "step": 16210
    },
    {
      "epoch": 4.753810082063306,
      "grad_norm": 0.7170540690422058,
      "learning_rate": 0.0005788647546604372,
      "loss": 0.1528,
      "step": 16220
    },
    {
      "epoch": 4.756740914419695,
      "grad_norm": 0.6088005304336548,
      "learning_rate": 0.0005787809583728488,
      "loss": 0.1425,
      "step": 16230
    },
    {
      "epoch": 4.7596717467760845,
      "grad_norm": 0.7602380514144897,
      "learning_rate": 0.0005786971620852604,
      "loss": 0.1307,
      "step": 16240
    },
    {
      "epoch": 4.762602579132474,
      "grad_norm": 0.6141948699951172,
      "learning_rate": 0.0005786133657976721,
      "loss": 0.148,
      "step": 16250
    },
    {
      "epoch": 4.765533411488863,
      "grad_norm": 0.6240478754043579,
      "learning_rate": 0.0005785295695100838,
      "loss": 0.1553,
      "step": 16260
    },
    {
      "epoch": 4.768464243845252,
      "grad_norm": 0.6348169445991516,
      "learning_rate": 0.0005784457732224954,
      "loss": 0.1653,
      "step": 16270
    },
    {
      "epoch": 4.7713950762016415,
      "grad_norm": 0.7254724502563477,
      "learning_rate": 0.0005783619769349071,
      "loss": 0.1448,
      "step": 16280
    },
    {
      "epoch": 4.774325908558031,
      "grad_norm": 0.682482898235321,
      "learning_rate": 0.0005782781806473186,
      "loss": 0.1418,
      "step": 16290
    },
    {
      "epoch": 4.77725674091442,
      "grad_norm": 0.671492338180542,
      "learning_rate": 0.0005781943843597302,
      "loss": 0.1232,
      "step": 16300
    },
    {
      "epoch": 4.780187573270809,
      "grad_norm": 0.6995726227760315,
      "learning_rate": 0.0005781105880721419,
      "loss": 0.148,
      "step": 16310
    },
    {
      "epoch": 4.7831184056271985,
      "grad_norm": 0.6912011504173279,
      "learning_rate": 0.0005780267917845535,
      "loss": 0.1423,
      "step": 16320
    },
    {
      "epoch": 4.786049237983588,
      "grad_norm": 1.3865196704864502,
      "learning_rate": 0.0005779429954969653,
      "loss": 0.1632,
      "step": 16330
    },
    {
      "epoch": 4.788980070339976,
      "grad_norm": 0.9687703847885132,
      "learning_rate": 0.0005778591992093769,
      "loss": 0.1315,
      "step": 16340
    },
    {
      "epoch": 4.791910902696365,
      "grad_norm": 1.290203332901001,
      "learning_rate": 0.0005777754029217884,
      "loss": 0.1162,
      "step": 16350
    },
    {
      "epoch": 4.794841735052755,
      "grad_norm": 1.8276989459991455,
      "learning_rate": 0.0005776916066342001,
      "loss": 0.1494,
      "step": 16360
    },
    {
      "epoch": 4.797772567409144,
      "grad_norm": 1.2023049592971802,
      "learning_rate": 0.0005776078103466117,
      "loss": 0.1399,
      "step": 16370
    },
    {
      "epoch": 4.800703399765533,
      "grad_norm": 1.1006649732589722,
      "learning_rate": 0.0005775240140590234,
      "loss": 0.1453,
      "step": 16380
    },
    {
      "epoch": 4.803634232121922,
      "grad_norm": 0.5126649737358093,
      "learning_rate": 0.000577440217771435,
      "loss": 0.1293,
      "step": 16390
    },
    {
      "epoch": 4.8065650644783116,
      "grad_norm": 1.2498576641082764,
      "learning_rate": 0.0005773564214838466,
      "loss": 0.1491,
      "step": 16400
    },
    {
      "epoch": 4.809495896834701,
      "grad_norm": 0.5770997405052185,
      "learning_rate": 0.0005772726251962582,
      "loss": 0.1586,
      "step": 16410
    },
    {
      "epoch": 4.81242672919109,
      "grad_norm": 0.5942208170890808,
      "learning_rate": 0.0005771888289086699,
      "loss": 0.1357,
      "step": 16420
    },
    {
      "epoch": 4.815357561547479,
      "grad_norm": 0.6652844548225403,
      "learning_rate": 0.0005771050326210816,
      "loss": 0.1509,
      "step": 16430
    },
    {
      "epoch": 4.8182883939038685,
      "grad_norm": 0.41137057542800903,
      "learning_rate": 0.0005770212363334932,
      "loss": 0.1399,
      "step": 16440
    },
    {
      "epoch": 4.821219226260258,
      "grad_norm": 0.6687323451042175,
      "learning_rate": 0.0005769374400459048,
      "loss": 0.1387,
      "step": 16450
    },
    {
      "epoch": 4.824150058616647,
      "grad_norm": 0.37420397996902466,
      "learning_rate": 0.0005768536437583164,
      "loss": 0.1389,
      "step": 16460
    },
    {
      "epoch": 4.827080890973036,
      "grad_norm": 0.440769761800766,
      "learning_rate": 0.000576769847470728,
      "loss": 0.1372,
      "step": 16470
    },
    {
      "epoch": 4.8300117233294255,
      "grad_norm": 1.000969409942627,
      "learning_rate": 0.0005766860511831397,
      "loss": 0.1444,
      "step": 16480
    },
    {
      "epoch": 4.832942555685815,
      "grad_norm": 0.6929011344909668,
      "learning_rate": 0.0005766022548955513,
      "loss": 0.1421,
      "step": 16490
    },
    {
      "epoch": 4.835873388042204,
      "grad_norm": 0.5990868210792542,
      "learning_rate": 0.000576518458607963,
      "loss": 0.156,
      "step": 16500
    },
    {
      "epoch": 4.838804220398593,
      "grad_norm": 0.7563186883926392,
      "learning_rate": 0.0005764346623203746,
      "loss": 0.13,
      "step": 16510
    },
    {
      "epoch": 4.8417350527549825,
      "grad_norm": 2.2753102779388428,
      "learning_rate": 0.0005763508660327862,
      "loss": 0.1488,
      "step": 16520
    },
    {
      "epoch": 4.844665885111372,
      "grad_norm": 0.7836152911186218,
      "learning_rate": 0.0005762670697451979,
      "loss": 0.1282,
      "step": 16530
    },
    {
      "epoch": 4.847596717467761,
      "grad_norm": 0.7806035876274109,
      "learning_rate": 0.0005761832734576095,
      "loss": 0.1488,
      "step": 16540
    },
    {
      "epoch": 4.85052754982415,
      "grad_norm": 0.6441959738731384,
      "learning_rate": 0.0005760994771700211,
      "loss": 0.1079,
      "step": 16550
    },
    {
      "epoch": 4.8534583821805395,
      "grad_norm": 0.5411359071731567,
      "learning_rate": 0.0005760156808824327,
      "loss": 0.121,
      "step": 16560
    },
    {
      "epoch": 4.856389214536929,
      "grad_norm": 0.5881704688072205,
      "learning_rate": 0.0005759318845948443,
      "loss": 0.1578,
      "step": 16570
    },
    {
      "epoch": 4.859320046893318,
      "grad_norm": 1.0403302907943726,
      "learning_rate": 0.000575848088307256,
      "loss": 0.1218,
      "step": 16580
    },
    {
      "epoch": 4.862250879249707,
      "grad_norm": 0.4767270088195801,
      "learning_rate": 0.0005757642920196677,
      "loss": 0.1496,
      "step": 16590
    },
    {
      "epoch": 4.8651817116060965,
      "grad_norm": 0.7287773489952087,
      "learning_rate": 0.0005756804957320793,
      "loss": 0.1212,
      "step": 16600
    },
    {
      "epoch": 4.868112543962486,
      "grad_norm": 0.5559797883033752,
      "learning_rate": 0.0005755966994444909,
      "loss": 0.1207,
      "step": 16610
    },
    {
      "epoch": 4.871043376318875,
      "grad_norm": 0.5149220824241638,
      "learning_rate": 0.0005755129031569026,
      "loss": 0.1426,
      "step": 16620
    },
    {
      "epoch": 4.873974208675264,
      "grad_norm": 0.6600340604782104,
      "learning_rate": 0.0005754291068693142,
      "loss": 0.1456,
      "step": 16630
    },
    {
      "epoch": 4.8769050410316535,
      "grad_norm": 0.7577974796295166,
      "learning_rate": 0.0005753453105817258,
      "loss": 0.1352,
      "step": 16640
    },
    {
      "epoch": 4.879835873388043,
      "grad_norm": 0.49896490573883057,
      "learning_rate": 0.0005752615142941374,
      "loss": 0.1237,
      "step": 16650
    },
    {
      "epoch": 4.882766705744431,
      "grad_norm": 0.7344579100608826,
      "learning_rate": 0.000575177718006549,
      "loss": 0.156,
      "step": 16660
    },
    {
      "epoch": 4.88569753810082,
      "grad_norm": 0.9948780536651611,
      "learning_rate": 0.0005750939217189607,
      "loss": 0.1383,
      "step": 16670
    },
    {
      "epoch": 4.8886283704572095,
      "grad_norm": 0.7960237264633179,
      "learning_rate": 0.0005750101254313724,
      "loss": 0.1604,
      "step": 16680
    },
    {
      "epoch": 4.891559202813599,
      "grad_norm": 1.392627477645874,
      "learning_rate": 0.000574926329143784,
      "loss": 0.128,
      "step": 16690
    },
    {
      "epoch": 4.894490035169988,
      "grad_norm": 0.48157981038093567,
      "learning_rate": 0.0005748425328561957,
      "loss": 0.1438,
      "step": 16700
    },
    {
      "epoch": 4.897420867526377,
      "grad_norm": 0.618719220161438,
      "learning_rate": 0.0005747587365686072,
      "loss": 0.1418,
      "step": 16710
    },
    {
      "epoch": 4.9003516998827665,
      "grad_norm": 1.0295392274856567,
      "learning_rate": 0.0005746749402810189,
      "loss": 0.1552,
      "step": 16720
    },
    {
      "epoch": 4.903282532239156,
      "grad_norm": 0.5312371850013733,
      "learning_rate": 0.0005745911439934305,
      "loss": 0.1572,
      "step": 16730
    },
    {
      "epoch": 4.906213364595545,
      "grad_norm": 0.5193799734115601,
      "learning_rate": 0.0005745073477058421,
      "loss": 0.113,
      "step": 16740
    },
    {
      "epoch": 4.909144196951934,
      "grad_norm": 0.890750527381897,
      "learning_rate": 0.0005744235514182538,
      "loss": 0.1219,
      "step": 16750
    },
    {
      "epoch": 4.9120750293083235,
      "grad_norm": 0.8260401487350464,
      "learning_rate": 0.0005743397551306653,
      "loss": 0.1605,
      "step": 16760
    },
    {
      "epoch": 4.915005861664713,
      "grad_norm": 0.7251513600349426,
      "learning_rate": 0.0005742559588430771,
      "loss": 0.1292,
      "step": 16770
    },
    {
      "epoch": 4.917936694021102,
      "grad_norm": 1.025389313697815,
      "learning_rate": 0.0005741721625554887,
      "loss": 0.1358,
      "step": 16780
    },
    {
      "epoch": 4.920867526377491,
      "grad_norm": 0.3817495107650757,
      "learning_rate": 0.0005740883662679003,
      "loss": 0.1366,
      "step": 16790
    },
    {
      "epoch": 4.9237983587338805,
      "grad_norm": 1.3859559297561646,
      "learning_rate": 0.000574004569980312,
      "loss": 0.1269,
      "step": 16800
    },
    {
      "epoch": 4.92672919109027,
      "grad_norm": 0.663697361946106,
      "learning_rate": 0.0005739207736927236,
      "loss": 0.1379,
      "step": 16810
    },
    {
      "epoch": 4.929660023446659,
      "grad_norm": 0.8019875884056091,
      "learning_rate": 0.0005738369774051352,
      "loss": 0.1303,
      "step": 16820
    },
    {
      "epoch": 4.932590855803048,
      "grad_norm": 0.6396894454956055,
      "learning_rate": 0.0005737531811175468,
      "loss": 0.1304,
      "step": 16830
    },
    {
      "epoch": 4.9355216881594375,
      "grad_norm": 1.03276789188385,
      "learning_rate": 0.0005736693848299585,
      "loss": 0.1439,
      "step": 16840
    },
    {
      "epoch": 4.938452520515827,
      "grad_norm": 1.0808115005493164,
      "learning_rate": 0.0005735855885423702,
      "loss": 0.139,
      "step": 16850
    },
    {
      "epoch": 4.941383352872216,
      "grad_norm": 1.4603439569473267,
      "learning_rate": 0.0005735017922547818,
      "loss": 0.1487,
      "step": 16860
    },
    {
      "epoch": 4.944314185228605,
      "grad_norm": 0.5847682952880859,
      "learning_rate": 0.0005734179959671934,
      "loss": 0.1298,
      "step": 16870
    },
    {
      "epoch": 4.9472450175849945,
      "grad_norm": 0.9182359576225281,
      "learning_rate": 0.000573334199679605,
      "loss": 0.132,
      "step": 16880
    },
    {
      "epoch": 4.950175849941384,
      "grad_norm": 1.6543805599212646,
      "learning_rate": 0.0005732504033920167,
      "loss": 0.1129,
      "step": 16890
    },
    {
      "epoch": 4.953106682297773,
      "grad_norm": 0.9689706563949585,
      "learning_rate": 0.0005731666071044283,
      "loss": 0.1649,
      "step": 16900
    },
    {
      "epoch": 4.956037514654161,
      "grad_norm": 0.8628659248352051,
      "learning_rate": 0.0005730828108168399,
      "loss": 0.1464,
      "step": 16910
    },
    {
      "epoch": 4.958968347010551,
      "grad_norm": 0.8545655012130737,
      "learning_rate": 0.0005729990145292515,
      "loss": 0.1476,
      "step": 16920
    },
    {
      "epoch": 4.96189917936694,
      "grad_norm": 1.2560245990753174,
      "learning_rate": 0.0005729152182416631,
      "loss": 0.1501,
      "step": 16930
    },
    {
      "epoch": 4.964830011723329,
      "grad_norm": 0.7266835570335388,
      "learning_rate": 0.0005728314219540749,
      "loss": 0.1371,
      "step": 16940
    },
    {
      "epoch": 4.967760844079718,
      "grad_norm": 0.6529867053031921,
      "learning_rate": 0.0005727476256664865,
      "loss": 0.14,
      "step": 16950
    },
    {
      "epoch": 4.9706916764361075,
      "grad_norm": 0.5777420401573181,
      "learning_rate": 0.0005726638293788981,
      "loss": 0.1399,
      "step": 16960
    },
    {
      "epoch": 4.973622508792497,
      "grad_norm": 0.41529858112335205,
      "learning_rate": 0.0005725800330913097,
      "loss": 0.1286,
      "step": 16970
    },
    {
      "epoch": 4.976553341148886,
      "grad_norm": 0.8863609433174133,
      "learning_rate": 0.0005724962368037213,
      "loss": 0.1663,
      "step": 16980
    },
    {
      "epoch": 4.979484173505275,
      "grad_norm": 0.8300988078117371,
      "learning_rate": 0.000572412440516133,
      "loss": 0.1418,
      "step": 16990
    },
    {
      "epoch": 4.9824150058616645,
      "grad_norm": 0.5197386741638184,
      "learning_rate": 0.0005723286442285446,
      "loss": 0.1427,
      "step": 17000
    },
    {
      "epoch": 4.985345838218054,
      "grad_norm": 1.021162748336792,
      "learning_rate": 0.0005722448479409562,
      "loss": 0.1151,
      "step": 17010
    },
    {
      "epoch": 4.988276670574443,
      "grad_norm": 0.7337222099304199,
      "learning_rate": 0.0005721610516533678,
      "loss": 0.1208,
      "step": 17020
    },
    {
      "epoch": 4.991207502930832,
      "grad_norm": 0.6527363657951355,
      "learning_rate": 0.0005720772553657795,
      "loss": 0.1443,
      "step": 17030
    },
    {
      "epoch": 4.9941383352872215,
      "grad_norm": 0.6069468259811401,
      "learning_rate": 0.0005719934590781912,
      "loss": 0.1773,
      "step": 17040
    },
    {
      "epoch": 4.997069167643611,
      "grad_norm": 0.6581974625587463,
      "learning_rate": 0.0005719096627906028,
      "loss": 0.1394,
      "step": 17050
    },
    {
      "epoch": 5.0,
      "grad_norm": 1.222957730293274,
      "learning_rate": 0.0005718258665030144,
      "loss": 0.1354,
      "step": 17060
    },
    {
      "epoch": 5.0,
      "eval_accuracy": 0.43257500824266404,
      "eval_f1_macro": 0.3431123802433734,
      "eval_f1_micro": 0.5625993289775737,
      "eval_f1_weighted": 0.5026889880335084,
      "eval_loss": 0.15015628933906555,
      "eval_roc_auc": 0.7241276252656856,
      "eval_runtime": 163.1086,
      "eval_samples_per_second": 18.595,
      "eval_steps_per_second": 2.33,
      "step": 17060
    },
    {
      "epoch": 5.002930832356389,
      "grad_norm": 0.8499008417129517,
      "learning_rate": 0.000571742070215426,
      "loss": 0.1615,
      "step": 17070
    },
    {
      "epoch": 5.0058616647127785,
      "grad_norm": 0.6855140924453735,
      "learning_rate": 0.0005716582739278376,
      "loss": 0.1409,
      "step": 17080
    },
    {
      "epoch": 5.008792497069168,
      "grad_norm": 0.6345043182373047,
      "learning_rate": 0.0005715744776402493,
      "loss": 0.1145,
      "step": 17090
    },
    {
      "epoch": 5.011723329425557,
      "grad_norm": 1.407209038734436,
      "learning_rate": 0.0005714906813526609,
      "loss": 0.1251,
      "step": 17100
    },
    {
      "epoch": 5.014654161781946,
      "grad_norm": 0.7356075048446655,
      "learning_rate": 0.0005714068850650727,
      "loss": 0.1064,
      "step": 17110
    },
    {
      "epoch": 5.0175849941383355,
      "grad_norm": 0.7609923481941223,
      "learning_rate": 0.0005713230887774842,
      "loss": 0.1692,
      "step": 17120
    },
    {
      "epoch": 5.020515826494725,
      "grad_norm": 0.8550623655319214,
      "learning_rate": 0.0005712392924898958,
      "loss": 0.1376,
      "step": 17130
    },
    {
      "epoch": 5.023446658851114,
      "grad_norm": 1.0356990098953247,
      "learning_rate": 0.0005711554962023075,
      "loss": 0.1185,
      "step": 17140
    },
    {
      "epoch": 5.026377491207503,
      "grad_norm": 0.7155194282531738,
      "learning_rate": 0.0005710716999147191,
      "loss": 0.1277,
      "step": 17150
    },
    {
      "epoch": 5.0293083235638925,
      "grad_norm": 1.0887466669082642,
      "learning_rate": 0.0005709879036271308,
      "loss": 0.1371,
      "step": 17160
    },
    {
      "epoch": 5.032239155920282,
      "grad_norm": 0.7326943874359131,
      "learning_rate": 0.0005709041073395424,
      "loss": 0.1589,
      "step": 17170
    },
    {
      "epoch": 5.035169988276671,
      "grad_norm": 0.5179531574249268,
      "learning_rate": 0.000570820311051954,
      "loss": 0.1147,
      "step": 17180
    },
    {
      "epoch": 5.03810082063306,
      "grad_norm": 0.7570958733558655,
      "learning_rate": 0.0005707365147643656,
      "loss": 0.1323,
      "step": 17190
    },
    {
      "epoch": 5.041031652989449,
      "grad_norm": 0.6615656614303589,
      "learning_rate": 0.0005706527184767773,
      "loss": 0.1431,
      "step": 17200
    },
    {
      "epoch": 5.043962485345838,
      "grad_norm": 0.7678660750389099,
      "learning_rate": 0.000570568922189189,
      "loss": 0.1181,
      "step": 17210
    },
    {
      "epoch": 5.046893317702227,
      "grad_norm": 0.6541253924369812,
      "learning_rate": 0.0005704851259016006,
      "loss": 0.1393,
      "step": 17220
    },
    {
      "epoch": 5.049824150058616,
      "grad_norm": 0.5886680483818054,
      "learning_rate": 0.0005704013296140122,
      "loss": 0.1241,
      "step": 17230
    },
    {
      "epoch": 5.0527549824150055,
      "grad_norm": 0.5638585686683655,
      "learning_rate": 0.0005703175333264238,
      "loss": 0.1444,
      "step": 17240
    },
    {
      "epoch": 5.055685814771395,
      "grad_norm": 0.5506039261817932,
      "learning_rate": 0.0005702337370388354,
      "loss": 0.1191,
      "step": 17250
    },
    {
      "epoch": 5.058616647127784,
      "grad_norm": 1.1763949394226074,
      "learning_rate": 0.0005701499407512471,
      "loss": 0.1298,
      "step": 17260
    },
    {
      "epoch": 5.061547479484173,
      "grad_norm": 0.5656128525733948,
      "learning_rate": 0.0005700661444636587,
      "loss": 0.1244,
      "step": 17270
    },
    {
      "epoch": 5.0644783118405625,
      "grad_norm": 0.7480099201202393,
      "learning_rate": 0.0005699823481760704,
      "loss": 0.1199,
      "step": 17280
    },
    {
      "epoch": 5.067409144196952,
      "grad_norm": 0.6797440648078918,
      "learning_rate": 0.000569898551888482,
      "loss": 0.1411,
      "step": 17290
    },
    {
      "epoch": 5.070339976553341,
      "grad_norm": 1.0020759105682373,
      "learning_rate": 0.0005698147556008936,
      "loss": 0.1238,
      "step": 17300
    },
    {
      "epoch": 5.07327080890973,
      "grad_norm": 0.8329079747200012,
      "learning_rate": 0.0005697309593133053,
      "loss": 0.1103,
      "step": 17310
    },
    {
      "epoch": 5.0762016412661195,
      "grad_norm": 0.5852659344673157,
      "learning_rate": 0.0005696471630257169,
      "loss": 0.1144,
      "step": 17320
    },
    {
      "epoch": 5.079132473622509,
      "grad_norm": 0.87580806016922,
      "learning_rate": 0.0005695633667381285,
      "loss": 0.1141,
      "step": 17330
    },
    {
      "epoch": 5.082063305978898,
      "grad_norm": 0.7134191989898682,
      "learning_rate": 0.0005694795704505401,
      "loss": 0.1339,
      "step": 17340
    },
    {
      "epoch": 5.084994138335287,
      "grad_norm": 1.2602770328521729,
      "learning_rate": 0.0005693957741629518,
      "loss": 0.126,
      "step": 17350
    },
    {
      "epoch": 5.0879249706916765,
      "grad_norm": 1.122302532196045,
      "learning_rate": 0.0005693119778753634,
      "loss": 0.1318,
      "step": 17360
    },
    {
      "epoch": 5.090855803048066,
      "grad_norm": 0.7182183861732483,
      "learning_rate": 0.0005692281815877751,
      "loss": 0.1552,
      "step": 17370
    },
    {
      "epoch": 5.093786635404455,
      "grad_norm": 0.7674416899681091,
      "learning_rate": 0.0005691443853001867,
      "loss": 0.1292,
      "step": 17380
    },
    {
      "epoch": 5.096717467760844,
      "grad_norm": 0.7677318453788757,
      "learning_rate": 0.0005690605890125983,
      "loss": 0.1109,
      "step": 17390
    },
    {
      "epoch": 5.0996483001172335,
      "grad_norm": 0.9841979742050171,
      "learning_rate": 0.00056897679272501,
      "loss": 0.1179,
      "step": 17400
    },
    {
      "epoch": 5.102579132473623,
      "grad_norm": 1.4363189935684204,
      "learning_rate": 0.0005688929964374216,
      "loss": 0.1319,
      "step": 17410
    },
    {
      "epoch": 5.105509964830012,
      "grad_norm": 0.8223081231117249,
      "learning_rate": 0.0005688092001498332,
      "loss": 0.1271,
      "step": 17420
    },
    {
      "epoch": 5.108440797186401,
      "grad_norm": 0.7464905977249146,
      "learning_rate": 0.0005687254038622448,
      "loss": 0.1449,
      "step": 17430
    },
    {
      "epoch": 5.1113716295427905,
      "grad_norm": 0.6065189242362976,
      "learning_rate": 0.0005686416075746564,
      "loss": 0.1202,
      "step": 17440
    },
    {
      "epoch": 5.11430246189918,
      "grad_norm": 1.3502393960952759,
      "learning_rate": 0.0005685578112870681,
      "loss": 0.145,
      "step": 17450
    },
    {
      "epoch": 5.117233294255569,
      "grad_norm": 0.36114633083343506,
      "learning_rate": 0.0005684740149994798,
      "loss": 0.1099,
      "step": 17460
    },
    {
      "epoch": 5.120164126611958,
      "grad_norm": 0.5489041209220886,
      "learning_rate": 0.0005683902187118914,
      "loss": 0.1217,
      "step": 17470
    },
    {
      "epoch": 5.123094958968347,
      "grad_norm": 1.1929864883422852,
      "learning_rate": 0.000568306422424303,
      "loss": 0.1325,
      "step": 17480
    },
    {
      "epoch": 5.126025791324736,
      "grad_norm": 0.7742696404457092,
      "learning_rate": 0.0005682226261367146,
      "loss": 0.1409,
      "step": 17490
    },
    {
      "epoch": 5.128956623681125,
      "grad_norm": 1.2097315788269043,
      "learning_rate": 0.0005681388298491263,
      "loss": 0.1364,
      "step": 17500
    },
    {
      "epoch": 5.131887456037514,
      "grad_norm": 1.1664279699325562,
      "learning_rate": 0.0005680550335615379,
      "loss": 0.1377,
      "step": 17510
    },
    {
      "epoch": 5.1348182883939035,
      "grad_norm": 0.534003734588623,
      "learning_rate": 0.0005679712372739496,
      "loss": 0.1171,
      "step": 17520
    },
    {
      "epoch": 5.137749120750293,
      "grad_norm": 0.4419160485267639,
      "learning_rate": 0.0005678874409863611,
      "loss": 0.1111,
      "step": 17530
    },
    {
      "epoch": 5.140679953106682,
      "grad_norm": 1.4860483407974243,
      "learning_rate": 0.0005678036446987727,
      "loss": 0.1277,
      "step": 17540
    },
    {
      "epoch": 5.143610785463071,
      "grad_norm": 0.4605909287929535,
      "learning_rate": 0.0005677198484111845,
      "loss": 0.1225,
      "step": 17550
    },
    {
      "epoch": 5.1465416178194605,
      "grad_norm": 1.2372831106185913,
      "learning_rate": 0.0005676360521235961,
      "loss": 0.1215,
      "step": 17560
    },
    {
      "epoch": 5.14947245017585,
      "grad_norm": 1.4970009326934814,
      "learning_rate": 0.0005675522558360078,
      "loss": 0.123,
      "step": 17570
    },
    {
      "epoch": 5.152403282532239,
      "grad_norm": 1.1933764219284058,
      "learning_rate": 0.0005674684595484194,
      "loss": 0.1418,
      "step": 17580
    },
    {
      "epoch": 5.155334114888628,
      "grad_norm": 1.2504085302352905,
      "learning_rate": 0.0005673846632608309,
      "loss": 0.1636,
      "step": 17590
    },
    {
      "epoch": 5.1582649472450175,
      "grad_norm": 0.7561545372009277,
      "learning_rate": 0.0005673008669732426,
      "loss": 0.1285,
      "step": 17600
    },
    {
      "epoch": 5.161195779601407,
      "grad_norm": 1.1112775802612305,
      "learning_rate": 0.0005672170706856542,
      "loss": 0.1227,
      "step": 17610
    },
    {
      "epoch": 5.164126611957796,
      "grad_norm": 0.9076501727104187,
      "learning_rate": 0.000567133274398066,
      "loss": 0.1514,
      "step": 17620
    },
    {
      "epoch": 5.167057444314185,
      "grad_norm": 0.4220878481864929,
      "learning_rate": 0.0005670494781104776,
      "loss": 0.1359,
      "step": 17630
    },
    {
      "epoch": 5.1699882766705745,
      "grad_norm": 0.43775439262390137,
      "learning_rate": 0.0005669656818228891,
      "loss": 0.1533,
      "step": 17640
    },
    {
      "epoch": 5.172919109026964,
      "grad_norm": 1.7240334749221802,
      "learning_rate": 0.0005668818855353008,
      "loss": 0.1353,
      "step": 17650
    },
    {
      "epoch": 5.175849941383353,
      "grad_norm": 0.6213442087173462,
      "learning_rate": 0.0005667980892477124,
      "loss": 0.1258,
      "step": 17660
    },
    {
      "epoch": 5.178780773739742,
      "grad_norm": 0.9630747437477112,
      "learning_rate": 0.0005667142929601241,
      "loss": 0.1349,
      "step": 17670
    },
    {
      "epoch": 5.1817116060961315,
      "grad_norm": 0.8391885757446289,
      "learning_rate": 0.0005666304966725357,
      "loss": 0.137,
      "step": 17680
    },
    {
      "epoch": 5.184642438452521,
      "grad_norm": 0.6597908735275269,
      "learning_rate": 0.0005665467003849473,
      "loss": 0.1202,
      "step": 17690
    },
    {
      "epoch": 5.18757327080891,
      "grad_norm": 1.0468933582305908,
      "learning_rate": 0.0005664629040973589,
      "loss": 0.1375,
      "step": 17700
    },
    {
      "epoch": 5.190504103165299,
      "grad_norm": 0.7287130355834961,
      "learning_rate": 0.0005663791078097705,
      "loss": 0.1253,
      "step": 17710
    },
    {
      "epoch": 5.1934349355216884,
      "grad_norm": 0.5858505368232727,
      "learning_rate": 0.0005662953115221823,
      "loss": 0.1149,
      "step": 17720
    },
    {
      "epoch": 5.196365767878078,
      "grad_norm": 1.8088960647583008,
      "learning_rate": 0.0005662115152345939,
      "loss": 0.1114,
      "step": 17730
    },
    {
      "epoch": 5.199296600234467,
      "grad_norm": 2.031798839569092,
      "learning_rate": 0.0005661277189470055,
      "loss": 0.1378,
      "step": 17740
    },
    {
      "epoch": 5.202227432590856,
      "grad_norm": 1.2756346464157104,
      "learning_rate": 0.0005660439226594171,
      "loss": 0.159,
      "step": 17750
    },
    {
      "epoch": 5.205158264947245,
      "grad_norm": 1.7194310426712036,
      "learning_rate": 0.0005659601263718287,
      "loss": 0.1352,
      "step": 17760
    },
    {
      "epoch": 5.208089097303635,
      "grad_norm": 0.617595911026001,
      "learning_rate": 0.0005658763300842404,
      "loss": 0.1019,
      "step": 17770
    },
    {
      "epoch": 5.211019929660023,
      "grad_norm": 0.9508254528045654,
      "learning_rate": 0.000565792533796652,
      "loss": 0.1299,
      "step": 17780
    },
    {
      "epoch": 5.213950762016412,
      "grad_norm": 0.6597930788993835,
      "learning_rate": 0.0005657087375090636,
      "loss": 0.1284,
      "step": 17790
    },
    {
      "epoch": 5.2168815943728015,
      "grad_norm": 0.8876430988311768,
      "learning_rate": 0.0005656249412214752,
      "loss": 0.1477,
      "step": 17800
    },
    {
      "epoch": 5.219812426729191,
      "grad_norm": 1.6146697998046875,
      "learning_rate": 0.0005655411449338869,
      "loss": 0.1269,
      "step": 17810
    },
    {
      "epoch": 5.22274325908558,
      "grad_norm": 0.3069012761116028,
      "learning_rate": 0.0005654573486462986,
      "loss": 0.1088,
      "step": 17820
    },
    {
      "epoch": 5.225674091441969,
      "grad_norm": 0.4027922451496124,
      "learning_rate": 0.0005653735523587102,
      "loss": 0.1237,
      "step": 17830
    },
    {
      "epoch": 5.2286049237983585,
      "grad_norm": 0.9356071352958679,
      "learning_rate": 0.0005652897560711218,
      "loss": 0.1244,
      "step": 17840
    },
    {
      "epoch": 5.231535756154748,
      "grad_norm": 1.6239420175552368,
      "learning_rate": 0.0005652059597835334,
      "loss": 0.1321,
      "step": 17850
    },
    {
      "epoch": 5.234466588511137,
      "grad_norm": 0.6036698222160339,
      "learning_rate": 0.0005651221634959451,
      "loss": 0.1129,
      "step": 17860
    },
    {
      "epoch": 5.237397420867526,
      "grad_norm": 0.4367921054363251,
      "learning_rate": 0.0005650383672083567,
      "loss": 0.1084,
      "step": 17870
    },
    {
      "epoch": 5.2403282532239155,
      "grad_norm": 0.37500497698783875,
      "learning_rate": 0.0005649545709207683,
      "loss": 0.1183,
      "step": 17880
    },
    {
      "epoch": 5.243259085580305,
      "grad_norm": 0.815421462059021,
      "learning_rate": 0.00056487077463318,
      "loss": 0.1055,
      "step": 17890
    },
    {
      "epoch": 5.246189917936694,
      "grad_norm": 0.6279557347297668,
      "learning_rate": 0.0005647869783455916,
      "loss": 0.1535,
      "step": 17900
    },
    {
      "epoch": 5.249120750293083,
      "grad_norm": 0.8549084067344666,
      "learning_rate": 0.0005647031820580033,
      "loss": 0.1359,
      "step": 17910
    },
    {
      "epoch": 5.2520515826494725,
      "grad_norm": 1.0756505727767944,
      "learning_rate": 0.0005646193857704149,
      "loss": 0.1226,
      "step": 17920
    },
    {
      "epoch": 5.254982415005862,
      "grad_norm": 0.9811694025993347,
      "learning_rate": 0.0005645355894828265,
      "loss": 0.1487,
      "step": 17930
    },
    {
      "epoch": 5.257913247362251,
      "grad_norm": 0.41367167234420776,
      "learning_rate": 0.0005644517931952382,
      "loss": 0.1176,
      "step": 17940
    },
    {
      "epoch": 5.26084407971864,
      "grad_norm": 0.7741619348526001,
      "learning_rate": 0.0005643679969076497,
      "loss": 0.1362,
      "step": 17950
    },
    {
      "epoch": 5.2637749120750295,
      "grad_norm": 0.840394139289856,
      "learning_rate": 0.0005642842006200614,
      "loss": 0.1438,
      "step": 17960
    },
    {
      "epoch": 5.266705744431419,
      "grad_norm": 0.962238609790802,
      "learning_rate": 0.000564200404332473,
      "loss": 0.1285,
      "step": 17970
    },
    {
      "epoch": 5.269636576787808,
      "grad_norm": 0.8427801132202148,
      "learning_rate": 0.0005641166080448847,
      "loss": 0.1348,
      "step": 17980
    },
    {
      "epoch": 5.272567409144197,
      "grad_norm": 0.7021342515945435,
      "learning_rate": 0.0005640328117572964,
      "loss": 0.1111,
      "step": 17990
    },
    {
      "epoch": 5.275498241500586,
      "grad_norm": 0.3538466989994049,
      "learning_rate": 0.0005639490154697079,
      "loss": 0.1149,
      "step": 18000
    },
    {
      "epoch": 5.278429073856976,
      "grad_norm": 0.5973894596099854,
      "learning_rate": 0.0005638652191821196,
      "loss": 0.1321,
      "step": 18010
    },
    {
      "epoch": 5.281359906213365,
      "grad_norm": 1.1293494701385498,
      "learning_rate": 0.0005637814228945312,
      "loss": 0.1333,
      "step": 18020
    },
    {
      "epoch": 5.284290738569754,
      "grad_norm": 1.0968835353851318,
      "learning_rate": 0.0005636976266069428,
      "loss": 0.1191,
      "step": 18030
    },
    {
      "epoch": 5.287221570926143,
      "grad_norm": 0.5451864004135132,
      "learning_rate": 0.0005636138303193545,
      "loss": 0.1075,
      "step": 18040
    },
    {
      "epoch": 5.290152403282532,
      "grad_norm": 1.734391689300537,
      "learning_rate": 0.0005635300340317661,
      "loss": 0.1313,
      "step": 18050
    },
    {
      "epoch": 5.293083235638921,
      "grad_norm": 1.0419270992279053,
      "learning_rate": 0.0005634462377441778,
      "loss": 0.1337,
      "step": 18060
    },
    {
      "epoch": 5.29601406799531,
      "grad_norm": 1.176497459411621,
      "learning_rate": 0.0005633624414565894,
      "loss": 0.117,
      "step": 18070
    },
    {
      "epoch": 5.2989449003516995,
      "grad_norm": 1.0074820518493652,
      "learning_rate": 0.0005632786451690011,
      "loss": 0.1728,
      "step": 18080
    },
    {
      "epoch": 5.301875732708089,
      "grad_norm": 0.7095877528190613,
      "learning_rate": 0.0005631948488814127,
      "loss": 0.147,
      "step": 18090
    },
    {
      "epoch": 5.304806565064478,
      "grad_norm": 1.0110901594161987,
      "learning_rate": 0.0005631110525938243,
      "loss": 0.1457,
      "step": 18100
    },
    {
      "epoch": 5.307737397420867,
      "grad_norm": 0.7038344144821167,
      "learning_rate": 0.0005630272563062359,
      "loss": 0.1326,
      "step": 18110
    },
    {
      "epoch": 5.3106682297772565,
      "grad_norm": 1.603237509727478,
      "learning_rate": 0.0005629434600186475,
      "loss": 0.1294,
      "step": 18120
    },
    {
      "epoch": 5.313599062133646,
      "grad_norm": 0.8613898158073425,
      "learning_rate": 0.0005628596637310592,
      "loss": 0.1328,
      "step": 18130
    },
    {
      "epoch": 5.316529894490035,
      "grad_norm": 1.919837236404419,
      "learning_rate": 0.0005627758674434708,
      "loss": 0.1348,
      "step": 18140
    },
    {
      "epoch": 5.319460726846424,
      "grad_norm": 0.6593901515007019,
      "learning_rate": 0.0005626920711558825,
      "loss": 0.1191,
      "step": 18150
    },
    {
      "epoch": 5.3223915592028135,
      "grad_norm": 1.5102900266647339,
      "learning_rate": 0.0005626082748682941,
      "loss": 0.1221,
      "step": 18160
    },
    {
      "epoch": 5.325322391559203,
      "grad_norm": 0.5018246173858643,
      "learning_rate": 0.0005625244785807057,
      "loss": 0.1549,
      "step": 18170
    },
    {
      "epoch": 5.328253223915592,
      "grad_norm": 0.44252270460128784,
      "learning_rate": 0.0005624406822931174,
      "loss": 0.101,
      "step": 18180
    },
    {
      "epoch": 5.331184056271981,
      "grad_norm": 0.520231306552887,
      "learning_rate": 0.000562356886005529,
      "loss": 0.1329,
      "step": 18190
    },
    {
      "epoch": 5.3341148886283705,
      "grad_norm": 0.542466402053833,
      "learning_rate": 0.0005622730897179406,
      "loss": 0.1305,
      "step": 18200
    },
    {
      "epoch": 5.33704572098476,
      "grad_norm": 0.8075933456420898,
      "learning_rate": 0.0005621892934303522,
      "loss": 0.1351,
      "step": 18210
    },
    {
      "epoch": 5.339976553341149,
      "grad_norm": 1.028774380683899,
      "learning_rate": 0.0005621054971427638,
      "loss": 0.1348,
      "step": 18220
    },
    {
      "epoch": 5.342907385697538,
      "grad_norm": 0.9835783839225769,
      "learning_rate": 0.0005620217008551755,
      "loss": 0.1254,
      "step": 18230
    },
    {
      "epoch": 5.3458382180539274,
      "grad_norm": 0.7083999514579773,
      "learning_rate": 0.0005619379045675872,
      "loss": 0.1221,
      "step": 18240
    },
    {
      "epoch": 5.348769050410317,
      "grad_norm": 1.0456470251083374,
      "learning_rate": 0.0005618541082799988,
      "loss": 0.108,
      "step": 18250
    },
    {
      "epoch": 5.351699882766706,
      "grad_norm": 1.818509578704834,
      "learning_rate": 0.0005617703119924104,
      "loss": 0.127,
      "step": 18260
    },
    {
      "epoch": 5.354630715123095,
      "grad_norm": 1.7991172075271606,
      "learning_rate": 0.000561686515704822,
      "loss": 0.1341,
      "step": 18270
    },
    {
      "epoch": 5.357561547479484,
      "grad_norm": 0.44946250319480896,
      "learning_rate": 0.0005616027194172337,
      "loss": 0.1186,
      "step": 18280
    },
    {
      "epoch": 5.360492379835874,
      "grad_norm": 1.0809478759765625,
      "learning_rate": 0.0005615189231296453,
      "loss": 0.1094,
      "step": 18290
    },
    {
      "epoch": 5.363423212192263,
      "grad_norm": 1.008803129196167,
      "learning_rate": 0.0005614351268420569,
      "loss": 0.1132,
      "step": 18300
    },
    {
      "epoch": 5.366354044548652,
      "grad_norm": 0.7420584559440613,
      "learning_rate": 0.0005613513305544685,
      "loss": 0.136,
      "step": 18310
    },
    {
      "epoch": 5.369284876905041,
      "grad_norm": 0.9021771550178528,
      "learning_rate": 0.0005612675342668801,
      "loss": 0.16,
      "step": 18320
    },
    {
      "epoch": 5.372215709261431,
      "grad_norm": 0.43088191747665405,
      "learning_rate": 0.0005611837379792919,
      "loss": 0.1337,
      "step": 18330
    },
    {
      "epoch": 5.37514654161782,
      "grad_norm": 0.878779947757721,
      "learning_rate": 0.0005610999416917035,
      "loss": 0.1383,
      "step": 18340
    },
    {
      "epoch": 5.378077373974208,
      "grad_norm": 0.6697827577590942,
      "learning_rate": 0.0005610161454041152,
      "loss": 0.15,
      "step": 18350
    },
    {
      "epoch": 5.3810082063305975,
      "grad_norm": 2.339784860610962,
      "learning_rate": 0.0005609323491165267,
      "loss": 0.1397,
      "step": 18360
    },
    {
      "epoch": 5.383939038686987,
      "grad_norm": 1.0725181102752686,
      "learning_rate": 0.0005608485528289383,
      "loss": 0.1707,
      "step": 18370
    },
    {
      "epoch": 5.386869871043376,
      "grad_norm": 0.6224737763404846,
      "learning_rate": 0.00056076475654135,
      "loss": 0.1297,
      "step": 18380
    },
    {
      "epoch": 5.389800703399765,
      "grad_norm": 0.6555064916610718,
      "learning_rate": 0.0005606809602537616,
      "loss": 0.1346,
      "step": 18390
    },
    {
      "epoch": 5.3927315357561545,
      "grad_norm": 0.6885536313056946,
      "learning_rate": 0.0005605971639661733,
      "loss": 0.1238,
      "step": 18400
    },
    {
      "epoch": 5.395662368112544,
      "grad_norm": 1.1249281167984009,
      "learning_rate": 0.0005605133676785848,
      "loss": 0.1576,
      "step": 18410
    },
    {
      "epoch": 5.398593200468933,
      "grad_norm": 0.7888476252555847,
      "learning_rate": 0.0005604295713909966,
      "loss": 0.1001,
      "step": 18420
    },
    {
      "epoch": 5.401524032825322,
      "grad_norm": 0.7083701491355896,
      "learning_rate": 0.0005603457751034082,
      "loss": 0.136,
      "step": 18430
    },
    {
      "epoch": 5.4044548651817115,
      "grad_norm": 0.9373849630355835,
      "learning_rate": 0.0005602619788158198,
      "loss": 0.1485,
      "step": 18440
    },
    {
      "epoch": 5.407385697538101,
      "grad_norm": 0.6515558958053589,
      "learning_rate": 0.0005601781825282315,
      "loss": 0.1581,
      "step": 18450
    },
    {
      "epoch": 5.41031652989449,
      "grad_norm": 0.7168446183204651,
      "learning_rate": 0.0005600943862406431,
      "loss": 0.1585,
      "step": 18460
    },
    {
      "epoch": 5.413247362250879,
      "grad_norm": 1.0834680795669556,
      "learning_rate": 0.0005600105899530547,
      "loss": 0.1224,
      "step": 18470
    },
    {
      "epoch": 5.4161781946072685,
      "grad_norm": 1.0403971672058105,
      "learning_rate": 0.0005599267936654663,
      "loss": 0.1298,
      "step": 18480
    },
    {
      "epoch": 5.419109026963658,
      "grad_norm": 0.9803380370140076,
      "learning_rate": 0.0005598429973778779,
      "loss": 0.1174,
      "step": 18490
    },
    {
      "epoch": 5.422039859320047,
      "grad_norm": 0.5725975632667542,
      "learning_rate": 0.0005597592010902897,
      "loss": 0.1311,
      "step": 18500
    },
    {
      "epoch": 5.424970691676436,
      "grad_norm": 0.5891751646995544,
      "learning_rate": 0.0005596754048027013,
      "loss": 0.1177,
      "step": 18510
    },
    {
      "epoch": 5.427901524032825,
      "grad_norm": 1.2532925605773926,
      "learning_rate": 0.0005595916085151129,
      "loss": 0.1378,
      "step": 18520
    },
    {
      "epoch": 5.430832356389215,
      "grad_norm": 0.9883378148078918,
      "learning_rate": 0.0005595078122275245,
      "loss": 0.1269,
      "step": 18530
    },
    {
      "epoch": 5.433763188745604,
      "grad_norm": 0.8593657612800598,
      "learning_rate": 0.0005594240159399361,
      "loss": 0.1399,
      "step": 18540
    },
    {
      "epoch": 5.436694021101993,
      "grad_norm": 0.8032885789871216,
      "learning_rate": 0.0005593402196523478,
      "loss": 0.133,
      "step": 18550
    },
    {
      "epoch": 5.439624853458382,
      "grad_norm": 0.9828415513038635,
      "learning_rate": 0.0005592564233647594,
      "loss": 0.1328,
      "step": 18560
    },
    {
      "epoch": 5.442555685814772,
      "grad_norm": 0.4364296793937683,
      "learning_rate": 0.000559172627077171,
      "loss": 0.1467,
      "step": 18570
    },
    {
      "epoch": 5.445486518171161,
      "grad_norm": 0.9743790030479431,
      "learning_rate": 0.0005590888307895826,
      "loss": 0.1279,
      "step": 18580
    },
    {
      "epoch": 5.44841735052755,
      "grad_norm": 1.3198875188827515,
      "learning_rate": 0.0005590050345019944,
      "loss": 0.134,
      "step": 18590
    },
    {
      "epoch": 5.451348182883939,
      "grad_norm": 0.6052128672599792,
      "learning_rate": 0.000558921238214406,
      "loss": 0.1172,
      "step": 18600
    },
    {
      "epoch": 5.454279015240329,
      "grad_norm": 1.1728343963623047,
      "learning_rate": 0.0005588374419268176,
      "loss": 0.1191,
      "step": 18610
    },
    {
      "epoch": 5.457209847596717,
      "grad_norm": 0.6746457815170288,
      "learning_rate": 0.0005587536456392292,
      "loss": 0.1284,
      "step": 18620
    },
    {
      "epoch": 5.460140679953106,
      "grad_norm": 1.101935625076294,
      "learning_rate": 0.0005586698493516408,
      "loss": 0.1385,
      "step": 18630
    },
    {
      "epoch": 5.4630715123094955,
      "grad_norm": 0.7662265300750732,
      "learning_rate": 0.0005585860530640525,
      "loss": 0.1518,
      "step": 18640
    },
    {
      "epoch": 5.466002344665885,
      "grad_norm": 1.1968721151351929,
      "learning_rate": 0.0005585022567764641,
      "loss": 0.1638,
      "step": 18650
    },
    {
      "epoch": 5.468933177022274,
      "grad_norm": 0.8698707222938538,
      "learning_rate": 0.0005584184604888757,
      "loss": 0.1256,
      "step": 18660
    },
    {
      "epoch": 5.471864009378663,
      "grad_norm": 0.7870505452156067,
      "learning_rate": 0.0005583346642012874,
      "loss": 0.1346,
      "step": 18670
    },
    {
      "epoch": 5.4747948417350525,
      "grad_norm": 1.1640576124191284,
      "learning_rate": 0.000558250867913699,
      "loss": 0.0944,
      "step": 18680
    },
    {
      "epoch": 5.477725674091442,
      "grad_norm": 1.5918610095977783,
      "learning_rate": 0.0005581670716261107,
      "loss": 0.1058,
      "step": 18690
    },
    {
      "epoch": 5.480656506447831,
      "grad_norm": 0.7940149307250977,
      "learning_rate": 0.0005580832753385223,
      "loss": 0.1395,
      "step": 18700
    },
    {
      "epoch": 5.48358733880422,
      "grad_norm": 0.9250445365905762,
      "learning_rate": 0.0005579994790509339,
      "loss": 0.1078,
      "step": 18710
    },
    {
      "epoch": 5.4865181711606095,
      "grad_norm": 0.6117343902587891,
      "learning_rate": 0.0005579156827633455,
      "loss": 0.1325,
      "step": 18720
    },
    {
      "epoch": 5.489449003516999,
      "grad_norm": 1.1505650281906128,
      "learning_rate": 0.0005578318864757571,
      "loss": 0.1339,
      "step": 18730
    },
    {
      "epoch": 5.492379835873388,
      "grad_norm": 0.7200290560722351,
      "learning_rate": 0.0005577480901881688,
      "loss": 0.1424,
      "step": 18740
    },
    {
      "epoch": 5.495310668229777,
      "grad_norm": 2.355423927307129,
      "learning_rate": 0.0005576642939005804,
      "loss": 0.1232,
      "step": 18750
    },
    {
      "epoch": 5.4982415005861665,
      "grad_norm": 1.1974471807479858,
      "learning_rate": 0.0005575804976129922,
      "loss": 0.1151,
      "step": 18760
    },
    {
      "epoch": 5.501172332942556,
      "grad_norm": 1.0297425985336304,
      "learning_rate": 0.0005574967013254037,
      "loss": 0.1639,
      "step": 18770
    },
    {
      "epoch": 5.504103165298945,
      "grad_norm": 0.7954526543617249,
      "learning_rate": 0.0005574129050378153,
      "loss": 0.1531,
      "step": 18780
    },
    {
      "epoch": 5.507033997655334,
      "grad_norm": 0.6829692721366882,
      "learning_rate": 0.000557329108750227,
      "loss": 0.1443,
      "step": 18790
    },
    {
      "epoch": 5.509964830011723,
      "grad_norm": 0.6087918281555176,
      "learning_rate": 0.0005572453124626386,
      "loss": 0.1264,
      "step": 18800
    },
    {
      "epoch": 5.512895662368113,
      "grad_norm": 0.7084112167358398,
      "learning_rate": 0.0005571615161750503,
      "loss": 0.1217,
      "step": 18810
    },
    {
      "epoch": 5.515826494724502,
      "grad_norm": 0.5342421531677246,
      "learning_rate": 0.0005570777198874619,
      "loss": 0.1532,
      "step": 18820
    },
    {
      "epoch": 5.518757327080891,
      "grad_norm": 0.6464768648147583,
      "learning_rate": 0.0005569939235998734,
      "loss": 0.1303,
      "step": 18830
    },
    {
      "epoch": 5.52168815943728,
      "grad_norm": 1.2535068988800049,
      "learning_rate": 0.0005569101273122852,
      "loss": 0.1562,
      "step": 18840
    },
    {
      "epoch": 5.52461899179367,
      "grad_norm": 0.8914763927459717,
      "learning_rate": 0.0005568263310246968,
      "loss": 0.1222,
      "step": 18850
    },
    {
      "epoch": 5.527549824150059,
      "grad_norm": 0.47062772512435913,
      "learning_rate": 0.0005567425347371085,
      "loss": 0.1192,
      "step": 18860
    },
    {
      "epoch": 5.530480656506448,
      "grad_norm": 1.074949026107788,
      "learning_rate": 0.0005566587384495201,
      "loss": 0.1227,
      "step": 18870
    },
    {
      "epoch": 5.533411488862837,
      "grad_norm": 0.5785755515098572,
      "learning_rate": 0.0005565749421619316,
      "loss": 0.1399,
      "step": 18880
    },
    {
      "epoch": 5.536342321219227,
      "grad_norm": 1.1751829385757446,
      "learning_rate": 0.0005564911458743433,
      "loss": 0.1395,
      "step": 18890
    },
    {
      "epoch": 5.539273153575616,
      "grad_norm": 0.7289455533027649,
      "learning_rate": 0.0005564073495867549,
      "loss": 0.1159,
      "step": 18900
    },
    {
      "epoch": 5.542203985932005,
      "grad_norm": 0.4234676659107208,
      "learning_rate": 0.0005563235532991666,
      "loss": 0.1192,
      "step": 18910
    },
    {
      "epoch": 5.545134818288394,
      "grad_norm": 1.2053649425506592,
      "learning_rate": 0.0005562397570115782,
      "loss": 0.1341,
      "step": 18920
    },
    {
      "epoch": 5.548065650644783,
      "grad_norm": 0.5627450346946716,
      "learning_rate": 0.0005561559607239899,
      "loss": 0.1532,
      "step": 18930
    },
    {
      "epoch": 5.550996483001172,
      "grad_norm": 0.7001364827156067,
      "learning_rate": 0.0005560721644364015,
      "loss": 0.1432,
      "step": 18940
    },
    {
      "epoch": 5.553927315357561,
      "grad_norm": 0.7142157554626465,
      "learning_rate": 0.0005559883681488131,
      "loss": 0.1287,
      "step": 18950
    },
    {
      "epoch": 5.5568581477139505,
      "grad_norm": 0.42303746938705444,
      "learning_rate": 0.0005559045718612248,
      "loss": 0.1433,
      "step": 18960
    },
    {
      "epoch": 5.55978898007034,
      "grad_norm": 0.8955310583114624,
      "learning_rate": 0.0005558207755736364,
      "loss": 0.1462,
      "step": 18970
    },
    {
      "epoch": 5.562719812426729,
      "grad_norm": 0.9125142097473145,
      "learning_rate": 0.000555736979286048,
      "loss": 0.1192,
      "step": 18980
    },
    {
      "epoch": 5.565650644783118,
      "grad_norm": 0.6616767048835754,
      "learning_rate": 0.0005556531829984596,
      "loss": 0.0844,
      "step": 18990
    },
    {
      "epoch": 5.5685814771395075,
      "grad_norm": 0.7521824240684509,
      "learning_rate": 0.0005555693867108712,
      "loss": 0.1176,
      "step": 19000
    },
    {
      "epoch": 5.571512309495897,
      "grad_norm": 0.6685177087783813,
      "learning_rate": 0.000555485590423283,
      "loss": 0.1229,
      "step": 19010
    },
    {
      "epoch": 5.574443141852286,
      "grad_norm": 1.1358932256698608,
      "learning_rate": 0.0005554017941356946,
      "loss": 0.1332,
      "step": 19020
    },
    {
      "epoch": 5.577373974208675,
      "grad_norm": 0.8244005441665649,
      "learning_rate": 0.0005553179978481062,
      "loss": 0.1387,
      "step": 19030
    },
    {
      "epoch": 5.5803048065650644,
      "grad_norm": 1.0379245281219482,
      "learning_rate": 0.0005552342015605178,
      "loss": 0.1171,
      "step": 19040
    },
    {
      "epoch": 5.583235638921454,
      "grad_norm": 1.2688500881195068,
      "learning_rate": 0.0005551504052729294,
      "loss": 0.1256,
      "step": 19050
    },
    {
      "epoch": 5.586166471277843,
      "grad_norm": 0.8177664279937744,
      "learning_rate": 0.0005550666089853411,
      "loss": 0.1186,
      "step": 19060
    },
    {
      "epoch": 5.589097303634232,
      "grad_norm": 1.2330597639083862,
      "learning_rate": 0.0005549828126977527,
      "loss": 0.1137,
      "step": 19070
    },
    {
      "epoch": 5.592028135990621,
      "grad_norm": 0.7514917254447937,
      "learning_rate": 0.0005548990164101643,
      "loss": 0.1447,
      "step": 19080
    },
    {
      "epoch": 5.594958968347011,
      "grad_norm": 0.7815567255020142,
      "learning_rate": 0.0005548152201225759,
      "loss": 0.1233,
      "step": 19090
    },
    {
      "epoch": 5.5978898007034,
      "grad_norm": 0.6994972825050354,
      "learning_rate": 0.0005547314238349875,
      "loss": 0.1387,
      "step": 19100
    },
    {
      "epoch": 5.600820633059789,
      "grad_norm": 1.0387290716171265,
      "learning_rate": 0.0005546476275473993,
      "loss": 0.1229,
      "step": 19110
    },
    {
      "epoch": 5.603751465416178,
      "grad_norm": 0.800321102142334,
      "learning_rate": 0.0005545638312598109,
      "loss": 0.1359,
      "step": 19120
    },
    {
      "epoch": 5.606682297772568,
      "grad_norm": 0.9151670932769775,
      "learning_rate": 0.0005544800349722225,
      "loss": 0.1349,
      "step": 19130
    },
    {
      "epoch": 5.609613130128957,
      "grad_norm": 0.5419270396232605,
      "learning_rate": 0.0005543962386846341,
      "loss": 0.1323,
      "step": 19140
    },
    {
      "epoch": 5.612543962485346,
      "grad_norm": 0.5549265742301941,
      "learning_rate": 0.0005543124423970458,
      "loss": 0.1301,
      "step": 19150
    },
    {
      "epoch": 5.615474794841735,
      "grad_norm": 0.8350614309310913,
      "learning_rate": 0.0005542286461094574,
      "loss": 0.151,
      "step": 19160
    },
    {
      "epoch": 5.618405627198125,
      "grad_norm": 1.0837582349777222,
      "learning_rate": 0.000554144849821869,
      "loss": 0.1326,
      "step": 19170
    },
    {
      "epoch": 5.621336459554513,
      "grad_norm": 0.9993409514427185,
      "learning_rate": 0.0005540610535342806,
      "loss": 0.1433,
      "step": 19180
    },
    {
      "epoch": 5.624267291910902,
      "grad_norm": 0.4054555594921112,
      "learning_rate": 0.0005539772572466922,
      "loss": 0.1032,
      "step": 19190
    },
    {
      "epoch": 5.6271981242672915,
      "grad_norm": 0.990673303604126,
      "learning_rate": 0.000553893460959104,
      "loss": 0.1204,
      "step": 19200
    },
    {
      "epoch": 5.630128956623681,
      "grad_norm": 0.7844060659408569,
      "learning_rate": 0.0005538096646715156,
      "loss": 0.1138,
      "step": 19210
    },
    {
      "epoch": 5.63305978898007,
      "grad_norm": 1.5498148202896118,
      "learning_rate": 0.0005537258683839272,
      "loss": 0.1313,
      "step": 19220
    },
    {
      "epoch": 5.635990621336459,
      "grad_norm": 0.8102915287017822,
      "learning_rate": 0.0005536420720963389,
      "loss": 0.1291,
      "step": 19230
    },
    {
      "epoch": 5.6389214536928485,
      "grad_norm": 0.8395740985870361,
      "learning_rate": 0.0005535582758087504,
      "loss": 0.122,
      "step": 19240
    },
    {
      "epoch": 5.641852286049238,
      "grad_norm": 0.8555880188941956,
      "learning_rate": 0.0005534744795211621,
      "loss": 0.1141,
      "step": 19250
    },
    {
      "epoch": 5.644783118405627,
      "grad_norm": 0.9092628359794617,
      "learning_rate": 0.0005533906832335737,
      "loss": 0.105,
      "step": 19260
    },
    {
      "epoch": 5.647713950762016,
      "grad_norm": 0.8519371151924133,
      "learning_rate": 0.0005533068869459853,
      "loss": 0.1199,
      "step": 19270
    },
    {
      "epoch": 5.6506447831184055,
      "grad_norm": 1.160767912864685,
      "learning_rate": 0.0005532230906583971,
      "loss": 0.1574,
      "step": 19280
    },
    {
      "epoch": 5.653575615474795,
      "grad_norm": 1.119551181793213,
      "learning_rate": 0.0005531392943708086,
      "loss": 0.1387,
      "step": 19290
    },
    {
      "epoch": 5.656506447831184,
      "grad_norm": 0.92168128490448,
      "learning_rate": 0.0005530554980832203,
      "loss": 0.1147,
      "step": 19300
    },
    {
      "epoch": 5.659437280187573,
      "grad_norm": 0.5231457352638245,
      "learning_rate": 0.0005529717017956319,
      "loss": 0.1212,
      "step": 19310
    },
    {
      "epoch": 5.662368112543962,
      "grad_norm": 1.5721036195755005,
      "learning_rate": 0.0005528879055080436,
      "loss": 0.1384,
      "step": 19320
    },
    {
      "epoch": 5.665298944900352,
      "grad_norm": 2.3226001262664795,
      "learning_rate": 0.0005528041092204552,
      "loss": 0.1221,
      "step": 19330
    },
    {
      "epoch": 5.668229777256741,
      "grad_norm": 1.2438102960586548,
      "learning_rate": 0.0005527203129328668,
      "loss": 0.1353,
      "step": 19340
    },
    {
      "epoch": 5.67116060961313,
      "grad_norm": 1.1855543851852417,
      "learning_rate": 0.0005526365166452784,
      "loss": 0.1207,
      "step": 19350
    },
    {
      "epoch": 5.674091441969519,
      "grad_norm": 1.530274510383606,
      "learning_rate": 0.00055255272035769,
      "loss": 0.1304,
      "step": 19360
    },
    {
      "epoch": 5.677022274325909,
      "grad_norm": 0.563027024269104,
      "learning_rate": 0.0005524689240701018,
      "loss": 0.1244,
      "step": 19370
    },
    {
      "epoch": 5.679953106682298,
      "grad_norm": 1.1436538696289062,
      "learning_rate": 0.0005523851277825134,
      "loss": 0.1202,
      "step": 19380
    },
    {
      "epoch": 5.682883939038687,
      "grad_norm": 1.0622807741165161,
      "learning_rate": 0.000552301331494925,
      "loss": 0.1187,
      "step": 19390
    },
    {
      "epoch": 5.685814771395076,
      "grad_norm": 0.8142120838165283,
      "learning_rate": 0.0005522175352073366,
      "loss": 0.1231,
      "step": 19400
    },
    {
      "epoch": 5.688745603751466,
      "grad_norm": 0.8083395957946777,
      "learning_rate": 0.0005521337389197482,
      "loss": 0.1232,
      "step": 19410
    },
    {
      "epoch": 5.691676436107855,
      "grad_norm": 1.4028041362762451,
      "learning_rate": 0.0005520499426321599,
      "loss": 0.1295,
      "step": 19420
    },
    {
      "epoch": 5.694607268464244,
      "grad_norm": 0.7365537881851196,
      "learning_rate": 0.0005519661463445715,
      "loss": 0.1429,
      "step": 19430
    },
    {
      "epoch": 5.697538100820633,
      "grad_norm": 0.4827195703983307,
      "learning_rate": 0.0005518823500569831,
      "loss": 0.1719,
      "step": 19440
    },
    {
      "epoch": 5.700468933177023,
      "grad_norm": 0.6890540719032288,
      "learning_rate": 0.0005517985537693948,
      "loss": 0.1219,
      "step": 19450
    },
    {
      "epoch": 5.703399765533412,
      "grad_norm": 0.7415304183959961,
      "learning_rate": 0.0005517147574818064,
      "loss": 0.1344,
      "step": 19460
    },
    {
      "epoch": 5.706330597889801,
      "grad_norm": 0.4895358085632324,
      "learning_rate": 0.0005516309611942181,
      "loss": 0.1274,
      "step": 19470
    },
    {
      "epoch": 5.70926143024619,
      "grad_norm": 0.6567111611366272,
      "learning_rate": 0.0005515471649066297,
      "loss": 0.1207,
      "step": 19480
    },
    {
      "epoch": 5.71219226260258,
      "grad_norm": 1.4555696249008179,
      "learning_rate": 0.0005514633686190413,
      "loss": 0.1112,
      "step": 19490
    },
    {
      "epoch": 5.715123094958968,
      "grad_norm": 1.8051118850708008,
      "learning_rate": 0.0005513795723314529,
      "loss": 0.1282,
      "step": 19500
    },
    {
      "epoch": 5.718053927315357,
      "grad_norm": 1.0142451524734497,
      "learning_rate": 0.0005512957760438645,
      "loss": 0.1338,
      "step": 19510
    },
    {
      "epoch": 5.7209847596717465,
      "grad_norm": 0.7157071232795715,
      "learning_rate": 0.0005512119797562762,
      "loss": 0.1048,
      "step": 19520
    },
    {
      "epoch": 5.723915592028136,
      "grad_norm": 0.9067635536193848,
      "learning_rate": 0.0005511281834686878,
      "loss": 0.1311,
      "step": 19530
    },
    {
      "epoch": 5.726846424384525,
      "grad_norm": 0.5587062835693359,
      "learning_rate": 0.0005510443871810995,
      "loss": 0.1167,
      "step": 19540
    },
    {
      "epoch": 5.729777256740914,
      "grad_norm": 0.8133501410484314,
      "learning_rate": 0.0005509605908935111,
      "loss": 0.112,
      "step": 19550
    },
    {
      "epoch": 5.7327080890973034,
      "grad_norm": 0.9334540963172913,
      "learning_rate": 0.0005508767946059227,
      "loss": 0.1365,
      "step": 19560
    },
    {
      "epoch": 5.735638921453693,
      "grad_norm": 0.8334629535675049,
      "learning_rate": 0.0005507929983183344,
      "loss": 0.116,
      "step": 19570
    },
    {
      "epoch": 5.738569753810082,
      "grad_norm": 0.6679830551147461,
      "learning_rate": 0.000550709202030746,
      "loss": 0.1103,
      "step": 19580
    },
    {
      "epoch": 5.741500586166471,
      "grad_norm": 0.9491525888442993,
      "learning_rate": 0.0005506254057431577,
      "loss": 0.1285,
      "step": 19590
    },
    {
      "epoch": 5.74443141852286,
      "grad_norm": 0.5157546997070312,
      "learning_rate": 0.0005505416094555692,
      "loss": 0.1436,
      "step": 19600
    },
    {
      "epoch": 5.74736225087925,
      "grad_norm": 0.8812384009361267,
      "learning_rate": 0.0005504578131679808,
      "loss": 0.1415,
      "step": 19610
    },
    {
      "epoch": 5.750293083235639,
      "grad_norm": 0.710528552532196,
      "learning_rate": 0.0005503740168803926,
      "loss": 0.1535,
      "step": 19620
    },
    {
      "epoch": 5.753223915592028,
      "grad_norm": 0.8397709131240845,
      "learning_rate": 0.0005502902205928042,
      "loss": 0.1221,
      "step": 19630
    },
    {
      "epoch": 5.756154747948417,
      "grad_norm": 1.2017772197723389,
      "learning_rate": 0.0005502064243052159,
      "loss": 0.1503,
      "step": 19640
    },
    {
      "epoch": 5.759085580304807,
      "grad_norm": 1.536128282546997,
      "learning_rate": 0.0005501226280176274,
      "loss": 0.1237,
      "step": 19650
    },
    {
      "epoch": 5.762016412661196,
      "grad_norm": 0.9116607308387756,
      "learning_rate": 0.0005500388317300391,
      "loss": 0.1356,
      "step": 19660
    },
    {
      "epoch": 5.764947245017585,
      "grad_norm": 1.1954375505447388,
      "learning_rate": 0.0005499550354424507,
      "loss": 0.1193,
      "step": 19670
    },
    {
      "epoch": 5.767878077373974,
      "grad_norm": 1.137370228767395,
      "learning_rate": 0.0005498712391548623,
      "loss": 0.1339,
      "step": 19680
    },
    {
      "epoch": 5.770808909730364,
      "grad_norm": 0.4752744734287262,
      "learning_rate": 0.000549787442867274,
      "loss": 0.1095,
      "step": 19690
    },
    {
      "epoch": 5.773739742086753,
      "grad_norm": 0.5443919897079468,
      "learning_rate": 0.0005497036465796856,
      "loss": 0.1449,
      "step": 19700
    },
    {
      "epoch": 5.776670574443142,
      "grad_norm": 0.9712052345275879,
      "learning_rate": 0.0005496198502920973,
      "loss": 0.1456,
      "step": 19710
    },
    {
      "epoch": 5.779601406799531,
      "grad_norm": 0.67578125,
      "learning_rate": 0.0005495360540045089,
      "loss": 0.1096,
      "step": 19720
    },
    {
      "epoch": 5.782532239155921,
      "grad_norm": 1.1534864902496338,
      "learning_rate": 0.0005494522577169205,
      "loss": 0.1036,
      "step": 19730
    },
    {
      "epoch": 5.78546307151231,
      "grad_norm": 0.9076052904129028,
      "learning_rate": 0.0005493684614293322,
      "loss": 0.1408,
      "step": 19740
    },
    {
      "epoch": 5.788393903868698,
      "grad_norm": 0.6439017653465271,
      "learning_rate": 0.0005492846651417438,
      "loss": 0.1095,
      "step": 19750
    },
    {
      "epoch": 5.7913247362250875,
      "grad_norm": 1.0754207372665405,
      "learning_rate": 0.0005492008688541554,
      "loss": 0.131,
      "step": 19760
    },
    {
      "epoch": 5.794255568581477,
      "grad_norm": 0.5095780491828918,
      "learning_rate": 0.000549117072566567,
      "loss": 0.1103,
      "step": 19770
    },
    {
      "epoch": 5.797186400937866,
      "grad_norm": 0.736488401889801,
      "learning_rate": 0.0005490332762789786,
      "loss": 0.1124,
      "step": 19780
    },
    {
      "epoch": 5.800117233294255,
      "grad_norm": 1.145850419998169,
      "learning_rate": 0.0005489494799913903,
      "loss": 0.1039,
      "step": 19790
    },
    {
      "epoch": 5.8030480656506445,
      "grad_norm": 1.0922191143035889,
      "learning_rate": 0.000548865683703802,
      "loss": 0.1271,
      "step": 19800
    },
    {
      "epoch": 5.805978898007034,
      "grad_norm": 0.7161542773246765,
      "learning_rate": 0.0005487818874162136,
      "loss": 0.0946,
      "step": 19810
    },
    {
      "epoch": 5.808909730363423,
      "grad_norm": 0.8889434933662415,
      "learning_rate": 0.0005486980911286252,
      "loss": 0.0961,
      "step": 19820
    },
    {
      "epoch": 5.811840562719812,
      "grad_norm": 1.4376366138458252,
      "learning_rate": 0.0005486142948410369,
      "loss": 0.1244,
      "step": 19830
    },
    {
      "epoch": 5.814771395076201,
      "grad_norm": 0.6628817915916443,
      "learning_rate": 0.0005485304985534485,
      "loss": 0.1425,
      "step": 19840
    },
    {
      "epoch": 5.817702227432591,
      "grad_norm": 0.8007327318191528,
      "learning_rate": 0.0005484467022658601,
      "loss": 0.1236,
      "step": 19850
    },
    {
      "epoch": 5.82063305978898,
      "grad_norm": 0.8769775032997131,
      "learning_rate": 0.0005483629059782717,
      "loss": 0.1175,
      "step": 19860
    },
    {
      "epoch": 5.823563892145369,
      "grad_norm": 2.473891019821167,
      "learning_rate": 0.0005482791096906833,
      "loss": 0.1263,
      "step": 19870
    },
    {
      "epoch": 5.826494724501758,
      "grad_norm": 0.6267533302307129,
      "learning_rate": 0.000548195313403095,
      "loss": 0.1397,
      "step": 19880
    },
    {
      "epoch": 5.829425556858148,
      "grad_norm": 0.8963913321495056,
      "learning_rate": 0.0005481115171155067,
      "loss": 0.1202,
      "step": 19890
    },
    {
      "epoch": 5.832356389214537,
      "grad_norm": 0.45599913597106934,
      "learning_rate": 0.0005480277208279183,
      "loss": 0.1155,
      "step": 19900
    },
    {
      "epoch": 5.835287221570926,
      "grad_norm": 1.2152206897735596,
      "learning_rate": 0.0005479439245403299,
      "loss": 0.107,
      "step": 19910
    },
    {
      "epoch": 5.838218053927315,
      "grad_norm": 0.8262843489646912,
      "learning_rate": 0.0005478601282527415,
      "loss": 0.1262,
      "step": 19920
    },
    {
      "epoch": 5.841148886283705,
      "grad_norm": 0.7370158433914185,
      "learning_rate": 0.0005477763319651532,
      "loss": 0.0934,
      "step": 19930
    },
    {
      "epoch": 5.844079718640094,
      "grad_norm": 0.45794036984443665,
      "learning_rate": 0.0005476925356775648,
      "loss": 0.1441,
      "step": 19940
    },
    {
      "epoch": 5.847010550996483,
      "grad_norm": 1.250684142112732,
      "learning_rate": 0.0005476087393899764,
      "loss": 0.1399,
      "step": 19950
    },
    {
      "epoch": 5.849941383352872,
      "grad_norm": 1.090957760810852,
      "learning_rate": 0.000547524943102388,
      "loss": 0.1323,
      "step": 19960
    },
    {
      "epoch": 5.852872215709262,
      "grad_norm": 1.1681822538375854,
      "learning_rate": 0.0005474411468147996,
      "loss": 0.111,
      "step": 19970
    },
    {
      "epoch": 5.855803048065651,
      "grad_norm": 1.341980218887329,
      "learning_rate": 0.0005473573505272114,
      "loss": 0.1289,
      "step": 19980
    },
    {
      "epoch": 5.85873388042204,
      "grad_norm": 0.626215398311615,
      "learning_rate": 0.000547273554239623,
      "loss": 0.1172,
      "step": 19990
    },
    {
      "epoch": 5.861664712778429,
      "grad_norm": 1.156921625137329,
      "learning_rate": 0.0005471897579520347,
      "loss": 0.1464,
      "step": 20000
    },
    {
      "epoch": 5.864595545134819,
      "grad_norm": 0.9572468400001526,
      "learning_rate": 0.0005471059616644462,
      "loss": 0.1289,
      "step": 20010
    },
    {
      "epoch": 5.867526377491208,
      "grad_norm": 0.46966683864593506,
      "learning_rate": 0.0005470221653768578,
      "loss": 0.1329,
      "step": 20020
    },
    {
      "epoch": 5.870457209847597,
      "grad_norm": 1.0331642627716064,
      "learning_rate": 0.0005469383690892695,
      "loss": 0.1166,
      "step": 20030
    },
    {
      "epoch": 5.873388042203986,
      "grad_norm": 0.514424741268158,
      "learning_rate": 0.0005468545728016811,
      "loss": 0.1286,
      "step": 20040
    },
    {
      "epoch": 5.876318874560376,
      "grad_norm": 0.7296290397644043,
      "learning_rate": 0.0005467707765140929,
      "loss": 0.1218,
      "step": 20050
    },
    {
      "epoch": 5.879249706916765,
      "grad_norm": 0.48257753252983093,
      "learning_rate": 0.0005466869802265044,
      "loss": 0.1378,
      "step": 20060
    },
    {
      "epoch": 5.882180539273153,
      "grad_norm": 0.9097143411636353,
      "learning_rate": 0.000546603183938916,
      "loss": 0.1192,
      "step": 20070
    },
    {
      "epoch": 5.8851113716295425,
      "grad_norm": 0.6374155879020691,
      "learning_rate": 0.0005465193876513277,
      "loss": 0.13,
      "step": 20080
    },
    {
      "epoch": 5.888042203985932,
      "grad_norm": 0.6656860709190369,
      "learning_rate": 0.0005464355913637393,
      "loss": 0.1358,
      "step": 20090
    },
    {
      "epoch": 5.890973036342321,
      "grad_norm": 0.7763903737068176,
      "learning_rate": 0.000546351795076151,
      "loss": 0.1405,
      "step": 20100
    },
    {
      "epoch": 5.89390386869871,
      "grad_norm": 0.8327959775924683,
      "learning_rate": 0.0005462679987885626,
      "loss": 0.1123,
      "step": 20110
    },
    {
      "epoch": 5.896834701055099,
      "grad_norm": 0.36577439308166504,
      "learning_rate": 0.0005461842025009741,
      "loss": 0.126,
      "step": 20120
    },
    {
      "epoch": 5.899765533411489,
      "grad_norm": 0.8226999044418335,
      "learning_rate": 0.0005461004062133858,
      "loss": 0.1404,
      "step": 20130
    },
    {
      "epoch": 5.902696365767878,
      "grad_norm": 0.9509454965591431,
      "learning_rate": 0.0005460166099257974,
      "loss": 0.1321,
      "step": 20140
    },
    {
      "epoch": 5.905627198124267,
      "grad_norm": 0.6283875107765198,
      "learning_rate": 0.0005459328136382092,
      "loss": 0.1545,
      "step": 20150
    },
    {
      "epoch": 5.908558030480656,
      "grad_norm": 0.9809292554855347,
      "learning_rate": 0.0005458490173506208,
      "loss": 0.0995,
      "step": 20160
    },
    {
      "epoch": 5.911488862837046,
      "grad_norm": 1.2084729671478271,
      "learning_rate": 0.0005457652210630324,
      "loss": 0.1295,
      "step": 20170
    },
    {
      "epoch": 5.914419695193435,
      "grad_norm": 0.6342999339103699,
      "learning_rate": 0.000545681424775444,
      "loss": 0.1212,
      "step": 20180
    },
    {
      "epoch": 5.917350527549824,
      "grad_norm": 0.4885539412498474,
      "learning_rate": 0.0005455976284878556,
      "loss": 0.0884,
      "step": 20190
    },
    {
      "epoch": 5.920281359906213,
      "grad_norm": 0.5975760221481323,
      "learning_rate": 0.0005455138322002673,
      "loss": 0.1052,
      "step": 20200
    },
    {
      "epoch": 5.923212192262603,
      "grad_norm": 1.2776199579238892,
      "learning_rate": 0.0005454300359126789,
      "loss": 0.1159,
      "step": 20210
    },
    {
      "epoch": 5.926143024618992,
      "grad_norm": 1.0050417184829712,
      "learning_rate": 0.0005453462396250905,
      "loss": 0.1277,
      "step": 20220
    },
    {
      "epoch": 5.929073856975381,
      "grad_norm": 1.5839133262634277,
      "learning_rate": 0.0005452624433375022,
      "loss": 0.1352,
      "step": 20230
    },
    {
      "epoch": 5.93200468933177,
      "grad_norm": 0.9006708264350891,
      "learning_rate": 0.0005451786470499138,
      "loss": 0.1176,
      "step": 20240
    },
    {
      "epoch": 5.93493552168816,
      "grad_norm": 1.0520004034042358,
      "learning_rate": 0.0005450948507623255,
      "loss": 0.1313,
      "step": 20250
    },
    {
      "epoch": 5.937866354044549,
      "grad_norm": 1.2610946893692017,
      "learning_rate": 0.0005450110544747371,
      "loss": 0.1475,
      "step": 20260
    },
    {
      "epoch": 5.940797186400938,
      "grad_norm": 0.6139042377471924,
      "learning_rate": 0.0005449272581871487,
      "loss": 0.1316,
      "step": 20270
    },
    {
      "epoch": 5.943728018757327,
      "grad_norm": 0.795705258846283,
      "learning_rate": 0.0005448434618995603,
      "loss": 0.1345,
      "step": 20280
    },
    {
      "epoch": 5.946658851113717,
      "grad_norm": 1.1193310022354126,
      "learning_rate": 0.0005447596656119719,
      "loss": 0.1596,
      "step": 20290
    },
    {
      "epoch": 5.949589683470106,
      "grad_norm": 0.7997594475746155,
      "learning_rate": 0.0005446758693243836,
      "loss": 0.1153,
      "step": 20300
    },
    {
      "epoch": 5.952520515826495,
      "grad_norm": 1.487941861152649,
      "learning_rate": 0.0005445920730367952,
      "loss": 0.1386,
      "step": 20310
    },
    {
      "epoch": 5.9554513481828835,
      "grad_norm": 1.2579479217529297,
      "learning_rate": 0.0005445082767492069,
      "loss": 0.1418,
      "step": 20320
    },
    {
      "epoch": 5.958382180539273,
      "grad_norm": 0.8659457564353943,
      "learning_rate": 0.0005444244804616185,
      "loss": 0.1226,
      "step": 20330
    },
    {
      "epoch": 5.961313012895662,
      "grad_norm": 0.8369041085243225,
      "learning_rate": 0.0005443406841740301,
      "loss": 0.1289,
      "step": 20340
    },
    {
      "epoch": 5.964243845252051,
      "grad_norm": 0.8136445879936218,
      "learning_rate": 0.0005442568878864418,
      "loss": 0.1291,
      "step": 20350
    },
    {
      "epoch": 5.9671746776084404,
      "grad_norm": 0.7389373779296875,
      "learning_rate": 0.0005441730915988534,
      "loss": 0.1226,
      "step": 20360
    },
    {
      "epoch": 5.97010550996483,
      "grad_norm": 0.44309934973716736,
      "learning_rate": 0.000544089295311265,
      "loss": 0.1464,
      "step": 20370
    },
    {
      "epoch": 5.973036342321219,
      "grad_norm": 0.2934356927871704,
      "learning_rate": 0.0005440054990236766,
      "loss": 0.1279,
      "step": 20380
    },
    {
      "epoch": 5.975967174677608,
      "grad_norm": 0.9983945488929749,
      "learning_rate": 0.0005439217027360883,
      "loss": 0.121,
      "step": 20390
    },
    {
      "epoch": 5.978898007033997,
      "grad_norm": 0.6398016810417175,
      "learning_rate": 0.0005438379064485,
      "loss": 0.1219,
      "step": 20400
    },
    {
      "epoch": 5.981828839390387,
      "grad_norm": 1.383539080619812,
      "learning_rate": 0.0005437541101609116,
      "loss": 0.1162,
      "step": 20410
    },
    {
      "epoch": 5.984759671746776,
      "grad_norm": 0.7892111539840698,
      "learning_rate": 0.0005436703138733232,
      "loss": 0.1454,
      "step": 20420
    },
    {
      "epoch": 5.987690504103165,
      "grad_norm": 0.5502342581748962,
      "learning_rate": 0.0005435865175857348,
      "loss": 0.1058,
      "step": 20430
    },
    {
      "epoch": 5.990621336459554,
      "grad_norm": 0.4172750413417816,
      "learning_rate": 0.0005435027212981465,
      "loss": 0.1422,
      "step": 20440
    },
    {
      "epoch": 5.993552168815944,
      "grad_norm": 0.7186890244483948,
      "learning_rate": 0.0005434189250105581,
      "loss": 0.1374,
      "step": 20450
    },
    {
      "epoch": 5.996483001172333,
      "grad_norm": 0.8840590715408325,
      "learning_rate": 0.0005433351287229697,
      "loss": 0.131,
      "step": 20460
    },
    {
      "epoch": 5.999413833528722,
      "grad_norm": 0.7256273031234741,
      "learning_rate": 0.0005432513324353814,
      "loss": 0.1565,
      "step": 20470
    },
    {
      "epoch": 6.0,
      "eval_accuracy": 0.5133531157270029,
      "eval_f1_macro": 0.5052123236603647,
      "eval_f1_micro": 0.6442625805328226,
      "eval_f1_weighted": 0.6169238485271663,
      "eval_loss": 0.12211699783802032,
      "eval_roc_auc": 0.7638802936900476,
      "eval_runtime": 194.9447,
      "eval_samples_per_second": 15.558,
      "eval_steps_per_second": 1.949,
      "step": 20472
    },
    {
      "epoch": 6.002344665885111,
      "grad_norm": 0.5274643301963806,
      "learning_rate": 0.0005431675361477929,
      "loss": 0.1385,
      "step": 20480
    },
    {
      "epoch": 6.005275498241501,
      "grad_norm": 0.7518572807312012,
      "learning_rate": 0.0005430837398602047,
      "loss": 0.0935,
      "step": 20490
    },
    {
      "epoch": 6.00820633059789,
      "grad_norm": 1.291403889656067,
      "learning_rate": 0.0005429999435726163,
      "loss": 0.1244,
      "step": 20500
    },
    {
      "epoch": 6.011137162954279,
      "grad_norm": 1.6162688732147217,
      "learning_rate": 0.0005429161472850279,
      "loss": 0.1187,
      "step": 20510
    },
    {
      "epoch": 6.014067995310668,
      "grad_norm": 1.542545199394226,
      "learning_rate": 0.0005428323509974396,
      "loss": 0.1139,
      "step": 20520
    },
    {
      "epoch": 6.016998827667058,
      "grad_norm": 0.7937474250793457,
      "learning_rate": 0.0005427485547098511,
      "loss": 0.1426,
      "step": 20530
    },
    {
      "epoch": 6.019929660023447,
      "grad_norm": 0.6933817863464355,
      "learning_rate": 0.0005426647584222628,
      "loss": 0.1134,
      "step": 20540
    },
    {
      "epoch": 6.022860492379836,
      "grad_norm": 0.5127484202384949,
      "learning_rate": 0.0005425809621346744,
      "loss": 0.1277,
      "step": 20550
    },
    {
      "epoch": 6.025791324736225,
      "grad_norm": 0.7021339535713196,
      "learning_rate": 0.0005424971658470861,
      "loss": 0.1292,
      "step": 20560
    },
    {
      "epoch": 6.028722157092615,
      "grad_norm": 0.6641174554824829,
      "learning_rate": 0.0005424133695594977,
      "loss": 0.1012,
      "step": 20570
    },
    {
      "epoch": 6.031652989449004,
      "grad_norm": 1.1040226221084595,
      "learning_rate": 0.0005423295732719094,
      "loss": 0.1317,
      "step": 20580
    },
    {
      "epoch": 6.034583821805393,
      "grad_norm": 1.1050565242767334,
      "learning_rate": 0.000542245776984321,
      "loss": 0.1257,
      "step": 20590
    },
    {
      "epoch": 6.037514654161782,
      "grad_norm": 0.5908833146095276,
      "learning_rate": 0.0005421619806967326,
      "loss": 0.112,
      "step": 20600
    },
    {
      "epoch": 6.040445486518172,
      "grad_norm": 1.2454324960708618,
      "learning_rate": 0.0005420781844091443,
      "loss": 0.1115,
      "step": 20610
    },
    {
      "epoch": 6.04337631887456,
      "grad_norm": 0.7572891712188721,
      "learning_rate": 0.0005419943881215559,
      "loss": 0.1213,
      "step": 20620
    },
    {
      "epoch": 6.046307151230949,
      "grad_norm": 0.972054123878479,
      "learning_rate": 0.0005419105918339675,
      "loss": 0.0928,
      "step": 20630
    },
    {
      "epoch": 6.049237983587338,
      "grad_norm": 0.630652666091919,
      "learning_rate": 0.0005418267955463791,
      "loss": 0.1068,
      "step": 20640
    },
    {
      "epoch": 6.052168815943728,
      "grad_norm": 0.9965735077857971,
      "learning_rate": 0.0005417429992587907,
      "loss": 0.0963,
      "step": 20650
    },
    {
      "epoch": 6.055099648300117,
      "grad_norm": 0.7058550119400024,
      "learning_rate": 0.0005416592029712025,
      "loss": 0.1209,
      "step": 20660
    },
    {
      "epoch": 6.058030480656506,
      "grad_norm": 1.1134010553359985,
      "learning_rate": 0.0005415754066836141,
      "loss": 0.132,
      "step": 20670
    },
    {
      "epoch": 6.060961313012895,
      "grad_norm": 0.44938361644744873,
      "learning_rate": 0.0005414916103960257,
      "loss": 0.1125,
      "step": 20680
    },
    {
      "epoch": 6.063892145369285,
      "grad_norm": 0.521569550037384,
      "learning_rate": 0.0005414078141084373,
      "loss": 0.1181,
      "step": 20690
    },
    {
      "epoch": 6.066822977725674,
      "grad_norm": 1.0585002899169922,
      "learning_rate": 0.0005413240178208489,
      "loss": 0.1127,
      "step": 20700
    },
    {
      "epoch": 6.069753810082063,
      "grad_norm": 0.7913745045661926,
      "learning_rate": 0.0005412402215332606,
      "loss": 0.1367,
      "step": 20710
    },
    {
      "epoch": 6.072684642438452,
      "grad_norm": 1.1583064794540405,
      "learning_rate": 0.0005411564252456722,
      "loss": 0.1154,
      "step": 20720
    },
    {
      "epoch": 6.075615474794842,
      "grad_norm": 0.9468548893928528,
      "learning_rate": 0.0005410726289580838,
      "loss": 0.1097,
      "step": 20730
    },
    {
      "epoch": 6.078546307151231,
      "grad_norm": 0.7807832360267639,
      "learning_rate": 0.0005409888326704954,
      "loss": 0.1465,
      "step": 20740
    },
    {
      "epoch": 6.08147713950762,
      "grad_norm": 0.8951379060745239,
      "learning_rate": 0.000540905036382907,
      "loss": 0.1189,
      "step": 20750
    },
    {
      "epoch": 6.084407971864009,
      "grad_norm": 0.6772780418395996,
      "learning_rate": 0.0005408212400953188,
      "loss": 0.1158,
      "step": 20760
    },
    {
      "epoch": 6.087338804220399,
      "grad_norm": 0.5604348182678223,
      "learning_rate": 0.0005407374438077304,
      "loss": 0.1169,
      "step": 20770
    },
    {
      "epoch": 6.090269636576788,
      "grad_norm": 1.4371635913848877,
      "learning_rate": 0.000540653647520142,
      "loss": 0.0905,
      "step": 20780
    },
    {
      "epoch": 6.093200468933177,
      "grad_norm": 1.954490065574646,
      "learning_rate": 0.0005405698512325536,
      "loss": 0.1385,
      "step": 20790
    },
    {
      "epoch": 6.096131301289566,
      "grad_norm": 1.2453798055648804,
      "learning_rate": 0.0005404860549449652,
      "loss": 0.1258,
      "step": 20800
    },
    {
      "epoch": 6.099062133645956,
      "grad_norm": 1.3259299993515015,
      "learning_rate": 0.0005404022586573769,
      "loss": 0.129,
      "step": 20810
    },
    {
      "epoch": 6.101992966002345,
      "grad_norm": 0.4453980624675751,
      "learning_rate": 0.0005403184623697885,
      "loss": 0.1154,
      "step": 20820
    },
    {
      "epoch": 6.104923798358734,
      "grad_norm": 0.5759934782981873,
      "learning_rate": 0.0005402346660822001,
      "loss": 0.122,
      "step": 20830
    },
    {
      "epoch": 6.107854630715123,
      "grad_norm": 1.0275412797927856,
      "learning_rate": 0.0005401508697946118,
      "loss": 0.1219,
      "step": 20840
    },
    {
      "epoch": 6.110785463071513,
      "grad_norm": 0.5987879633903503,
      "learning_rate": 0.0005400670735070234,
      "loss": 0.1065,
      "step": 20850
    },
    {
      "epoch": 6.113716295427902,
      "grad_norm": 0.509411633014679,
      "learning_rate": 0.0005399832772194351,
      "loss": 0.1057,
      "step": 20860
    },
    {
      "epoch": 6.116647127784291,
      "grad_norm": 0.6980689167976379,
      "learning_rate": 0.0005398994809318467,
      "loss": 0.1029,
      "step": 20870
    },
    {
      "epoch": 6.11957796014068,
      "grad_norm": 0.6999555826187134,
      "learning_rate": 0.0005398156846442584,
      "loss": 0.1229,
      "step": 20880
    },
    {
      "epoch": 6.12250879249707,
      "grad_norm": 0.9958440661430359,
      "learning_rate": 0.0005397318883566699,
      "loss": 0.1468,
      "step": 20890
    },
    {
      "epoch": 6.125439624853458,
      "grad_norm": 0.7693983912467957,
      "learning_rate": 0.0005396480920690816,
      "loss": 0.1053,
      "step": 20900
    },
    {
      "epoch": 6.128370457209847,
      "grad_norm": 0.7917369604110718,
      "learning_rate": 0.0005395642957814932,
      "loss": 0.119,
      "step": 20910
    },
    {
      "epoch": 6.131301289566236,
      "grad_norm": 2.1234347820281982,
      "learning_rate": 0.0005394804994939048,
      "loss": 0.1357,
      "step": 20920
    },
    {
      "epoch": 6.134232121922626,
      "grad_norm": 0.6159390807151794,
      "learning_rate": 0.0005393967032063166,
      "loss": 0.1102,
      "step": 20930
    },
    {
      "epoch": 6.137162954279015,
      "grad_norm": 0.8082758784294128,
      "learning_rate": 0.0005393129069187282,
      "loss": 0.1006,
      "step": 20940
    },
    {
      "epoch": 6.140093786635404,
      "grad_norm": 0.7986807227134705,
      "learning_rate": 0.0005392291106311398,
      "loss": 0.1303,
      "step": 20950
    },
    {
      "epoch": 6.143024618991793,
      "grad_norm": 0.7578585147857666,
      "learning_rate": 0.0005391453143435514,
      "loss": 0.1188,
      "step": 20960
    },
    {
      "epoch": 6.145955451348183,
      "grad_norm": 0.8770901560783386,
      "learning_rate": 0.000539061518055963,
      "loss": 0.1161,
      "step": 20970
    },
    {
      "epoch": 6.148886283704572,
      "grad_norm": 2.0356853008270264,
      "learning_rate": 0.0005389777217683747,
      "loss": 0.1293,
      "step": 20980
    },
    {
      "epoch": 6.151817116060961,
      "grad_norm": 0.694066047668457,
      "learning_rate": 0.0005388939254807863,
      "loss": 0.1237,
      "step": 20990
    },
    {
      "epoch": 6.15474794841735,
      "grad_norm": 1.0737533569335938,
      "learning_rate": 0.0005388101291931979,
      "loss": 0.1337,
      "step": 21000
    },
    {
      "epoch": 6.15767878077374,
      "grad_norm": 0.6836756467819214,
      "learning_rate": 0.0005387263329056096,
      "loss": 0.1224,
      "step": 21010
    },
    {
      "epoch": 6.160609613130129,
      "grad_norm": 0.7659578919410706,
      "learning_rate": 0.0005386425366180212,
      "loss": 0.135,
      "step": 21020
    },
    {
      "epoch": 6.163540445486518,
      "grad_norm": 1.383249282836914,
      "learning_rate": 0.0005385587403304329,
      "loss": 0.1289,
      "step": 21030
    },
    {
      "epoch": 6.166471277842907,
      "grad_norm": 0.8881345987319946,
      "learning_rate": 0.0005384749440428445,
      "loss": 0.1255,
      "step": 21040
    },
    {
      "epoch": 6.169402110199297,
      "grad_norm": 0.8200391530990601,
      "learning_rate": 0.0005383911477552561,
      "loss": 0.1059,
      "step": 21050
    },
    {
      "epoch": 6.172332942555686,
      "grad_norm": 0.8922130465507507,
      "learning_rate": 0.0005383073514676677,
      "loss": 0.124,
      "step": 21060
    },
    {
      "epoch": 6.175263774912075,
      "grad_norm": 1.4189221858978271,
      "learning_rate": 0.0005382235551800794,
      "loss": 0.1118,
      "step": 21070
    },
    {
      "epoch": 6.178194607268464,
      "grad_norm": 0.5806325674057007,
      "learning_rate": 0.000538139758892491,
      "loss": 0.1505,
      "step": 21080
    },
    {
      "epoch": 6.181125439624854,
      "grad_norm": 1.43015456199646,
      "learning_rate": 0.0005380559626049026,
      "loss": 0.1406,
      "step": 21090
    },
    {
      "epoch": 6.184056271981243,
      "grad_norm": 0.7859344482421875,
      "learning_rate": 0.0005379721663173143,
      "loss": 0.1262,
      "step": 21100
    },
    {
      "epoch": 6.186987104337632,
      "grad_norm": 0.8693854808807373,
      "learning_rate": 0.0005378883700297259,
      "loss": 0.0931,
      "step": 21110
    },
    {
      "epoch": 6.189917936694021,
      "grad_norm": 1.1008232831954956,
      "learning_rate": 0.0005378045737421376,
      "loss": 0.113,
      "step": 21120
    },
    {
      "epoch": 6.192848769050411,
      "grad_norm": 0.3703156113624573,
      "learning_rate": 0.0005377207774545492,
      "loss": 0.1279,
      "step": 21130
    },
    {
      "epoch": 6.1957796014068,
      "grad_norm": 1.3619972467422485,
      "learning_rate": 0.0005376369811669608,
      "loss": 0.1102,
      "step": 21140
    },
    {
      "epoch": 6.198710433763189,
      "grad_norm": 0.7068637013435364,
      "learning_rate": 0.0005375531848793724,
      "loss": 0.1297,
      "step": 21150
    },
    {
      "epoch": 6.201641266119578,
      "grad_norm": 0.7477927207946777,
      "learning_rate": 0.000537469388591784,
      "loss": 0.1201,
      "step": 21160
    },
    {
      "epoch": 6.204572098475968,
      "grad_norm": 1.4380910396575928,
      "learning_rate": 0.0005373855923041957,
      "loss": 0.1086,
      "step": 21170
    },
    {
      "epoch": 6.207502930832357,
      "grad_norm": 0.8913083076477051,
      "learning_rate": 0.0005373017960166073,
      "loss": 0.1113,
      "step": 21180
    },
    {
      "epoch": 6.210433763188745,
      "grad_norm": 1.3904861211776733,
      "learning_rate": 0.000537217999729019,
      "loss": 0.1157,
      "step": 21190
    },
    {
      "epoch": 6.213364595545134,
      "grad_norm": 0.9320228099822998,
      "learning_rate": 0.0005371342034414306,
      "loss": 0.1418,
      "step": 21200
    },
    {
      "epoch": 6.216295427901524,
      "grad_norm": 0.670960009098053,
      "learning_rate": 0.0005370504071538422,
      "loss": 0.1171,
      "step": 21210
    },
    {
      "epoch": 6.219226260257913,
      "grad_norm": 0.9708728194236755,
      "learning_rate": 0.0005369666108662539,
      "loss": 0.11,
      "step": 21220
    },
    {
      "epoch": 6.222157092614302,
      "grad_norm": 0.551527202129364,
      "learning_rate": 0.0005368828145786655,
      "loss": 0.0931,
      "step": 21230
    },
    {
      "epoch": 6.225087924970691,
      "grad_norm": 1.5040879249572754,
      "learning_rate": 0.0005367990182910772,
      "loss": 0.1306,
      "step": 21240
    },
    {
      "epoch": 6.228018757327081,
      "grad_norm": 0.7073246240615845,
      "learning_rate": 0.0005367152220034887,
      "loss": 0.1291,
      "step": 21250
    },
    {
      "epoch": 6.23094958968347,
      "grad_norm": 0.9173598289489746,
      "learning_rate": 0.0005366314257159003,
      "loss": 0.1158,
      "step": 21260
    },
    {
      "epoch": 6.233880422039859,
      "grad_norm": 0.9014860987663269,
      "learning_rate": 0.000536547629428312,
      "loss": 0.1288,
      "step": 21270
    },
    {
      "epoch": 6.236811254396248,
      "grad_norm": 0.9326080679893494,
      "learning_rate": 0.0005364638331407237,
      "loss": 0.1385,
      "step": 21280
    },
    {
      "epoch": 6.239742086752638,
      "grad_norm": 1.3352571725845337,
      "learning_rate": 0.0005363800368531354,
      "loss": 0.115,
      "step": 21290
    },
    {
      "epoch": 6.242672919109027,
      "grad_norm": 0.7893192768096924,
      "learning_rate": 0.0005362962405655469,
      "loss": 0.1209,
      "step": 21300
    },
    {
      "epoch": 6.245603751465416,
      "grad_norm": 1.1707135438919067,
      "learning_rate": 0.0005362124442779585,
      "loss": 0.1313,
      "step": 21310
    },
    {
      "epoch": 6.248534583821805,
      "grad_norm": 1.318389892578125,
      "learning_rate": 0.0005361286479903702,
      "loss": 0.1304,
      "step": 21320
    },
    {
      "epoch": 6.251465416178195,
      "grad_norm": 0.5308125019073486,
      "learning_rate": 0.0005360448517027818,
      "loss": 0.1482,
      "step": 21330
    },
    {
      "epoch": 6.254396248534584,
      "grad_norm": 0.723980188369751,
      "learning_rate": 0.0005359610554151935,
      "loss": 0.1361,
      "step": 21340
    },
    {
      "epoch": 6.257327080890973,
      "grad_norm": 1.5719813108444214,
      "learning_rate": 0.0005358772591276051,
      "loss": 0.1171,
      "step": 21350
    },
    {
      "epoch": 6.260257913247362,
      "grad_norm": 0.45988771319389343,
      "learning_rate": 0.0005357934628400166,
      "loss": 0.1126,
      "step": 21360
    },
    {
      "epoch": 6.263188745603752,
      "grad_norm": 0.6263363361358643,
      "learning_rate": 0.0005357096665524284,
      "loss": 0.1257,
      "step": 21370
    },
    {
      "epoch": 6.266119577960141,
      "grad_norm": 1.4040136337280273,
      "learning_rate": 0.00053562587026484,
      "loss": 0.1109,
      "step": 21380
    },
    {
      "epoch": 6.26905041031653,
      "grad_norm": 1.0758371353149414,
      "learning_rate": 0.0005355420739772517,
      "loss": 0.136,
      "step": 21390
    },
    {
      "epoch": 6.271981242672919,
      "grad_norm": 2.035226583480835,
      "learning_rate": 0.0005354582776896633,
      "loss": 0.1333,
      "step": 21400
    },
    {
      "epoch": 6.274912075029309,
      "grad_norm": 0.447725385427475,
      "learning_rate": 0.0005353744814020748,
      "loss": 0.1091,
      "step": 21410
    },
    {
      "epoch": 6.277842907385698,
      "grad_norm": 0.8893126845359802,
      "learning_rate": 0.0005352906851144865,
      "loss": 0.1202,
      "step": 21420
    },
    {
      "epoch": 6.280773739742087,
      "grad_norm": 1.428743839263916,
      "learning_rate": 0.0005352068888268981,
      "loss": 0.1195,
      "step": 21430
    },
    {
      "epoch": 6.283704572098476,
      "grad_norm": 1.0946869850158691,
      "learning_rate": 0.0005351230925393099,
      "loss": 0.12,
      "step": 21440
    },
    {
      "epoch": 6.286635404454866,
      "grad_norm": 0.8970282077789307,
      "learning_rate": 0.0005350392962517215,
      "loss": 0.0883,
      "step": 21450
    },
    {
      "epoch": 6.289566236811254,
      "grad_norm": 0.6368216276168823,
      "learning_rate": 0.0005349554999641331,
      "loss": 0.1318,
      "step": 21460
    },
    {
      "epoch": 6.292497069167643,
      "grad_norm": 0.7290704846382141,
      "learning_rate": 0.0005348717036765447,
      "loss": 0.1331,
      "step": 21470
    },
    {
      "epoch": 6.295427901524032,
      "grad_norm": 1.377102255821228,
      "learning_rate": 0.0005347879073889563,
      "loss": 0.1159,
      "step": 21480
    },
    {
      "epoch": 6.298358733880422,
      "grad_norm": 0.4516224265098572,
      "learning_rate": 0.000534704111101368,
      "loss": 0.1056,
      "step": 21490
    },
    {
      "epoch": 6.301289566236811,
      "grad_norm": 0.8717057108879089,
      "learning_rate": 0.0005346203148137796,
      "loss": 0.1092,
      "step": 21500
    },
    {
      "epoch": 6.3042203985932,
      "grad_norm": 0.8652967214584351,
      "learning_rate": 0.0005345365185261912,
      "loss": 0.1196,
      "step": 21510
    },
    {
      "epoch": 6.307151230949589,
      "grad_norm": 0.8930631875991821,
      "learning_rate": 0.0005344527222386028,
      "loss": 0.1193,
      "step": 21520
    },
    {
      "epoch": 6.310082063305979,
      "grad_norm": 0.7760114669799805,
      "learning_rate": 0.0005343689259510144,
      "loss": 0.121,
      "step": 21530
    },
    {
      "epoch": 6.313012895662368,
      "grad_norm": 0.7959727644920349,
      "learning_rate": 0.0005342851296634262,
      "loss": 0.1184,
      "step": 21540
    },
    {
      "epoch": 6.315943728018757,
      "grad_norm": 0.805739164352417,
      "learning_rate": 0.0005342013333758378,
      "loss": 0.133,
      "step": 21550
    },
    {
      "epoch": 6.318874560375146,
      "grad_norm": 0.9452225565910339,
      "learning_rate": 0.0005341175370882494,
      "loss": 0.1589,
      "step": 21560
    },
    {
      "epoch": 6.321805392731536,
      "grad_norm": 1.2539992332458496,
      "learning_rate": 0.000534033740800661,
      "loss": 0.1337,
      "step": 21570
    },
    {
      "epoch": 6.324736225087925,
      "grad_norm": 0.5440332889556885,
      "learning_rate": 0.0005339499445130726,
      "loss": 0.1138,
      "step": 21580
    },
    {
      "epoch": 6.327667057444314,
      "grad_norm": 0.5356559753417969,
      "learning_rate": 0.0005338661482254843,
      "loss": 0.1056,
      "step": 21590
    },
    {
      "epoch": 6.330597889800703,
      "grad_norm": 0.5328781008720398,
      "learning_rate": 0.0005337823519378959,
      "loss": 0.12,
      "step": 21600
    },
    {
      "epoch": 6.333528722157093,
      "grad_norm": 0.5467849373817444,
      "learning_rate": 0.0005336985556503075,
      "loss": 0.1238,
      "step": 21610
    },
    {
      "epoch": 6.336459554513482,
      "grad_norm": 0.7639093399047852,
      "learning_rate": 0.0005336147593627192,
      "loss": 0.086,
      "step": 21620
    },
    {
      "epoch": 6.339390386869871,
      "grad_norm": 1.7709250450134277,
      "learning_rate": 0.0005335309630751309,
      "loss": 0.1273,
      "step": 21630
    },
    {
      "epoch": 6.34232121922626,
      "grad_norm": 0.5518251657485962,
      "learning_rate": 0.0005334471667875425,
      "loss": 0.1162,
      "step": 21640
    },
    {
      "epoch": 6.34525205158265,
      "grad_norm": 1.3209995031356812,
      "learning_rate": 0.0005333633704999541,
      "loss": 0.1537,
      "step": 21650
    },
    {
      "epoch": 6.348182883939039,
      "grad_norm": 1.2025693655014038,
      "learning_rate": 0.0005332795742123657,
      "loss": 0.1406,
      "step": 21660
    },
    {
      "epoch": 6.351113716295428,
      "grad_norm": 0.7544507384300232,
      "learning_rate": 0.0005331957779247773,
      "loss": 0.1447,
      "step": 21670
    },
    {
      "epoch": 6.354044548651817,
      "grad_norm": 0.7717786431312561,
      "learning_rate": 0.000533111981637189,
      "loss": 0.1315,
      "step": 21680
    },
    {
      "epoch": 6.356975381008207,
      "grad_norm": 0.5964828133583069,
      "learning_rate": 0.0005330281853496006,
      "loss": 0.1038,
      "step": 21690
    },
    {
      "epoch": 6.359906213364596,
      "grad_norm": 0.9177064299583435,
      "learning_rate": 0.0005329443890620122,
      "loss": 0.1126,
      "step": 21700
    },
    {
      "epoch": 6.362837045720985,
      "grad_norm": 0.528215765953064,
      "learning_rate": 0.0005328605927744239,
      "loss": 0.1314,
      "step": 21710
    },
    {
      "epoch": 6.365767878077374,
      "grad_norm": 0.7425424456596375,
      "learning_rate": 0.0005327767964868355,
      "loss": 0.1165,
      "step": 21720
    },
    {
      "epoch": 6.368698710433764,
      "grad_norm": 0.7270386219024658,
      "learning_rate": 0.0005326930001992472,
      "loss": 0.1319,
      "step": 21730
    },
    {
      "epoch": 6.371629542790153,
      "grad_norm": 0.9041935801506042,
      "learning_rate": 0.0005326092039116588,
      "loss": 0.1266,
      "step": 21740
    },
    {
      "epoch": 6.374560375146542,
      "grad_norm": 0.8429412245750427,
      "learning_rate": 0.0005325254076240704,
      "loss": 0.1267,
      "step": 21750
    },
    {
      "epoch": 6.377491207502931,
      "grad_norm": 0.7994561195373535,
      "learning_rate": 0.0005324416113364821,
      "loss": 0.1117,
      "step": 21760
    },
    {
      "epoch": 6.38042203985932,
      "grad_norm": 0.7891921401023865,
      "learning_rate": 0.0005323578150488936,
      "loss": 0.119,
      "step": 21770
    },
    {
      "epoch": 6.383352872215709,
      "grad_norm": 0.7017212510108948,
      "learning_rate": 0.0005322740187613053,
      "loss": 0.1093,
      "step": 21780
    },
    {
      "epoch": 6.386283704572098,
      "grad_norm": 0.5098856091499329,
      "learning_rate": 0.000532190222473717,
      "loss": 0.1428,
      "step": 21790
    },
    {
      "epoch": 6.389214536928487,
      "grad_norm": 1.4421091079711914,
      "learning_rate": 0.0005321064261861287,
      "loss": 0.1407,
      "step": 21800
    },
    {
      "epoch": 6.392145369284877,
      "grad_norm": 0.7451670169830322,
      "learning_rate": 0.0005320226298985403,
      "loss": 0.1177,
      "step": 21810
    },
    {
      "epoch": 6.395076201641266,
      "grad_norm": 1.199355959892273,
      "learning_rate": 0.0005319388336109519,
      "loss": 0.1452,
      "step": 21820
    },
    {
      "epoch": 6.398007033997655,
      "grad_norm": 0.3685338497161865,
      "learning_rate": 0.0005318550373233635,
      "loss": 0.1085,
      "step": 21830
    },
    {
      "epoch": 6.400937866354044,
      "grad_norm": 0.8738268613815308,
      "learning_rate": 0.0005317712410357751,
      "loss": 0.1155,
      "step": 21840
    },
    {
      "epoch": 6.403868698710434,
      "grad_norm": 0.4541265368461609,
      "learning_rate": 0.0005316874447481868,
      "loss": 0.1108,
      "step": 21850
    },
    {
      "epoch": 6.406799531066823,
      "grad_norm": 0.6212108731269836,
      "learning_rate": 0.0005316036484605984,
      "loss": 0.1167,
      "step": 21860
    },
    {
      "epoch": 6.409730363423212,
      "grad_norm": 0.71646648645401,
      "learning_rate": 0.00053151985217301,
      "loss": 0.1001,
      "step": 21870
    },
    {
      "epoch": 6.412661195779601,
      "grad_norm": 0.7531129121780396,
      "learning_rate": 0.0005314360558854217,
      "loss": 0.1135,
      "step": 21880
    },
    {
      "epoch": 6.415592028135991,
      "grad_norm": 1.3554564714431763,
      "learning_rate": 0.0005313522595978333,
      "loss": 0.1351,
      "step": 21890
    },
    {
      "epoch": 6.41852286049238,
      "grad_norm": 0.4460228979587555,
      "learning_rate": 0.000531268463310245,
      "loss": 0.1263,
      "step": 21900
    },
    {
      "epoch": 6.421453692848769,
      "grad_norm": 0.8006187677383423,
      "learning_rate": 0.0005311846670226566,
      "loss": 0.088,
      "step": 21910
    },
    {
      "epoch": 6.424384525205158,
      "grad_norm": 1.3398529291152954,
      "learning_rate": 0.0005311008707350682,
      "loss": 0.1192,
      "step": 21920
    },
    {
      "epoch": 6.427315357561548,
      "grad_norm": 0.8022570013999939,
      "learning_rate": 0.0005310170744474798,
      "loss": 0.1258,
      "step": 21930
    },
    {
      "epoch": 6.430246189917937,
      "grad_norm": 0.7912986874580383,
      "learning_rate": 0.0005309332781598914,
      "loss": 0.1429,
      "step": 21940
    },
    {
      "epoch": 6.433177022274326,
      "grad_norm": 0.5668529868125916,
      "learning_rate": 0.0005308494818723031,
      "loss": 0.1067,
      "step": 21950
    },
    {
      "epoch": 6.436107854630715,
      "grad_norm": 0.3848589062690735,
      "learning_rate": 0.0005307656855847147,
      "loss": 0.1109,
      "step": 21960
    },
    {
      "epoch": 6.439038686987105,
      "grad_norm": 0.804742157459259,
      "learning_rate": 0.0005306818892971264,
      "loss": 0.1044,
      "step": 21970
    },
    {
      "epoch": 6.441969519343494,
      "grad_norm": 0.9012132883071899,
      "learning_rate": 0.000530598093009538,
      "loss": 0.1227,
      "step": 21980
    },
    {
      "epoch": 6.444900351699883,
      "grad_norm": 1.7091703414916992,
      "learning_rate": 0.0005305142967219496,
      "loss": 0.1079,
      "step": 21990
    },
    {
      "epoch": 6.447831184056272,
      "grad_norm": 1.1443321704864502,
      "learning_rate": 0.0005304305004343613,
      "loss": 0.1264,
      "step": 22000
    },
    {
      "epoch": 6.450762016412662,
      "grad_norm": 0.5658637881278992,
      "learning_rate": 0.0005303467041467729,
      "loss": 0.1308,
      "step": 22010
    },
    {
      "epoch": 6.453692848769051,
      "grad_norm": 1.0180076360702515,
      "learning_rate": 0.0005302629078591845,
      "loss": 0.1262,
      "step": 22020
    },
    {
      "epoch": 6.456623681125439,
      "grad_norm": 0.6026532649993896,
      "learning_rate": 0.0005301791115715961,
      "loss": 0.114,
      "step": 22030
    },
    {
      "epoch": 6.459554513481828,
      "grad_norm": 0.4279485046863556,
      "learning_rate": 0.0005300953152840077,
      "loss": 0.1154,
      "step": 22040
    },
    {
      "epoch": 6.462485345838218,
      "grad_norm": 0.9092355966567993,
      "learning_rate": 0.0005300115189964195,
      "loss": 0.1216,
      "step": 22050
    },
    {
      "epoch": 6.465416178194607,
      "grad_norm": 0.7378544807434082,
      "learning_rate": 0.0005299277227088311,
      "loss": 0.1292,
      "step": 22060
    },
    {
      "epoch": 6.468347010550996,
      "grad_norm": 1.1955889463424683,
      "learning_rate": 0.0005298439264212427,
      "loss": 0.1222,
      "step": 22070
    },
    {
      "epoch": 6.471277842907385,
      "grad_norm": 0.53024822473526,
      "learning_rate": 0.0005297601301336543,
      "loss": 0.1254,
      "step": 22080
    },
    {
      "epoch": 6.474208675263775,
      "grad_norm": 0.4491753876209259,
      "learning_rate": 0.0005296763338460659,
      "loss": 0.1149,
      "step": 22090
    },
    {
      "epoch": 6.477139507620164,
      "grad_norm": 0.6862837076187134,
      "learning_rate": 0.0005295925375584776,
      "loss": 0.1274,
      "step": 22100
    },
    {
      "epoch": 6.480070339976553,
      "grad_norm": 0.7048207521438599,
      "learning_rate": 0.0005295087412708892,
      "loss": 0.1124,
      "step": 22110
    },
    {
      "epoch": 6.483001172332942,
      "grad_norm": 0.4849906861782074,
      "learning_rate": 0.0005294249449833009,
      "loss": 0.1142,
      "step": 22120
    },
    {
      "epoch": 6.485932004689332,
      "grad_norm": 1.9332094192504883,
      "learning_rate": 0.0005293411486957124,
      "loss": 0.1337,
      "step": 22130
    },
    {
      "epoch": 6.488862837045721,
      "grad_norm": 1.1961029767990112,
      "learning_rate": 0.0005292573524081242,
      "loss": 0.1176,
      "step": 22140
    },
    {
      "epoch": 6.49179366940211,
      "grad_norm": 0.9151391386985779,
      "learning_rate": 0.0005291735561205358,
      "loss": 0.1079,
      "step": 22150
    },
    {
      "epoch": 6.494724501758499,
      "grad_norm": 0.8120861649513245,
      "learning_rate": 0.0005290897598329474,
      "loss": 0.0981,
      "step": 22160
    },
    {
      "epoch": 6.497655334114889,
      "grad_norm": 0.9181799292564392,
      "learning_rate": 0.0005290059635453591,
      "loss": 0.157,
      "step": 22170
    },
    {
      "epoch": 6.500586166471278,
      "grad_norm": 1.2405415773391724,
      "learning_rate": 0.0005289221672577706,
      "loss": 0.108,
      "step": 22180
    },
    {
      "epoch": 6.503516998827667,
      "grad_norm": 0.9305843114852905,
      "learning_rate": 0.0005288383709701823,
      "loss": 0.1041,
      "step": 22190
    },
    {
      "epoch": 6.506447831184056,
      "grad_norm": 0.5917807817459106,
      "learning_rate": 0.0005287545746825939,
      "loss": 0.1082,
      "step": 22200
    },
    {
      "epoch": 6.509378663540446,
      "grad_norm": 0.7043387293815613,
      "learning_rate": 0.0005286707783950055,
      "loss": 0.1266,
      "step": 22210
    },
    {
      "epoch": 6.512309495896835,
      "grad_norm": 1.6628015041351318,
      "learning_rate": 0.0005285869821074173,
      "loss": 0.1174,
      "step": 22220
    },
    {
      "epoch": 6.515240328253224,
      "grad_norm": 0.7064293026924133,
      "learning_rate": 0.0005285031858198289,
      "loss": 0.0982,
      "step": 22230
    },
    {
      "epoch": 6.518171160609613,
      "grad_norm": 1.179416537284851,
      "learning_rate": 0.0005284193895322405,
      "loss": 0.1145,
      "step": 22240
    },
    {
      "epoch": 6.521101992966003,
      "grad_norm": 0.875758945941925,
      "learning_rate": 0.0005283355932446521,
      "loss": 0.1389,
      "step": 22250
    },
    {
      "epoch": 6.524032825322392,
      "grad_norm": 0.7560788989067078,
      "learning_rate": 0.0005282517969570637,
      "loss": 0.0958,
      "step": 22260
    },
    {
      "epoch": 6.526963657678781,
      "grad_norm": 0.7337411046028137,
      "learning_rate": 0.0005281680006694754,
      "loss": 0.0908,
      "step": 22270
    },
    {
      "epoch": 6.52989449003517,
      "grad_norm": 0.8592016100883484,
      "learning_rate": 0.000528084204381887,
      "loss": 0.1082,
      "step": 22280
    },
    {
      "epoch": 6.5328253223915596,
      "grad_norm": 1.43669593334198,
      "learning_rate": 0.0005280004080942986,
      "loss": 0.1199,
      "step": 22290
    },
    {
      "epoch": 6.535756154747949,
      "grad_norm": 1.0285533666610718,
      "learning_rate": 0.0005279166118067102,
      "loss": 0.1216,
      "step": 22300
    },
    {
      "epoch": 6.538686987104338,
      "grad_norm": 0.4298758804798126,
      "learning_rate": 0.000527832815519122,
      "loss": 0.1185,
      "step": 22310
    },
    {
      "epoch": 6.541617819460727,
      "grad_norm": 0.44823044538497925,
      "learning_rate": 0.0005277490192315336,
      "loss": 0.1198,
      "step": 22320
    },
    {
      "epoch": 6.5445486518171165,
      "grad_norm": 1.4564862251281738,
      "learning_rate": 0.0005276652229439452,
      "loss": 0.1197,
      "step": 22330
    },
    {
      "epoch": 6.547479484173505,
      "grad_norm": 0.7410118579864502,
      "learning_rate": 0.0005275814266563568,
      "loss": 0.1311,
      "step": 22340
    },
    {
      "epoch": 6.550410316529894,
      "grad_norm": 0.615795373916626,
      "learning_rate": 0.0005274976303687684,
      "loss": 0.1012,
      "step": 22350
    },
    {
      "epoch": 6.553341148886283,
      "grad_norm": 0.9223909974098206,
      "learning_rate": 0.0005274138340811801,
      "loss": 0.1118,
      "step": 22360
    },
    {
      "epoch": 6.556271981242673,
      "grad_norm": 0.720700740814209,
      "learning_rate": 0.0005273300377935917,
      "loss": 0.1364,
      "step": 22370
    },
    {
      "epoch": 6.559202813599062,
      "grad_norm": 2.3844127655029297,
      "learning_rate": 0.0005272462415060033,
      "loss": 0.1398,
      "step": 22380
    },
    {
      "epoch": 6.562133645955451,
      "grad_norm": 0.2583869695663452,
      "learning_rate": 0.0005271624452184149,
      "loss": 0.0887,
      "step": 22390
    },
    {
      "epoch": 6.56506447831184,
      "grad_norm": 0.5900037884712219,
      "learning_rate": 0.0005270786489308266,
      "loss": 0.1225,
      "step": 22400
    },
    {
      "epoch": 6.56799531066823,
      "grad_norm": 0.9080224633216858,
      "learning_rate": 0.0005269948526432383,
      "loss": 0.1277,
      "step": 22410
    },
    {
      "epoch": 6.570926143024619,
      "grad_norm": 0.4769621789455414,
      "learning_rate": 0.0005269110563556499,
      "loss": 0.1316,
      "step": 22420
    },
    {
      "epoch": 6.573856975381008,
      "grad_norm": 1.108832597732544,
      "learning_rate": 0.0005268272600680615,
      "loss": 0.1238,
      "step": 22430
    },
    {
      "epoch": 6.576787807737397,
      "grad_norm": 0.9840369820594788,
      "learning_rate": 0.0005267434637804731,
      "loss": 0.1072,
      "step": 22440
    },
    {
      "epoch": 6.579718640093787,
      "grad_norm": 1.2877789735794067,
      "learning_rate": 0.0005266596674928847,
      "loss": 0.1258,
      "step": 22450
    },
    {
      "epoch": 6.582649472450176,
      "grad_norm": 0.6774393320083618,
      "learning_rate": 0.0005265758712052964,
      "loss": 0.0967,
      "step": 22460
    },
    {
      "epoch": 6.585580304806565,
      "grad_norm": 0.6433420181274414,
      "learning_rate": 0.000526492074917708,
      "loss": 0.1328,
      "step": 22470
    },
    {
      "epoch": 6.588511137162954,
      "grad_norm": 2.1469621658325195,
      "learning_rate": 0.0005264082786301196,
      "loss": 0.1296,
      "step": 22480
    },
    {
      "epoch": 6.591441969519344,
      "grad_norm": 0.6200151443481445,
      "learning_rate": 0.0005263244823425313,
      "loss": 0.1203,
      "step": 22490
    },
    {
      "epoch": 6.594372801875733,
      "grad_norm": 0.565528929233551,
      "learning_rate": 0.0005262406860549429,
      "loss": 0.1307,
      "step": 22500
    },
    {
      "epoch": 6.597303634232122,
      "grad_norm": 1.1293429136276245,
      "learning_rate": 0.0005261568897673546,
      "loss": 0.1166,
      "step": 22510
    },
    {
      "epoch": 6.600234466588511,
      "grad_norm": 0.8239080905914307,
      "learning_rate": 0.0005260730934797662,
      "loss": 0.1105,
      "step": 22520
    },
    {
      "epoch": 6.603165298944901,
      "grad_norm": 2.0871026515960693,
      "learning_rate": 0.0005259892971921779,
      "loss": 0.1445,
      "step": 22530
    },
    {
      "epoch": 6.60609613130129,
      "grad_norm": 0.8633973598480225,
      "learning_rate": 0.0005259055009045894,
      "loss": 0.093,
      "step": 22540
    },
    {
      "epoch": 6.609026963657679,
      "grad_norm": 1.516729474067688,
      "learning_rate": 0.000525821704617001,
      "loss": 0.1027,
      "step": 22550
    },
    {
      "epoch": 6.611957796014068,
      "grad_norm": 0.9027478694915771,
      "learning_rate": 0.0005257379083294127,
      "loss": 0.1435,
      "step": 22560
    },
    {
      "epoch": 6.6148886283704575,
      "grad_norm": 0.6156712174415588,
      "learning_rate": 0.0005256541120418244,
      "loss": 0.1215,
      "step": 22570
    },
    {
      "epoch": 6.617819460726847,
      "grad_norm": 0.7295686602592468,
      "learning_rate": 0.0005255703157542361,
      "loss": 0.0941,
      "step": 22580
    },
    {
      "epoch": 6.620750293083235,
      "grad_norm": 1.1680843830108643,
      "learning_rate": 0.0005254865194666477,
      "loss": 0.1286,
      "step": 22590
    },
    {
      "epoch": 6.623681125439624,
      "grad_norm": 1.102866291999817,
      "learning_rate": 0.0005254027231790592,
      "loss": 0.1268,
      "step": 22600
    },
    {
      "epoch": 6.626611957796014,
      "grad_norm": 1.0721967220306396,
      "learning_rate": 0.0005253189268914709,
      "loss": 0.1262,
      "step": 22610
    },
    {
      "epoch": 6.629542790152403,
      "grad_norm": 0.8129352331161499,
      "learning_rate": 0.0005252351306038825,
      "loss": 0.123,
      "step": 22620
    },
    {
      "epoch": 6.632473622508792,
      "grad_norm": 0.931634783744812,
      "learning_rate": 0.0005251513343162942,
      "loss": 0.106,
      "step": 22630
    },
    {
      "epoch": 6.635404454865181,
      "grad_norm": 0.7160994410514832,
      "learning_rate": 0.0005250675380287058,
      "loss": 0.1177,
      "step": 22640
    },
    {
      "epoch": 6.638335287221571,
      "grad_norm": 0.5142033100128174,
      "learning_rate": 0.0005249837417411173,
      "loss": 0.1371,
      "step": 22650
    },
    {
      "epoch": 6.64126611957796,
      "grad_norm": 0.7913150787353516,
      "learning_rate": 0.0005248999454535291,
      "loss": 0.135,
      "step": 22660
    },
    {
      "epoch": 6.644196951934349,
      "grad_norm": 0.9680182933807373,
      "learning_rate": 0.0005248161491659407,
      "loss": 0.1156,
      "step": 22670
    },
    {
      "epoch": 6.647127784290738,
      "grad_norm": 1.1568756103515625,
      "learning_rate": 0.0005247323528783524,
      "loss": 0.1112,
      "step": 22680
    },
    {
      "epoch": 6.650058616647128,
      "grad_norm": 0.47811856865882874,
      "learning_rate": 0.000524648556590764,
      "loss": 0.1441,
      "step": 22690
    },
    {
      "epoch": 6.652989449003517,
      "grad_norm": 0.5283049941062927,
      "learning_rate": 0.0005245647603031756,
      "loss": 0.1153,
      "step": 22700
    },
    {
      "epoch": 6.655920281359906,
      "grad_norm": 1.195639967918396,
      "learning_rate": 0.0005244809640155872,
      "loss": 0.1357,
      "step": 22710
    },
    {
      "epoch": 6.658851113716295,
      "grad_norm": 1.2982453107833862,
      "learning_rate": 0.0005243971677279988,
      "loss": 0.1078,
      "step": 22720
    },
    {
      "epoch": 6.661781946072685,
      "grad_norm": 1.6653655767440796,
      "learning_rate": 0.0005243133714404105,
      "loss": 0.1171,
      "step": 22730
    },
    {
      "epoch": 6.664712778429074,
      "grad_norm": 1.6987791061401367,
      "learning_rate": 0.0005242295751528221,
      "loss": 0.1315,
      "step": 22740
    },
    {
      "epoch": 6.667643610785463,
      "grad_norm": 0.587846040725708,
      "learning_rate": 0.0005241457788652338,
      "loss": 0.1119,
      "step": 22750
    },
    {
      "epoch": 6.670574443141852,
      "grad_norm": 0.6866642236709595,
      "learning_rate": 0.0005240619825776454,
      "loss": 0.1359,
      "step": 22760
    },
    {
      "epoch": 6.673505275498242,
      "grad_norm": 1.001128911972046,
      "learning_rate": 0.000523978186290057,
      "loss": 0.1149,
      "step": 22770
    },
    {
      "epoch": 6.676436107854631,
      "grad_norm": 0.5191981792449951,
      "learning_rate": 0.0005238943900024687,
      "loss": 0.1163,
      "step": 22780
    },
    {
      "epoch": 6.67936694021102,
      "grad_norm": 0.9826027154922485,
      "learning_rate": 0.0005238105937148803,
      "loss": 0.1002,
      "step": 22790
    },
    {
      "epoch": 6.682297772567409,
      "grad_norm": 1.130207896232605,
      "learning_rate": 0.0005237267974272919,
      "loss": 0.1131,
      "step": 22800
    },
    {
      "epoch": 6.685228604923799,
      "grad_norm": 0.4819737672805786,
      "learning_rate": 0.0005236430011397035,
      "loss": 0.1008,
      "step": 22810
    },
    {
      "epoch": 6.688159437280188,
      "grad_norm": 1.0218082666397095,
      "learning_rate": 0.0005235592048521151,
      "loss": 0.1351,
      "step": 22820
    },
    {
      "epoch": 6.691090269636577,
      "grad_norm": 0.7460787892341614,
      "learning_rate": 0.0005234754085645269,
      "loss": 0.1297,
      "step": 22830
    },
    {
      "epoch": 6.694021101992966,
      "grad_norm": 1.028617262840271,
      "learning_rate": 0.0005233916122769385,
      "loss": 0.1524,
      "step": 22840
    },
    {
      "epoch": 6.6969519343493555,
      "grad_norm": 0.7777985334396362,
      "learning_rate": 0.0005233078159893501,
      "loss": 0.1075,
      "step": 22850
    },
    {
      "epoch": 6.699882766705745,
      "grad_norm": 1.416845679283142,
      "learning_rate": 0.0005232240197017617,
      "loss": 0.0912,
      "step": 22860
    },
    {
      "epoch": 6.702813599062134,
      "grad_norm": 0.8446124792098999,
      "learning_rate": 0.0005231402234141734,
      "loss": 0.1075,
      "step": 22870
    },
    {
      "epoch": 6.705744431418523,
      "grad_norm": 2.17828369140625,
      "learning_rate": 0.000523056427126585,
      "loss": 0.1243,
      "step": 22880
    },
    {
      "epoch": 6.7086752637749125,
      "grad_norm": 0.6730513572692871,
      "learning_rate": 0.0005229726308389966,
      "loss": 0.1138,
      "step": 22890
    },
    {
      "epoch": 6.711606096131302,
      "grad_norm": 0.8919965624809265,
      "learning_rate": 0.0005228888345514082,
      "loss": 0.1207,
      "step": 22900
    },
    {
      "epoch": 6.71453692848769,
      "grad_norm": 1.80734121799469,
      "learning_rate": 0.0005228050382638198,
      "loss": 0.1239,
      "step": 22910
    },
    {
      "epoch": 6.717467760844079,
      "grad_norm": 0.7308464050292969,
      "learning_rate": 0.0005227212419762316,
      "loss": 0.116,
      "step": 22920
    },
    {
      "epoch": 6.720398593200469,
      "grad_norm": 0.7504040002822876,
      "learning_rate": 0.0005226374456886432,
      "loss": 0.1142,
      "step": 22930
    },
    {
      "epoch": 6.723329425556858,
      "grad_norm": 0.5327960848808289,
      "learning_rate": 0.0005225536494010548,
      "loss": 0.1244,
      "step": 22940
    },
    {
      "epoch": 6.726260257913247,
      "grad_norm": 0.6090463995933533,
      "learning_rate": 0.0005224698531134664,
      "loss": 0.1032,
      "step": 22950
    },
    {
      "epoch": 6.729191090269636,
      "grad_norm": 1.0692229270935059,
      "learning_rate": 0.000522386056825878,
      "loss": 0.1162,
      "step": 22960
    },
    {
      "epoch": 6.732121922626026,
      "grad_norm": 0.7922887802124023,
      "learning_rate": 0.0005223022605382897,
      "loss": 0.1273,
      "step": 22970
    },
    {
      "epoch": 6.735052754982415,
      "grad_norm": 0.872128963470459,
      "learning_rate": 0.0005222184642507013,
      "loss": 0.1311,
      "step": 22980
    },
    {
      "epoch": 6.737983587338804,
      "grad_norm": 0.7958377599716187,
      "learning_rate": 0.0005221346679631129,
      "loss": 0.1068,
      "step": 22990
    },
    {
      "epoch": 6.740914419695193,
      "grad_norm": 1.0005677938461304,
      "learning_rate": 0.0005220508716755247,
      "loss": 0.1099,
      "step": 23000
    },
    {
      "epoch": 6.743845252051583,
      "grad_norm": 0.5323265194892883,
      "learning_rate": 0.0005219670753879362,
      "loss": 0.1186,
      "step": 23010
    },
    {
      "epoch": 6.746776084407972,
      "grad_norm": 0.47164544463157654,
      "learning_rate": 0.0005218832791003479,
      "loss": 0.1195,
      "step": 23020
    },
    {
      "epoch": 6.749706916764361,
      "grad_norm": 0.6402738094329834,
      "learning_rate": 0.0005217994828127595,
      "loss": 0.1224,
      "step": 23030
    },
    {
      "epoch": 6.75263774912075,
      "grad_norm": 1.369758129119873,
      "learning_rate": 0.0005217156865251712,
      "loss": 0.1016,
      "step": 23040
    },
    {
      "epoch": 6.75556858147714,
      "grad_norm": 0.9867219924926758,
      "learning_rate": 0.0005216318902375828,
      "loss": 0.106,
      "step": 23050
    },
    {
      "epoch": 6.758499413833529,
      "grad_norm": 0.609300434589386,
      "learning_rate": 0.0005215480939499943,
      "loss": 0.0989,
      "step": 23060
    },
    {
      "epoch": 6.761430246189918,
      "grad_norm": 0.8018832802772522,
      "learning_rate": 0.000521464297662406,
      "loss": 0.1329,
      "step": 23070
    },
    {
      "epoch": 6.764361078546307,
      "grad_norm": 0.9599236249923706,
      "learning_rate": 0.0005213805013748176,
      "loss": 0.1198,
      "step": 23080
    },
    {
      "epoch": 6.7672919109026966,
      "grad_norm": 1.0562981367111206,
      "learning_rate": 0.0005212967050872294,
      "loss": 0.1146,
      "step": 23090
    },
    {
      "epoch": 6.770222743259086,
      "grad_norm": 0.5539676547050476,
      "learning_rate": 0.000521212908799641,
      "loss": 0.1326,
      "step": 23100
    },
    {
      "epoch": 6.773153575615475,
      "grad_norm": 0.45720264315605164,
      "learning_rate": 0.0005211291125120526,
      "loss": 0.1121,
      "step": 23110
    },
    {
      "epoch": 6.776084407971864,
      "grad_norm": 0.9359744787216187,
      "learning_rate": 0.0005210453162244642,
      "loss": 0.1261,
      "step": 23120
    },
    {
      "epoch": 6.7790152403282535,
      "grad_norm": 0.3781041204929352,
      "learning_rate": 0.0005209615199368758,
      "loss": 0.1203,
      "step": 23130
    },
    {
      "epoch": 6.781946072684643,
      "grad_norm": 0.8064543008804321,
      "learning_rate": 0.0005208777236492875,
      "loss": 0.1211,
      "step": 23140
    },
    {
      "epoch": 6.784876905041032,
      "grad_norm": 0.9419729709625244,
      "learning_rate": 0.0005207939273616991,
      "loss": 0.1272,
      "step": 23150
    },
    {
      "epoch": 6.78780773739742,
      "grad_norm": 0.38589227199554443,
      "learning_rate": 0.0005207101310741107,
      "loss": 0.1163,
      "step": 23160
    },
    {
      "epoch": 6.79073856975381,
      "grad_norm": 1.4618641138076782,
      "learning_rate": 0.0005206263347865223,
      "loss": 0.1188,
      "step": 23170
    },
    {
      "epoch": 6.793669402110199,
      "grad_norm": 0.6946274042129517,
      "learning_rate": 0.000520542538498934,
      "loss": 0.0923,
      "step": 23180
    },
    {
      "epoch": 6.796600234466588,
      "grad_norm": 0.4145899713039398,
      "learning_rate": 0.0005204587422113457,
      "loss": 0.1026,
      "step": 23190
    },
    {
      "epoch": 6.799531066822977,
      "grad_norm": 1.3657300472259521,
      "learning_rate": 0.0005203749459237573,
      "loss": 0.1092,
      "step": 23200
    },
    {
      "epoch": 6.802461899179367,
      "grad_norm": 0.47956544160842896,
      "learning_rate": 0.0005202911496361689,
      "loss": 0.1149,
      "step": 23210
    },
    {
      "epoch": 6.805392731535756,
      "grad_norm": 0.7006860375404358,
      "learning_rate": 0.0005202073533485805,
      "loss": 0.1252,
      "step": 23220
    },
    {
      "epoch": 6.808323563892145,
      "grad_norm": 1.7569559812545776,
      "learning_rate": 0.0005201235570609921,
      "loss": 0.1309,
      "step": 23230
    },
    {
      "epoch": 6.811254396248534,
      "grad_norm": 0.8714809417724609,
      "learning_rate": 0.0005200397607734038,
      "loss": 0.1264,
      "step": 23240
    },
    {
      "epoch": 6.814185228604924,
      "grad_norm": 1.035255789756775,
      "learning_rate": 0.0005199559644858154,
      "loss": 0.0948,
      "step": 23250
    },
    {
      "epoch": 6.817116060961313,
      "grad_norm": 0.5232076048851013,
      "learning_rate": 0.000519872168198227,
      "loss": 0.1264,
      "step": 23260
    },
    {
      "epoch": 6.820046893317702,
      "grad_norm": 1.0431240797042847,
      "learning_rate": 0.0005197883719106387,
      "loss": 0.1262,
      "step": 23270
    },
    {
      "epoch": 6.822977725674091,
      "grad_norm": 1.174593448638916,
      "learning_rate": 0.0005197045756230503,
      "loss": 0.122,
      "step": 23280
    },
    {
      "epoch": 6.825908558030481,
      "grad_norm": 0.9955304861068726,
      "learning_rate": 0.000519620779335462,
      "loss": 0.1117,
      "step": 23290
    },
    {
      "epoch": 6.82883939038687,
      "grad_norm": 1.1165376901626587,
      "learning_rate": 0.0005195369830478736,
      "loss": 0.1327,
      "step": 23300
    },
    {
      "epoch": 6.831770222743259,
      "grad_norm": 1.0213152170181274,
      "learning_rate": 0.0005194531867602852,
      "loss": 0.113,
      "step": 23310
    },
    {
      "epoch": 6.834701055099648,
      "grad_norm": 1.149231195449829,
      "learning_rate": 0.0005193693904726968,
      "loss": 0.1013,
      "step": 23320
    },
    {
      "epoch": 6.837631887456038,
      "grad_norm": 0.6167883276939392,
      "learning_rate": 0.0005192855941851084,
      "loss": 0.1192,
      "step": 23330
    },
    {
      "epoch": 6.840562719812427,
      "grad_norm": 1.1469064950942993,
      "learning_rate": 0.0005192017978975201,
      "loss": 0.1141,
      "step": 23340
    },
    {
      "epoch": 6.843493552168816,
      "grad_norm": 0.4155070185661316,
      "learning_rate": 0.0005191180016099318,
      "loss": 0.1395,
      "step": 23350
    },
    {
      "epoch": 6.846424384525205,
      "grad_norm": 0.697663426399231,
      "learning_rate": 0.0005190342053223435,
      "loss": 0.1147,
      "step": 23360
    },
    {
      "epoch": 6.8493552168815945,
      "grad_norm": 0.8632169961929321,
      "learning_rate": 0.000518950409034755,
      "loss": 0.1516,
      "step": 23370
    },
    {
      "epoch": 6.852286049237984,
      "grad_norm": 0.6390863060951233,
      "learning_rate": 0.0005188666127471667,
      "loss": 0.1001,
      "step": 23380
    },
    {
      "epoch": 6.855216881594373,
      "grad_norm": 0.7919893264770508,
      "learning_rate": 0.0005187828164595783,
      "loss": 0.1267,
      "step": 23390
    },
    {
      "epoch": 6.858147713950762,
      "grad_norm": 0.52251797914505,
      "learning_rate": 0.0005186990201719899,
      "loss": 0.109,
      "step": 23400
    },
    {
      "epoch": 6.8610785463071515,
      "grad_norm": 1.2731739282608032,
      "learning_rate": 0.0005186152238844016,
      "loss": 0.1194,
      "step": 23410
    },
    {
      "epoch": 6.864009378663541,
      "grad_norm": 0.7515690326690674,
      "learning_rate": 0.0005185314275968131,
      "loss": 0.1005,
      "step": 23420
    },
    {
      "epoch": 6.86694021101993,
      "grad_norm": 1.8904368877410889,
      "learning_rate": 0.0005184476313092248,
      "loss": 0.1198,
      "step": 23430
    },
    {
      "epoch": 6.869871043376319,
      "grad_norm": 0.9370625019073486,
      "learning_rate": 0.0005183638350216365,
      "loss": 0.1046,
      "step": 23440
    },
    {
      "epoch": 6.8728018757327085,
      "grad_norm": 1.0106699466705322,
      "learning_rate": 0.0005182800387340481,
      "loss": 0.126,
      "step": 23450
    },
    {
      "epoch": 6.875732708089098,
      "grad_norm": 1.5547412633895874,
      "learning_rate": 0.0005181962424464598,
      "loss": 0.1261,
      "step": 23460
    },
    {
      "epoch": 6.878663540445487,
      "grad_norm": 1.5524957180023193,
      "learning_rate": 0.0005181124461588714,
      "loss": 0.1341,
      "step": 23470
    },
    {
      "epoch": 6.881594372801875,
      "grad_norm": 0.6751664280891418,
      "learning_rate": 0.000518028649871283,
      "loss": 0.0856,
      "step": 23480
    },
    {
      "epoch": 6.884525205158265,
      "grad_norm": 0.7438635230064392,
      "learning_rate": 0.0005179448535836946,
      "loss": 0.0934,
      "step": 23490
    },
    {
      "epoch": 6.887456037514654,
      "grad_norm": 0.9091341495513916,
      "learning_rate": 0.0005178610572961062,
      "loss": 0.1122,
      "step": 23500
    },
    {
      "epoch": 6.890386869871043,
      "grad_norm": 0.829333484172821,
      "learning_rate": 0.0005177772610085179,
      "loss": 0.1075,
      "step": 23510
    },
    {
      "epoch": 6.893317702227432,
      "grad_norm": 0.6295780539512634,
      "learning_rate": 0.0005176934647209295,
      "loss": 0.1258,
      "step": 23520
    },
    {
      "epoch": 6.896248534583822,
      "grad_norm": 1.0111552476882935,
      "learning_rate": 0.0005176096684333412,
      "loss": 0.1061,
      "step": 23530
    },
    {
      "epoch": 6.899179366940211,
      "grad_norm": 0.556617021560669,
      "learning_rate": 0.0005175258721457528,
      "loss": 0.1259,
      "step": 23540
    },
    {
      "epoch": 6.9021101992966,
      "grad_norm": 1.5079271793365479,
      "learning_rate": 0.0005174420758581645,
      "loss": 0.1211,
      "step": 23550
    },
    {
      "epoch": 6.905041031652989,
      "grad_norm": 0.6808505654335022,
      "learning_rate": 0.0005173582795705761,
      "loss": 0.1331,
      "step": 23560
    },
    {
      "epoch": 6.907971864009379,
      "grad_norm": 0.7105540633201599,
      "learning_rate": 0.0005172744832829877,
      "loss": 0.1379,
      "step": 23570
    },
    {
      "epoch": 6.910902696365768,
      "grad_norm": 0.8998695611953735,
      "learning_rate": 0.0005171906869953993,
      "loss": 0.1151,
      "step": 23580
    },
    {
      "epoch": 6.913833528722157,
      "grad_norm": 1.3965855836868286,
      "learning_rate": 0.0005171068907078109,
      "loss": 0.1144,
      "step": 23590
    },
    {
      "epoch": 6.916764361078546,
      "grad_norm": 1.359275460243225,
      "learning_rate": 0.0005170230944202226,
      "loss": 0.1176,
      "step": 23600
    },
    {
      "epoch": 6.9196951934349356,
      "grad_norm": 0.9892827272415161,
      "learning_rate": 0.0005169392981326343,
      "loss": 0.1304,
      "step": 23610
    },
    {
      "epoch": 6.922626025791325,
      "grad_norm": 1.5892882347106934,
      "learning_rate": 0.0005168555018450459,
      "loss": 0.0994,
      "step": 23620
    },
    {
      "epoch": 6.925556858147714,
      "grad_norm": 1.5623935461044312,
      "learning_rate": 0.0005167717055574575,
      "loss": 0.118,
      "step": 23630
    },
    {
      "epoch": 6.928487690504103,
      "grad_norm": 1.0766147375106812,
      "learning_rate": 0.0005166879092698691,
      "loss": 0.1186,
      "step": 23640
    },
    {
      "epoch": 6.9314185228604925,
      "grad_norm": 0.7780326008796692,
      "learning_rate": 0.0005166041129822808,
      "loss": 0.1053,
      "step": 23650
    },
    {
      "epoch": 6.934349355216882,
      "grad_norm": 1.218907117843628,
      "learning_rate": 0.0005165203166946924,
      "loss": 0.1005,
      "step": 23660
    },
    {
      "epoch": 6.937280187573271,
      "grad_norm": 0.6760881543159485,
      "learning_rate": 0.000516436520407104,
      "loss": 0.1221,
      "step": 23670
    },
    {
      "epoch": 6.94021101992966,
      "grad_norm": 1.8119596242904663,
      "learning_rate": 0.0005163527241195156,
      "loss": 0.1225,
      "step": 23680
    },
    {
      "epoch": 6.9431418522860495,
      "grad_norm": 0.9071987271308899,
      "learning_rate": 0.0005162689278319272,
      "loss": 0.1267,
      "step": 23690
    },
    {
      "epoch": 6.946072684642439,
      "grad_norm": 1.8411890268325806,
      "learning_rate": 0.000516185131544339,
      "loss": 0.1192,
      "step": 23700
    },
    {
      "epoch": 6.949003516998828,
      "grad_norm": 1.0487487316131592,
      "learning_rate": 0.0005161013352567506,
      "loss": 0.1084,
      "step": 23710
    },
    {
      "epoch": 6.951934349355217,
      "grad_norm": 0.43616077303886414,
      "learning_rate": 0.0005160175389691622,
      "loss": 0.1013,
      "step": 23720
    },
    {
      "epoch": 6.954865181711606,
      "grad_norm": 1.462868332862854,
      "learning_rate": 0.0005159337426815738,
      "loss": 0.1167,
      "step": 23730
    },
    {
      "epoch": 6.957796014067995,
      "grad_norm": 1.1720362901687622,
      "learning_rate": 0.0005158499463939854,
      "loss": 0.132,
      "step": 23740
    },
    {
      "epoch": 6.960726846424384,
      "grad_norm": 0.6567161679267883,
      "learning_rate": 0.0005157661501063971,
      "loss": 0.1244,
      "step": 23750
    },
    {
      "epoch": 6.963657678780773,
      "grad_norm": 0.44980525970458984,
      "learning_rate": 0.0005156823538188087,
      "loss": 0.1409,
      "step": 23760
    },
    {
      "epoch": 6.966588511137163,
      "grad_norm": 0.9029614925384521,
      "learning_rate": 0.0005155985575312204,
      "loss": 0.1306,
      "step": 23770
    },
    {
      "epoch": 6.969519343493552,
      "grad_norm": 0.7528419494628906,
      "learning_rate": 0.0005155147612436319,
      "loss": 0.1368,
      "step": 23780
    },
    {
      "epoch": 6.972450175849941,
      "grad_norm": 0.5169209837913513,
      "learning_rate": 0.0005154309649560436,
      "loss": 0.1265,
      "step": 23790
    },
    {
      "epoch": 6.97538100820633,
      "grad_norm": 0.6766040325164795,
      "learning_rate": 0.0005153471686684553,
      "loss": 0.1231,
      "step": 23800
    },
    {
      "epoch": 6.97831184056272,
      "grad_norm": 0.7861598134040833,
      "learning_rate": 0.0005152633723808669,
      "loss": 0.0987,
      "step": 23810
    },
    {
      "epoch": 6.981242672919109,
      "grad_norm": 1.5047669410705566,
      "learning_rate": 0.0005151795760932786,
      "loss": 0.1242,
      "step": 23820
    },
    {
      "epoch": 6.984173505275498,
      "grad_norm": 0.735123336315155,
      "learning_rate": 0.0005150957798056901,
      "loss": 0.1259,
      "step": 23830
    },
    {
      "epoch": 6.987104337631887,
      "grad_norm": 1.0893150568008423,
      "learning_rate": 0.0005150119835181017,
      "loss": 0.1442,
      "step": 23840
    },
    {
      "epoch": 6.990035169988277,
      "grad_norm": 1.0731505155563354,
      "learning_rate": 0.0005149281872305134,
      "loss": 0.1081,
      "step": 23850
    },
    {
      "epoch": 6.992966002344666,
      "grad_norm": 0.8481832146644592,
      "learning_rate": 0.000514844390942925,
      "loss": 0.1417,
      "step": 23860
    },
    {
      "epoch": 6.995896834701055,
      "grad_norm": 0.8608702421188354,
      "learning_rate": 0.0005147605946553368,
      "loss": 0.1022,
      "step": 23870
    },
    {
      "epoch": 6.998827667057444,
      "grad_norm": 0.5820047855377197,
      "learning_rate": 0.0005146767983677484,
      "loss": 0.1233,
      "step": 23880
    },
    {
      "epoch": 7.0,
      "eval_accuracy": 0.6109462578305308,
      "eval_f1_macro": 0.5700533111398822,
      "eval_f1_micro": 0.7072576716999514,
      "eval_f1_weighted": 0.6748788611117201,
      "eval_loss": 0.1142551526427269,
      "eval_roc_auc": 0.8105943781530496,
      "eval_runtime": 162.183,
      "eval_samples_per_second": 18.701,
      "eval_steps_per_second": 2.343,
      "step": 23884
    },
    {
      "epoch": 7.0017584994138335,
      "grad_norm": 3.550384998321533,
      "learning_rate": 0.0005145930020801599,
      "loss": 0.1235,
      "step": 23890
    },
    {
      "epoch": 7.004689331770223,
      "grad_norm": 0.5729696750640869,
      "learning_rate": 0.0005145092057925716,
      "loss": 0.1155,
      "step": 23900
    },
    {
      "epoch": 7.007620164126612,
      "grad_norm": 0.8809893131256104,
      "learning_rate": 0.0005144254095049832,
      "loss": 0.1472,
      "step": 23910
    },
    {
      "epoch": 7.010550996483001,
      "grad_norm": 0.640714704990387,
      "learning_rate": 0.0005143416132173949,
      "loss": 0.1138,
      "step": 23920
    },
    {
      "epoch": 7.0134818288393905,
      "grad_norm": 1.2678909301757812,
      "learning_rate": 0.0005142578169298065,
      "loss": 0.0687,
      "step": 23930
    },
    {
      "epoch": 7.01641266119578,
      "grad_norm": 0.9634881615638733,
      "learning_rate": 0.0005141740206422181,
      "loss": 0.124,
      "step": 23940
    },
    {
      "epoch": 7.019343493552169,
      "grad_norm": 1.6586594581604004,
      "learning_rate": 0.0005140902243546297,
      "loss": 0.116,
      "step": 23950
    },
    {
      "epoch": 7.022274325908558,
      "grad_norm": 0.9787765741348267,
      "learning_rate": 0.0005140064280670414,
      "loss": 0.1129,
      "step": 23960
    },
    {
      "epoch": 7.0252051582649475,
      "grad_norm": 0.7713385820388794,
      "learning_rate": 0.0005139226317794531,
      "loss": 0.1121,
      "step": 23970
    },
    {
      "epoch": 7.028135990621337,
      "grad_norm": 0.8939084410667419,
      "learning_rate": 0.0005138388354918647,
      "loss": 0.1063,
      "step": 23980
    },
    {
      "epoch": 7.031066822977726,
      "grad_norm": 1.965360164642334,
      "learning_rate": 0.0005137550392042763,
      "loss": 0.0926,
      "step": 23990
    },
    {
      "epoch": 7.033997655334115,
      "grad_norm": 0.8434505462646484,
      "learning_rate": 0.0005136712429166879,
      "loss": 0.1165,
      "step": 24000
    },
    {
      "epoch": 7.0369284876905045,
      "grad_norm": 1.0945814847946167,
      "learning_rate": 0.0005135874466290995,
      "loss": 0.1199,
      "step": 24010
    },
    {
      "epoch": 7.039859320046894,
      "grad_norm": 0.7617234587669373,
      "learning_rate": 0.0005135036503415112,
      "loss": 0.1161,
      "step": 24020
    },
    {
      "epoch": 7.042790152403282,
      "grad_norm": 1.1833754777908325,
      "learning_rate": 0.0005134198540539228,
      "loss": 0.1278,
      "step": 24030
    },
    {
      "epoch": 7.045720984759671,
      "grad_norm": 0.8681219220161438,
      "learning_rate": 0.0005133360577663344,
      "loss": 0.1046,
      "step": 24040
    },
    {
      "epoch": 7.048651817116061,
      "grad_norm": 0.34039783477783203,
      "learning_rate": 0.0005132522614787461,
      "loss": 0.0985,
      "step": 24050
    },
    {
      "epoch": 7.05158264947245,
      "grad_norm": 0.6819936037063599,
      "learning_rate": 0.0005131684651911577,
      "loss": 0.1058,
      "step": 24060
    },
    {
      "epoch": 7.054513481828839,
      "grad_norm": 1.0223510265350342,
      "learning_rate": 0.0005130846689035694,
      "loss": 0.1132,
      "step": 24070
    },
    {
      "epoch": 7.057444314185228,
      "grad_norm": 0.6777737140655518,
      "learning_rate": 0.000513000872615981,
      "loss": 0.1327,
      "step": 24080
    },
    {
      "epoch": 7.060375146541618,
      "grad_norm": 0.5994179844856262,
      "learning_rate": 0.0005129170763283926,
      "loss": 0.0877,
      "step": 24090
    },
    {
      "epoch": 7.063305978898007,
      "grad_norm": 1.034410834312439,
      "learning_rate": 0.0005128332800408042,
      "loss": 0.1231,
      "step": 24100
    },
    {
      "epoch": 7.066236811254396,
      "grad_norm": 1.9833468198776245,
      "learning_rate": 0.0005127494837532159,
      "loss": 0.1191,
      "step": 24110
    },
    {
      "epoch": 7.069167643610785,
      "grad_norm": 0.6211405992507935,
      "learning_rate": 0.0005126656874656275,
      "loss": 0.0957,
      "step": 24120
    },
    {
      "epoch": 7.072098475967175,
      "grad_norm": 0.9831978678703308,
      "learning_rate": 0.0005125818911780392,
      "loss": 0.1323,
      "step": 24130
    },
    {
      "epoch": 7.075029308323564,
      "grad_norm": 0.8539133667945862,
      "learning_rate": 0.0005124980948904508,
      "loss": 0.122,
      "step": 24140
    },
    {
      "epoch": 7.077960140679953,
      "grad_norm": 1.2345314025878906,
      "learning_rate": 0.0005124142986028624,
      "loss": 0.1336,
      "step": 24150
    },
    {
      "epoch": 7.080890973036342,
      "grad_norm": 0.8982236385345459,
      "learning_rate": 0.0005123305023152741,
      "loss": 0.1147,
      "step": 24160
    },
    {
      "epoch": 7.0838218053927315,
      "grad_norm": 0.5812134742736816,
      "learning_rate": 0.0005122467060276857,
      "loss": 0.1224,
      "step": 24170
    },
    {
      "epoch": 7.086752637749121,
      "grad_norm": 0.7036316394805908,
      "learning_rate": 0.0005121629097400973,
      "loss": 0.1151,
      "step": 24180
    },
    {
      "epoch": 7.08968347010551,
      "grad_norm": 1.2218159437179565,
      "learning_rate": 0.0005120791134525089,
      "loss": 0.1258,
      "step": 24190
    },
    {
      "epoch": 7.092614302461899,
      "grad_norm": 1.7794743776321411,
      "learning_rate": 0.0005119953171649205,
      "loss": 0.0985,
      "step": 24200
    },
    {
      "epoch": 7.0955451348182885,
      "grad_norm": 0.46352389454841614,
      "learning_rate": 0.0005119115208773322,
      "loss": 0.119,
      "step": 24210
    },
    {
      "epoch": 7.098475967174678,
      "grad_norm": 1.2675226926803589,
      "learning_rate": 0.0005118277245897439,
      "loss": 0.1255,
      "step": 24220
    },
    {
      "epoch": 7.101406799531067,
      "grad_norm": 0.6828895807266235,
      "learning_rate": 0.0005117439283021555,
      "loss": 0.0973,
      "step": 24230
    },
    {
      "epoch": 7.104337631887456,
      "grad_norm": 0.6413421630859375,
      "learning_rate": 0.0005116601320145672,
      "loss": 0.1072,
      "step": 24240
    },
    {
      "epoch": 7.1072684642438455,
      "grad_norm": 1.2603510618209839,
      "learning_rate": 0.0005115763357269787,
      "loss": 0.0974,
      "step": 24250
    },
    {
      "epoch": 7.110199296600235,
      "grad_norm": 0.5688678622245789,
      "learning_rate": 0.0005114925394393904,
      "loss": 0.1103,
      "step": 24260
    },
    {
      "epoch": 7.113130128956624,
      "grad_norm": 1.1734793186187744,
      "learning_rate": 0.000511408743151802,
      "loss": 0.0944,
      "step": 24270
    },
    {
      "epoch": 7.116060961313013,
      "grad_norm": 0.7417131066322327,
      "learning_rate": 0.0005113249468642137,
      "loss": 0.1303,
      "step": 24280
    },
    {
      "epoch": 7.1189917936694025,
      "grad_norm": 0.6695602536201477,
      "learning_rate": 0.0005112411505766253,
      "loss": 0.1197,
      "step": 24290
    },
    {
      "epoch": 7.121922626025792,
      "grad_norm": 0.7740633487701416,
      "learning_rate": 0.0005111573542890368,
      "loss": 0.1329,
      "step": 24300
    },
    {
      "epoch": 7.12485345838218,
      "grad_norm": 0.8083699345588684,
      "learning_rate": 0.0005110735580014486,
      "loss": 0.096,
      "step": 24310
    },
    {
      "epoch": 7.127784290738569,
      "grad_norm": 0.725562334060669,
      "learning_rate": 0.0005109897617138602,
      "loss": 0.1157,
      "step": 24320
    },
    {
      "epoch": 7.130715123094959,
      "grad_norm": 2.3832449913024902,
      "learning_rate": 0.0005109059654262719,
      "loss": 0.1171,
      "step": 24330
    },
    {
      "epoch": 7.133645955451348,
      "grad_norm": 1.808760166168213,
      "learning_rate": 0.0005108221691386835,
      "loss": 0.1258,
      "step": 24340
    },
    {
      "epoch": 7.136576787807737,
      "grad_norm": 0.6084378361701965,
      "learning_rate": 0.0005107383728510951,
      "loss": 0.1169,
      "step": 24350
    },
    {
      "epoch": 7.139507620164126,
      "grad_norm": 1.0408341884613037,
      "learning_rate": 0.0005106545765635067,
      "loss": 0.1236,
      "step": 24360
    },
    {
      "epoch": 7.142438452520516,
      "grad_norm": 0.64695143699646,
      "learning_rate": 0.0005105707802759183,
      "loss": 0.0996,
      "step": 24370
    },
    {
      "epoch": 7.145369284876905,
      "grad_norm": 0.8668547868728638,
      "learning_rate": 0.00051048698398833,
      "loss": 0.129,
      "step": 24380
    },
    {
      "epoch": 7.148300117233294,
      "grad_norm": 0.9397488832473755,
      "learning_rate": 0.0005104031877007417,
      "loss": 0.1383,
      "step": 24390
    },
    {
      "epoch": 7.151230949589683,
      "grad_norm": 0.7448166012763977,
      "learning_rate": 0.0005103193914131533,
      "loss": 0.1163,
      "step": 24400
    },
    {
      "epoch": 7.1541617819460726,
      "grad_norm": 0.9478644728660583,
      "learning_rate": 0.0005102355951255649,
      "loss": 0.0907,
      "step": 24410
    },
    {
      "epoch": 7.157092614302462,
      "grad_norm": 1.0385464429855347,
      "learning_rate": 0.0005101517988379765,
      "loss": 0.0989,
      "step": 24420
    },
    {
      "epoch": 7.160023446658851,
      "grad_norm": 0.9295490980148315,
      "learning_rate": 0.0005100680025503882,
      "loss": 0.1386,
      "step": 24430
    },
    {
      "epoch": 7.16295427901524,
      "grad_norm": 1.910788655281067,
      "learning_rate": 0.0005099842062627998,
      "loss": 0.1259,
      "step": 24440
    },
    {
      "epoch": 7.1658851113716295,
      "grad_norm": 1.0395084619522095,
      "learning_rate": 0.0005099004099752114,
      "loss": 0.103,
      "step": 24450
    },
    {
      "epoch": 7.168815943728019,
      "grad_norm": 1.1745209693908691,
      "learning_rate": 0.000509816613687623,
      "loss": 0.0977,
      "step": 24460
    },
    {
      "epoch": 7.171746776084408,
      "grad_norm": 0.8688961863517761,
      "learning_rate": 0.0005097328174000346,
      "loss": 0.102,
      "step": 24470
    },
    {
      "epoch": 7.174677608440797,
      "grad_norm": 0.6763483881950378,
      "learning_rate": 0.0005096490211124464,
      "loss": 0.0858,
      "step": 24480
    },
    {
      "epoch": 7.1776084407971865,
      "grad_norm": 0.9551393985748291,
      "learning_rate": 0.000509565224824858,
      "loss": 0.1294,
      "step": 24490
    },
    {
      "epoch": 7.180539273153576,
      "grad_norm": 0.9537851810455322,
      "learning_rate": 0.0005094814285372696,
      "loss": 0.1137,
      "step": 24500
    },
    {
      "epoch": 7.183470105509965,
      "grad_norm": 0.5782511234283447,
      "learning_rate": 0.0005093976322496812,
      "loss": 0.1253,
      "step": 24510
    },
    {
      "epoch": 7.186400937866354,
      "grad_norm": 0.815977156162262,
      "learning_rate": 0.0005093138359620928,
      "loss": 0.1332,
      "step": 24520
    },
    {
      "epoch": 7.1893317702227435,
      "grad_norm": 0.598101019859314,
      "learning_rate": 0.0005092300396745045,
      "loss": 0.1189,
      "step": 24530
    },
    {
      "epoch": 7.192262602579133,
      "grad_norm": 0.9213472008705139,
      "learning_rate": 0.0005091462433869161,
      "loss": 0.1003,
      "step": 24540
    },
    {
      "epoch": 7.195193434935522,
      "grad_norm": 0.885405957698822,
      "learning_rate": 0.0005090624470993277,
      "loss": 0.1261,
      "step": 24550
    },
    {
      "epoch": 7.198124267291911,
      "grad_norm": 0.47913965582847595,
      "learning_rate": 0.0005089786508117393,
      "loss": 0.1032,
      "step": 24560
    },
    {
      "epoch": 7.2010550996483005,
      "grad_norm": 0.851875364780426,
      "learning_rate": 0.000508894854524151,
      "loss": 0.0903,
      "step": 24570
    },
    {
      "epoch": 7.20398593200469,
      "grad_norm": 0.9605023264884949,
      "learning_rate": 0.0005088110582365627,
      "loss": 0.0999,
      "step": 24580
    },
    {
      "epoch": 7.206916764361079,
      "grad_norm": 1.0674073696136475,
      "learning_rate": 0.0005087272619489743,
      "loss": 0.1183,
      "step": 24590
    },
    {
      "epoch": 7.209847596717467,
      "grad_norm": 0.9226682186126709,
      "learning_rate": 0.0005086434656613859,
      "loss": 0.0841,
      "step": 24600
    },
    {
      "epoch": 7.212778429073857,
      "grad_norm": 0.2564548850059509,
      "learning_rate": 0.0005085596693737975,
      "loss": 0.1097,
      "step": 24610
    },
    {
      "epoch": 7.215709261430246,
      "grad_norm": 0.8319751620292664,
      "learning_rate": 0.0005084758730862092,
      "loss": 0.1114,
      "step": 24620
    },
    {
      "epoch": 7.218640093786635,
      "grad_norm": 1.4095778465270996,
      "learning_rate": 0.0005083920767986208,
      "loss": 0.1275,
      "step": 24630
    },
    {
      "epoch": 7.221570926143024,
      "grad_norm": 0.8459200859069824,
      "learning_rate": 0.0005083082805110324,
      "loss": 0.1206,
      "step": 24640
    },
    {
      "epoch": 7.224501758499414,
      "grad_norm": 0.6949003338813782,
      "learning_rate": 0.0005082244842234442,
      "loss": 0.1195,
      "step": 24650
    },
    {
      "epoch": 7.227432590855803,
      "grad_norm": 0.8744308352470398,
      "learning_rate": 0.0005081406879358557,
      "loss": 0.1274,
      "step": 24660
    },
    {
      "epoch": 7.230363423212192,
      "grad_norm": 0.6149123311042786,
      "learning_rate": 0.0005080568916482674,
      "loss": 0.0887,
      "step": 24670
    },
    {
      "epoch": 7.233294255568581,
      "grad_norm": 0.7059335708618164,
      "learning_rate": 0.000507973095360679,
      "loss": 0.0893,
      "step": 24680
    },
    {
      "epoch": 7.2362250879249705,
      "grad_norm": 2.2010343074798584,
      "learning_rate": 0.0005078892990730906,
      "loss": 0.1012,
      "step": 24690
    },
    {
      "epoch": 7.23915592028136,
      "grad_norm": 0.6742129325866699,
      "learning_rate": 0.0005078055027855023,
      "loss": 0.116,
      "step": 24700
    },
    {
      "epoch": 7.242086752637749,
      "grad_norm": 1.775510311126709,
      "learning_rate": 0.0005077217064979138,
      "loss": 0.1215,
      "step": 24710
    },
    {
      "epoch": 7.245017584994138,
      "grad_norm": 0.7043710947036743,
      "learning_rate": 0.0005076379102103255,
      "loss": 0.0919,
      "step": 24720
    },
    {
      "epoch": 7.2479484173505275,
      "grad_norm": 0.763817548751831,
      "learning_rate": 0.0005075541139227371,
      "loss": 0.1178,
      "step": 24730
    },
    {
      "epoch": 7.250879249706917,
      "grad_norm": 0.7792488932609558,
      "learning_rate": 0.0005074703176351488,
      "loss": 0.1172,
      "step": 24740
    },
    {
      "epoch": 7.253810082063306,
      "grad_norm": 0.7281783223152161,
      "learning_rate": 0.0005073865213475605,
      "loss": 0.0867,
      "step": 24750
    },
    {
      "epoch": 7.256740914419695,
      "grad_norm": 1.738938331604004,
      "learning_rate": 0.0005073027250599721,
      "loss": 0.0999,
      "step": 24760
    },
    {
      "epoch": 7.2596717467760845,
      "grad_norm": 0.9001278877258301,
      "learning_rate": 0.0005072189287723837,
      "loss": 0.1107,
      "step": 24770
    },
    {
      "epoch": 7.262602579132474,
      "grad_norm": 1.0447461605072021,
      "learning_rate": 0.0005071351324847953,
      "loss": 0.1182,
      "step": 24780
    },
    {
      "epoch": 7.265533411488863,
      "grad_norm": 1.3116410970687866,
      "learning_rate": 0.0005070513361972069,
      "loss": 0.1364,
      "step": 24790
    },
    {
      "epoch": 7.268464243845252,
      "grad_norm": 0.2953316271305084,
      "learning_rate": 0.0005069675399096186,
      "loss": 0.088,
      "step": 24800
    },
    {
      "epoch": 7.2713950762016415,
      "grad_norm": 0.8830611705780029,
      "learning_rate": 0.0005068837436220302,
      "loss": 0.122,
      "step": 24810
    },
    {
      "epoch": 7.274325908558031,
      "grad_norm": 0.7819790244102478,
      "learning_rate": 0.0005067999473344418,
      "loss": 0.0857,
      "step": 24820
    },
    {
      "epoch": 7.27725674091442,
      "grad_norm": 0.9205616116523743,
      "learning_rate": 0.0005067161510468535,
      "loss": 0.1239,
      "step": 24830
    },
    {
      "epoch": 7.280187573270809,
      "grad_norm": 0.2853579819202423,
      "learning_rate": 0.0005066323547592652,
      "loss": 0.0887,
      "step": 24840
    },
    {
      "epoch": 7.2831184056271985,
      "grad_norm": 0.9300666451454163,
      "learning_rate": 0.0005065485584716768,
      "loss": 0.0998,
      "step": 24850
    },
    {
      "epoch": 7.286049237983588,
      "grad_norm": 0.7750013470649719,
      "learning_rate": 0.0005064647621840884,
      "loss": 0.1451,
      "step": 24860
    },
    {
      "epoch": 7.288980070339977,
      "grad_norm": 0.8037611842155457,
      "learning_rate": 0.0005063809658965,
      "loss": 0.1159,
      "step": 24870
    },
    {
      "epoch": 7.291910902696365,
      "grad_norm": 0.8586869239807129,
      "learning_rate": 0.0005062971696089116,
      "loss": 0.101,
      "step": 24880
    },
    {
      "epoch": 7.294841735052755,
      "grad_norm": 0.8689338564872742,
      "learning_rate": 0.0005062133733213233,
      "loss": 0.1326,
      "step": 24890
    },
    {
      "epoch": 7.297772567409144,
      "grad_norm": 1.1009613275527954,
      "learning_rate": 0.0005061295770337349,
      "loss": 0.1098,
      "step": 24900
    },
    {
      "epoch": 7.300703399765533,
      "grad_norm": 1.1427656412124634,
      "learning_rate": 0.0005060457807461466,
      "loss": 0.1019,
      "step": 24910
    },
    {
      "epoch": 7.303634232121922,
      "grad_norm": 1.1274787187576294,
      "learning_rate": 0.0005059619844585582,
      "loss": 0.1395,
      "step": 24920
    },
    {
      "epoch": 7.3065650644783116,
      "grad_norm": 1.318595290184021,
      "learning_rate": 0.0005058781881709698,
      "loss": 0.1055,
      "step": 24930
    },
    {
      "epoch": 7.309495896834701,
      "grad_norm": 0.8308205008506775,
      "learning_rate": 0.0005057943918833815,
      "loss": 0.0996,
      "step": 24940
    },
    {
      "epoch": 7.31242672919109,
      "grad_norm": 0.6504967212677002,
      "learning_rate": 0.0005057105955957931,
      "loss": 0.1125,
      "step": 24950
    },
    {
      "epoch": 7.315357561547479,
      "grad_norm": 0.39087381958961487,
      "learning_rate": 0.0005056267993082047,
      "loss": 0.0859,
      "step": 24960
    },
    {
      "epoch": 7.3182883939038685,
      "grad_norm": 0.8418640494346619,
      "learning_rate": 0.0005055430030206163,
      "loss": 0.1139,
      "step": 24970
    },
    {
      "epoch": 7.321219226260258,
      "grad_norm": 0.510008692741394,
      "learning_rate": 0.0005054592067330279,
      "loss": 0.0919,
      "step": 24980
    },
    {
      "epoch": 7.324150058616647,
      "grad_norm": 1.4261136054992676,
      "learning_rate": 0.0005053754104454396,
      "loss": 0.1155,
      "step": 24990
    },
    {
      "epoch": 7.327080890973036,
      "grad_norm": 0.6800909042358398,
      "learning_rate": 0.0005052916141578513,
      "loss": 0.1075,
      "step": 25000
    },
    {
      "epoch": 7.3300117233294255,
      "grad_norm": 0.8962802290916443,
      "learning_rate": 0.000505207817870263,
      "loss": 0.092,
      "step": 25010
    },
    {
      "epoch": 7.332942555685815,
      "grad_norm": 0.7712864279747009,
      "learning_rate": 0.0005051240215826745,
      "loss": 0.1084,
      "step": 25020
    },
    {
      "epoch": 7.335873388042204,
      "grad_norm": 1.5902003049850464,
      "learning_rate": 0.0005050402252950861,
      "loss": 0.1253,
      "step": 25030
    },
    {
      "epoch": 7.338804220398593,
      "grad_norm": 1.278531551361084,
      "learning_rate": 0.0005049564290074978,
      "loss": 0.1041,
      "step": 25040
    },
    {
      "epoch": 7.3417350527549825,
      "grad_norm": 0.6569145321846008,
      "learning_rate": 0.0005048726327199094,
      "loss": 0.1151,
      "step": 25050
    },
    {
      "epoch": 7.344665885111372,
      "grad_norm": 0.7701902985572815,
      "learning_rate": 0.0005047888364323211,
      "loss": 0.1003,
      "step": 25060
    },
    {
      "epoch": 7.347596717467761,
      "grad_norm": 1.0410276651382446,
      "learning_rate": 0.0005047050401447326,
      "loss": 0.1103,
      "step": 25070
    },
    {
      "epoch": 7.35052754982415,
      "grad_norm": 1.1432169675827026,
      "learning_rate": 0.0005046212438571442,
      "loss": 0.1306,
      "step": 25080
    },
    {
      "epoch": 7.3534583821805395,
      "grad_norm": 0.6640624403953552,
      "learning_rate": 0.000504537447569556,
      "loss": 0.1153,
      "step": 25090
    },
    {
      "epoch": 7.356389214536929,
      "grad_norm": 0.9544304013252258,
      "learning_rate": 0.0005044536512819676,
      "loss": 0.1019,
      "step": 25100
    },
    {
      "epoch": 7.359320046893318,
      "grad_norm": 0.791942834854126,
      "learning_rate": 0.0005043698549943793,
      "loss": 0.1048,
      "step": 25110
    },
    {
      "epoch": 7.362250879249707,
      "grad_norm": 0.6325312256813049,
      "learning_rate": 0.0005042860587067909,
      "loss": 0.1441,
      "step": 25120
    },
    {
      "epoch": 7.3651817116060965,
      "grad_norm": 0.7177265882492065,
      "learning_rate": 0.0005042022624192024,
      "loss": 0.1001,
      "step": 25130
    },
    {
      "epoch": 7.368112543962486,
      "grad_norm": 0.5661335587501526,
      "learning_rate": 0.0005041184661316141,
      "loss": 0.1099,
      "step": 25140
    },
    {
      "epoch": 7.371043376318875,
      "grad_norm": 0.8036679625511169,
      "learning_rate": 0.0005040346698440257,
      "loss": 0.1219,
      "step": 25150
    },
    {
      "epoch": 7.373974208675264,
      "grad_norm": 1.9238916635513306,
      "learning_rate": 0.0005039508735564374,
      "loss": 0.1231,
      "step": 25160
    },
    {
      "epoch": 7.3769050410316535,
      "grad_norm": 0.7336757183074951,
      "learning_rate": 0.000503867077268849,
      "loss": 0.1137,
      "step": 25170
    },
    {
      "epoch": 7.379835873388042,
      "grad_norm": 1.4174147844314575,
      "learning_rate": 0.0005037832809812607,
      "loss": 0.0999,
      "step": 25180
    },
    {
      "epoch": 7.382766705744431,
      "grad_norm": 0.9840916991233826,
      "learning_rate": 0.0005036994846936723,
      "loss": 0.1247,
      "step": 25190
    },
    {
      "epoch": 7.38569753810082,
      "grad_norm": 0.4574427008628845,
      "learning_rate": 0.0005036156884060839,
      "loss": 0.1038,
      "step": 25200
    },
    {
      "epoch": 7.3886283704572095,
      "grad_norm": 1.264955759048462,
      "learning_rate": 0.0005035318921184956,
      "loss": 0.0962,
      "step": 25210
    },
    {
      "epoch": 7.391559202813599,
      "grad_norm": 1.0036109685897827,
      "learning_rate": 0.0005034480958309072,
      "loss": 0.1414,
      "step": 25220
    },
    {
      "epoch": 7.394490035169988,
      "grad_norm": 0.7081435322761536,
      "learning_rate": 0.0005033642995433188,
      "loss": 0.0917,
      "step": 25230
    },
    {
      "epoch": 7.397420867526377,
      "grad_norm": 1.425925374031067,
      "learning_rate": 0.0005032805032557304,
      "loss": 0.1173,
      "step": 25240
    },
    {
      "epoch": 7.4003516998827665,
      "grad_norm": 1.86116361618042,
      "learning_rate": 0.000503196706968142,
      "loss": 0.117,
      "step": 25250
    },
    {
      "epoch": 7.403282532239156,
      "grad_norm": 0.7086683511734009,
      "learning_rate": 0.0005031129106805538,
      "loss": 0.1001,
      "step": 25260
    },
    {
      "epoch": 7.406213364595545,
      "grad_norm": 1.5969613790512085,
      "learning_rate": 0.0005030291143929654,
      "loss": 0.1032,
      "step": 25270
    },
    {
      "epoch": 7.409144196951934,
      "grad_norm": 0.9808691143989563,
      "learning_rate": 0.000502945318105377,
      "loss": 0.1252,
      "step": 25280
    },
    {
      "epoch": 7.4120750293083235,
      "grad_norm": 0.8406767845153809,
      "learning_rate": 0.0005028615218177886,
      "loss": 0.1065,
      "step": 25290
    },
    {
      "epoch": 7.415005861664713,
      "grad_norm": 0.7625494599342346,
      "learning_rate": 0.0005027777255302002,
      "loss": 0.1115,
      "step": 25300
    },
    {
      "epoch": 7.417936694021102,
      "grad_norm": 0.6558920741081238,
      "learning_rate": 0.0005026939292426119,
      "loss": 0.098,
      "step": 25310
    },
    {
      "epoch": 7.420867526377491,
      "grad_norm": 0.38040274381637573,
      "learning_rate": 0.0005026101329550235,
      "loss": 0.1095,
      "step": 25320
    },
    {
      "epoch": 7.4237983587338805,
      "grad_norm": 1.0408247709274292,
      "learning_rate": 0.0005025263366674351,
      "loss": 0.1003,
      "step": 25330
    },
    {
      "epoch": 7.42672919109027,
      "grad_norm": 1.8129132986068726,
      "learning_rate": 0.0005024425403798467,
      "loss": 0.1177,
      "step": 25340
    },
    {
      "epoch": 7.429660023446659,
      "grad_norm": 0.7185381054878235,
      "learning_rate": 0.0005023587440922585,
      "loss": 0.0835,
      "step": 25350
    },
    {
      "epoch": 7.432590855803048,
      "grad_norm": 2.7094852924346924,
      "learning_rate": 0.0005022749478046701,
      "loss": 0.0999,
      "step": 25360
    },
    {
      "epoch": 7.4355216881594375,
      "grad_norm": 0.9342043399810791,
      "learning_rate": 0.0005021911515170817,
      "loss": 0.0938,
      "step": 25370
    },
    {
      "epoch": 7.438452520515827,
      "grad_norm": 1.3265314102172852,
      "learning_rate": 0.0005021073552294933,
      "loss": 0.1352,
      "step": 25380
    },
    {
      "epoch": 7.441383352872216,
      "grad_norm": 0.6920299530029297,
      "learning_rate": 0.0005020235589419049,
      "loss": 0.0914,
      "step": 25390
    },
    {
      "epoch": 7.444314185228605,
      "grad_norm": 0.8956431746482849,
      "learning_rate": 0.0005019397626543166,
      "loss": 0.1113,
      "step": 25400
    },
    {
      "epoch": 7.4472450175849945,
      "grad_norm": 1.5073055028915405,
      "learning_rate": 0.0005018559663667282,
      "loss": 0.1069,
      "step": 25410
    },
    {
      "epoch": 7.450175849941384,
      "grad_norm": 0.38806310296058655,
      "learning_rate": 0.0005017721700791398,
      "loss": 0.0894,
      "step": 25420
    },
    {
      "epoch": 7.453106682297773,
      "grad_norm": 0.8458338975906372,
      "learning_rate": 0.0005016883737915514,
      "loss": 0.0849,
      "step": 25430
    },
    {
      "epoch": 7.456037514654161,
      "grad_norm": 1.3436652421951294,
      "learning_rate": 0.0005016045775039631,
      "loss": 0.116,
      "step": 25440
    },
    {
      "epoch": 7.458968347010551,
      "grad_norm": 0.38840875029563904,
      "learning_rate": 0.0005015207812163748,
      "loss": 0.1014,
      "step": 25450
    },
    {
      "epoch": 7.46189917936694,
      "grad_norm": 0.7473304271697998,
      "learning_rate": 0.0005014369849287864,
      "loss": 0.1205,
      "step": 25460
    },
    {
      "epoch": 7.464830011723329,
      "grad_norm": 1.4836221933364868,
      "learning_rate": 0.000501353188641198,
      "loss": 0.1101,
      "step": 25470
    },
    {
      "epoch": 7.467760844079718,
      "grad_norm": 0.7910987138748169,
      "learning_rate": 0.0005012693923536096,
      "loss": 0.12,
      "step": 25480
    },
    {
      "epoch": 7.4706916764361075,
      "grad_norm": 0.6006105542182922,
      "learning_rate": 0.0005011855960660212,
      "loss": 0.1023,
      "step": 25490
    },
    {
      "epoch": 7.473622508792497,
      "grad_norm": 0.8663644194602966,
      "learning_rate": 0.0005011017997784329,
      "loss": 0.1227,
      "step": 25500
    },
    {
      "epoch": 7.476553341148886,
      "grad_norm": 0.7360313534736633,
      "learning_rate": 0.0005010180034908445,
      "loss": 0.1085,
      "step": 25510
    },
    {
      "epoch": 7.479484173505275,
      "grad_norm": 1.1783642768859863,
      "learning_rate": 0.0005009342072032563,
      "loss": 0.1083,
      "step": 25520
    },
    {
      "epoch": 7.4824150058616645,
      "grad_norm": 0.45056772232055664,
      "learning_rate": 0.0005008504109156679,
      "loss": 0.0763,
      "step": 25530
    },
    {
      "epoch": 7.485345838218054,
      "grad_norm": 1.2916218042373657,
      "learning_rate": 0.0005007666146280794,
      "loss": 0.1121,
      "step": 25540
    },
    {
      "epoch": 7.488276670574443,
      "grad_norm": 0.6403579711914062,
      "learning_rate": 0.0005006828183404911,
      "loss": 0.0966,
      "step": 25550
    },
    {
      "epoch": 7.491207502930832,
      "grad_norm": 0.7610029578208923,
      "learning_rate": 0.0005005990220529027,
      "loss": 0.0883,
      "step": 25560
    },
    {
      "epoch": 7.4941383352872215,
      "grad_norm": 0.4449959397315979,
      "learning_rate": 0.0005005152257653144,
      "loss": 0.0982,
      "step": 25570
    },
    {
      "epoch": 7.497069167643611,
      "grad_norm": 0.9628700613975525,
      "learning_rate": 0.000500431429477726,
      "loss": 0.0943,
      "step": 25580
    },
    {
      "epoch": 7.5,
      "grad_norm": 0.909546434879303,
      "learning_rate": 0.0005003476331901376,
      "loss": 0.1171,
      "step": 25590
    },
    {
      "epoch": 7.502930832356389,
      "grad_norm": 0.93479323387146,
      "learning_rate": 0.0005002638369025492,
      "loss": 0.1133,
      "step": 25600
    },
    {
      "epoch": 7.5058616647127785,
      "grad_norm": 1.2740148305892944,
      "learning_rate": 0.0005001800406149609,
      "loss": 0.1296,
      "step": 25610
    },
    {
      "epoch": 7.508792497069168,
      "grad_norm": 2.37857985496521,
      "learning_rate": 0.0005000962443273726,
      "loss": 0.0964,
      "step": 25620
    },
    {
      "epoch": 7.511723329425557,
      "grad_norm": 0.9153687953948975,
      "learning_rate": 0.0005000124480397842,
      "loss": 0.1169,
      "step": 25630
    },
    {
      "epoch": 7.514654161781946,
      "grad_norm": 0.6910721063613892,
      "learning_rate": 0.0004999286517521958,
      "loss": 0.0966,
      "step": 25640
    },
    {
      "epoch": 7.5175849941383355,
      "grad_norm": 0.4887755513191223,
      "learning_rate": 0.0004998448554646074,
      "loss": 0.1153,
      "step": 25650
    },
    {
      "epoch": 7.520515826494725,
      "grad_norm": 1.06424081325531,
      "learning_rate": 0.000499761059177019,
      "loss": 0.1231,
      "step": 25660
    },
    {
      "epoch": 7.523446658851114,
      "grad_norm": 1.1469311714172363,
      "learning_rate": 0.0004996772628894307,
      "loss": 0.1051,
      "step": 25670
    },
    {
      "epoch": 7.526377491207503,
      "grad_norm": 0.813825249671936,
      "learning_rate": 0.0004995934666018423,
      "loss": 0.0895,
      "step": 25680
    },
    {
      "epoch": 7.5293083235638925,
      "grad_norm": 0.4570331871509552,
      "learning_rate": 0.000499509670314254,
      "loss": 0.1374,
      "step": 25690
    },
    {
      "epoch": 7.532239155920282,
      "grad_norm": 0.5877935886383057,
      "learning_rate": 0.0004994258740266656,
      "loss": 0.1217,
      "step": 25700
    },
    {
      "epoch": 7.535169988276671,
      "grad_norm": 0.8364202380180359,
      "learning_rate": 0.0004993420777390772,
      "loss": 0.1161,
      "step": 25710
    },
    {
      "epoch": 7.53810082063306,
      "grad_norm": 0.761953592300415,
      "learning_rate": 0.0004992582814514889,
      "loss": 0.1003,
      "step": 25720
    },
    {
      "epoch": 7.541031652989449,
      "grad_norm": 0.5686449408531189,
      "learning_rate": 0.0004991744851639005,
      "loss": 0.1116,
      "step": 25730
    },
    {
      "epoch": 7.543962485345839,
      "grad_norm": 1.4898761510849,
      "learning_rate": 0.0004990906888763121,
      "loss": 0.1351,
      "step": 25740
    },
    {
      "epoch": 7.546893317702227,
      "grad_norm": 1.2310510873794556,
      "learning_rate": 0.0004990068925887237,
      "loss": 0.125,
      "step": 25750
    },
    {
      "epoch": 7.549824150058616,
      "grad_norm": 0.6843822598457336,
      "learning_rate": 0.0004989230963011353,
      "loss": 0.1008,
      "step": 25760
    },
    {
      "epoch": 7.5527549824150055,
      "grad_norm": 0.9573968648910522,
      "learning_rate": 0.000498839300013547,
      "loss": 0.1089,
      "step": 25770
    },
    {
      "epoch": 7.555685814771395,
      "grad_norm": 1.00589120388031,
      "learning_rate": 0.0004987555037259587,
      "loss": 0.1348,
      "step": 25780
    },
    {
      "epoch": 7.558616647127784,
      "grad_norm": 1.3189446926116943,
      "learning_rate": 0.0004986717074383703,
      "loss": 0.1122,
      "step": 25790
    },
    {
      "epoch": 7.561547479484173,
      "grad_norm": 0.6544076800346375,
      "learning_rate": 0.0004985879111507819,
      "loss": 0.1221,
      "step": 25800
    },
    {
      "epoch": 7.5644783118405625,
      "grad_norm": 0.6538845896720886,
      "learning_rate": 0.0004985041148631935,
      "loss": 0.091,
      "step": 25810
    },
    {
      "epoch": 7.567409144196952,
      "grad_norm": 0.3541991114616394,
      "learning_rate": 0.0004984203185756052,
      "loss": 0.0774,
      "step": 25820
    },
    {
      "epoch": 7.570339976553341,
      "grad_norm": 1.4908643960952759,
      "learning_rate": 0.0004983365222880168,
      "loss": 0.1029,
      "step": 25830
    },
    {
      "epoch": 7.57327080890973,
      "grad_norm": 0.6930798888206482,
      "learning_rate": 0.0004982527260004284,
      "loss": 0.0995,
      "step": 25840
    },
    {
      "epoch": 7.5762016412661195,
      "grad_norm": 0.975421130657196,
      "learning_rate": 0.00049816892971284,
      "loss": 0.1235,
      "step": 25850
    },
    {
      "epoch": 7.579132473622509,
      "grad_norm": 0.8756507039070129,
      "learning_rate": 0.0004980851334252517,
      "loss": 0.1344,
      "step": 25860
    },
    {
      "epoch": 7.582063305978898,
      "grad_norm": 0.5522035956382751,
      "learning_rate": 0.0004980013371376634,
      "loss": 0.1247,
      "step": 25870
    },
    {
      "epoch": 7.584994138335287,
      "grad_norm": 0.8887229561805725,
      "learning_rate": 0.000497917540850075,
      "loss": 0.1103,
      "step": 25880
    },
    {
      "epoch": 7.5879249706916765,
      "grad_norm": 0.6533347964286804,
      "learning_rate": 0.0004978337445624867,
      "loss": 0.0949,
      "step": 25890
    },
    {
      "epoch": 7.590855803048066,
      "grad_norm": 2.0875916481018066,
      "learning_rate": 0.0004977499482748982,
      "loss": 0.1056,
      "step": 25900
    },
    {
      "epoch": 7.593786635404455,
      "grad_norm": 1.2818325757980347,
      "learning_rate": 0.0004976661519873099,
      "loss": 0.1279,
      "step": 25910
    },
    {
      "epoch": 7.596717467760844,
      "grad_norm": 0.9441434144973755,
      "learning_rate": 0.0004975823556997215,
      "loss": 0.1001,
      "step": 25920
    },
    {
      "epoch": 7.5996483001172335,
      "grad_norm": 0.736204206943512,
      "learning_rate": 0.0004974985594121331,
      "loss": 0.1058,
      "step": 25930
    },
    {
      "epoch": 7.602579132473623,
      "grad_norm": 0.7888314127922058,
      "learning_rate": 0.0004974147631245448,
      "loss": 0.1031,
      "step": 25940
    },
    {
      "epoch": 7.605509964830012,
      "grad_norm": 1.6424256563186646,
      "learning_rate": 0.0004973309668369563,
      "loss": 0.1341,
      "step": 25950
    },
    {
      "epoch": 7.608440797186401,
      "grad_norm": 0.7448120713233948,
      "learning_rate": 0.0004972471705493681,
      "loss": 0.0904,
      "step": 25960
    },
    {
      "epoch": 7.6113716295427905,
      "grad_norm": 0.9695875644683838,
      "learning_rate": 0.0004971633742617797,
      "loss": 0.1081,
      "step": 25970
    },
    {
      "epoch": 7.61430246189918,
      "grad_norm": 0.9748044013977051,
      "learning_rate": 0.0004970795779741913,
      "loss": 0.1061,
      "step": 25980
    },
    {
      "epoch": 7.617233294255569,
      "grad_norm": 0.8191220164299011,
      "learning_rate": 0.000496995781686603,
      "loss": 0.1201,
      "step": 25990
    },
    {
      "epoch": 7.620164126611957,
      "grad_norm": 0.6076970100402832,
      "learning_rate": 0.0004969119853990146,
      "loss": 0.1043,
      "step": 26000
    },
    {
      "epoch": 7.6230949589683465,
      "grad_norm": 0.9962372779846191,
      "learning_rate": 0.0004968281891114262,
      "loss": 0.1176,
      "step": 26010
    },
    {
      "epoch": 7.626025791324736,
      "grad_norm": 0.8582522869110107,
      "learning_rate": 0.0004967443928238378,
      "loss": 0.1,
      "step": 26020
    },
    {
      "epoch": 7.628956623681125,
      "grad_norm": 0.9815994501113892,
      "learning_rate": 0.0004966605965362494,
      "loss": 0.1264,
      "step": 26030
    },
    {
      "epoch": 7.631887456037514,
      "grad_norm": 1.5301517248153687,
      "learning_rate": 0.0004965768002486612,
      "loss": 0.1296,
      "step": 26040
    },
    {
      "epoch": 7.6348182883939035,
      "grad_norm": 0.6397955417633057,
      "learning_rate": 0.0004964930039610728,
      "loss": 0.1201,
      "step": 26050
    },
    {
      "epoch": 7.637749120750293,
      "grad_norm": 0.5904095768928528,
      "learning_rate": 0.0004964092076734844,
      "loss": 0.1097,
      "step": 26060
    },
    {
      "epoch": 7.640679953106682,
      "grad_norm": 1.4150418043136597,
      "learning_rate": 0.000496325411385896,
      "loss": 0.0998,
      "step": 26070
    },
    {
      "epoch": 7.643610785463071,
      "grad_norm": 0.7216018438339233,
      "learning_rate": 0.0004962416150983077,
      "loss": 0.1295,
      "step": 26080
    },
    {
      "epoch": 7.6465416178194605,
      "grad_norm": 0.5221068263053894,
      "learning_rate": 0.0004961578188107193,
      "loss": 0.0893,
      "step": 26090
    },
    {
      "epoch": 7.64947245017585,
      "grad_norm": 1.3393256664276123,
      "learning_rate": 0.0004960740225231309,
      "loss": 0.103,
      "step": 26100
    },
    {
      "epoch": 7.652403282532239,
      "grad_norm": 0.5724062323570251,
      "learning_rate": 0.0004959902262355425,
      "loss": 0.1113,
      "step": 26110
    },
    {
      "epoch": 7.655334114888628,
      "grad_norm": 0.7557424902915955,
      "learning_rate": 0.0004959064299479541,
      "loss": 0.1143,
      "step": 26120
    },
    {
      "epoch": 7.6582649472450175,
      "grad_norm": 0.6531981825828552,
      "learning_rate": 0.0004958226336603659,
      "loss": 0.0789,
      "step": 26130
    },
    {
      "epoch": 7.661195779601407,
      "grad_norm": 1.1975162029266357,
      "learning_rate": 0.0004957388373727775,
      "loss": 0.1072,
      "step": 26140
    },
    {
      "epoch": 7.664126611957796,
      "grad_norm": 0.6669493913650513,
      "learning_rate": 0.0004956550410851891,
      "loss": 0.0997,
      "step": 26150
    },
    {
      "epoch": 7.667057444314185,
      "grad_norm": 0.9737816452980042,
      "learning_rate": 0.0004955712447976007,
      "loss": 0.1231,
      "step": 26160
    },
    {
      "epoch": 7.6699882766705745,
      "grad_norm": 1.0662533044815063,
      "learning_rate": 0.0004954874485100123,
      "loss": 0.1379,
      "step": 26170
    },
    {
      "epoch": 7.672919109026964,
      "grad_norm": 0.7458712458610535,
      "learning_rate": 0.000495403652222424,
      "loss": 0.116,
      "step": 26180
    },
    {
      "epoch": 7.675849941383353,
      "grad_norm": 0.625099241733551,
      "learning_rate": 0.0004953198559348356,
      "loss": 0.1022,
      "step": 26190
    },
    {
      "epoch": 7.678780773739742,
      "grad_norm": 1.7385451793670654,
      "learning_rate": 0.0004952360596472472,
      "loss": 0.1264,
      "step": 26200
    },
    {
      "epoch": 7.6817116060961315,
      "grad_norm": 0.8727821111679077,
      "learning_rate": 0.0004951522633596588,
      "loss": 0.0974,
      "step": 26210
    },
    {
      "epoch": 7.684642438452521,
      "grad_norm": 1.2044177055358887,
      "learning_rate": 0.0004950684670720705,
      "loss": 0.1381,
      "step": 26220
    },
    {
      "epoch": 7.68757327080891,
      "grad_norm": 0.6715930700302124,
      "learning_rate": 0.0004949846707844822,
      "loss": 0.1015,
      "step": 26230
    },
    {
      "epoch": 7.690504103165299,
      "grad_norm": 0.8137679696083069,
      "learning_rate": 0.0004949008744968938,
      "loss": 0.1592,
      "step": 26240
    },
    {
      "epoch": 7.6934349355216884,
      "grad_norm": 0.8525349497795105,
      "learning_rate": 0.0004948170782093054,
      "loss": 0.1248,
      "step": 26250
    },
    {
      "epoch": 7.696365767878078,
      "grad_norm": 1.4868179559707642,
      "learning_rate": 0.000494733281921717,
      "loss": 0.1027,
      "step": 26260
    },
    {
      "epoch": 7.699296600234467,
      "grad_norm": 0.8448368310928345,
      "learning_rate": 0.0004946494856341286,
      "loss": 0.0977,
      "step": 26270
    },
    {
      "epoch": 7.702227432590856,
      "grad_norm": 1.2125742435455322,
      "learning_rate": 0.0004945656893465403,
      "loss": 0.0945,
      "step": 26280
    },
    {
      "epoch": 7.705158264947245,
      "grad_norm": 0.36546042561531067,
      "learning_rate": 0.0004944818930589519,
      "loss": 0.0947,
      "step": 26290
    },
    {
      "epoch": 7.708089097303635,
      "grad_norm": 0.8318564891815186,
      "learning_rate": 0.0004943980967713637,
      "loss": 0.1338,
      "step": 26300
    },
    {
      "epoch": 7.711019929660024,
      "grad_norm": 1.0883134603500366,
      "learning_rate": 0.0004943143004837752,
      "loss": 0.0823,
      "step": 26310
    },
    {
      "epoch": 7.713950762016412,
      "grad_norm": 0.777272641658783,
      "learning_rate": 0.0004942305041961868,
      "loss": 0.1041,
      "step": 26320
    },
    {
      "epoch": 7.7168815943728015,
      "grad_norm": 1.9069668054580688,
      "learning_rate": 0.0004941467079085985,
      "loss": 0.1114,
      "step": 26330
    },
    {
      "epoch": 7.719812426729191,
      "grad_norm": 0.9335911870002747,
      "learning_rate": 0.0004940629116210101,
      "loss": 0.1346,
      "step": 26340
    },
    {
      "epoch": 7.72274325908558,
      "grad_norm": 0.42824283242225647,
      "learning_rate": 0.0004939791153334218,
      "loss": 0.103,
      "step": 26350
    },
    {
      "epoch": 7.725674091441969,
      "grad_norm": 1.4305142164230347,
      "learning_rate": 0.0004938953190458334,
      "loss": 0.1054,
      "step": 26360
    },
    {
      "epoch": 7.7286049237983585,
      "grad_norm": 0.5915818810462952,
      "learning_rate": 0.0004938115227582449,
      "loss": 0.1469,
      "step": 26370
    },
    {
      "epoch": 7.731535756154748,
      "grad_norm": 1.1129481792449951,
      "learning_rate": 0.0004937277264706566,
      "loss": 0.1096,
      "step": 26380
    },
    {
      "epoch": 7.734466588511137,
      "grad_norm": 0.3456172049045563,
      "learning_rate": 0.0004936439301830683,
      "loss": 0.1146,
      "step": 26390
    },
    {
      "epoch": 7.737397420867526,
      "grad_norm": 0.43911516666412354,
      "learning_rate": 0.00049356013389548,
      "loss": 0.0838,
      "step": 26400
    },
    {
      "epoch": 7.7403282532239155,
      "grad_norm": 1.428281307220459,
      "learning_rate": 0.0004934763376078916,
      "loss": 0.1136,
      "step": 26410
    },
    {
      "epoch": 7.743259085580305,
      "grad_norm": 0.7297935485839844,
      "learning_rate": 0.0004933925413203032,
      "loss": 0.1198,
      "step": 26420
    },
    {
      "epoch": 7.746189917936694,
      "grad_norm": 1.184890627861023,
      "learning_rate": 0.0004933087450327148,
      "loss": 0.1018,
      "step": 26430
    },
    {
      "epoch": 7.749120750293083,
      "grad_norm": 1.0029031038284302,
      "learning_rate": 0.0004932249487451264,
      "loss": 0.1134,
      "step": 26440
    },
    {
      "epoch": 7.7520515826494725,
      "grad_norm": 0.7985628843307495,
      "learning_rate": 0.0004931411524575381,
      "loss": 0.1334,
      "step": 26450
    },
    {
      "epoch": 7.754982415005862,
      "grad_norm": 1.4043428897857666,
      "learning_rate": 0.0004930573561699497,
      "loss": 0.0963,
      "step": 26460
    },
    {
      "epoch": 7.757913247362251,
      "grad_norm": 1.1489946842193604,
      "learning_rate": 0.0004929735598823614,
      "loss": 0.1437,
      "step": 26470
    },
    {
      "epoch": 7.76084407971864,
      "grad_norm": 0.3322591483592987,
      "learning_rate": 0.000492889763594773,
      "loss": 0.079,
      "step": 26480
    },
    {
      "epoch": 7.7637749120750295,
      "grad_norm": 0.3788503408432007,
      "learning_rate": 0.0004928059673071846,
      "loss": 0.1212,
      "step": 26490
    },
    {
      "epoch": 7.766705744431419,
      "grad_norm": 0.9479835033416748,
      "learning_rate": 0.0004927221710195963,
      "loss": 0.1131,
      "step": 26500
    },
    {
      "epoch": 7.769636576787808,
      "grad_norm": 0.7744758129119873,
      "learning_rate": 0.0004926383747320079,
      "loss": 0.119,
      "step": 26510
    },
    {
      "epoch": 7.772567409144197,
      "grad_norm": 0.9925634264945984,
      "learning_rate": 0.0004925545784444195,
      "loss": 0.1274,
      "step": 26520
    },
    {
      "epoch": 7.775498241500586,
      "grad_norm": 0.6220388412475586,
      "learning_rate": 0.0004924707821568311,
      "loss": 0.1083,
      "step": 26530
    },
    {
      "epoch": 7.778429073856976,
      "grad_norm": 1.1401455402374268,
      "learning_rate": 0.0004923869858692427,
      "loss": 0.1135,
      "step": 26540
    },
    {
      "epoch": 7.781359906213365,
      "grad_norm": 0.6772053241729736,
      "learning_rate": 0.0004923031895816544,
      "loss": 0.1153,
      "step": 26550
    },
    {
      "epoch": 7.784290738569754,
      "grad_norm": 1.149107575416565,
      "learning_rate": 0.0004922193932940661,
      "loss": 0.1012,
      "step": 26560
    },
    {
      "epoch": 7.7872215709261425,
      "grad_norm": 1.1615054607391357,
      "learning_rate": 0.0004921355970064777,
      "loss": 0.1168,
      "step": 26570
    },
    {
      "epoch": 7.790152403282532,
      "grad_norm": 0.6059349179267883,
      "learning_rate": 0.0004920518007188893,
      "loss": 0.0984,
      "step": 26580
    },
    {
      "epoch": 7.793083235638921,
      "grad_norm": 0.9121401906013489,
      "learning_rate": 0.000491968004431301,
      "loss": 0.1091,
      "step": 26590
    },
    {
      "epoch": 7.79601406799531,
      "grad_norm": 0.6545374989509583,
      "learning_rate": 0.0004918842081437126,
      "loss": 0.083,
      "step": 26600
    },
    {
      "epoch": 7.7989449003516995,
      "grad_norm": 1.177030324935913,
      "learning_rate": 0.0004918004118561242,
      "loss": 0.1182,
      "step": 26610
    },
    {
      "epoch": 7.801875732708089,
      "grad_norm": 1.1708152294158936,
      "learning_rate": 0.0004917166155685358,
      "loss": 0.0885,
      "step": 26620
    },
    {
      "epoch": 7.804806565064478,
      "grad_norm": 1.0830309391021729,
      "learning_rate": 0.0004916328192809474,
      "loss": 0.1031,
      "step": 26630
    },
    {
      "epoch": 7.807737397420867,
      "grad_norm": 0.7172645330429077,
      "learning_rate": 0.0004915490229933591,
      "loss": 0.1148,
      "step": 26640
    },
    {
      "epoch": 7.8106682297772565,
      "grad_norm": 1.4322633743286133,
      "learning_rate": 0.0004914652267057708,
      "loss": 0.1267,
      "step": 26650
    },
    {
      "epoch": 7.813599062133646,
      "grad_norm": 0.6064997911453247,
      "learning_rate": 0.0004913814304181824,
      "loss": 0.1159,
      "step": 26660
    },
    {
      "epoch": 7.816529894490035,
      "grad_norm": 0.5029308795928955,
      "learning_rate": 0.000491297634130594,
      "loss": 0.1289,
      "step": 26670
    },
    {
      "epoch": 7.819460726846424,
      "grad_norm": 0.7860140800476074,
      "learning_rate": 0.0004912138378430056,
      "loss": 0.1297,
      "step": 26680
    },
    {
      "epoch": 7.8223915592028135,
      "grad_norm": 0.6336295008659363,
      "learning_rate": 0.0004911300415554173,
      "loss": 0.0941,
      "step": 26690
    },
    {
      "epoch": 7.825322391559203,
      "grad_norm": 2.078540563583374,
      "learning_rate": 0.0004910462452678289,
      "loss": 0.1217,
      "step": 26700
    },
    {
      "epoch": 7.828253223915592,
      "grad_norm": 2.016660213470459,
      "learning_rate": 0.0004909624489802405,
      "loss": 0.1017,
      "step": 26710
    },
    {
      "epoch": 7.831184056271981,
      "grad_norm": 0.8017416596412659,
      "learning_rate": 0.0004908786526926521,
      "loss": 0.131,
      "step": 26720
    },
    {
      "epoch": 7.8341148886283705,
      "grad_norm": 1.1794586181640625,
      "learning_rate": 0.0004907948564050637,
      "loss": 0.1083,
      "step": 26730
    },
    {
      "epoch": 7.83704572098476,
      "grad_norm": 0.825408935546875,
      "learning_rate": 0.0004907110601174755,
      "loss": 0.0932,
      "step": 26740
    },
    {
      "epoch": 7.839976553341149,
      "grad_norm": 1.0644564628601074,
      "learning_rate": 0.0004906272638298871,
      "loss": 0.1173,
      "step": 26750
    },
    {
      "epoch": 7.842907385697538,
      "grad_norm": 0.7772612571716309,
      "learning_rate": 0.0004905434675422988,
      "loss": 0.0977,
      "step": 26760
    },
    {
      "epoch": 7.8458382180539274,
      "grad_norm": 0.6417045593261719,
      "learning_rate": 0.0004904596712547104,
      "loss": 0.0937,
      "step": 26770
    },
    {
      "epoch": 7.848769050410317,
      "grad_norm": 0.6056572794914246,
      "learning_rate": 0.0004903758749671219,
      "loss": 0.101,
      "step": 26780
    },
    {
      "epoch": 7.851699882766706,
      "grad_norm": 0.9185227751731873,
      "learning_rate": 0.0004902920786795336,
      "loss": 0.1204,
      "step": 26790
    },
    {
      "epoch": 7.854630715123095,
      "grad_norm": 0.9160441756248474,
      "learning_rate": 0.0004902082823919452,
      "loss": 0.0984,
      "step": 26800
    },
    {
      "epoch": 7.857561547479484,
      "grad_norm": 0.515841543674469,
      "learning_rate": 0.000490124486104357,
      "loss": 0.1157,
      "step": 26810
    },
    {
      "epoch": 7.860492379835874,
      "grad_norm": 0.2573738992214203,
      "learning_rate": 0.0004900406898167686,
      "loss": 0.1011,
      "step": 26820
    },
    {
      "epoch": 7.863423212192263,
      "grad_norm": 0.7177868485450745,
      "learning_rate": 0.0004899568935291801,
      "loss": 0.0951,
      "step": 26830
    },
    {
      "epoch": 7.866354044548652,
      "grad_norm": 0.6738060116767883,
      "learning_rate": 0.0004898730972415918,
      "loss": 0.0989,
      "step": 26840
    },
    {
      "epoch": 7.869284876905041,
      "grad_norm": 0.9912840127944946,
      "learning_rate": 0.0004897893009540034,
      "loss": 0.0943,
      "step": 26850
    },
    {
      "epoch": 7.872215709261431,
      "grad_norm": 0.41123777627944946,
      "learning_rate": 0.0004897055046664151,
      "loss": 0.1099,
      "step": 26860
    },
    {
      "epoch": 7.87514654161782,
      "grad_norm": 1.7550698518753052,
      "learning_rate": 0.0004896217083788267,
      "loss": 0.1236,
      "step": 26870
    },
    {
      "epoch": 7.878077373974209,
      "grad_norm": 0.7669817805290222,
      "learning_rate": 0.0004895379120912383,
      "loss": 0.0802,
      "step": 26880
    },
    {
      "epoch": 7.8810082063305975,
      "grad_norm": 0.4192041754722595,
      "learning_rate": 0.0004894541158036499,
      "loss": 0.0918,
      "step": 26890
    },
    {
      "epoch": 7.883939038686987,
      "grad_norm": 0.9138805866241455,
      "learning_rate": 0.0004893703195160615,
      "loss": 0.09,
      "step": 26900
    },
    {
      "epoch": 7.886869871043376,
      "grad_norm": 0.8841928243637085,
      "learning_rate": 0.0004892865232284733,
      "loss": 0.1085,
      "step": 26910
    },
    {
      "epoch": 7.889800703399765,
      "grad_norm": 0.4241308271884918,
      "learning_rate": 0.0004892027269408849,
      "loss": 0.1201,
      "step": 26920
    },
    {
      "epoch": 7.8927315357561545,
      "grad_norm": 1.2056362628936768,
      "learning_rate": 0.0004891189306532965,
      "loss": 0.1083,
      "step": 26930
    },
    {
      "epoch": 7.895662368112544,
      "grad_norm": 0.5604645013809204,
      "learning_rate": 0.0004890351343657081,
      "loss": 0.1079,
      "step": 26940
    },
    {
      "epoch": 7.898593200468933,
      "grad_norm": 0.7671579122543335,
      "learning_rate": 0.0004889513380781197,
      "loss": 0.0994,
      "step": 26950
    },
    {
      "epoch": 7.901524032825322,
      "grad_norm": 0.7565087676048279,
      "learning_rate": 0.0004888675417905314,
      "loss": 0.1083,
      "step": 26960
    },
    {
      "epoch": 7.9044548651817115,
      "grad_norm": 0.9615740180015564,
      "learning_rate": 0.000488783745502943,
      "loss": 0.132,
      "step": 26970
    },
    {
      "epoch": 7.907385697538101,
      "grad_norm": 0.5694160461425781,
      "learning_rate": 0.0004886999492153546,
      "loss": 0.1119,
      "step": 26980
    },
    {
      "epoch": 7.91031652989449,
      "grad_norm": 0.6750121116638184,
      "learning_rate": 0.0004886161529277662,
      "loss": 0.0873,
      "step": 26990
    },
    {
      "epoch": 7.913247362250879,
      "grad_norm": 0.8368257284164429,
      "learning_rate": 0.0004885323566401779,
      "loss": 0.0988,
      "step": 27000
    },
    {
      "epoch": 7.9161781946072685,
      "grad_norm": 1.3994249105453491,
      "learning_rate": 0.0004884485603525896,
      "loss": 0.1044,
      "step": 27010
    },
    {
      "epoch": 7.919109026963658,
      "grad_norm": 1.6645231246948242,
      "learning_rate": 0.0004883647640650012,
      "loss": 0.1118,
      "step": 27020
    },
    {
      "epoch": 7.922039859320047,
      "grad_norm": 0.5240078568458557,
      "learning_rate": 0.00048828096777741285,
      "loss": 0.0962,
      "step": 27030
    },
    {
      "epoch": 7.924970691676436,
      "grad_norm": 0.7384213805198669,
      "learning_rate": 0.00048819717148982446,
      "loss": 0.1307,
      "step": 27040
    },
    {
      "epoch": 7.927901524032825,
      "grad_norm": 0.8169671893119812,
      "learning_rate": 0.000488113375202236,
      "loss": 0.1344,
      "step": 27050
    },
    {
      "epoch": 7.930832356389215,
      "grad_norm": 0.7688769102096558,
      "learning_rate": 0.00048802957891464773,
      "loss": 0.1126,
      "step": 27060
    },
    {
      "epoch": 7.933763188745604,
      "grad_norm": 1.0219894647598267,
      "learning_rate": 0.00048794578262705934,
      "loss": 0.1058,
      "step": 27070
    },
    {
      "epoch": 7.936694021101993,
      "grad_norm": 0.9246783256530762,
      "learning_rate": 0.000487861986339471,
      "loss": 0.0834,
      "step": 27080
    },
    {
      "epoch": 7.939624853458382,
      "grad_norm": 0.8655222058296204,
      "learning_rate": 0.0004877781900518826,
      "loss": 0.1008,
      "step": 27090
    },
    {
      "epoch": 7.942555685814772,
      "grad_norm": 0.642376184463501,
      "learning_rate": 0.0004876943937642942,
      "loss": 0.1021,
      "step": 27100
    },
    {
      "epoch": 7.945486518171161,
      "grad_norm": 2.190490245819092,
      "learning_rate": 0.0004876105974767059,
      "loss": 0.1102,
      "step": 27110
    },
    {
      "epoch": 7.94841735052755,
      "grad_norm": 1.823133945465088,
      "learning_rate": 0.0004875268011891175,
      "loss": 0.1146,
      "step": 27120
    },
    {
      "epoch": 7.951348182883939,
      "grad_norm": 1.6928187608718872,
      "learning_rate": 0.00048744300490152916,
      "loss": 0.125,
      "step": 27130
    },
    {
      "epoch": 7.954279015240328,
      "grad_norm": 1.6328976154327393,
      "learning_rate": 0.00048735920861394077,
      "loss": 0.1301,
      "step": 27140
    },
    {
      "epoch": 7.957209847596717,
      "grad_norm": 0.8507264852523804,
      "learning_rate": 0.00048727541232635244,
      "loss": 0.1012,
      "step": 27150
    },
    {
      "epoch": 7.960140679953106,
      "grad_norm": 0.8762220144271851,
      "learning_rate": 0.00048719161603876405,
      "loss": 0.1209,
      "step": 27160
    },
    {
      "epoch": 7.9630715123094955,
      "grad_norm": 1.3078618049621582,
      "learning_rate": 0.00048710781975117566,
      "loss": 0.1167,
      "step": 27170
    },
    {
      "epoch": 7.966002344665885,
      "grad_norm": 0.9391302466392517,
      "learning_rate": 0.0004870240234635873,
      "loss": 0.1312,
      "step": 27180
    },
    {
      "epoch": 7.968933177022274,
      "grad_norm": 2.5870115756988525,
      "learning_rate": 0.00048694022717599893,
      "loss": 0.12,
      "step": 27190
    },
    {
      "epoch": 7.971864009378663,
      "grad_norm": 1.291646122932434,
      "learning_rate": 0.0004868564308884106,
      "loss": 0.118,
      "step": 27200
    },
    {
      "epoch": 7.9747948417350525,
      "grad_norm": 1.3934048414230347,
      "learning_rate": 0.0004867726346008222,
      "loss": 0.0921,
      "step": 27210
    },
    {
      "epoch": 7.977725674091442,
      "grad_norm": 0.45057162642478943,
      "learning_rate": 0.0004866888383132338,
      "loss": 0.1209,
      "step": 27220
    },
    {
      "epoch": 7.980656506447831,
      "grad_norm": 0.7920573353767395,
      "learning_rate": 0.0004866050420256455,
      "loss": 0.112,
      "step": 27230
    },
    {
      "epoch": 7.98358733880422,
      "grad_norm": 0.5331494212150574,
      "learning_rate": 0.0004865212457380571,
      "loss": 0.1375,
      "step": 27240
    },
    {
      "epoch": 7.9865181711606095,
      "grad_norm": 0.974520206451416,
      "learning_rate": 0.00048643744945046875,
      "loss": 0.1023,
      "step": 27250
    },
    {
      "epoch": 7.989449003516999,
      "grad_norm": 1.1121803522109985,
      "learning_rate": 0.00048635365316288036,
      "loss": 0.0841,
      "step": 27260
    },
    {
      "epoch": 7.992379835873388,
      "grad_norm": 1.0490878820419312,
      "learning_rate": 0.00048626985687529197,
      "loss": 0.1459,
      "step": 27270
    },
    {
      "epoch": 7.995310668229777,
      "grad_norm": 0.5207968354225159,
      "learning_rate": 0.00048618606058770363,
      "loss": 0.0933,
      "step": 27280
    },
    {
      "epoch": 7.9982415005861665,
      "grad_norm": 0.940913736820221,
      "learning_rate": 0.00048610226430011524,
      "loss": 0.1022,
      "step": 27290
    },
    {
      "epoch": 8.0,
      "eval_accuracy": 0.6340257171117705,
      "eval_f1_macro": 0.6048843593198426,
      "eval_f1_micro": 0.7242050616482804,
      "eval_f1_weighted": 0.7086462422864449,
      "eval_loss": 0.10571015626192093,
      "eval_roc_auc": 0.8190983488110686,
      "eval_runtime": 152.4553,
      "eval_samples_per_second": 19.894,
      "eval_steps_per_second": 2.493,
      "step": 27296
    },
    {
      "epoch": 8.001172332942556,
      "grad_norm": 0.5952522158622742,
      "learning_rate": 0.0004860184680125269,
      "loss": 0.1198,
      "step": 27300
    },
    {
      "epoch": 8.004103165298945,
      "grad_norm": 1.072372317314148,
      "learning_rate": 0.0004859346717249385,
      "loss": 0.1021,
      "step": 27310
    },
    {
      "epoch": 8.007033997655334,
      "grad_norm": 0.8739628791809082,
      "learning_rate": 0.00048585087543735023,
      "loss": 0.0925,
      "step": 27320
    },
    {
      "epoch": 8.009964830011723,
      "grad_norm": 0.42216840386390686,
      "learning_rate": 0.0004857670791497618,
      "loss": 0.1106,
      "step": 27330
    },
    {
      "epoch": 8.012895662368113,
      "grad_norm": 1.321088194847107,
      "learning_rate": 0.0004856832828621734,
      "loss": 0.0957,
      "step": 27340
    },
    {
      "epoch": 8.015826494724502,
      "grad_norm": 0.783901572227478,
      "learning_rate": 0.0004855994865745851,
      "loss": 0.0907,
      "step": 27350
    },
    {
      "epoch": 8.018757327080891,
      "grad_norm": 1.1192644834518433,
      "learning_rate": 0.0004855156902869967,
      "loss": 0.106,
      "step": 27360
    },
    {
      "epoch": 8.02168815943728,
      "grad_norm": 0.894284725189209,
      "learning_rate": 0.0004854318939994084,
      "loss": 0.1136,
      "step": 27370
    },
    {
      "epoch": 8.02461899179367,
      "grad_norm": 0.8783031702041626,
      "learning_rate": 0.00048534809771181995,
      "loss": 0.0963,
      "step": 27380
    },
    {
      "epoch": 8.027549824150059,
      "grad_norm": 0.5219975113868713,
      "learning_rate": 0.00048526430142423156,
      "loss": 0.1034,
      "step": 27390
    },
    {
      "epoch": 8.030480656506448,
      "grad_norm": 0.7516340017318726,
      "learning_rate": 0.0004851805051366433,
      "loss": 0.093,
      "step": 27400
    },
    {
      "epoch": 8.033411488862837,
      "grad_norm": 1.016099452972412,
      "learning_rate": 0.00048509670884905483,
      "loss": 0.1111,
      "step": 27410
    },
    {
      "epoch": 8.036342321219227,
      "grad_norm": 1.0972539186477661,
      "learning_rate": 0.00048501291256146655,
      "loss": 0.0763,
      "step": 27420
    },
    {
      "epoch": 8.039273153575616,
      "grad_norm": 0.6556985378265381,
      "learning_rate": 0.00048492911627387816,
      "loss": 0.0824,
      "step": 27430
    },
    {
      "epoch": 8.042203985932005,
      "grad_norm": 1.701069951057434,
      "learning_rate": 0.0004848453199862897,
      "loss": 0.1005,
      "step": 27440
    },
    {
      "epoch": 8.045134818288394,
      "grad_norm": 0.5706660151481628,
      "learning_rate": 0.00048476152369870143,
      "loss": 0.0749,
      "step": 27450
    },
    {
      "epoch": 8.048065650644784,
      "grad_norm": 1.1216356754302979,
      "learning_rate": 0.00048467772741111304,
      "loss": 0.1167,
      "step": 27460
    },
    {
      "epoch": 8.050996483001173,
      "grad_norm": 0.44767317175865173,
      "learning_rate": 0.0004845939311235247,
      "loss": 0.0963,
      "step": 27470
    },
    {
      "epoch": 8.053927315357562,
      "grad_norm": 0.583162784576416,
      "learning_rate": 0.0004845101348359363,
      "loss": 0.0916,
      "step": 27480
    },
    {
      "epoch": 8.056858147713951,
      "grad_norm": 0.7618022561073303,
      "learning_rate": 0.000484426338548348,
      "loss": 0.0755,
      "step": 27490
    },
    {
      "epoch": 8.05978898007034,
      "grad_norm": 0.6121408343315125,
      "learning_rate": 0.0004843425422607596,
      "loss": 0.0971,
      "step": 27500
    },
    {
      "epoch": 8.06271981242673,
      "grad_norm": 1.1220741271972656,
      "learning_rate": 0.0004842587459731712,
      "loss": 0.1113,
      "step": 27510
    },
    {
      "epoch": 8.065650644783119,
      "grad_norm": 0.907589316368103,
      "learning_rate": 0.00048417494968558286,
      "loss": 0.105,
      "step": 27520
    },
    {
      "epoch": 8.068581477139508,
      "grad_norm": 0.6941221952438354,
      "learning_rate": 0.00048409115339799447,
      "loss": 0.0919,
      "step": 27530
    },
    {
      "epoch": 8.071512309495898,
      "grad_norm": 1.5834264755249023,
      "learning_rate": 0.00048400735711040614,
      "loss": 0.1014,
      "step": 27540
    },
    {
      "epoch": 8.074443141852287,
      "grad_norm": 1.2796615362167358,
      "learning_rate": 0.00048392356082281775,
      "loss": 0.0824,
      "step": 27550
    },
    {
      "epoch": 8.077373974208676,
      "grad_norm": 0.8905047178268433,
      "learning_rate": 0.00048383976453522936,
      "loss": 0.0971,
      "step": 27560
    },
    {
      "epoch": 8.080304806565065,
      "grad_norm": 0.9730421900749207,
      "learning_rate": 0.000483755968247641,
      "loss": 0.1088,
      "step": 27570
    },
    {
      "epoch": 8.083235638921455,
      "grad_norm": 0.7397729158401489,
      "learning_rate": 0.00048367217196005263,
      "loss": 0.0965,
      "step": 27580
    },
    {
      "epoch": 8.086166471277842,
      "grad_norm": 1.0992168188095093,
      "learning_rate": 0.0004835883756724643,
      "loss": 0.099,
      "step": 27590
    },
    {
      "epoch": 8.089097303634231,
      "grad_norm": 0.7891396880149841,
      "learning_rate": 0.0004835045793848759,
      "loss": 0.1315,
      "step": 27600
    },
    {
      "epoch": 8.09202813599062,
      "grad_norm": 1.284735918045044,
      "learning_rate": 0.0004834207830972875,
      "loss": 0.11,
      "step": 27610
    },
    {
      "epoch": 8.09495896834701,
      "grad_norm": 1.1814879179000854,
      "learning_rate": 0.0004833369868096992,
      "loss": 0.1081,
      "step": 27620
    },
    {
      "epoch": 8.097889800703399,
      "grad_norm": 1.573901891708374,
      "learning_rate": 0.0004832531905221108,
      "loss": 0.1093,
      "step": 27630
    },
    {
      "epoch": 8.100820633059788,
      "grad_norm": 0.30914536118507385,
      "learning_rate": 0.00048316939423452245,
      "loss": 0.0957,
      "step": 27640
    },
    {
      "epoch": 8.103751465416178,
      "grad_norm": 1.2632678747177124,
      "learning_rate": 0.00048308559794693406,
      "loss": 0.0946,
      "step": 27650
    },
    {
      "epoch": 8.106682297772567,
      "grad_norm": 0.42558541893959045,
      "learning_rate": 0.0004830018016593457,
      "loss": 0.082,
      "step": 27660
    },
    {
      "epoch": 8.109613130128956,
      "grad_norm": 0.6997050046920776,
      "learning_rate": 0.00048291800537175733,
      "loss": 0.1039,
      "step": 27670
    },
    {
      "epoch": 8.112543962485345,
      "grad_norm": 1.6650800704956055,
      "learning_rate": 0.00048283420908416894,
      "loss": 0.1107,
      "step": 27680
    },
    {
      "epoch": 8.115474794841735,
      "grad_norm": 0.9562669992446899,
      "learning_rate": 0.0004827504127965806,
      "loss": 0.1049,
      "step": 27690
    },
    {
      "epoch": 8.118405627198124,
      "grad_norm": 0.37192052602767944,
      "learning_rate": 0.0004826666165089922,
      "loss": 0.0972,
      "step": 27700
    },
    {
      "epoch": 8.121336459554513,
      "grad_norm": 0.5566682815551758,
      "learning_rate": 0.0004825828202214039,
      "loss": 0.0918,
      "step": 27710
    },
    {
      "epoch": 8.124267291910902,
      "grad_norm": 1.1566517353057861,
      "learning_rate": 0.0004824990239338155,
      "loss": 0.1058,
      "step": 27720
    },
    {
      "epoch": 8.127198124267291,
      "grad_norm": 1.8504234552383423,
      "learning_rate": 0.0004824152276462271,
      "loss": 0.0917,
      "step": 27730
    },
    {
      "epoch": 8.13012895662368,
      "grad_norm": 0.5723855495452881,
      "learning_rate": 0.00048233143135863876,
      "loss": 0.0984,
      "step": 27740
    },
    {
      "epoch": 8.13305978898007,
      "grad_norm": 1.543541431427002,
      "learning_rate": 0.0004822476350710504,
      "loss": 0.1098,
      "step": 27750
    },
    {
      "epoch": 8.13599062133646,
      "grad_norm": 0.7289904356002808,
      "learning_rate": 0.0004821638387834621,
      "loss": 0.1073,
      "step": 27760
    },
    {
      "epoch": 8.138921453692848,
      "grad_norm": 0.6510140895843506,
      "learning_rate": 0.00048208004249587365,
      "loss": 0.0848,
      "step": 27770
    },
    {
      "epoch": 8.141852286049238,
      "grad_norm": 0.5196846127510071,
      "learning_rate": 0.00048199624620828526,
      "loss": 0.0921,
      "step": 27780
    },
    {
      "epoch": 8.144783118405627,
      "grad_norm": 1.1963106393814087,
      "learning_rate": 0.000481912449920697,
      "loss": 0.0982,
      "step": 27790
    },
    {
      "epoch": 8.147713950762016,
      "grad_norm": 0.9237231016159058,
      "learning_rate": 0.00048182865363310853,
      "loss": 0.0831,
      "step": 27800
    },
    {
      "epoch": 8.150644783118405,
      "grad_norm": 0.2816806435585022,
      "learning_rate": 0.00048174485734552025,
      "loss": 0.099,
      "step": 27810
    },
    {
      "epoch": 8.153575615474795,
      "grad_norm": 0.8130453824996948,
      "learning_rate": 0.0004816610610579318,
      "loss": 0.0888,
      "step": 27820
    },
    {
      "epoch": 8.156506447831184,
      "grad_norm": 1.041825294494629,
      "learning_rate": 0.0004815772647703435,
      "loss": 0.0875,
      "step": 27830
    },
    {
      "epoch": 8.159437280187573,
      "grad_norm": 1.0369057655334473,
      "learning_rate": 0.00048149346848275513,
      "loss": 0.1185,
      "step": 27840
    },
    {
      "epoch": 8.162368112543962,
      "grad_norm": 0.6262798309326172,
      "learning_rate": 0.0004814096721951667,
      "loss": 0.1402,
      "step": 27850
    },
    {
      "epoch": 8.165298944900352,
      "grad_norm": 0.5694050788879395,
      "learning_rate": 0.0004813258759075784,
      "loss": 0.1053,
      "step": 27860
    },
    {
      "epoch": 8.168229777256741,
      "grad_norm": 0.649961531162262,
      "learning_rate": 0.00048124207961999,
      "loss": 0.1136,
      "step": 27870
    },
    {
      "epoch": 8.17116060961313,
      "grad_norm": 0.8112384080886841,
      "learning_rate": 0.0004811582833324017,
      "loss": 0.081,
      "step": 27880
    },
    {
      "epoch": 8.17409144196952,
      "grad_norm": 1.3266546726226807,
      "learning_rate": 0.0004810744870448133,
      "loss": 0.0955,
      "step": 27890
    },
    {
      "epoch": 8.177022274325909,
      "grad_norm": 1.10960853099823,
      "learning_rate": 0.0004809906907572249,
      "loss": 0.0896,
      "step": 27900
    },
    {
      "epoch": 8.179953106682298,
      "grad_norm": 1.8417680263519287,
      "learning_rate": 0.00048090689446963656,
      "loss": 0.1104,
      "step": 27910
    },
    {
      "epoch": 8.182883939038687,
      "grad_norm": 0.3537755310535431,
      "learning_rate": 0.00048082309818204817,
      "loss": 0.0913,
      "step": 27920
    },
    {
      "epoch": 8.185814771395076,
      "grad_norm": 0.9447631239891052,
      "learning_rate": 0.00048073930189445984,
      "loss": 0.1086,
      "step": 27930
    },
    {
      "epoch": 8.188745603751466,
      "grad_norm": 0.9883641004562378,
      "learning_rate": 0.00048065550560687145,
      "loss": 0.1057,
      "step": 27940
    },
    {
      "epoch": 8.191676436107855,
      "grad_norm": 1.4217090606689453,
      "learning_rate": 0.00048057170931928306,
      "loss": 0.0943,
      "step": 27950
    },
    {
      "epoch": 8.194607268464244,
      "grad_norm": 1.039765477180481,
      "learning_rate": 0.0004804879130316947,
      "loss": 0.124,
      "step": 27960
    },
    {
      "epoch": 8.197538100820633,
      "grad_norm": 0.7756993174552917,
      "learning_rate": 0.00048040411674410633,
      "loss": 0.0884,
      "step": 27970
    },
    {
      "epoch": 8.200468933177023,
      "grad_norm": 0.786526083946228,
      "learning_rate": 0.000480320320456518,
      "loss": 0.108,
      "step": 27980
    },
    {
      "epoch": 8.203399765533412,
      "grad_norm": 0.299309104681015,
      "learning_rate": 0.0004802365241689296,
      "loss": 0.1001,
      "step": 27990
    },
    {
      "epoch": 8.206330597889801,
      "grad_norm": 1.0046566724777222,
      "learning_rate": 0.00048015272788134127,
      "loss": 0.1121,
      "step": 28000
    },
    {
      "epoch": 8.20926143024619,
      "grad_norm": 2.4338016510009766,
      "learning_rate": 0.0004800689315937529,
      "loss": 0.1028,
      "step": 28010
    },
    {
      "epoch": 8.21219226260258,
      "grad_norm": 0.7105827927589417,
      "learning_rate": 0.0004799851353061645,
      "loss": 0.0798,
      "step": 28020
    },
    {
      "epoch": 8.215123094958969,
      "grad_norm": 1.4748728275299072,
      "learning_rate": 0.00047990133901857615,
      "loss": 0.1304,
      "step": 28030
    },
    {
      "epoch": 8.218053927315358,
      "grad_norm": 0.9254090785980225,
      "learning_rate": 0.00047981754273098776,
      "loss": 0.0936,
      "step": 28040
    },
    {
      "epoch": 8.220984759671747,
      "grad_norm": 0.9008448719978333,
      "learning_rate": 0.0004797337464433994,
      "loss": 0.0909,
      "step": 28050
    },
    {
      "epoch": 8.223915592028137,
      "grad_norm": 1.418418526649475,
      "learning_rate": 0.00047964995015581103,
      "loss": 0.1003,
      "step": 28060
    },
    {
      "epoch": 8.226846424384526,
      "grad_norm": 1.834731101989746,
      "learning_rate": 0.00047956615386822264,
      "loss": 0.0861,
      "step": 28070
    },
    {
      "epoch": 8.229777256740915,
      "grad_norm": 0.9147148728370667,
      "learning_rate": 0.0004794823575806343,
      "loss": 0.1024,
      "step": 28080
    },
    {
      "epoch": 8.232708089097304,
      "grad_norm": 0.9125981330871582,
      "learning_rate": 0.0004793985612930459,
      "loss": 0.1261,
      "step": 28090
    },
    {
      "epoch": 8.235638921453694,
      "grad_norm": 0.42697572708129883,
      "learning_rate": 0.0004793147650054576,
      "loss": 0.1027,
      "step": 28100
    },
    {
      "epoch": 8.238569753810083,
      "grad_norm": 0.9189801216125488,
      "learning_rate": 0.0004792309687178692,
      "loss": 0.1034,
      "step": 28110
    },
    {
      "epoch": 8.241500586166472,
      "grad_norm": 0.9162117838859558,
      "learning_rate": 0.0004791471724302808,
      "loss": 0.1249,
      "step": 28120
    },
    {
      "epoch": 8.244431418522861,
      "grad_norm": 1.1161304712295532,
      "learning_rate": 0.00047906337614269246,
      "loss": 0.1018,
      "step": 28130
    },
    {
      "epoch": 8.24736225087925,
      "grad_norm": 0.6496874690055847,
      "learning_rate": 0.0004789795798551041,
      "loss": 0.1164,
      "step": 28140
    },
    {
      "epoch": 8.25029308323564,
      "grad_norm": 0.471068799495697,
      "learning_rate": 0.00047889578356751574,
      "loss": 0.1007,
      "step": 28150
    },
    {
      "epoch": 8.253223915592027,
      "grad_norm": 0.8974026441574097,
      "learning_rate": 0.00047881198727992735,
      "loss": 0.1069,
      "step": 28160
    },
    {
      "epoch": 8.256154747948417,
      "grad_norm": 0.7612598538398743,
      "learning_rate": 0.00047872819099233907,
      "loss": 0.1302,
      "step": 28170
    },
    {
      "epoch": 8.259085580304806,
      "grad_norm": 0.4526318311691284,
      "learning_rate": 0.0004786443947047506,
      "loss": 0.1023,
      "step": 28180
    },
    {
      "epoch": 8.262016412661195,
      "grad_norm": 1.0331552028656006,
      "learning_rate": 0.00047856059841716223,
      "loss": 0.0977,
      "step": 28190
    },
    {
      "epoch": 8.264947245017584,
      "grad_norm": 1.2178401947021484,
      "learning_rate": 0.00047847680212957395,
      "loss": 0.1094,
      "step": 28200
    },
    {
      "epoch": 8.267878077373974,
      "grad_norm": 0.665644645690918,
      "learning_rate": 0.0004783930058419855,
      "loss": 0.0848,
      "step": 28210
    },
    {
      "epoch": 8.270808909730363,
      "grad_norm": 1.6029819250106812,
      "learning_rate": 0.0004783092095543972,
      "loss": 0.1105,
      "step": 28220
    },
    {
      "epoch": 8.273739742086752,
      "grad_norm": 0.747970700263977,
      "learning_rate": 0.00047822541326680883,
      "loss": 0.1235,
      "step": 28230
    },
    {
      "epoch": 8.276670574443141,
      "grad_norm": 0.8573651313781738,
      "learning_rate": 0.0004781416169792204,
      "loss": 0.1001,
      "step": 28240
    },
    {
      "epoch": 8.27960140679953,
      "grad_norm": 1.0779399871826172,
      "learning_rate": 0.0004780578206916321,
      "loss": 0.1309,
      "step": 28250
    },
    {
      "epoch": 8.28253223915592,
      "grad_norm": 0.9628547430038452,
      "learning_rate": 0.00047797402440404366,
      "loss": 0.1359,
      "step": 28260
    },
    {
      "epoch": 8.285463071512309,
      "grad_norm": 1.3103331327438354,
      "learning_rate": 0.0004778902281164554,
      "loss": 0.0938,
      "step": 28270
    },
    {
      "epoch": 8.288393903868698,
      "grad_norm": 0.7047619819641113,
      "learning_rate": 0.000477806431828867,
      "loss": 0.1126,
      "step": 28280
    },
    {
      "epoch": 8.291324736225087,
      "grad_norm": 0.7609540820121765,
      "learning_rate": 0.00047772263554127854,
      "loss": 0.0991,
      "step": 28290
    },
    {
      "epoch": 8.294255568581477,
      "grad_norm": 2.381138563156128,
      "learning_rate": 0.00047763883925369026,
      "loss": 0.1099,
      "step": 28300
    },
    {
      "epoch": 8.297186400937866,
      "grad_norm": 0.881575882434845,
      "learning_rate": 0.00047755504296610187,
      "loss": 0.0952,
      "step": 28310
    },
    {
      "epoch": 8.300117233294255,
      "grad_norm": 1.8984193801879883,
      "learning_rate": 0.00047747124667851354,
      "loss": 0.078,
      "step": 28320
    },
    {
      "epoch": 8.303048065650644,
      "grad_norm": 1.2619024515151978,
      "learning_rate": 0.00047738745039092515,
      "loss": 0.0964,
      "step": 28330
    },
    {
      "epoch": 8.305978898007034,
      "grad_norm": 1.0904896259307861,
      "learning_rate": 0.00047730365410333676,
      "loss": 0.0694,
      "step": 28340
    },
    {
      "epoch": 8.308909730363423,
      "grad_norm": 0.7918073534965515,
      "learning_rate": 0.0004772198578157484,
      "loss": 0.0945,
      "step": 28350
    },
    {
      "epoch": 8.311840562719812,
      "grad_norm": 1.084898591041565,
      "learning_rate": 0.00047713606152816003,
      "loss": 0.0955,
      "step": 28360
    },
    {
      "epoch": 8.314771395076201,
      "grad_norm": 0.9408053755760193,
      "learning_rate": 0.0004770522652405717,
      "loss": 0.0852,
      "step": 28370
    },
    {
      "epoch": 8.31770222743259,
      "grad_norm": 1.1315962076187134,
      "learning_rate": 0.0004769684689529833,
      "loss": 0.121,
      "step": 28380
    },
    {
      "epoch": 8.32063305978898,
      "grad_norm": 1.4708253145217896,
      "learning_rate": 0.00047688467266539497,
      "loss": 0.1017,
      "step": 28390
    },
    {
      "epoch": 8.32356389214537,
      "grad_norm": 0.744425892829895,
      "learning_rate": 0.0004768008763778066,
      "loss": 0.0838,
      "step": 28400
    },
    {
      "epoch": 8.326494724501758,
      "grad_norm": 0.7561160326004028,
      "learning_rate": 0.0004767170800902182,
      "loss": 0.0914,
      "step": 28410
    },
    {
      "epoch": 8.329425556858148,
      "grad_norm": 0.7669458985328674,
      "learning_rate": 0.00047663328380262985,
      "loss": 0.1085,
      "step": 28420
    },
    {
      "epoch": 8.332356389214537,
      "grad_norm": 0.8855232000350952,
      "learning_rate": 0.00047654948751504146,
      "loss": 0.1165,
      "step": 28430
    },
    {
      "epoch": 8.335287221570926,
      "grad_norm": 0.47084781527519226,
      "learning_rate": 0.0004764656912274531,
      "loss": 0.124,
      "step": 28440
    },
    {
      "epoch": 8.338218053927315,
      "grad_norm": 1.430322527885437,
      "learning_rate": 0.00047638189493986473,
      "loss": 0.1307,
      "step": 28450
    },
    {
      "epoch": 8.341148886283705,
      "grad_norm": 0.7821995615959167,
      "learning_rate": 0.00047629809865227634,
      "loss": 0.1286,
      "step": 28460
    },
    {
      "epoch": 8.344079718640094,
      "grad_norm": 1.2837978601455688,
      "learning_rate": 0.000476214302364688,
      "loss": 0.1087,
      "step": 28470
    },
    {
      "epoch": 8.347010550996483,
      "grad_norm": 0.5108187794685364,
      "learning_rate": 0.0004761305060770996,
      "loss": 0.1192,
      "step": 28480
    },
    {
      "epoch": 8.349941383352872,
      "grad_norm": 2.6653401851654053,
      "learning_rate": 0.0004760467097895113,
      "loss": 0.112,
      "step": 28490
    },
    {
      "epoch": 8.352872215709262,
      "grad_norm": 0.8201626539230347,
      "learning_rate": 0.0004759629135019229,
      "loss": 0.0826,
      "step": 28500
    },
    {
      "epoch": 8.35580304806565,
      "grad_norm": 1.239530324935913,
      "learning_rate": 0.0004758791172143345,
      "loss": 0.0853,
      "step": 28510
    },
    {
      "epoch": 8.35873388042204,
      "grad_norm": 1.0357346534729004,
      "learning_rate": 0.00047579532092674616,
      "loss": 0.1011,
      "step": 28520
    },
    {
      "epoch": 8.36166471277843,
      "grad_norm": 0.8163228034973145,
      "learning_rate": 0.0004757115246391578,
      "loss": 0.0982,
      "step": 28530
    },
    {
      "epoch": 8.364595545134819,
      "grad_norm": 0.6478409171104431,
      "learning_rate": 0.00047562772835156944,
      "loss": 0.088,
      "step": 28540
    },
    {
      "epoch": 8.367526377491208,
      "grad_norm": 0.6976346373558044,
      "learning_rate": 0.00047554393206398105,
      "loss": 0.1321,
      "step": 28550
    },
    {
      "epoch": 8.370457209847597,
      "grad_norm": 1.1250079870224,
      "learning_rate": 0.00047546013577639277,
      "loss": 0.0909,
      "step": 28560
    },
    {
      "epoch": 8.373388042203986,
      "grad_norm": 0.30918580293655396,
      "learning_rate": 0.0004753763394888043,
      "loss": 0.0755,
      "step": 28570
    },
    {
      "epoch": 8.376318874560376,
      "grad_norm": 1.05021071434021,
      "learning_rate": 0.00047529254320121593,
      "loss": 0.1126,
      "step": 28580
    },
    {
      "epoch": 8.379249706916765,
      "grad_norm": 0.5577785968780518,
      "learning_rate": 0.0004752087469136276,
      "loss": 0.0998,
      "step": 28590
    },
    {
      "epoch": 8.382180539273154,
      "grad_norm": 0.6202614903450012,
      "learning_rate": 0.0004751249506260392,
      "loss": 0.1007,
      "step": 28600
    },
    {
      "epoch": 8.385111371629543,
      "grad_norm": 0.8661931753158569,
      "learning_rate": 0.0004750411543384509,
      "loss": 0.1165,
      "step": 28610
    },
    {
      "epoch": 8.388042203985933,
      "grad_norm": 0.36048269271850586,
      "learning_rate": 0.0004749573580508625,
      "loss": 0.0969,
      "step": 28620
    },
    {
      "epoch": 8.390973036342322,
      "grad_norm": 1.1235886812210083,
      "learning_rate": 0.0004748735617632741,
      "loss": 0.0928,
      "step": 28630
    },
    {
      "epoch": 8.393903868698711,
      "grad_norm": 1.7042958736419678,
      "learning_rate": 0.0004747897654756858,
      "loss": 0.1375,
      "step": 28640
    },
    {
      "epoch": 8.3968347010551,
      "grad_norm": 0.9173551797866821,
      "learning_rate": 0.00047470596918809736,
      "loss": 0.0992,
      "step": 28650
    },
    {
      "epoch": 8.39976553341149,
      "grad_norm": 0.837835967540741,
      "learning_rate": 0.0004746221729005091,
      "loss": 0.1043,
      "step": 28660
    },
    {
      "epoch": 8.402696365767879,
      "grad_norm": 0.4891829788684845,
      "learning_rate": 0.0004745383766129207,
      "loss": 0.0798,
      "step": 28670
    },
    {
      "epoch": 8.405627198124268,
      "grad_norm": 0.6708767414093018,
      "learning_rate": 0.00047445458032533224,
      "loss": 0.1083,
      "step": 28680
    },
    {
      "epoch": 8.408558030480657,
      "grad_norm": 1.3761005401611328,
      "learning_rate": 0.00047437078403774396,
      "loss": 0.0853,
      "step": 28690
    },
    {
      "epoch": 8.411488862837047,
      "grad_norm": 0.7369723916053772,
      "learning_rate": 0.0004742869877501555,
      "loss": 0.0823,
      "step": 28700
    },
    {
      "epoch": 8.414419695193434,
      "grad_norm": 1.1972905397415161,
      "learning_rate": 0.00047420319146256724,
      "loss": 0.1046,
      "step": 28710
    },
    {
      "epoch": 8.417350527549825,
      "grad_norm": 0.7406697273254395,
      "learning_rate": 0.00047411939517497885,
      "loss": 0.0926,
      "step": 28720
    },
    {
      "epoch": 8.420281359906213,
      "grad_norm": 0.9914187788963318,
      "learning_rate": 0.0004740355988873905,
      "loss": 0.1041,
      "step": 28730
    },
    {
      "epoch": 8.423212192262602,
      "grad_norm": 0.9294220209121704,
      "learning_rate": 0.0004739518025998021,
      "loss": 0.113,
      "step": 28740
    },
    {
      "epoch": 8.426143024618991,
      "grad_norm": 0.7862476110458374,
      "learning_rate": 0.00047386800631221373,
      "loss": 0.1176,
      "step": 28750
    },
    {
      "epoch": 8.42907385697538,
      "grad_norm": 0.9532790184020996,
      "learning_rate": 0.0004737842100246254,
      "loss": 0.0899,
      "step": 28760
    },
    {
      "epoch": 8.43200468933177,
      "grad_norm": 0.9331392645835876,
      "learning_rate": 0.000473700413737037,
      "loss": 0.1015,
      "step": 28770
    },
    {
      "epoch": 8.434935521688159,
      "grad_norm": 0.6605994701385498,
      "learning_rate": 0.00047361661744944867,
      "loss": 0.0754,
      "step": 28780
    },
    {
      "epoch": 8.437866354044548,
      "grad_norm": 1.018039584159851,
      "learning_rate": 0.0004735328211618603,
      "loss": 0.0983,
      "step": 28790
    },
    {
      "epoch": 8.440797186400937,
      "grad_norm": 0.7884618043899536,
      "learning_rate": 0.0004734490248742719,
      "loss": 0.0951,
      "step": 28800
    },
    {
      "epoch": 8.443728018757326,
      "grad_norm": 0.6522725224494934,
      "learning_rate": 0.00047336522858668355,
      "loss": 0.0937,
      "step": 28810
    },
    {
      "epoch": 8.446658851113716,
      "grad_norm": 0.8133108615875244,
      "learning_rate": 0.00047328143229909516,
      "loss": 0.1063,
      "step": 28820
    },
    {
      "epoch": 8.449589683470105,
      "grad_norm": 0.996816873550415,
      "learning_rate": 0.0004731976360115068,
      "loss": 0.1288,
      "step": 28830
    },
    {
      "epoch": 8.452520515826494,
      "grad_norm": 1.107086420059204,
      "learning_rate": 0.00047311383972391843,
      "loss": 0.114,
      "step": 28840
    },
    {
      "epoch": 8.455451348182883,
      "grad_norm": 0.7588015794754028,
      "learning_rate": 0.00047303004343633004,
      "loss": 0.09,
      "step": 28850
    },
    {
      "epoch": 8.458382180539273,
      "grad_norm": 0.717461109161377,
      "learning_rate": 0.0004729462471487417,
      "loss": 0.1315,
      "step": 28860
    },
    {
      "epoch": 8.461313012895662,
      "grad_norm": 0.576545000076294,
      "learning_rate": 0.0004728624508611533,
      "loss": 0.0974,
      "step": 28870
    },
    {
      "epoch": 8.464243845252051,
      "grad_norm": 0.7328510284423828,
      "learning_rate": 0.000472778654573565,
      "loss": 0.1078,
      "step": 28880
    },
    {
      "epoch": 8.46717467760844,
      "grad_norm": 0.707255482673645,
      "learning_rate": 0.0004726948582859766,
      "loss": 0.1311,
      "step": 28890
    },
    {
      "epoch": 8.47010550996483,
      "grad_norm": 1.8043957948684692,
      "learning_rate": 0.00047261106199838825,
      "loss": 0.1111,
      "step": 28900
    },
    {
      "epoch": 8.473036342321219,
      "grad_norm": 0.9847107529640198,
      "learning_rate": 0.00047252726571079986,
      "loss": 0.1225,
      "step": 28910
    },
    {
      "epoch": 8.475967174677608,
      "grad_norm": 1.3394933938980103,
      "learning_rate": 0.0004724434694232115,
      "loss": 0.0922,
      "step": 28920
    },
    {
      "epoch": 8.478898007033997,
      "grad_norm": 0.8918478488922119,
      "learning_rate": 0.00047235967313562314,
      "loss": 0.1175,
      "step": 28930
    },
    {
      "epoch": 8.481828839390387,
      "grad_norm": 1.6472409963607788,
      "learning_rate": 0.00047227587684803475,
      "loss": 0.134,
      "step": 28940
    },
    {
      "epoch": 8.484759671746776,
      "grad_norm": 0.5322094559669495,
      "learning_rate": 0.0004721920805604464,
      "loss": 0.1093,
      "step": 28950
    },
    {
      "epoch": 8.487690504103165,
      "grad_norm": 1.5331600904464722,
      "learning_rate": 0.000472108284272858,
      "loss": 0.0994,
      "step": 28960
    },
    {
      "epoch": 8.490621336459554,
      "grad_norm": 1.2482059001922607,
      "learning_rate": 0.00047202448798526963,
      "loss": 0.0939,
      "step": 28970
    },
    {
      "epoch": 8.493552168815944,
      "grad_norm": 1.3014603853225708,
      "learning_rate": 0.0004719406916976813,
      "loss": 0.0941,
      "step": 28980
    },
    {
      "epoch": 8.496483001172333,
      "grad_norm": 0.6127099394798279,
      "learning_rate": 0.0004718568954100929,
      "loss": 0.093,
      "step": 28990
    },
    {
      "epoch": 8.499413833528722,
      "grad_norm": 1.3000110387802124,
      "learning_rate": 0.0004717730991225046,
      "loss": 0.0938,
      "step": 29000
    },
    {
      "epoch": 8.502344665885111,
      "grad_norm": 0.8966729044914246,
      "learning_rate": 0.0004716893028349162,
      "loss": 0.0918,
      "step": 29010
    },
    {
      "epoch": 8.5052754982415,
      "grad_norm": 1.3613706827163696,
      "learning_rate": 0.0004716055065473278,
      "loss": 0.0976,
      "step": 29020
    },
    {
      "epoch": 8.50820633059789,
      "grad_norm": 1.334797978401184,
      "learning_rate": 0.00047152171025973945,
      "loss": 0.0891,
      "step": 29030
    },
    {
      "epoch": 8.51113716295428,
      "grad_norm": 0.68816739320755,
      "learning_rate": 0.00047143791397215106,
      "loss": 0.097,
      "step": 29040
    },
    {
      "epoch": 8.514067995310668,
      "grad_norm": 1.6426360607147217,
      "learning_rate": 0.0004713541176845628,
      "loss": 0.1025,
      "step": 29050
    },
    {
      "epoch": 8.516998827667058,
      "grad_norm": 2.4173834323883057,
      "learning_rate": 0.00047127032139697433,
      "loss": 0.0939,
      "step": 29060
    },
    {
      "epoch": 8.519929660023447,
      "grad_norm": 0.6256209015846252,
      "learning_rate": 0.00047118652510938605,
      "loss": 0.1202,
      "step": 29070
    },
    {
      "epoch": 8.522860492379836,
      "grad_norm": 0.8322795629501343,
      "learning_rate": 0.00047110272882179766,
      "loss": 0.096,
      "step": 29080
    },
    {
      "epoch": 8.525791324736225,
      "grad_norm": 0.7867786288261414,
      "learning_rate": 0.0004710189325342092,
      "loss": 0.099,
      "step": 29090
    },
    {
      "epoch": 8.528722157092615,
      "grad_norm": 1.2807254791259766,
      "learning_rate": 0.00047093513624662094,
      "loss": 0.0994,
      "step": 29100
    },
    {
      "epoch": 8.531652989449004,
      "grad_norm": 3.1347970962524414,
      "learning_rate": 0.00047085133995903255,
      "loss": 0.1038,
      "step": 29110
    },
    {
      "epoch": 8.534583821805393,
      "grad_norm": 1.6613614559173584,
      "learning_rate": 0.0004707675436714442,
      "loss": 0.126,
      "step": 29120
    },
    {
      "epoch": 8.537514654161782,
      "grad_norm": 0.8478803038597107,
      "learning_rate": 0.0004706837473838558,
      "loss": 0.0987,
      "step": 29130
    },
    {
      "epoch": 8.540445486518172,
      "grad_norm": 0.8669002652168274,
      "learning_rate": 0.00047059995109626743,
      "loss": 0.1054,
      "step": 29140
    },
    {
      "epoch": 8.54337631887456,
      "grad_norm": 0.8734533190727234,
      "learning_rate": 0.0004705161548086791,
      "loss": 0.1034,
      "step": 29150
    },
    {
      "epoch": 8.54630715123095,
      "grad_norm": 1.0205714702606201,
      "learning_rate": 0.0004704323585210907,
      "loss": 0.1203,
      "step": 29160
    },
    {
      "epoch": 8.54923798358734,
      "grad_norm": 0.3795599639415741,
      "learning_rate": 0.00047034856223350237,
      "loss": 0.0912,
      "step": 29170
    },
    {
      "epoch": 8.552168815943729,
      "grad_norm": 0.7364940047264099,
      "learning_rate": 0.000470264765945914,
      "loss": 0.0712,
      "step": 29180
    },
    {
      "epoch": 8.555099648300118,
      "grad_norm": 1.4030781984329224,
      "learning_rate": 0.0004701809696583256,
      "loss": 0.1255,
      "step": 29190
    },
    {
      "epoch": 8.558030480656507,
      "grad_norm": 0.7423578500747681,
      "learning_rate": 0.00047009717337073725,
      "loss": 0.1255,
      "step": 29200
    },
    {
      "epoch": 8.560961313012896,
      "grad_norm": 0.8162586688995361,
      "learning_rate": 0.00047001337708314886,
      "loss": 0.1034,
      "step": 29210
    },
    {
      "epoch": 8.563892145369286,
      "grad_norm": 1.0390517711639404,
      "learning_rate": 0.0004699295807955605,
      "loss": 0.1122,
      "step": 29220
    },
    {
      "epoch": 8.566822977725675,
      "grad_norm": 1.4150575399398804,
      "learning_rate": 0.00046984578450797213,
      "loss": 0.1165,
      "step": 29230
    },
    {
      "epoch": 8.569753810082064,
      "grad_norm": 1.42728590965271,
      "learning_rate": 0.0004697619882203838,
      "loss": 0.0824,
      "step": 29240
    },
    {
      "epoch": 8.572684642438453,
      "grad_norm": 1.4986743927001953,
      "learning_rate": 0.0004696781919327954,
      "loss": 0.1084,
      "step": 29250
    },
    {
      "epoch": 8.575615474794843,
      "grad_norm": 1.1622140407562256,
      "learning_rate": 0.000469594395645207,
      "loss": 0.1203,
      "step": 29260
    },
    {
      "epoch": 8.578546307151232,
      "grad_norm": 0.6234248280525208,
      "learning_rate": 0.0004695105993576187,
      "loss": 0.0894,
      "step": 29270
    },
    {
      "epoch": 8.58147713950762,
      "grad_norm": 1.4340585470199585,
      "learning_rate": 0.0004694268030700303,
      "loss": 0.1119,
      "step": 29280
    },
    {
      "epoch": 8.58440797186401,
      "grad_norm": 1.2962006330490112,
      "learning_rate": 0.00046934300678244195,
      "loss": 0.114,
      "step": 29290
    },
    {
      "epoch": 8.587338804220398,
      "grad_norm": 0.9586053490638733,
      "learning_rate": 0.00046925921049485356,
      "loss": 0.1103,
      "step": 29300
    },
    {
      "epoch": 8.590269636576787,
      "grad_norm": 1.7237027883529663,
      "learning_rate": 0.0004691754142072652,
      "loss": 0.1047,
      "step": 29310
    },
    {
      "epoch": 8.593200468933176,
      "grad_norm": 0.6813157200813293,
      "learning_rate": 0.00046909161791967684,
      "loss": 0.1114,
      "step": 29320
    },
    {
      "epoch": 8.596131301289565,
      "grad_norm": 1.3114773035049438,
      "learning_rate": 0.00046900782163208845,
      "loss": 0.0919,
      "step": 29330
    },
    {
      "epoch": 8.599062133645955,
      "grad_norm": 0.2690736949443817,
      "learning_rate": 0.0004689240253445001,
      "loss": 0.0844,
      "step": 29340
    },
    {
      "epoch": 8.601992966002344,
      "grad_norm": 0.5543291568756104,
      "learning_rate": 0.0004688402290569117,
      "loss": 0.0945,
      "step": 29350
    },
    {
      "epoch": 8.604923798358733,
      "grad_norm": 1.249734878540039,
      "learning_rate": 0.00046875643276932333,
      "loss": 0.1036,
      "step": 29360
    },
    {
      "epoch": 8.607854630715122,
      "grad_norm": 0.8101787567138672,
      "learning_rate": 0.000468672636481735,
      "loss": 0.1092,
      "step": 29370
    },
    {
      "epoch": 8.610785463071512,
      "grad_norm": 0.6904309988021851,
      "learning_rate": 0.0004685888401941466,
      "loss": 0.1113,
      "step": 29380
    },
    {
      "epoch": 8.613716295427901,
      "grad_norm": 1.1816279888153076,
      "learning_rate": 0.00046850504390655827,
      "loss": 0.0974,
      "step": 29390
    },
    {
      "epoch": 8.61664712778429,
      "grad_norm": 0.9050284624099731,
      "learning_rate": 0.0004684212476189699,
      "loss": 0.0806,
      "step": 29400
    },
    {
      "epoch": 8.61957796014068,
      "grad_norm": 1.1374855041503906,
      "learning_rate": 0.0004683374513313815,
      "loss": 0.0847,
      "step": 29410
    },
    {
      "epoch": 8.622508792497069,
      "grad_norm": 0.6379604339599609,
      "learning_rate": 0.00046825365504379315,
      "loss": 0.0945,
      "step": 29420
    },
    {
      "epoch": 8.625439624853458,
      "grad_norm": 1.6336017847061157,
      "learning_rate": 0.00046816985875620476,
      "loss": 0.0738,
      "step": 29430
    },
    {
      "epoch": 8.628370457209847,
      "grad_norm": 1.248153567314148,
      "learning_rate": 0.0004680860624686165,
      "loss": 0.1115,
      "step": 29440
    },
    {
      "epoch": 8.631301289566236,
      "grad_norm": 1.0775251388549805,
      "learning_rate": 0.00046800226618102803,
      "loss": 0.1031,
      "step": 29450
    },
    {
      "epoch": 8.634232121922626,
      "grad_norm": 1.0371402502059937,
      "learning_rate": 0.00046791846989343975,
      "loss": 0.1292,
      "step": 29460
    },
    {
      "epoch": 8.637162954279015,
      "grad_norm": 0.5387911200523376,
      "learning_rate": 0.0004678346736058513,
      "loss": 0.0865,
      "step": 29470
    },
    {
      "epoch": 8.640093786635404,
      "grad_norm": 1.1000488996505737,
      "learning_rate": 0.0004677508773182629,
      "loss": 0.1099,
      "step": 29480
    },
    {
      "epoch": 8.643024618991793,
      "grad_norm": 0.9488044381141663,
      "learning_rate": 0.00046766708103067464,
      "loss": 0.1115,
      "step": 29490
    },
    {
      "epoch": 8.645955451348183,
      "grad_norm": 0.9757809638977051,
      "learning_rate": 0.0004675832847430862,
      "loss": 0.093,
      "step": 29500
    },
    {
      "epoch": 8.648886283704572,
      "grad_norm": 0.8484904766082764,
      "learning_rate": 0.0004674994884554979,
      "loss": 0.1031,
      "step": 29510
    },
    {
      "epoch": 8.651817116060961,
      "grad_norm": 0.7521646022796631,
      "learning_rate": 0.0004674156921679095,
      "loss": 0.1356,
      "step": 29520
    },
    {
      "epoch": 8.65474794841735,
      "grad_norm": 0.8256587982177734,
      "learning_rate": 0.0004673318958803211,
      "loss": 0.1283,
      "step": 29530
    },
    {
      "epoch": 8.65767878077374,
      "grad_norm": 0.4548223316669464,
      "learning_rate": 0.0004672480995927328,
      "loss": 0.0868,
      "step": 29540
    },
    {
      "epoch": 8.660609613130129,
      "grad_norm": 0.8213234543800354,
      "learning_rate": 0.0004671643033051444,
      "loss": 0.1023,
      "step": 29550
    },
    {
      "epoch": 8.663540445486518,
      "grad_norm": 0.7500166893005371,
      "learning_rate": 0.00046708050701755607,
      "loss": 0.1046,
      "step": 29560
    },
    {
      "epoch": 8.666471277842907,
      "grad_norm": 0.9687725305557251,
      "learning_rate": 0.0004669967107299677,
      "loss": 0.1236,
      "step": 29570
    },
    {
      "epoch": 8.669402110199297,
      "grad_norm": 1.583682656288147,
      "learning_rate": 0.0004669129144423793,
      "loss": 0.1298,
      "step": 29580
    },
    {
      "epoch": 8.672332942555686,
      "grad_norm": 0.6380344033241272,
      "learning_rate": 0.00046682911815479095,
      "loss": 0.0906,
      "step": 29590
    },
    {
      "epoch": 8.675263774912075,
      "grad_norm": 1.390140414237976,
      "learning_rate": 0.00046674532186720256,
      "loss": 0.1084,
      "step": 29600
    },
    {
      "epoch": 8.678194607268464,
      "grad_norm": 1.3047640323638916,
      "learning_rate": 0.0004666615255796142,
      "loss": 0.0864,
      "step": 29610
    },
    {
      "epoch": 8.681125439624854,
      "grad_norm": 1.0710569620132446,
      "learning_rate": 0.00046657772929202583,
      "loss": 0.1139,
      "step": 29620
    },
    {
      "epoch": 8.684056271981243,
      "grad_norm": 0.7895805835723877,
      "learning_rate": 0.0004664939330044375,
      "loss": 0.1057,
      "step": 29630
    },
    {
      "epoch": 8.686987104337632,
      "grad_norm": 0.797432541847229,
      "learning_rate": 0.0004664101367168491,
      "loss": 0.0874,
      "step": 29640
    },
    {
      "epoch": 8.689917936694021,
      "grad_norm": 1.0360952615737915,
      "learning_rate": 0.0004663263404292607,
      "loss": 0.1243,
      "step": 29650
    },
    {
      "epoch": 8.69284876905041,
      "grad_norm": 0.6992159485816956,
      "learning_rate": 0.0004662425441416724,
      "loss": 0.1242,
      "step": 29660
    },
    {
      "epoch": 8.6957796014068,
      "grad_norm": 1.6026966571807861,
      "learning_rate": 0.000466158747854084,
      "loss": 0.0848,
      "step": 29670
    },
    {
      "epoch": 8.698710433763189,
      "grad_norm": 0.7057841420173645,
      "learning_rate": 0.00046607495156649565,
      "loss": 0.1026,
      "step": 29680
    },
    {
      "epoch": 8.701641266119578,
      "grad_norm": 0.839275062084198,
      "learning_rate": 0.00046599115527890726,
      "loss": 0.0937,
      "step": 29690
    },
    {
      "epoch": 8.704572098475968,
      "grad_norm": 1.1799743175506592,
      "learning_rate": 0.0004659073589913189,
      "loss": 0.099,
      "step": 29700
    },
    {
      "epoch": 8.707502930832357,
      "grad_norm": 0.9130209684371948,
      "learning_rate": 0.00046582356270373054,
      "loss": 0.1152,
      "step": 29710
    },
    {
      "epoch": 8.710433763188746,
      "grad_norm": 1.0452951192855835,
      "learning_rate": 0.00046573976641614215,
      "loss": 0.1132,
      "step": 29720
    },
    {
      "epoch": 8.713364595545135,
      "grad_norm": 0.9009170532226562,
      "learning_rate": 0.0004656559701285538,
      "loss": 0.1076,
      "step": 29730
    },
    {
      "epoch": 8.716295427901525,
      "grad_norm": 0.37239646911621094,
      "learning_rate": 0.0004655721738409654,
      "loss": 0.0997,
      "step": 29740
    },
    {
      "epoch": 8.719226260257914,
      "grad_norm": 0.6240997314453125,
      "learning_rate": 0.00046548837755337703,
      "loss": 0.0984,
      "step": 29750
    },
    {
      "epoch": 8.722157092614303,
      "grad_norm": 1.0956491231918335,
      "learning_rate": 0.0004654045812657887,
      "loss": 0.0891,
      "step": 29760
    },
    {
      "epoch": 8.725087924970692,
      "grad_norm": 1.1709777116775513,
      "learning_rate": 0.0004653207849782003,
      "loss": 0.0838,
      "step": 29770
    },
    {
      "epoch": 8.728018757327082,
      "grad_norm": 0.955532968044281,
      "learning_rate": 0.00046523698869061197,
      "loss": 0.1005,
      "step": 29780
    },
    {
      "epoch": 8.73094958968347,
      "grad_norm": 1.6284472942352295,
      "learning_rate": 0.0004651531924030236,
      "loss": 0.1541,
      "step": 29790
    },
    {
      "epoch": 8.73388042203986,
      "grad_norm": 1.446706771850586,
      "learning_rate": 0.00046506939611543524,
      "loss": 0.102,
      "step": 29800
    },
    {
      "epoch": 8.73681125439625,
      "grad_norm": 0.8565634489059448,
      "learning_rate": 0.00046498559982784685,
      "loss": 0.1018,
      "step": 29810
    },
    {
      "epoch": 8.739742086752639,
      "grad_norm": 1.7435405254364014,
      "learning_rate": 0.00046490180354025846,
      "loss": 0.1073,
      "step": 29820
    },
    {
      "epoch": 8.742672919109028,
      "grad_norm": 0.6501768231391907,
      "learning_rate": 0.0004648180072526701,
      "loss": 0.1037,
      "step": 29830
    },
    {
      "epoch": 8.745603751465417,
      "grad_norm": 0.8725658059120178,
      "learning_rate": 0.00046473421096508173,
      "loss": 0.0952,
      "step": 29840
    },
    {
      "epoch": 8.748534583821804,
      "grad_norm": 0.5038749575614929,
      "learning_rate": 0.00046465041467749345,
      "loss": 0.1051,
      "step": 29850
    },
    {
      "epoch": 8.751465416178196,
      "grad_norm": 0.5078597664833069,
      "learning_rate": 0.000464566618389905,
      "loss": 0.077,
      "step": 29860
    },
    {
      "epoch": 8.754396248534583,
      "grad_norm": 0.46097707748413086,
      "learning_rate": 0.0004644828221023166,
      "loss": 0.0694,
      "step": 29870
    },
    {
      "epoch": 8.757327080890972,
      "grad_norm": 1.2329089641571045,
      "learning_rate": 0.00046439902581472834,
      "loss": 0.0954,
      "step": 29880
    },
    {
      "epoch": 8.760257913247361,
      "grad_norm": 0.8838796615600586,
      "learning_rate": 0.0004643152295271399,
      "loss": 0.1183,
      "step": 29890
    },
    {
      "epoch": 8.76318874560375,
      "grad_norm": 0.6577491164207458,
      "learning_rate": 0.0004642314332395516,
      "loss": 0.116,
      "step": 29900
    },
    {
      "epoch": 8.76611957796014,
      "grad_norm": 0.8995466828346252,
      "learning_rate": 0.0004641476369519632,
      "loss": 0.0883,
      "step": 29910
    },
    {
      "epoch": 8.76905041031653,
      "grad_norm": 1.1972911357879639,
      "learning_rate": 0.0004640638406643748,
      "loss": 0.1084,
      "step": 29920
    },
    {
      "epoch": 8.771981242672918,
      "grad_norm": 0.8356617093086243,
      "learning_rate": 0.0004639800443767865,
      "loss": 0.1729,
      "step": 29930
    },
    {
      "epoch": 8.774912075029308,
      "grad_norm": 1.3046860694885254,
      "learning_rate": 0.00046389624808919805,
      "loss": 0.1,
      "step": 29940
    },
    {
      "epoch": 8.777842907385697,
      "grad_norm": 0.43847474455833435,
      "learning_rate": 0.00046381245180160977,
      "loss": 0.0824,
      "step": 29950
    },
    {
      "epoch": 8.780773739742086,
      "grad_norm": 0.3294776976108551,
      "learning_rate": 0.0004637286555140214,
      "loss": 0.1201,
      "step": 29960
    },
    {
      "epoch": 8.783704572098475,
      "grad_norm": 1.4460102319717407,
      "learning_rate": 0.00046364485922643304,
      "loss": 0.0717,
      "step": 29970
    },
    {
      "epoch": 8.786635404454865,
      "grad_norm": 1.2148784399032593,
      "learning_rate": 0.00046356106293884465,
      "loss": 0.0957,
      "step": 29980
    },
    {
      "epoch": 8.789566236811254,
      "grad_norm": 1.0991767644882202,
      "learning_rate": 0.00046347726665125626,
      "loss": 0.1097,
      "step": 29990
    },
    {
      "epoch": 8.792497069167643,
      "grad_norm": 0.8958939909934998,
      "learning_rate": 0.0004633934703636679,
      "loss": 0.1002,
      "step": 30000
    },
    {
      "epoch": 8.795427901524032,
      "grad_norm": 0.6785247921943665,
      "learning_rate": 0.00046330967407607953,
      "loss": 0.1119,
      "step": 30010
    },
    {
      "epoch": 8.798358733880422,
      "grad_norm": 0.8432401418685913,
      "learning_rate": 0.0004632258777884912,
      "loss": 0.1056,
      "step": 30020
    },
    {
      "epoch": 8.801289566236811,
      "grad_norm": 0.7196900248527527,
      "learning_rate": 0.0004631420815009028,
      "loss": 0.0997,
      "step": 30030
    },
    {
      "epoch": 8.8042203985932,
      "grad_norm": 0.9263116717338562,
      "learning_rate": 0.0004630582852133144,
      "loss": 0.0835,
      "step": 30040
    },
    {
      "epoch": 8.80715123094959,
      "grad_norm": 1.396619200706482,
      "learning_rate": 0.0004629744889257261,
      "loss": 0.1022,
      "step": 30050
    },
    {
      "epoch": 8.810082063305979,
      "grad_norm": 1.83524489402771,
      "learning_rate": 0.0004628906926381377,
      "loss": 0.1055,
      "step": 30060
    },
    {
      "epoch": 8.813012895662368,
      "grad_norm": 1.0287693738937378,
      "learning_rate": 0.00046280689635054935,
      "loss": 0.1083,
      "step": 30070
    },
    {
      "epoch": 8.815943728018757,
      "grad_norm": 1.239495038986206,
      "learning_rate": 0.00046272310006296096,
      "loss": 0.1098,
      "step": 30080
    },
    {
      "epoch": 8.818874560375146,
      "grad_norm": 1.2188507318496704,
      "learning_rate": 0.0004626393037753726,
      "loss": 0.1049,
      "step": 30090
    },
    {
      "epoch": 8.821805392731536,
      "grad_norm": 0.939777672290802,
      "learning_rate": 0.00046255550748778424,
      "loss": 0.1142,
      "step": 30100
    },
    {
      "epoch": 8.824736225087925,
      "grad_norm": 0.8690223693847656,
      "learning_rate": 0.00046247171120019585,
      "loss": 0.1045,
      "step": 30110
    },
    {
      "epoch": 8.827667057444314,
      "grad_norm": 1.0254552364349365,
      "learning_rate": 0.0004623879149126075,
      "loss": 0.1152,
      "step": 30120
    },
    {
      "epoch": 8.830597889800703,
      "grad_norm": 0.954290509223938,
      "learning_rate": 0.0004623041186250191,
      "loss": 0.0932,
      "step": 30130
    },
    {
      "epoch": 8.833528722157093,
      "grad_norm": 0.7346832156181335,
      "learning_rate": 0.0004622203223374308,
      "loss": 0.0708,
      "step": 30140
    },
    {
      "epoch": 8.836459554513482,
      "grad_norm": 0.646622896194458,
      "learning_rate": 0.0004621365260498424,
      "loss": 0.1079,
      "step": 30150
    },
    {
      "epoch": 8.839390386869871,
      "grad_norm": 1.0865827798843384,
      "learning_rate": 0.000462052729762254,
      "loss": 0.0951,
      "step": 30160
    },
    {
      "epoch": 8.84232121922626,
      "grad_norm": 0.2591914236545563,
      "learning_rate": 0.00046196893347466567,
      "loss": 0.1122,
      "step": 30170
    },
    {
      "epoch": 8.84525205158265,
      "grad_norm": 0.878814160823822,
      "learning_rate": 0.0004618851371870773,
      "loss": 0.0767,
      "step": 30180
    },
    {
      "epoch": 8.848182883939039,
      "grad_norm": 0.9359546303749084,
      "learning_rate": 0.00046180134089948894,
      "loss": 0.0923,
      "step": 30190
    },
    {
      "epoch": 8.851113716295428,
      "grad_norm": 1.6284713745117188,
      "learning_rate": 0.00046171754461190055,
      "loss": 0.1414,
      "step": 30200
    },
    {
      "epoch": 8.854044548651817,
      "grad_norm": 1.4359256029129028,
      "learning_rate": 0.00046163374832431216,
      "loss": 0.1258,
      "step": 30210
    },
    {
      "epoch": 8.856975381008207,
      "grad_norm": 0.7260029315948486,
      "learning_rate": 0.0004615499520367238,
      "loss": 0.1017,
      "step": 30220
    },
    {
      "epoch": 8.859906213364596,
      "grad_norm": 1.4012514352798462,
      "learning_rate": 0.00046146615574913543,
      "loss": 0.1283,
      "step": 30230
    },
    {
      "epoch": 8.862837045720985,
      "grad_norm": 0.45434248447418213,
      "learning_rate": 0.0004613823594615471,
      "loss": 0.0866,
      "step": 30240
    },
    {
      "epoch": 8.865767878077374,
      "grad_norm": 1.5257309675216675,
      "learning_rate": 0.0004612985631739587,
      "loss": 0.0893,
      "step": 30250
    },
    {
      "epoch": 8.868698710433764,
      "grad_norm": 0.6887279152870178,
      "learning_rate": 0.0004612147668863703,
      "loss": 0.0959,
      "step": 30260
    },
    {
      "epoch": 8.871629542790153,
      "grad_norm": 1.0425379276275635,
      "learning_rate": 0.000461130970598782,
      "loss": 0.1483,
      "step": 30270
    },
    {
      "epoch": 8.874560375146542,
      "grad_norm": 0.9473314881324768,
      "learning_rate": 0.0004610471743111936,
      "loss": 0.1317,
      "step": 30280
    },
    {
      "epoch": 8.877491207502931,
      "grad_norm": 1.157994270324707,
      "learning_rate": 0.0004609633780236053,
      "loss": 0.0963,
      "step": 30290
    },
    {
      "epoch": 8.88042203985932,
      "grad_norm": 0.6221473217010498,
      "learning_rate": 0.00046087958173601686,
      "loss": 0.0948,
      "step": 30300
    },
    {
      "epoch": 8.88335287221571,
      "grad_norm": 1.221745491027832,
      "learning_rate": 0.0004607957854484286,
      "loss": 0.1116,
      "step": 30310
    },
    {
      "epoch": 8.886283704572099,
      "grad_norm": 0.5626525282859802,
      "learning_rate": 0.0004607119891608402,
      "loss": 0.1102,
      "step": 30320
    },
    {
      "epoch": 8.889214536928488,
      "grad_norm": 0.5346011519432068,
      "learning_rate": 0.00046062819287325175,
      "loss": 0.1041,
      "step": 30330
    },
    {
      "epoch": 8.892145369284878,
      "grad_norm": 1.3323423862457275,
      "learning_rate": 0.00046054439658566347,
      "loss": 0.1005,
      "step": 30340
    },
    {
      "epoch": 8.895076201641267,
      "grad_norm": 1.3507169485092163,
      "learning_rate": 0.0004604606002980751,
      "loss": 0.0951,
      "step": 30350
    },
    {
      "epoch": 8.898007033997656,
      "grad_norm": 1.1321301460266113,
      "learning_rate": 0.00046037680401048674,
      "loss": 0.119,
      "step": 30360
    },
    {
      "epoch": 8.900937866354045,
      "grad_norm": 1.0188931226730347,
      "learning_rate": 0.00046029300772289835,
      "loss": 0.1052,
      "step": 30370
    },
    {
      "epoch": 8.903868698710435,
      "grad_norm": 1.4623668193817139,
      "learning_rate": 0.0004602092114353099,
      "loss": 0.1001,
      "step": 30380
    },
    {
      "epoch": 8.906799531066824,
      "grad_norm": 0.9596614241600037,
      "learning_rate": 0.0004601254151477216,
      "loss": 0.1059,
      "step": 30390
    },
    {
      "epoch": 8.909730363423213,
      "grad_norm": 1.2152308225631714,
      "learning_rate": 0.00046004161886013323,
      "loss": 0.081,
      "step": 30400
    },
    {
      "epoch": 8.912661195779602,
      "grad_norm": 0.8869110345840454,
      "learning_rate": 0.0004599578225725449,
      "loss": 0.0915,
      "step": 30410
    },
    {
      "epoch": 8.91559202813599,
      "grad_norm": 0.7702096700668335,
      "learning_rate": 0.0004598740262849565,
      "loss": 0.1122,
      "step": 30420
    },
    {
      "epoch": 8.91852286049238,
      "grad_norm": 0.9173003435134888,
      "learning_rate": 0.0004597902299973681,
      "loss": 0.0913,
      "step": 30430
    },
    {
      "epoch": 8.921453692848768,
      "grad_norm": 1.3791741132736206,
      "learning_rate": 0.0004597064337097798,
      "loss": 0.0924,
      "step": 30440
    },
    {
      "epoch": 8.924384525205157,
      "grad_norm": 1.052954912185669,
      "learning_rate": 0.0004596226374221914,
      "loss": 0.1032,
      "step": 30450
    },
    {
      "epoch": 8.927315357561547,
      "grad_norm": 1.5951370000839233,
      "learning_rate": 0.00045953884113460305,
      "loss": 0.0841,
      "step": 30460
    },
    {
      "epoch": 8.930246189917936,
      "grad_norm": 0.7756171822547913,
      "learning_rate": 0.00045945504484701466,
      "loss": 0.0922,
      "step": 30470
    },
    {
      "epoch": 8.933177022274325,
      "grad_norm": 0.7657930850982666,
      "learning_rate": 0.00045937124855942633,
      "loss": 0.0914,
      "step": 30480
    },
    {
      "epoch": 8.936107854630714,
      "grad_norm": 1.6887133121490479,
      "learning_rate": 0.00045928745227183794,
      "loss": 0.1136,
      "step": 30490
    },
    {
      "epoch": 8.939038686987104,
      "grad_norm": 0.8545253276824951,
      "learning_rate": 0.00045920365598424955,
      "loss": 0.089,
      "step": 30500
    },
    {
      "epoch": 8.941969519343493,
      "grad_norm": 0.7291433215141296,
      "learning_rate": 0.0004591198596966612,
      "loss": 0.1041,
      "step": 30510
    },
    {
      "epoch": 8.944900351699882,
      "grad_norm": 1.316425085067749,
      "learning_rate": 0.0004590360634090728,
      "loss": 0.1205,
      "step": 30520
    },
    {
      "epoch": 8.947831184056271,
      "grad_norm": 1.061384677886963,
      "learning_rate": 0.0004589522671214845,
      "loss": 0.1052,
      "step": 30530
    },
    {
      "epoch": 8.95076201641266,
      "grad_norm": 0.7785199880599976,
      "learning_rate": 0.0004588684708338961,
      "loss": 0.114,
      "step": 30540
    },
    {
      "epoch": 8.95369284876905,
      "grad_norm": 1.704937219619751,
      "learning_rate": 0.0004587846745463077,
      "loss": 0.0867,
      "step": 30550
    },
    {
      "epoch": 8.95662368112544,
      "grad_norm": 0.8856827020645142,
      "learning_rate": 0.00045870087825871937,
      "loss": 0.1059,
      "step": 30560
    },
    {
      "epoch": 8.959554513481828,
      "grad_norm": 1.226067304611206,
      "learning_rate": 0.000458617081971131,
      "loss": 0.1124,
      "step": 30570
    },
    {
      "epoch": 8.962485345838218,
      "grad_norm": 0.8795140385627747,
      "learning_rate": 0.00045853328568354264,
      "loss": 0.1114,
      "step": 30580
    },
    {
      "epoch": 8.965416178194607,
      "grad_norm": 0.4876221716403961,
      "learning_rate": 0.00045844948939595425,
      "loss": 0.0936,
      "step": 30590
    },
    {
      "epoch": 8.968347010550996,
      "grad_norm": 0.695682942867279,
      "learning_rate": 0.00045836569310836586,
      "loss": 0.0986,
      "step": 30600
    },
    {
      "epoch": 8.971277842907385,
      "grad_norm": 0.7653408646583557,
      "learning_rate": 0.0004582818968207775,
      "loss": 0.1116,
      "step": 30610
    },
    {
      "epoch": 8.974208675263775,
      "grad_norm": 0.8159955143928528,
      "learning_rate": 0.00045819810053318913,
      "loss": 0.0933,
      "step": 30620
    },
    {
      "epoch": 8.977139507620164,
      "grad_norm": 0.8208440542221069,
      "learning_rate": 0.0004581143042456008,
      "loss": 0.1089,
      "step": 30630
    },
    {
      "epoch": 8.980070339976553,
      "grad_norm": 1.076439619064331,
      "learning_rate": 0.0004580305079580124,
      "loss": 0.1388,
      "step": 30640
    },
    {
      "epoch": 8.983001172332942,
      "grad_norm": 0.5928246378898621,
      "learning_rate": 0.000457946711670424,
      "loss": 0.1072,
      "step": 30650
    },
    {
      "epoch": 8.985932004689332,
      "grad_norm": 0.6874586939811707,
      "learning_rate": 0.0004578629153828357,
      "loss": 0.065,
      "step": 30660
    },
    {
      "epoch": 8.98886283704572,
      "grad_norm": 0.7329134345054626,
      "learning_rate": 0.0004577791190952473,
      "loss": 0.1072,
      "step": 30670
    },
    {
      "epoch": 8.99179366940211,
      "grad_norm": 1.9985265731811523,
      "learning_rate": 0.00045769532280765895,
      "loss": 0.1154,
      "step": 30680
    },
    {
      "epoch": 8.9947245017585,
      "grad_norm": 1.2538865804672241,
      "learning_rate": 0.00045761152652007056,
      "loss": 0.0928,
      "step": 30690
    },
    {
      "epoch": 8.997655334114889,
      "grad_norm": 1.110275149345398,
      "learning_rate": 0.0004575277302324823,
      "loss": 0.0931,
      "step": 30700
    },
    {
      "epoch": 9.0,
      "eval_accuracy": 0.6511704582921201,
      "eval_f1_macro": 0.6908243066414774,
      "eval_f1_micro": 0.7440532825880114,
      "eval_f1_weighted": 0.7355126613877293,
      "eval_loss": 0.09983232617378235,
      "eval_roc_auc": 0.8353674584267231,
      "eval_runtime": 240.0866,
      "eval_samples_per_second": 12.633,
      "eval_steps_per_second": 1.583,
      "step": 30708
    },
    {
      "epoch": 9.000586166471278,
      "grad_norm": 0.8146556615829468,
      "learning_rate": 0.00045744393394489384,
      "loss": 0.1135,
      "step": 30710
    },
    {
      "epoch": 9.003516998827667,
      "grad_norm": 0.34804946184158325,
      "learning_rate": 0.00045736013765730545,
      "loss": 0.0804,
      "step": 30720
    },
    {
      "epoch": 9.006447831184056,
      "grad_norm": 1.0265597105026245,
      "learning_rate": 0.00045727634136971717,
      "loss": 0.0523,
      "step": 30730
    },
    {
      "epoch": 9.009378663540446,
      "grad_norm": 0.6952730417251587,
      "learning_rate": 0.0004571925450821287,
      "loss": 0.1078,
      "step": 30740
    },
    {
      "epoch": 9.012309495896835,
      "grad_norm": 0.6344197988510132,
      "learning_rate": 0.00045710874879454044,
      "loss": 0.0782,
      "step": 30750
    },
    {
      "epoch": 9.015240328253224,
      "grad_norm": 1.0598249435424805,
      "learning_rate": 0.00045702495250695205,
      "loss": 0.0872,
      "step": 30760
    },
    {
      "epoch": 9.018171160609613,
      "grad_norm": 0.963806688785553,
      "learning_rate": 0.0004569411562193636,
      "loss": 0.1006,
      "step": 30770
    },
    {
      "epoch": 9.021101992966003,
      "grad_norm": 1.2286146879196167,
      "learning_rate": 0.0004568573599317753,
      "loss": 0.0907,
      "step": 30780
    },
    {
      "epoch": 9.024032825322392,
      "grad_norm": 1.2287077903747559,
      "learning_rate": 0.00045677356364418693,
      "loss": 0.096,
      "step": 30790
    },
    {
      "epoch": 9.026963657678781,
      "grad_norm": 1.1455761194229126,
      "learning_rate": 0.0004566897673565986,
      "loss": 0.1167,
      "step": 30800
    },
    {
      "epoch": 9.02989449003517,
      "grad_norm": 0.9286202788352966,
      "learning_rate": 0.0004566059710690102,
      "loss": 0.0712,
      "step": 30810
    },
    {
      "epoch": 9.03282532239156,
      "grad_norm": 0.836207389831543,
      "learning_rate": 0.00045652217478142176,
      "loss": 0.0951,
      "step": 30820
    },
    {
      "epoch": 9.035756154747949,
      "grad_norm": 0.7689431309700012,
      "learning_rate": 0.0004564383784938335,
      "loss": 0.0917,
      "step": 30830
    },
    {
      "epoch": 9.038686987104338,
      "grad_norm": 0.514183759689331,
      "learning_rate": 0.0004563545822062451,
      "loss": 0.0835,
      "step": 30840
    },
    {
      "epoch": 9.041617819460727,
      "grad_norm": 0.28438958525657654,
      "learning_rate": 0.00045627078591865675,
      "loss": 0.0966,
      "step": 30850
    },
    {
      "epoch": 9.044548651817117,
      "grad_norm": 0.6856771111488342,
      "learning_rate": 0.00045618698963106836,
      "loss": 0.0912,
      "step": 30860
    },
    {
      "epoch": 9.047479484173506,
      "grad_norm": 0.42000991106033325,
      "learning_rate": 0.00045610319334348,
      "loss": 0.0737,
      "step": 30870
    },
    {
      "epoch": 9.050410316529895,
      "grad_norm": 1.0722947120666504,
      "learning_rate": 0.00045601939705589164,
      "loss": 0.0934,
      "step": 30880
    },
    {
      "epoch": 9.053341148886284,
      "grad_norm": 0.8673533201217651,
      "learning_rate": 0.00045593560076830325,
      "loss": 0.0895,
      "step": 30890
    },
    {
      "epoch": 9.056271981242674,
      "grad_norm": 0.9486190676689148,
      "learning_rate": 0.0004558518044807149,
      "loss": 0.1039,
      "step": 30900
    },
    {
      "epoch": 9.059202813599063,
      "grad_norm": 0.4757077395915985,
      "learning_rate": 0.0004557680081931265,
      "loss": 0.0935,
      "step": 30910
    },
    {
      "epoch": 9.062133645955452,
      "grad_norm": 0.9144823551177979,
      "learning_rate": 0.0004556842119055382,
      "loss": 0.1021,
      "step": 30920
    },
    {
      "epoch": 9.065064478311841,
      "grad_norm": 1.2087305784225464,
      "learning_rate": 0.0004556004156179498,
      "loss": 0.0876,
      "step": 30930
    },
    {
      "epoch": 9.06799531066823,
      "grad_norm": 1.4767483472824097,
      "learning_rate": 0.0004555166193303614,
      "loss": 0.094,
      "step": 30940
    },
    {
      "epoch": 9.07092614302462,
      "grad_norm": 1.703413963317871,
      "learning_rate": 0.00045543282304277307,
      "loss": 0.1197,
      "step": 30950
    },
    {
      "epoch": 9.073856975381009,
      "grad_norm": 1.1357489824295044,
      "learning_rate": 0.0004553490267551847,
      "loss": 0.0952,
      "step": 30960
    },
    {
      "epoch": 9.076787807737398,
      "grad_norm": 1.3679994344711304,
      "learning_rate": 0.00045526523046759634,
      "loss": 0.0979,
      "step": 30970
    },
    {
      "epoch": 9.079718640093787,
      "grad_norm": 1.871288537979126,
      "learning_rate": 0.00045518143418000795,
      "loss": 0.0985,
      "step": 30980
    },
    {
      "epoch": 9.082649472450177,
      "grad_norm": 0.7406281232833862,
      "learning_rate": 0.00045509763789241956,
      "loss": 0.0971,
      "step": 30990
    },
    {
      "epoch": 9.085580304806564,
      "grad_norm": 0.8236909508705139,
      "learning_rate": 0.0004550138416048312,
      "loss": 0.0897,
      "step": 31000
    },
    {
      "epoch": 9.088511137162953,
      "grad_norm": 1.8470759391784668,
      "learning_rate": 0.00045493004531724283,
      "loss": 0.1116,
      "step": 31010
    },
    {
      "epoch": 9.091441969519343,
      "grad_norm": 0.752672553062439,
      "learning_rate": 0.0004548462490296545,
      "loss": 0.0921,
      "step": 31020
    },
    {
      "epoch": 9.094372801875732,
      "grad_norm": 1.3801946640014648,
      "learning_rate": 0.0004547624527420661,
      "loss": 0.0778,
      "step": 31030
    },
    {
      "epoch": 9.097303634232121,
      "grad_norm": 1.9253238439559937,
      "learning_rate": 0.00045467865645447777,
      "loss": 0.0918,
      "step": 31040
    },
    {
      "epoch": 9.10023446658851,
      "grad_norm": 0.9224393963813782,
      "learning_rate": 0.0004545948601668894,
      "loss": 0.1022,
      "step": 31050
    },
    {
      "epoch": 9.1031652989449,
      "grad_norm": 1.2658089399337769,
      "learning_rate": 0.000454511063879301,
      "loss": 0.0968,
      "step": 31060
    },
    {
      "epoch": 9.106096131301289,
      "grad_norm": 1.1169487237930298,
      "learning_rate": 0.00045442726759171265,
      "loss": 0.0905,
      "step": 31070
    },
    {
      "epoch": 9.109026963657678,
      "grad_norm": 0.6554105281829834,
      "learning_rate": 0.00045434347130412426,
      "loss": 0.0879,
      "step": 31080
    },
    {
      "epoch": 9.111957796014067,
      "grad_norm": 0.7148575782775879,
      "learning_rate": 0.000454259675016536,
      "loss": 0.1238,
      "step": 31090
    },
    {
      "epoch": 9.114888628370457,
      "grad_norm": 1.420708417892456,
      "learning_rate": 0.00045417587872894754,
      "loss": 0.1287,
      "step": 31100
    },
    {
      "epoch": 9.117819460726846,
      "grad_norm": 2.277353286743164,
      "learning_rate": 0.00045409208244135915,
      "loss": 0.0928,
      "step": 31110
    },
    {
      "epoch": 9.120750293083235,
      "grad_norm": 2.1110808849334717,
      "learning_rate": 0.00045400828615377087,
      "loss": 0.1137,
      "step": 31120
    },
    {
      "epoch": 9.123681125439624,
      "grad_norm": 0.14136819541454315,
      "learning_rate": 0.0004539244898661824,
      "loss": 0.0813,
      "step": 31130
    },
    {
      "epoch": 9.126611957796014,
      "grad_norm": 0.880661129951477,
      "learning_rate": 0.00045384069357859414,
      "loss": 0.0948,
      "step": 31140
    },
    {
      "epoch": 9.129542790152403,
      "grad_norm": 0.7148082852363586,
      "learning_rate": 0.0004537568972910057,
      "loss": 0.0921,
      "step": 31150
    },
    {
      "epoch": 9.132473622508792,
      "grad_norm": 1.0840898752212524,
      "learning_rate": 0.0004536731010034173,
      "loss": 0.1011,
      "step": 31160
    },
    {
      "epoch": 9.135404454865181,
      "grad_norm": 0.7554919719696045,
      "learning_rate": 0.000453589304715829,
      "loss": 0.1011,
      "step": 31170
    },
    {
      "epoch": 9.13833528722157,
      "grad_norm": 0.47841203212738037,
      "learning_rate": 0.0004535055084282406,
      "loss": 0.101,
      "step": 31180
    },
    {
      "epoch": 9.14126611957796,
      "grad_norm": 1.4605164527893066,
      "learning_rate": 0.0004534217121406523,
      "loss": 0.0864,
      "step": 31190
    },
    {
      "epoch": 9.14419695193435,
      "grad_norm": 0.5266279578208923,
      "learning_rate": 0.0004533379158530639,
      "loss": 0.0573,
      "step": 31200
    },
    {
      "epoch": 9.147127784290738,
      "grad_norm": 0.8512956500053406,
      "learning_rate": 0.00045325411956547557,
      "loss": 0.0872,
      "step": 31210
    },
    {
      "epoch": 9.150058616647128,
      "grad_norm": 0.9600860476493835,
      "learning_rate": 0.0004531703232778872,
      "loss": 0.088,
      "step": 31220
    },
    {
      "epoch": 9.152989449003517,
      "grad_norm": 0.6022906303405762,
      "learning_rate": 0.0004530865269902988,
      "loss": 0.1021,
      "step": 31230
    },
    {
      "epoch": 9.155920281359906,
      "grad_norm": 0.9987354874610901,
      "learning_rate": 0.00045300273070271045,
      "loss": 0.0877,
      "step": 31240
    },
    {
      "epoch": 9.158851113716295,
      "grad_norm": 1.3260834217071533,
      "learning_rate": 0.00045291893441512206,
      "loss": 0.0926,
      "step": 31250
    },
    {
      "epoch": 9.161781946072685,
      "grad_norm": 1.3084776401519775,
      "learning_rate": 0.0004528351381275337,
      "loss": 0.0885,
      "step": 31260
    },
    {
      "epoch": 9.164712778429074,
      "grad_norm": 0.6932283043861389,
      "learning_rate": 0.00045275134183994534,
      "loss": 0.0784,
      "step": 31270
    },
    {
      "epoch": 9.167643610785463,
      "grad_norm": 0.6021262407302856,
      "learning_rate": 0.00045266754555235695,
      "loss": 0.1024,
      "step": 31280
    },
    {
      "epoch": 9.170574443141852,
      "grad_norm": 0.8900943398475647,
      "learning_rate": 0.0004525837492647686,
      "loss": 0.093,
      "step": 31290
    },
    {
      "epoch": 9.173505275498242,
      "grad_norm": 1.105130910873413,
      "learning_rate": 0.0004524999529771802,
      "loss": 0.0942,
      "step": 31300
    },
    {
      "epoch": 9.17643610785463,
      "grad_norm": 0.8323207497596741,
      "learning_rate": 0.0004524161566895919,
      "loss": 0.0924,
      "step": 31310
    },
    {
      "epoch": 9.17936694021102,
      "grad_norm": 1.713187575340271,
      "learning_rate": 0.0004523323604020035,
      "loss": 0.1237,
      "step": 31320
    },
    {
      "epoch": 9.18229777256741,
      "grad_norm": 1.2672861814498901,
      "learning_rate": 0.0004522485641144151,
      "loss": 0.079,
      "step": 31330
    },
    {
      "epoch": 9.185228604923799,
      "grad_norm": 0.9109605550765991,
      "learning_rate": 0.00045216476782682677,
      "loss": 0.0942,
      "step": 31340
    },
    {
      "epoch": 9.188159437280188,
      "grad_norm": 0.8418381810188293,
      "learning_rate": 0.0004520809715392384,
      "loss": 0.0725,
      "step": 31350
    },
    {
      "epoch": 9.191090269636577,
      "grad_norm": 0.8852598071098328,
      "learning_rate": 0.00045199717525165004,
      "loss": 0.1017,
      "step": 31360
    },
    {
      "epoch": 9.194021101992966,
      "grad_norm": 1.2252472639083862,
      "learning_rate": 0.00045191337896406165,
      "loss": 0.1279,
      "step": 31370
    },
    {
      "epoch": 9.196951934349356,
      "grad_norm": 1.0486849546432495,
      "learning_rate": 0.0004518295826764733,
      "loss": 0.1095,
      "step": 31380
    },
    {
      "epoch": 9.199882766705745,
      "grad_norm": 0.5379083752632141,
      "learning_rate": 0.0004517457863888849,
      "loss": 0.0953,
      "step": 31390
    },
    {
      "epoch": 9.202813599062134,
      "grad_norm": 0.35029831528663635,
      "learning_rate": 0.00045166199010129653,
      "loss": 0.0988,
      "step": 31400
    },
    {
      "epoch": 9.205744431418523,
      "grad_norm": 0.5641648173332214,
      "learning_rate": 0.0004515781938137082,
      "loss": 0.0936,
      "step": 31410
    },
    {
      "epoch": 9.208675263774913,
      "grad_norm": 1.0616477727890015,
      "learning_rate": 0.0004514943975261198,
      "loss": 0.1,
      "step": 31420
    },
    {
      "epoch": 9.211606096131302,
      "grad_norm": 0.8393281698226929,
      "learning_rate": 0.00045141060123853147,
      "loss": 0.1027,
      "step": 31430
    },
    {
      "epoch": 9.214536928487691,
      "grad_norm": 0.9224027395248413,
      "learning_rate": 0.0004513268049509431,
      "loss": 0.123,
      "step": 31440
    },
    {
      "epoch": 9.21746776084408,
      "grad_norm": 0.5615242123603821,
      "learning_rate": 0.0004512430086633547,
      "loss": 0.0875,
      "step": 31450
    },
    {
      "epoch": 9.22039859320047,
      "grad_norm": 0.514786422252655,
      "learning_rate": 0.00045115921237576635,
      "loss": 0.0771,
      "step": 31460
    },
    {
      "epoch": 9.223329425556859,
      "grad_norm": 0.9645435214042664,
      "learning_rate": 0.00045107541608817796,
      "loss": 0.0906,
      "step": 31470
    },
    {
      "epoch": 9.226260257913248,
      "grad_norm": 1.0316439867019653,
      "learning_rate": 0.00045099161980058963,
      "loss": 0.0913,
      "step": 31480
    },
    {
      "epoch": 9.229191090269637,
      "grad_norm": 1.0240672826766968,
      "learning_rate": 0.00045090782351300124,
      "loss": 0.0851,
      "step": 31490
    },
    {
      "epoch": 9.232121922626026,
      "grad_norm": 0.8553158640861511,
      "learning_rate": 0.00045082402722541285,
      "loss": 0.1111,
      "step": 31500
    },
    {
      "epoch": 9.235052754982416,
      "grad_norm": 0.9689570665359497,
      "learning_rate": 0.0004507402309378245,
      "loss": 0.1165,
      "step": 31510
    },
    {
      "epoch": 9.237983587338805,
      "grad_norm": 0.47529757022857666,
      "learning_rate": 0.0004506564346502361,
      "loss": 0.0964,
      "step": 31520
    },
    {
      "epoch": 9.240914419695194,
      "grad_norm": 1.1465944051742554,
      "learning_rate": 0.00045057263836264784,
      "loss": 0.0911,
      "step": 31530
    },
    {
      "epoch": 9.243845252051583,
      "grad_norm": 1.7093878984451294,
      "learning_rate": 0.0004504888420750594,
      "loss": 0.118,
      "step": 31540
    },
    {
      "epoch": 9.246776084407973,
      "grad_norm": 0.9139065146446228,
      "learning_rate": 0.0004504050457874711,
      "loss": 0.0979,
      "step": 31550
    },
    {
      "epoch": 9.24970691676436,
      "grad_norm": 0.35485345125198364,
      "learning_rate": 0.0004503212494998827,
      "loss": 0.0854,
      "step": 31560
    },
    {
      "epoch": 9.25263774912075,
      "grad_norm": 0.6807276010513306,
      "learning_rate": 0.0004502374532122943,
      "loss": 0.0919,
      "step": 31570
    },
    {
      "epoch": 9.255568581477139,
      "grad_norm": 1.653315544128418,
      "learning_rate": 0.000450153656924706,
      "loss": 0.0999,
      "step": 31580
    },
    {
      "epoch": 9.258499413833528,
      "grad_norm": 0.6155381798744202,
      "learning_rate": 0.00045006986063711755,
      "loss": 0.1017,
      "step": 31590
    },
    {
      "epoch": 9.261430246189917,
      "grad_norm": 0.4724031984806061,
      "learning_rate": 0.00044998606434952927,
      "loss": 0.1115,
      "step": 31600
    },
    {
      "epoch": 9.264361078546306,
      "grad_norm": 0.5534321665763855,
      "learning_rate": 0.0004499022680619409,
      "loss": 0.0925,
      "step": 31610
    },
    {
      "epoch": 9.267291910902696,
      "grad_norm": 0.6416749358177185,
      "learning_rate": 0.00044981847177435244,
      "loss": 0.0986,
      "step": 31620
    },
    {
      "epoch": 9.270222743259085,
      "grad_norm": 1.239784836769104,
      "learning_rate": 0.00044973467548676415,
      "loss": 0.0744,
      "step": 31630
    },
    {
      "epoch": 9.273153575615474,
      "grad_norm": 0.8850623965263367,
      "learning_rate": 0.00044965087919917576,
      "loss": 0.0893,
      "step": 31640
    },
    {
      "epoch": 9.276084407971863,
      "grad_norm": 0.9623700976371765,
      "learning_rate": 0.0004495670829115874,
      "loss": 0.0873,
      "step": 31650
    },
    {
      "epoch": 9.279015240328253,
      "grad_norm": 0.5646083354949951,
      "learning_rate": 0.00044948328662399904,
      "loss": 0.0768,
      "step": 31660
    },
    {
      "epoch": 9.281946072684642,
      "grad_norm": 1.6966572999954224,
      "learning_rate": 0.00044939949033641065,
      "loss": 0.144,
      "step": 31670
    },
    {
      "epoch": 9.284876905041031,
      "grad_norm": 0.3440409302711487,
      "learning_rate": 0.0004493156940488223,
      "loss": 0.1038,
      "step": 31680
    },
    {
      "epoch": 9.28780773739742,
      "grad_norm": 0.9537009596824646,
      "learning_rate": 0.0004492318977612339,
      "loss": 0.1068,
      "step": 31690
    },
    {
      "epoch": 9.29073856975381,
      "grad_norm": 0.7279064655303955,
      "learning_rate": 0.0004491481014736456,
      "loss": 0.0951,
      "step": 31700
    },
    {
      "epoch": 9.293669402110199,
      "grad_norm": 0.8465149402618408,
      "learning_rate": 0.0004490643051860572,
      "loss": 0.097,
      "step": 31710
    },
    {
      "epoch": 9.296600234466588,
      "grad_norm": 1.0050575733184814,
      "learning_rate": 0.0004489805088984688,
      "loss": 0.1025,
      "step": 31720
    },
    {
      "epoch": 9.299531066822977,
      "grad_norm": 0.673556387424469,
      "learning_rate": 0.00044889671261088047,
      "loss": 0.0958,
      "step": 31730
    },
    {
      "epoch": 9.302461899179367,
      "grad_norm": 0.6365070343017578,
      "learning_rate": 0.0004488129163232921,
      "loss": 0.0863,
      "step": 31740
    },
    {
      "epoch": 9.305392731535756,
      "grad_norm": 0.2351492941379547,
      "learning_rate": 0.00044872912003570374,
      "loss": 0.0679,
      "step": 31750
    },
    {
      "epoch": 9.308323563892145,
      "grad_norm": 0.5862501859664917,
      "learning_rate": 0.00044864532374811535,
      "loss": 0.065,
      "step": 31760
    },
    {
      "epoch": 9.311254396248534,
      "grad_norm": 2.0217652320861816,
      "learning_rate": 0.000448561527460527,
      "loss": 0.0975,
      "step": 31770
    },
    {
      "epoch": 9.314185228604924,
      "grad_norm": 1.0606310367584229,
      "learning_rate": 0.0004484777311729386,
      "loss": 0.0885,
      "step": 31780
    },
    {
      "epoch": 9.317116060961313,
      "grad_norm": 0.6017016768455505,
      "learning_rate": 0.00044839393488535023,
      "loss": 0.0928,
      "step": 31790
    },
    {
      "epoch": 9.320046893317702,
      "grad_norm": 1.2503468990325928,
      "learning_rate": 0.0004483101385977619,
      "loss": 0.0968,
      "step": 31800
    },
    {
      "epoch": 9.322977725674091,
      "grad_norm": 1.2592129707336426,
      "learning_rate": 0.0004482263423101735,
      "loss": 0.0956,
      "step": 31810
    },
    {
      "epoch": 9.32590855803048,
      "grad_norm": 0.8657842874526978,
      "learning_rate": 0.00044814254602258517,
      "loss": 0.1139,
      "step": 31820
    },
    {
      "epoch": 9.32883939038687,
      "grad_norm": 1.1771920919418335,
      "learning_rate": 0.0004480587497349968,
      "loss": 0.0994,
      "step": 31830
    },
    {
      "epoch": 9.331770222743259,
      "grad_norm": 0.6492476463317871,
      "learning_rate": 0.0004479749534474084,
      "loss": 0.0484,
      "step": 31840
    },
    {
      "epoch": 9.334701055099648,
      "grad_norm": 1.6270339488983154,
      "learning_rate": 0.00044789115715982005,
      "loss": 0.0994,
      "step": 31850
    },
    {
      "epoch": 9.337631887456038,
      "grad_norm": 0.5956416130065918,
      "learning_rate": 0.00044780736087223166,
      "loss": 0.105,
      "step": 31860
    },
    {
      "epoch": 9.340562719812427,
      "grad_norm": 0.8525084257125854,
      "learning_rate": 0.00044772356458464333,
      "loss": 0.1098,
      "step": 31870
    },
    {
      "epoch": 9.343493552168816,
      "grad_norm": 0.8843398690223694,
      "learning_rate": 0.00044763976829705494,
      "loss": 0.1025,
      "step": 31880
    },
    {
      "epoch": 9.346424384525205,
      "grad_norm": 0.6232545971870422,
      "learning_rate": 0.00044755597200946655,
      "loss": 0.0877,
      "step": 31890
    },
    {
      "epoch": 9.349355216881595,
      "grad_norm": 1.0240592956542969,
      "learning_rate": 0.0004474721757218782,
      "loss": 0.0986,
      "step": 31900
    },
    {
      "epoch": 9.352286049237984,
      "grad_norm": 1.377327799797058,
      "learning_rate": 0.0004473883794342898,
      "loss": 0.113,
      "step": 31910
    },
    {
      "epoch": 9.355216881594373,
      "grad_norm": 0.519015908241272,
      "learning_rate": 0.0004473045831467015,
      "loss": 0.0728,
      "step": 31920
    },
    {
      "epoch": 9.358147713950762,
      "grad_norm": 0.9779667854309082,
      "learning_rate": 0.0004472207868591131,
      "loss": 0.1085,
      "step": 31930
    },
    {
      "epoch": 9.361078546307152,
      "grad_norm": 0.8388940691947937,
      "learning_rate": 0.0004471369905715248,
      "loss": 0.0829,
      "step": 31940
    },
    {
      "epoch": 9.36400937866354,
      "grad_norm": 0.49088388681411743,
      "learning_rate": 0.00044705319428393637,
      "loss": 0.1144,
      "step": 31950
    },
    {
      "epoch": 9.36694021101993,
      "grad_norm": 0.7516010403633118,
      "learning_rate": 0.000446969397996348,
      "loss": 0.1059,
      "step": 31960
    },
    {
      "epoch": 9.36987104337632,
      "grad_norm": 0.693341076374054,
      "learning_rate": 0.0004468856017087597,
      "loss": 0.1046,
      "step": 31970
    },
    {
      "epoch": 9.372801875732709,
      "grad_norm": 2.1669468879699707,
      "learning_rate": 0.00044680180542117125,
      "loss": 0.0916,
      "step": 31980
    },
    {
      "epoch": 9.375732708089098,
      "grad_norm": 0.9495393633842468,
      "learning_rate": 0.00044671800913358297,
      "loss": 0.076,
      "step": 31990
    },
    {
      "epoch": 9.378663540445487,
      "grad_norm": 1.19025719165802,
      "learning_rate": 0.0004466342128459946,
      "loss": 0.1099,
      "step": 32000
    },
    {
      "epoch": 9.381594372801876,
      "grad_norm": 1.1477012634277344,
      "learning_rate": 0.00044655041655840614,
      "loss": 0.1336,
      "step": 32010
    },
    {
      "epoch": 9.384525205158265,
      "grad_norm": 0.5821565985679626,
      "learning_rate": 0.00044646662027081785,
      "loss": 0.0888,
      "step": 32020
    },
    {
      "epoch": 9.387456037514655,
      "grad_norm": 0.7677128314971924,
      "learning_rate": 0.0004463828239832294,
      "loss": 0.0974,
      "step": 32030
    },
    {
      "epoch": 9.390386869871044,
      "grad_norm": 2.0000054836273193,
      "learning_rate": 0.0004462990276956411,
      "loss": 0.0986,
      "step": 32040
    },
    {
      "epoch": 9.393317702227433,
      "grad_norm": 0.4242720901966095,
      "learning_rate": 0.00044621523140805274,
      "loss": 0.0872,
      "step": 32050
    },
    {
      "epoch": 9.396248534583822,
      "grad_norm": 1.0552304983139038,
      "learning_rate": 0.0004461314351204643,
      "loss": 0.0952,
      "step": 32060
    },
    {
      "epoch": 9.399179366940212,
      "grad_norm": 1.1522109508514404,
      "learning_rate": 0.000446047638832876,
      "loss": 0.0989,
      "step": 32070
    },
    {
      "epoch": 9.402110199296601,
      "grad_norm": 1.6765782833099365,
      "learning_rate": 0.0004459638425452876,
      "loss": 0.0984,
      "step": 32080
    },
    {
      "epoch": 9.40504103165299,
      "grad_norm": 0.72381192445755,
      "learning_rate": 0.0004458800462576993,
      "loss": 0.085,
      "step": 32090
    },
    {
      "epoch": 9.40797186400938,
      "grad_norm": 1.5605961084365845,
      "learning_rate": 0.0004457962499701109,
      "loss": 0.1043,
      "step": 32100
    },
    {
      "epoch": 9.410902696365769,
      "grad_norm": 0.7665477991104126,
      "learning_rate": 0.00044571245368252256,
      "loss": 0.1223,
      "step": 32110
    },
    {
      "epoch": 9.413833528722158,
      "grad_norm": 0.5785242319107056,
      "learning_rate": 0.00044562865739493417,
      "loss": 0.0862,
      "step": 32120
    },
    {
      "epoch": 9.416764361078545,
      "grad_norm": 1.4032796621322632,
      "learning_rate": 0.0004455448611073458,
      "loss": 0.1101,
      "step": 32130
    },
    {
      "epoch": 9.419695193434935,
      "grad_norm": 0.5246685147285461,
      "learning_rate": 0.00044546106481975744,
      "loss": 0.047,
      "step": 32140
    },
    {
      "epoch": 9.422626025791324,
      "grad_norm": 0.7482849955558777,
      "learning_rate": 0.00044537726853216905,
      "loss": 0.0933,
      "step": 32150
    },
    {
      "epoch": 9.425556858147713,
      "grad_norm": 1.0382224321365356,
      "learning_rate": 0.0004452934722445807,
      "loss": 0.1051,
      "step": 32160
    },
    {
      "epoch": 9.428487690504102,
      "grad_norm": 1.2527649402618408,
      "learning_rate": 0.0004452096759569923,
      "loss": 0.0799,
      "step": 32170
    },
    {
      "epoch": 9.431418522860492,
      "grad_norm": 2.0043256282806396,
      "learning_rate": 0.00044512587966940393,
      "loss": 0.0937,
      "step": 32180
    },
    {
      "epoch": 9.434349355216881,
      "grad_norm": 0.845874011516571,
      "learning_rate": 0.0004450420833818156,
      "loss": 0.0867,
      "step": 32190
    },
    {
      "epoch": 9.43728018757327,
      "grad_norm": 0.6875812411308289,
      "learning_rate": 0.0004449582870942272,
      "loss": 0.0885,
      "step": 32200
    },
    {
      "epoch": 9.44021101992966,
      "grad_norm": 1.2391717433929443,
      "learning_rate": 0.00044487449080663887,
      "loss": 0.1352,
      "step": 32210
    },
    {
      "epoch": 9.443141852286049,
      "grad_norm": 1.0486514568328857,
      "learning_rate": 0.0004447906945190505,
      "loss": 0.0868,
      "step": 32220
    },
    {
      "epoch": 9.446072684642438,
      "grad_norm": 0.9895713329315186,
      "learning_rate": 0.0004447068982314621,
      "loss": 0.1113,
      "step": 32230
    },
    {
      "epoch": 9.449003516998827,
      "grad_norm": 0.8284003138542175,
      "learning_rate": 0.00044462310194387375,
      "loss": 0.084,
      "step": 32240
    },
    {
      "epoch": 9.451934349355216,
      "grad_norm": 1.1502337455749512,
      "learning_rate": 0.00044453930565628536,
      "loss": 0.1009,
      "step": 32250
    },
    {
      "epoch": 9.454865181711606,
      "grad_norm": 0.7387424111366272,
      "learning_rate": 0.00044445550936869703,
      "loss": 0.0728,
      "step": 32260
    },
    {
      "epoch": 9.457796014067995,
      "grad_norm": 1.5452525615692139,
      "learning_rate": 0.00044437171308110864,
      "loss": 0.1085,
      "step": 32270
    },
    {
      "epoch": 9.460726846424384,
      "grad_norm": 0.6425284743309021,
      "learning_rate": 0.0004442879167935203,
      "loss": 0.1219,
      "step": 32280
    },
    {
      "epoch": 9.463657678780773,
      "grad_norm": 0.7490608096122742,
      "learning_rate": 0.0004442041205059319,
      "loss": 0.11,
      "step": 32290
    },
    {
      "epoch": 9.466588511137163,
      "grad_norm": 0.6756064295768738,
      "learning_rate": 0.0004441203242183435,
      "loss": 0.1022,
      "step": 32300
    },
    {
      "epoch": 9.469519343493552,
      "grad_norm": 1.7472882270812988,
      "learning_rate": 0.0004440365279307552,
      "loss": 0.0973,
      "step": 32310
    },
    {
      "epoch": 9.472450175849941,
      "grad_norm": 0.7210726141929626,
      "learning_rate": 0.0004439527316431668,
      "loss": 0.0869,
      "step": 32320
    },
    {
      "epoch": 9.47538100820633,
      "grad_norm": 0.8636696934700012,
      "learning_rate": 0.0004438689353555785,
      "loss": 0.0767,
      "step": 32330
    },
    {
      "epoch": 9.47831184056272,
      "grad_norm": 0.6430782079696655,
      "learning_rate": 0.00044378513906799007,
      "loss": 0.1112,
      "step": 32340
    },
    {
      "epoch": 9.481242672919109,
      "grad_norm": 1.3115270137786865,
      "learning_rate": 0.0004437013427804017,
      "loss": 0.0965,
      "step": 32350
    },
    {
      "epoch": 9.484173505275498,
      "grad_norm": 1.7244161367416382,
      "learning_rate": 0.00044361754649281334,
      "loss": 0.0966,
      "step": 32360
    },
    {
      "epoch": 9.487104337631887,
      "grad_norm": 1.0832995176315308,
      "learning_rate": 0.00044353375020522495,
      "loss": 0.1088,
      "step": 32370
    },
    {
      "epoch": 9.490035169988277,
      "grad_norm": 0.4728039801120758,
      "learning_rate": 0.00044344995391763667,
      "loss": 0.1043,
      "step": 32380
    },
    {
      "epoch": 9.492966002344666,
      "grad_norm": 1.0006166696548462,
      "learning_rate": 0.0004433661576300482,
      "loss": 0.1132,
      "step": 32390
    },
    {
      "epoch": 9.495896834701055,
      "grad_norm": 1.13746976852417,
      "learning_rate": 0.00044328236134245983,
      "loss": 0.0824,
      "step": 32400
    },
    {
      "epoch": 9.498827667057444,
      "grad_norm": 0.7302780747413635,
      "learning_rate": 0.00044319856505487155,
      "loss": 0.083,
      "step": 32410
    },
    {
      "epoch": 9.501758499413834,
      "grad_norm": 0.8748328685760498,
      "learning_rate": 0.0004431147687672831,
      "loss": 0.0906,
      "step": 32420
    },
    {
      "epoch": 9.504689331770223,
      "grad_norm": 0.4130614399909973,
      "learning_rate": 0.0004430309724796948,
      "loss": 0.0793,
      "step": 32430
    },
    {
      "epoch": 9.507620164126612,
      "grad_norm": 1.4041513204574585,
      "learning_rate": 0.00044294717619210644,
      "loss": 0.1128,
      "step": 32440
    },
    {
      "epoch": 9.510550996483001,
      "grad_norm": 0.9009031653404236,
      "learning_rate": 0.0004428633799045181,
      "loss": 0.0666,
      "step": 32450
    },
    {
      "epoch": 9.51348182883939,
      "grad_norm": 1.0318470001220703,
      "learning_rate": 0.0004427795836169297,
      "loss": 0.1008,
      "step": 32460
    },
    {
      "epoch": 9.51641266119578,
      "grad_norm": 0.8636763095855713,
      "learning_rate": 0.00044269578732934127,
      "loss": 0.0829,
      "step": 32470
    },
    {
      "epoch": 9.519343493552169,
      "grad_norm": 1.5947937965393066,
      "learning_rate": 0.000442611991041753,
      "loss": 0.0909,
      "step": 32480
    },
    {
      "epoch": 9.522274325908558,
      "grad_norm": 1.1362730264663696,
      "learning_rate": 0.0004425281947541646,
      "loss": 0.118,
      "step": 32490
    },
    {
      "epoch": 9.525205158264948,
      "grad_norm": 1.3849924802780151,
      "learning_rate": 0.00044244439846657626,
      "loss": 0.0986,
      "step": 32500
    },
    {
      "epoch": 9.528135990621337,
      "grad_norm": 1.3929448127746582,
      "learning_rate": 0.00044236060217898787,
      "loss": 0.0932,
      "step": 32510
    },
    {
      "epoch": 9.531066822977726,
      "grad_norm": 1.4834814071655273,
      "learning_rate": 0.0004422768058913995,
      "loss": 0.0705,
      "step": 32520
    },
    {
      "epoch": 9.533997655334115,
      "grad_norm": 1.5297449827194214,
      "learning_rate": 0.00044219300960381114,
      "loss": 0.0798,
      "step": 32530
    },
    {
      "epoch": 9.536928487690504,
      "grad_norm": 0.7374861240386963,
      "learning_rate": 0.00044210921331622275,
      "loss": 0.0971,
      "step": 32540
    },
    {
      "epoch": 9.539859320046894,
      "grad_norm": 1.2774865627288818,
      "learning_rate": 0.0004420254170286344,
      "loss": 0.0844,
      "step": 32550
    },
    {
      "epoch": 9.542790152403283,
      "grad_norm": 0.8840625882148743,
      "learning_rate": 0.000441941620741046,
      "loss": 0.078,
      "step": 32560
    },
    {
      "epoch": 9.545720984759672,
      "grad_norm": 0.6337326765060425,
      "learning_rate": 0.00044185782445345763,
      "loss": 0.1038,
      "step": 32570
    },
    {
      "epoch": 9.548651817116061,
      "grad_norm": 0.8714467287063599,
      "learning_rate": 0.0004417740281658693,
      "loss": 0.0865,
      "step": 32580
    },
    {
      "epoch": 9.55158264947245,
      "grad_norm": 0.832607090473175,
      "learning_rate": 0.0004416902318782809,
      "loss": 0.1053,
      "step": 32590
    },
    {
      "epoch": 9.55451348182884,
      "grad_norm": 0.5095430612564087,
      "learning_rate": 0.00044160643559069257,
      "loss": 0.0961,
      "step": 32600
    },
    {
      "epoch": 9.55744431418523,
      "grad_norm": 0.4816778600215912,
      "learning_rate": 0.0004415226393031042,
      "loss": 0.0854,
      "step": 32610
    },
    {
      "epoch": 9.560375146541618,
      "grad_norm": 0.5535361170768738,
      "learning_rate": 0.00044143884301551584,
      "loss": 0.1008,
      "step": 32620
    },
    {
      "epoch": 9.563305978898008,
      "grad_norm": 3.383371591567993,
      "learning_rate": 0.00044135504672792745,
      "loss": 0.0923,
      "step": 32630
    },
    {
      "epoch": 9.566236811254397,
      "grad_norm": 1.0894477367401123,
      "learning_rate": 0.00044127125044033906,
      "loss": 0.1012,
      "step": 32640
    },
    {
      "epoch": 9.569167643610786,
      "grad_norm": 0.9366499781608582,
      "learning_rate": 0.00044118745415275073,
      "loss": 0.1082,
      "step": 32650
    },
    {
      "epoch": 9.572098475967175,
      "grad_norm": 1.0930486917495728,
      "learning_rate": 0.00044110365786516234,
      "loss": 0.088,
      "step": 32660
    },
    {
      "epoch": 9.575029308323565,
      "grad_norm": 0.8553963303565979,
      "learning_rate": 0.000441019861577574,
      "loss": 0.0659,
      "step": 32670
    },
    {
      "epoch": 9.577960140679954,
      "grad_norm": 2.4393961429595947,
      "learning_rate": 0.0004409360652899856,
      "loss": 0.1067,
      "step": 32680
    },
    {
      "epoch": 9.580890973036343,
      "grad_norm": 1.7402750253677368,
      "learning_rate": 0.0004408522690023972,
      "loss": 0.0829,
      "step": 32690
    },
    {
      "epoch": 9.58382180539273,
      "grad_norm": 0.6242197155952454,
      "learning_rate": 0.0004407684727148089,
      "loss": 0.1165,
      "step": 32700
    },
    {
      "epoch": 9.586752637749122,
      "grad_norm": 0.906583309173584,
      "learning_rate": 0.0004406846764272205,
      "loss": 0.111,
      "step": 32710
    },
    {
      "epoch": 9.58968347010551,
      "grad_norm": 0.5036402940750122,
      "learning_rate": 0.00044060088013963216,
      "loss": 0.1013,
      "step": 32720
    },
    {
      "epoch": 9.592614302461898,
      "grad_norm": 0.7606616020202637,
      "learning_rate": 0.00044051708385204377,
      "loss": 0.0792,
      "step": 32730
    },
    {
      "epoch": 9.595545134818288,
      "grad_norm": 0.5095638036727905,
      "learning_rate": 0.0004404332875644554,
      "loss": 0.0814,
      "step": 32740
    },
    {
      "epoch": 9.598475967174677,
      "grad_norm": 1.0845516920089722,
      "learning_rate": 0.00044034949127686704,
      "loss": 0.0792,
      "step": 32750
    },
    {
      "epoch": 9.601406799531066,
      "grad_norm": 1.1800320148468018,
      "learning_rate": 0.00044026569498927865,
      "loss": 0.0971,
      "step": 32760
    },
    {
      "epoch": 9.604337631887455,
      "grad_norm": 0.7229344248771667,
      "learning_rate": 0.00044018189870169037,
      "loss": 0.0935,
      "step": 32770
    },
    {
      "epoch": 9.607268464243845,
      "grad_norm": 0.9219183921813965,
      "learning_rate": 0.0004400981024141019,
      "loss": 0.0922,
      "step": 32780
    },
    {
      "epoch": 9.610199296600234,
      "grad_norm": 0.7297172546386719,
      "learning_rate": 0.00044001430612651364,
      "loss": 0.1205,
      "step": 32790
    },
    {
      "epoch": 9.613130128956623,
      "grad_norm": 0.546975314617157,
      "learning_rate": 0.0004399305098389252,
      "loss": 0.0968,
      "step": 32800
    },
    {
      "epoch": 9.616060961313012,
      "grad_norm": 0.4854944944381714,
      "learning_rate": 0.0004398467135513368,
      "loss": 0.0768,
      "step": 32810
    },
    {
      "epoch": 9.618991793669402,
      "grad_norm": 1.191948652267456,
      "learning_rate": 0.0004397629172637485,
      "loss": 0.0904,
      "step": 32820
    },
    {
      "epoch": 9.62192262602579,
      "grad_norm": 0.6081277132034302,
      "learning_rate": 0.0004396791209761601,
      "loss": 0.0911,
      "step": 32830
    },
    {
      "epoch": 9.62485345838218,
      "grad_norm": 1.121549367904663,
      "learning_rate": 0.0004395953246885718,
      "loss": 0.0867,
      "step": 32840
    },
    {
      "epoch": 9.62778429073857,
      "grad_norm": 0.7905793786048889,
      "learning_rate": 0.0004395115284009834,
      "loss": 0.0826,
      "step": 32850
    },
    {
      "epoch": 9.630715123094959,
      "grad_norm": 1.551177978515625,
      "learning_rate": 0.00043942773211339497,
      "loss": 0.0914,
      "step": 32860
    },
    {
      "epoch": 9.633645955451348,
      "grad_norm": 1.3704159259796143,
      "learning_rate": 0.0004393439358258067,
      "loss": 0.1157,
      "step": 32870
    },
    {
      "epoch": 9.636576787807737,
      "grad_norm": 1.0972551107406616,
      "learning_rate": 0.0004392601395382183,
      "loss": 0.1112,
      "step": 32880
    },
    {
      "epoch": 9.639507620164126,
      "grad_norm": 0.5689226984977722,
      "learning_rate": 0.00043917634325062996,
      "loss": 0.0784,
      "step": 32890
    },
    {
      "epoch": 9.642438452520516,
      "grad_norm": 1.0210754871368408,
      "learning_rate": 0.00043909254696304157,
      "loss": 0.122,
      "step": 32900
    },
    {
      "epoch": 9.645369284876905,
      "grad_norm": 1.1283401250839233,
      "learning_rate": 0.0004390087506754532,
      "loss": 0.1144,
      "step": 32910
    },
    {
      "epoch": 9.648300117233294,
      "grad_norm": 0.930870771408081,
      "learning_rate": 0.00043892495438786484,
      "loss": 0.1141,
      "step": 32920
    },
    {
      "epoch": 9.651230949589683,
      "grad_norm": 0.99509596824646,
      "learning_rate": 0.00043884115810027645,
      "loss": 0.1033,
      "step": 32930
    },
    {
      "epoch": 9.654161781946073,
      "grad_norm": 1.0511482954025269,
      "learning_rate": 0.0004387573618126881,
      "loss": 0.1127,
      "step": 32940
    },
    {
      "epoch": 9.657092614302462,
      "grad_norm": 0.8462114334106445,
      "learning_rate": 0.0004386735655250997,
      "loss": 0.0809,
      "step": 32950
    },
    {
      "epoch": 9.660023446658851,
      "grad_norm": 0.5727906823158264,
      "learning_rate": 0.00043858976923751133,
      "loss": 0.1002,
      "step": 32960
    },
    {
      "epoch": 9.66295427901524,
      "grad_norm": 1.3834259510040283,
      "learning_rate": 0.000438505972949923,
      "loss": 0.0961,
      "step": 32970
    },
    {
      "epoch": 9.66588511137163,
      "grad_norm": 0.7262815237045288,
      "learning_rate": 0.0004384221766623346,
      "loss": 0.0894,
      "step": 32980
    },
    {
      "epoch": 9.668815943728019,
      "grad_norm": 0.5356839299201965,
      "learning_rate": 0.00043833838037474627,
      "loss": 0.0868,
      "step": 32990
    },
    {
      "epoch": 9.671746776084408,
      "grad_norm": 1.759921908378601,
      "learning_rate": 0.0004382545840871579,
      "loss": 0.0751,
      "step": 33000
    },
    {
      "epoch": 9.674677608440797,
      "grad_norm": 0.537513792514801,
      "learning_rate": 0.00043817078779956954,
      "loss": 0.0965,
      "step": 33010
    },
    {
      "epoch": 9.677608440797187,
      "grad_norm": 1.8827744722366333,
      "learning_rate": 0.00043808699151198115,
      "loss": 0.1061,
      "step": 33020
    },
    {
      "epoch": 9.680539273153576,
      "grad_norm": 0.6694169640541077,
      "learning_rate": 0.00043800319522439276,
      "loss": 0.0857,
      "step": 33030
    },
    {
      "epoch": 9.683470105509965,
      "grad_norm": 1.0787569284439087,
      "learning_rate": 0.00043791939893680443,
      "loss": 0.1224,
      "step": 33040
    },
    {
      "epoch": 9.686400937866354,
      "grad_norm": 1.4700511693954468,
      "learning_rate": 0.00043783560264921604,
      "loss": 0.0901,
      "step": 33050
    },
    {
      "epoch": 9.689331770222744,
      "grad_norm": 1.3203041553497314,
      "learning_rate": 0.0004377518063616277,
      "loss": 0.093,
      "step": 33060
    },
    {
      "epoch": 9.692262602579133,
      "grad_norm": 1.0590648651123047,
      "learning_rate": 0.0004376680100740393,
      "loss": 0.0941,
      "step": 33070
    },
    {
      "epoch": 9.695193434935522,
      "grad_norm": 1.1019744873046875,
      "learning_rate": 0.0004375842137864509,
      "loss": 0.1076,
      "step": 33080
    },
    {
      "epoch": 9.698124267291911,
      "grad_norm": 0.9683600664138794,
      "learning_rate": 0.0004375004174988626,
      "loss": 0.0983,
      "step": 33090
    },
    {
      "epoch": 9.7010550996483,
      "grad_norm": 1.1223418712615967,
      "learning_rate": 0.0004374166212112742,
      "loss": 0.1027,
      "step": 33100
    },
    {
      "epoch": 9.70398593200469,
      "grad_norm": 0.8989694118499756,
      "learning_rate": 0.00043733282492368586,
      "loss": 0.0816,
      "step": 33110
    },
    {
      "epoch": 9.706916764361079,
      "grad_norm": 0.7415425181388855,
      "learning_rate": 0.00043724902863609747,
      "loss": 0.1137,
      "step": 33120
    },
    {
      "epoch": 9.709847596717468,
      "grad_norm": 0.7453848123550415,
      "learning_rate": 0.0004371652323485091,
      "loss": 0.1003,
      "step": 33130
    },
    {
      "epoch": 9.712778429073857,
      "grad_norm": 0.42650750279426575,
      "learning_rate": 0.00043708143606092074,
      "loss": 0.0855,
      "step": 33140
    },
    {
      "epoch": 9.715709261430247,
      "grad_norm": 0.7342669367790222,
      "learning_rate": 0.00043699763977333235,
      "loss": 0.0683,
      "step": 33150
    },
    {
      "epoch": 9.718640093786636,
      "grad_norm": 0.987615704536438,
      "learning_rate": 0.000436913843485744,
      "loss": 0.0778,
      "step": 33160
    },
    {
      "epoch": 9.721570926143025,
      "grad_norm": 1.1258094310760498,
      "learning_rate": 0.0004368300471981556,
      "loss": 0.0837,
      "step": 33170
    },
    {
      "epoch": 9.724501758499414,
      "grad_norm": 0.6695621609687805,
      "learning_rate": 0.00043674625091056734,
      "loss": 0.0931,
      "step": 33180
    },
    {
      "epoch": 9.727432590855804,
      "grad_norm": 0.4397259056568146,
      "learning_rate": 0.0004366624546229789,
      "loss": 0.0768,
      "step": 33190
    },
    {
      "epoch": 9.730363423212193,
      "grad_norm": 2.0167603492736816,
      "learning_rate": 0.0004365786583353905,
      "loss": 0.1059,
      "step": 33200
    },
    {
      "epoch": 9.733294255568582,
      "grad_norm": 0.904794454574585,
      "learning_rate": 0.0004364948620478022,
      "loss": 0.0802,
      "step": 33210
    },
    {
      "epoch": 9.736225087924971,
      "grad_norm": 1.0231845378875732,
      "learning_rate": 0.0004364110657602138,
      "loss": 0.0911,
      "step": 33220
    },
    {
      "epoch": 9.73915592028136,
      "grad_norm": 0.8836821913719177,
      "learning_rate": 0.0004363272694726255,
      "loss": 0.0887,
      "step": 33230
    },
    {
      "epoch": 9.74208675263775,
      "grad_norm": 1.0878865718841553,
      "learning_rate": 0.00043624347318503706,
      "loss": 0.0823,
      "step": 33240
    },
    {
      "epoch": 9.745017584994137,
      "grad_norm": 0.7929593324661255,
      "learning_rate": 0.00043615967689744867,
      "loss": 0.1016,
      "step": 33250
    },
    {
      "epoch": 9.747948417350528,
      "grad_norm": 0.7860913872718811,
      "learning_rate": 0.0004360758806098604,
      "loss": 0.1084,
      "step": 33260
    },
    {
      "epoch": 9.750879249706916,
      "grad_norm": 0.47147172689437866,
      "learning_rate": 0.00043599208432227194,
      "loss": 0.0915,
      "step": 33270
    },
    {
      "epoch": 9.753810082063307,
      "grad_norm": 0.3440004289150238,
      "learning_rate": 0.00043590828803468366,
      "loss": 0.0646,
      "step": 33280
    },
    {
      "epoch": 9.756740914419694,
      "grad_norm": 0.5434272885322571,
      "learning_rate": 0.00043582449174709527,
      "loss": 0.0844,
      "step": 33290
    },
    {
      "epoch": 9.759671746776084,
      "grad_norm": 1.432809591293335,
      "learning_rate": 0.0004357406954595068,
      "loss": 0.1252,
      "step": 33300
    },
    {
      "epoch": 9.762602579132473,
      "grad_norm": 1.168032169342041,
      "learning_rate": 0.00043565689917191854,
      "loss": 0.0715,
      "step": 33310
    },
    {
      "epoch": 9.765533411488862,
      "grad_norm": 0.7723042368888855,
      "learning_rate": 0.00043557310288433015,
      "loss": 0.0919,
      "step": 33320
    },
    {
      "epoch": 9.768464243845251,
      "grad_norm": 0.5779016017913818,
      "learning_rate": 0.0004354893065967418,
      "loss": 0.0947,
      "step": 33330
    },
    {
      "epoch": 9.77139507620164,
      "grad_norm": 0.9368350505828857,
      "learning_rate": 0.0004354055103091534,
      "loss": 0.0919,
      "step": 33340
    },
    {
      "epoch": 9.77432590855803,
      "grad_norm": 0.7198853492736816,
      "learning_rate": 0.0004353217140215651,
      "loss": 0.0658,
      "step": 33350
    },
    {
      "epoch": 9.777256740914419,
      "grad_norm": 0.625778317451477,
      "learning_rate": 0.0004352379177339767,
      "loss": 0.0787,
      "step": 33360
    },
    {
      "epoch": 9.780187573270808,
      "grad_norm": 0.8175680637359619,
      "learning_rate": 0.0004351541214463883,
      "loss": 0.1157,
      "step": 33370
    },
    {
      "epoch": 9.783118405627198,
      "grad_norm": 1.017068862915039,
      "learning_rate": 0.00043507032515879997,
      "loss": 0.0909,
      "step": 33380
    },
    {
      "epoch": 9.786049237983587,
      "grad_norm": 0.23505349457263947,
      "learning_rate": 0.0004349865288712116,
      "loss": 0.0952,
      "step": 33390
    },
    {
      "epoch": 9.788980070339976,
      "grad_norm": 0.6123917102813721,
      "learning_rate": 0.00043490273258362324,
      "loss": 0.0927,
      "step": 33400
    },
    {
      "epoch": 9.791910902696365,
      "grad_norm": 1.016704797744751,
      "learning_rate": 0.00043481893629603485,
      "loss": 0.0862,
      "step": 33410
    },
    {
      "epoch": 9.794841735052755,
      "grad_norm": 1.2636469602584839,
      "learning_rate": 0.00043473514000844646,
      "loss": 0.1127,
      "step": 33420
    },
    {
      "epoch": 9.797772567409144,
      "grad_norm": 0.5676947236061096,
      "learning_rate": 0.00043465134372085813,
      "loss": 0.0655,
      "step": 33430
    },
    {
      "epoch": 9.800703399765533,
      "grad_norm": 0.444964200258255,
      "learning_rate": 0.00043456754743326974,
      "loss": 0.1056,
      "step": 33440
    },
    {
      "epoch": 9.803634232121922,
      "grad_norm": 0.46507346630096436,
      "learning_rate": 0.0004344837511456814,
      "loss": 0.0615,
      "step": 33450
    },
    {
      "epoch": 9.806565064478312,
      "grad_norm": 0.9720153212547302,
      "learning_rate": 0.000434399954858093,
      "loss": 0.1174,
      "step": 33460
    },
    {
      "epoch": 9.8094958968347,
      "grad_norm": 1.0641913414001465,
      "learning_rate": 0.0004343161585705046,
      "loss": 0.1036,
      "step": 33470
    },
    {
      "epoch": 9.81242672919109,
      "grad_norm": 0.7884088754653931,
      "learning_rate": 0.0004342323622829163,
      "loss": 0.1275,
      "step": 33480
    },
    {
      "epoch": 9.81535756154748,
      "grad_norm": 0.32271862030029297,
      "learning_rate": 0.0004341485659953279,
      "loss": 0.0751,
      "step": 33490
    },
    {
      "epoch": 9.818288393903869,
      "grad_norm": 1.039992332458496,
      "learning_rate": 0.00043406476970773956,
      "loss": 0.0862,
      "step": 33500
    },
    {
      "epoch": 9.821219226260258,
      "grad_norm": 0.5838337540626526,
      "learning_rate": 0.00043398097342015117,
      "loss": 0.0856,
      "step": 33510
    },
    {
      "epoch": 9.824150058616647,
      "grad_norm": 1.2348953485488892,
      "learning_rate": 0.00043389717713256283,
      "loss": 0.0919,
      "step": 33520
    },
    {
      "epoch": 9.827080890973036,
      "grad_norm": 1.0386126041412354,
      "learning_rate": 0.00043381338084497444,
      "loss": 0.0887,
      "step": 33530
    },
    {
      "epoch": 9.830011723329426,
      "grad_norm": 0.9820998311042786,
      "learning_rate": 0.00043372958455738605,
      "loss": 0.0913,
      "step": 33540
    },
    {
      "epoch": 9.832942555685815,
      "grad_norm": 1.3077832460403442,
      "learning_rate": 0.0004336457882697977,
      "loss": 0.0925,
      "step": 33550
    },
    {
      "epoch": 9.835873388042204,
      "grad_norm": 1.0003947019577026,
      "learning_rate": 0.0004335619919822093,
      "loss": 0.1151,
      "step": 33560
    },
    {
      "epoch": 9.838804220398593,
      "grad_norm": 0.8992592692375183,
      "learning_rate": 0.000433478195694621,
      "loss": 0.1026,
      "step": 33570
    },
    {
      "epoch": 9.841735052754983,
      "grad_norm": 0.7066418528556824,
      "learning_rate": 0.0004333943994070326,
      "loss": 0.0597,
      "step": 33580
    },
    {
      "epoch": 9.844665885111372,
      "grad_norm": 0.37880539894104004,
      "learning_rate": 0.0004333106031194442,
      "loss": 0.0954,
      "step": 33590
    },
    {
      "epoch": 9.847596717467761,
      "grad_norm": 1.6239207983016968,
      "learning_rate": 0.00043322680683185587,
      "loss": 0.0965,
      "step": 33600
    },
    {
      "epoch": 9.85052754982415,
      "grad_norm": 0.6731612086296082,
      "learning_rate": 0.0004331430105442675,
      "loss": 0.0913,
      "step": 33610
    },
    {
      "epoch": 9.85345838218054,
      "grad_norm": 0.30875271558761597,
      "learning_rate": 0.0004330592142566792,
      "loss": 0.0983,
      "step": 33620
    },
    {
      "epoch": 9.856389214536929,
      "grad_norm": 1.0035573244094849,
      "learning_rate": 0.00043297541796909076,
      "loss": 0.0899,
      "step": 33630
    },
    {
      "epoch": 9.859320046893318,
      "grad_norm": 1.1277503967285156,
      "learning_rate": 0.00043289162168150237,
      "loss": 0.1193,
      "step": 33640
    },
    {
      "epoch": 9.862250879249707,
      "grad_norm": 1.4578877687454224,
      "learning_rate": 0.0004328078253939141,
      "loss": 0.1016,
      "step": 33650
    },
    {
      "epoch": 9.865181711606096,
      "grad_norm": 0.8018819093704224,
      "learning_rate": 0.00043272402910632564,
      "loss": 0.0926,
      "step": 33660
    },
    {
      "epoch": 9.868112543962486,
      "grad_norm": 0.8195152878761292,
      "learning_rate": 0.00043264023281873736,
      "loss": 0.0787,
      "step": 33670
    },
    {
      "epoch": 9.871043376318875,
      "grad_norm": 1.7884910106658936,
      "learning_rate": 0.0004325564365311489,
      "loss": 0.0924,
      "step": 33680
    },
    {
      "epoch": 9.873974208675264,
      "grad_norm": 1.7945256233215332,
      "learning_rate": 0.00043247264024356063,
      "loss": 0.098,
      "step": 33690
    },
    {
      "epoch": 9.876905041031653,
      "grad_norm": 1.072736144065857,
      "learning_rate": 0.00043238884395597224,
      "loss": 0.0835,
      "step": 33700
    },
    {
      "epoch": 9.879835873388043,
      "grad_norm": 0.7153169512748718,
      "learning_rate": 0.0004323050476683838,
      "loss": 0.0969,
      "step": 33710
    },
    {
      "epoch": 9.882766705744432,
      "grad_norm": 0.8186249732971191,
      "learning_rate": 0.0004322212513807955,
      "loss": 0.0881,
      "step": 33720
    },
    {
      "epoch": 9.885697538100821,
      "grad_norm": 0.984531819820404,
      "learning_rate": 0.0004321374550932071,
      "loss": 0.1086,
      "step": 33730
    },
    {
      "epoch": 9.88862837045721,
      "grad_norm": 0.8027106523513794,
      "learning_rate": 0.0004320536588056188,
      "loss": 0.0732,
      "step": 33740
    },
    {
      "epoch": 9.8915592028136,
      "grad_norm": 0.6286831498146057,
      "learning_rate": 0.0004319698625180304,
      "loss": 0.1103,
      "step": 33750
    },
    {
      "epoch": 9.894490035169989,
      "grad_norm": 0.4472239315509796,
      "learning_rate": 0.000431886066230442,
      "loss": 0.0909,
      "step": 33760
    },
    {
      "epoch": 9.897420867526378,
      "grad_norm": 1.0470004081726074,
      "learning_rate": 0.00043180226994285367,
      "loss": 0.0841,
      "step": 33770
    },
    {
      "epoch": 9.900351699882767,
      "grad_norm": 2.4935216903686523,
      "learning_rate": 0.0004317184736552653,
      "loss": 0.1214,
      "step": 33780
    },
    {
      "epoch": 9.903282532239157,
      "grad_norm": 0.797528088092804,
      "learning_rate": 0.00043163467736767694,
      "loss": 0.1034,
      "step": 33790
    },
    {
      "epoch": 9.906213364595546,
      "grad_norm": 0.8757821321487427,
      "learning_rate": 0.00043155088108008855,
      "loss": 0.1004,
      "step": 33800
    },
    {
      "epoch": 9.909144196951935,
      "grad_norm": 0.6024268269538879,
      "learning_rate": 0.00043146708479250016,
      "loss": 0.0868,
      "step": 33810
    },
    {
      "epoch": 9.912075029308323,
      "grad_norm": 0.5788748264312744,
      "learning_rate": 0.00043138328850491183,
      "loss": 0.1116,
      "step": 33820
    },
    {
      "epoch": 9.915005861664714,
      "grad_norm": 1.7226920127868652,
      "learning_rate": 0.00043129949221732344,
      "loss": 0.0991,
      "step": 33830
    },
    {
      "epoch": 9.917936694021101,
      "grad_norm": 0.9367091059684753,
      "learning_rate": 0.0004312156959297351,
      "loss": 0.0859,
      "step": 33840
    },
    {
      "epoch": 9.920867526377492,
      "grad_norm": 0.9577701091766357,
      "learning_rate": 0.0004311318996421467,
      "loss": 0.0695,
      "step": 33850
    },
    {
      "epoch": 9.92379835873388,
      "grad_norm": 1.153597116470337,
      "learning_rate": 0.0004310481033545584,
      "loss": 0.1108,
      "step": 33860
    },
    {
      "epoch": 9.926729191090269,
      "grad_norm": 1.5329267978668213,
      "learning_rate": 0.00043096430706697,
      "loss": 0.0831,
      "step": 33870
    },
    {
      "epoch": 9.929660023446658,
      "grad_norm": 1.2113326787948608,
      "learning_rate": 0.0004308805107793816,
      "loss": 0.0892,
      "step": 33880
    },
    {
      "epoch": 9.932590855803047,
      "grad_norm": 2.12334942817688,
      "learning_rate": 0.00043079671449179326,
      "loss": 0.0799,
      "step": 33890
    },
    {
      "epoch": 9.935521688159437,
      "grad_norm": 0.8927798271179199,
      "learning_rate": 0.00043071291820420487,
      "loss": 0.1343,
      "step": 33900
    },
    {
      "epoch": 9.938452520515826,
      "grad_norm": 0.4473942220211029,
      "learning_rate": 0.00043062912191661653,
      "loss": 0.0829,
      "step": 33910
    },
    {
      "epoch": 9.941383352872215,
      "grad_norm": 1.1489825248718262,
      "learning_rate": 0.00043054532562902814,
      "loss": 0.0888,
      "step": 33920
    },
    {
      "epoch": 9.944314185228604,
      "grad_norm": 0.35206761956214905,
      "learning_rate": 0.00043046152934143975,
      "loss": 0.0953,
      "step": 33930
    },
    {
      "epoch": 9.947245017584994,
      "grad_norm": 0.5799428820610046,
      "learning_rate": 0.0004303777330538514,
      "loss": 0.0929,
      "step": 33940
    },
    {
      "epoch": 9.950175849941383,
      "grad_norm": 0.6933925151824951,
      "learning_rate": 0.000430293936766263,
      "loss": 0.0774,
      "step": 33950
    },
    {
      "epoch": 9.953106682297772,
      "grad_norm": 1.249191403388977,
      "learning_rate": 0.0004302101404786747,
      "loss": 0.0804,
      "step": 33960
    },
    {
      "epoch": 9.956037514654161,
      "grad_norm": 0.8851504921913147,
      "learning_rate": 0.0004301263441910863,
      "loss": 0.0678,
      "step": 33970
    },
    {
      "epoch": 9.95896834701055,
      "grad_norm": 0.7101484537124634,
      "learning_rate": 0.0004300425479034979,
      "loss": 0.0785,
      "step": 33980
    },
    {
      "epoch": 9.96189917936694,
      "grad_norm": 0.9337666630744934,
      "learning_rate": 0.00042995875161590957,
      "loss": 0.0981,
      "step": 33990
    },
    {
      "epoch": 9.964830011723329,
      "grad_norm": 1.3630744218826294,
      "learning_rate": 0.0004298749553283212,
      "loss": 0.1059,
      "step": 34000
    },
    {
      "epoch": 9.967760844079718,
      "grad_norm": 2.0661046504974365,
      "learning_rate": 0.00042979115904073285,
      "loss": 0.0899,
      "step": 34010
    },
    {
      "epoch": 9.970691676436108,
      "grad_norm": 1.0624351501464844,
      "learning_rate": 0.00042970736275314446,
      "loss": 0.1219,
      "step": 34020
    },
    {
      "epoch": 9.973622508792497,
      "grad_norm": 0.8668650984764099,
      "learning_rate": 0.00042962356646555607,
      "loss": 0.0883,
      "step": 34030
    },
    {
      "epoch": 9.976553341148886,
      "grad_norm": 0.6819055080413818,
      "learning_rate": 0.00042953977017796773,
      "loss": 0.0827,
      "step": 34040
    },
    {
      "epoch": 9.979484173505275,
      "grad_norm": 1.0373536348342896,
      "learning_rate": 0.00042945597389037934,
      "loss": 0.0987,
      "step": 34050
    },
    {
      "epoch": 9.982415005861665,
      "grad_norm": 2.4660069942474365,
      "learning_rate": 0.00042937217760279106,
      "loss": 0.0948,
      "step": 34060
    },
    {
      "epoch": 9.985345838218054,
      "grad_norm": 1.0193369388580322,
      "learning_rate": 0.0004292883813152026,
      "loss": 0.1079,
      "step": 34070
    },
    {
      "epoch": 9.988276670574443,
      "grad_norm": 1.0958188772201538,
      "learning_rate": 0.00042920458502761433,
      "loss": 0.1077,
      "step": 34080
    },
    {
      "epoch": 9.991207502930832,
      "grad_norm": 0.9772732853889465,
      "learning_rate": 0.00042912078874002594,
      "loss": 0.104,
      "step": 34090
    },
    {
      "epoch": 9.994138335287222,
      "grad_norm": 0.6332451701164246,
      "learning_rate": 0.0004290369924524375,
      "loss": 0.0859,
      "step": 34100
    },
    {
      "epoch": 9.99706916764361,
      "grad_norm": 0.9128898978233337,
      "learning_rate": 0.0004289531961648492,
      "loss": 0.0982,
      "step": 34110
    },
    {
      "epoch": 10.0,
      "grad_norm": 3.7063615322113037,
      "learning_rate": 0.0004288693998772608,
      "loss": 0.0911,
      "step": 34120
    },
    {
      "epoch": 10.0,
      "eval_accuracy": 0.6627101879327398,
      "eval_f1_macro": 0.6769127369637934,
      "eval_f1_micro": 0.7558474847805191,
      "eval_f1_weighted": 0.7414186681739475,
      "eval_loss": 0.09644987434148788,
      "eval_roc_auc": 0.8382499274037434,
      "eval_runtime": 242.2029,
      "eval_samples_per_second": 12.523,
      "eval_steps_per_second": 1.569,
      "step": 34120
    },
    {
      "epoch": 10.00293083235639,
      "grad_norm": 0.9502073526382446,
      "learning_rate": 0.0004287856035896725,
      "loss": 0.0882,
      "step": 34130
    },
    {
      "epoch": 10.005861664712778,
      "grad_norm": 0.34971579909324646,
      "learning_rate": 0.0004287018073020841,
      "loss": 0.0862,
      "step": 34140
    },
    {
      "epoch": 10.008792497069168,
      "grad_norm": 0.5187122225761414,
      "learning_rate": 0.00042861801101449565,
      "loss": 0.0635,
      "step": 34150
    },
    {
      "epoch": 10.011723329425557,
      "grad_norm": 0.7260177135467529,
      "learning_rate": 0.00042853421472690737,
      "loss": 0.0722,
      "step": 34160
    },
    {
      "epoch": 10.014654161781946,
      "grad_norm": 1.8626364469528198,
      "learning_rate": 0.000428450418439319,
      "loss": 0.0743,
      "step": 34170
    },
    {
      "epoch": 10.017584994138335,
      "grad_norm": 0.7797979116439819,
      "learning_rate": 0.00042836662215173064,
      "loss": 0.0933,
      "step": 34180
    },
    {
      "epoch": 10.020515826494725,
      "grad_norm": 0.7570191621780396,
      "learning_rate": 0.00042828282586414225,
      "loss": 0.1156,
      "step": 34190
    },
    {
      "epoch": 10.023446658851114,
      "grad_norm": 0.7016076445579529,
      "learning_rate": 0.00042819902957655386,
      "loss": 0.094,
      "step": 34200
    },
    {
      "epoch": 10.026377491207503,
      "grad_norm": 1.585945725440979,
      "learning_rate": 0.00042811523328896553,
      "loss": 0.1085,
      "step": 34210
    },
    {
      "epoch": 10.029308323563892,
      "grad_norm": 0.7650853395462036,
      "learning_rate": 0.00042803143700137714,
      "loss": 0.1046,
      "step": 34220
    },
    {
      "epoch": 10.032239155920282,
      "grad_norm": 0.6791256070137024,
      "learning_rate": 0.0004279476407137888,
      "loss": 0.0924,
      "step": 34230
    },
    {
      "epoch": 10.035169988276671,
      "grad_norm": 0.8522852063179016,
      "learning_rate": 0.0004278638444262004,
      "loss": 0.1207,
      "step": 34240
    },
    {
      "epoch": 10.03810082063306,
      "grad_norm": 1.5356359481811523,
      "learning_rate": 0.0004277800481386121,
      "loss": 0.0985,
      "step": 34250
    },
    {
      "epoch": 10.04103165298945,
      "grad_norm": 1.0827741622924805,
      "learning_rate": 0.0004276962518510237,
      "loss": 0.0633,
      "step": 34260
    },
    {
      "epoch": 10.043962485345839,
      "grad_norm": 0.7467241883277893,
      "learning_rate": 0.0004276124555634353,
      "loss": 0.0875,
      "step": 34270
    },
    {
      "epoch": 10.046893317702228,
      "grad_norm": 1.6107009649276733,
      "learning_rate": 0.00042752865927584696,
      "loss": 0.0935,
      "step": 34280
    },
    {
      "epoch": 10.049824150058617,
      "grad_norm": 1.2087818384170532,
      "learning_rate": 0.00042744486298825857,
      "loss": 0.0759,
      "step": 34290
    },
    {
      "epoch": 10.052754982415006,
      "grad_norm": 0.6908083558082581,
      "learning_rate": 0.00042736106670067023,
      "loss": 0.0895,
      "step": 34300
    },
    {
      "epoch": 10.055685814771396,
      "grad_norm": 1.3926033973693848,
      "learning_rate": 0.00042727727041308184,
      "loss": 0.1065,
      "step": 34310
    },
    {
      "epoch": 10.058616647127785,
      "grad_norm": 1.8706386089324951,
      "learning_rate": 0.00042719347412549345,
      "loss": 0.1197,
      "step": 34320
    },
    {
      "epoch": 10.061547479484174,
      "grad_norm": 0.222838893532753,
      "learning_rate": 0.0004271096778379051,
      "loss": 0.1107,
      "step": 34330
    },
    {
      "epoch": 10.064478311840563,
      "grad_norm": 1.3181592226028442,
      "learning_rate": 0.0004270258815503167,
      "loss": 0.1207,
      "step": 34340
    },
    {
      "epoch": 10.067409144196953,
      "grad_norm": 0.42308104038238525,
      "learning_rate": 0.0004269420852627284,
      "loss": 0.1134,
      "step": 34350
    },
    {
      "epoch": 10.070339976553342,
      "grad_norm": 0.8000532388687134,
      "learning_rate": 0.00042685828897514,
      "loss": 0.0782,
      "step": 34360
    },
    {
      "epoch": 10.073270808909731,
      "grad_norm": 0.7141433954238892,
      "learning_rate": 0.0004267744926875516,
      "loss": 0.0887,
      "step": 34370
    },
    {
      "epoch": 10.07620164126612,
      "grad_norm": 1.6471210718154907,
      "learning_rate": 0.00042669069639996327,
      "loss": 0.1027,
      "step": 34380
    },
    {
      "epoch": 10.07913247362251,
      "grad_norm": 0.42946088314056396,
      "learning_rate": 0.0004266069001123749,
      "loss": 0.1006,
      "step": 34390
    },
    {
      "epoch": 10.082063305978899,
      "grad_norm": 1.3317060470581055,
      "learning_rate": 0.00042652310382478655,
      "loss": 0.1069,
      "step": 34400
    },
    {
      "epoch": 10.084994138335286,
      "grad_norm": 1.5109367370605469,
      "learning_rate": 0.00042643930753719816,
      "loss": 0.1039,
      "step": 34410
    },
    {
      "epoch": 10.087924970691676,
      "grad_norm": 0.5136106014251709,
      "learning_rate": 0.0004263555112496099,
      "loss": 0.089,
      "step": 34420
    },
    {
      "epoch": 10.090855803048065,
      "grad_norm": 1.19968843460083,
      "learning_rate": 0.00042627171496202143,
      "loss": 0.0669,
      "step": 34430
    },
    {
      "epoch": 10.093786635404454,
      "grad_norm": 0.6536751985549927,
      "learning_rate": 0.00042618791867443304,
      "loss": 0.0929,
      "step": 34440
    },
    {
      "epoch": 10.096717467760843,
      "grad_norm": 1.8862595558166504,
      "learning_rate": 0.0004261041223868447,
      "loss": 0.0922,
      "step": 34450
    },
    {
      "epoch": 10.099648300117233,
      "grad_norm": 0.34538835287094116,
      "learning_rate": 0.0004260203260992563,
      "loss": 0.0843,
      "step": 34460
    },
    {
      "epoch": 10.102579132473622,
      "grad_norm": 1.32676362991333,
      "learning_rate": 0.00042593652981166803,
      "loss": 0.0646,
      "step": 34470
    },
    {
      "epoch": 10.105509964830011,
      "grad_norm": 0.3796287775039673,
      "learning_rate": 0.0004258527335240796,
      "loss": 0.0516,
      "step": 34480
    },
    {
      "epoch": 10.1084407971864,
      "grad_norm": 1.0229930877685547,
      "learning_rate": 0.0004257689372364912,
      "loss": 0.0765,
      "step": 34490
    },
    {
      "epoch": 10.11137162954279,
      "grad_norm": 0.9195473790168762,
      "learning_rate": 0.0004256851409489029,
      "loss": 0.0898,
      "step": 34500
    },
    {
      "epoch": 10.114302461899179,
      "grad_norm": 1.016685128211975,
      "learning_rate": 0.00042560134466131447,
      "loss": 0.1097,
      "step": 34510
    },
    {
      "epoch": 10.117233294255568,
      "grad_norm": 1.4625844955444336,
      "learning_rate": 0.0004255175483737262,
      "loss": 0.0906,
      "step": 34520
    },
    {
      "epoch": 10.120164126611957,
      "grad_norm": 0.571341872215271,
      "learning_rate": 0.0004254337520861378,
      "loss": 0.1034,
      "step": 34530
    },
    {
      "epoch": 10.123094958968347,
      "grad_norm": 1.599228024482727,
      "learning_rate": 0.00042534995579854935,
      "loss": 0.1198,
      "step": 34540
    },
    {
      "epoch": 10.126025791324736,
      "grad_norm": 0.864144504070282,
      "learning_rate": 0.00042526615951096107,
      "loss": 0.0822,
      "step": 34550
    },
    {
      "epoch": 10.128956623681125,
      "grad_norm": 1.1704083681106567,
      "learning_rate": 0.0004251823632233727,
      "loss": 0.088,
      "step": 34560
    },
    {
      "epoch": 10.131887456037514,
      "grad_norm": 0.9885199666023254,
      "learning_rate": 0.00042509856693578434,
      "loss": 0.0943,
      "step": 34570
    },
    {
      "epoch": 10.134818288393904,
      "grad_norm": 0.8512581586837769,
      "learning_rate": 0.00042501477064819595,
      "loss": 0.1014,
      "step": 34580
    },
    {
      "epoch": 10.137749120750293,
      "grad_norm": 0.81071537733078,
      "learning_rate": 0.0004249309743606076,
      "loss": 0.0582,
      "step": 34590
    },
    {
      "epoch": 10.140679953106682,
      "grad_norm": 1.6591140031814575,
      "learning_rate": 0.00042484717807301923,
      "loss": 0.0869,
      "step": 34600
    },
    {
      "epoch": 10.143610785463071,
      "grad_norm": 0.7023012042045593,
      "learning_rate": 0.00042476338178543084,
      "loss": 0.0896,
      "step": 34610
    },
    {
      "epoch": 10.14654161781946,
      "grad_norm": 0.5548232197761536,
      "learning_rate": 0.0004246795854978425,
      "loss": 0.1114,
      "step": 34620
    },
    {
      "epoch": 10.14947245017585,
      "grad_norm": 0.6225626468658447,
      "learning_rate": 0.0004245957892102541,
      "loss": 0.1033,
      "step": 34630
    },
    {
      "epoch": 10.152403282532239,
      "grad_norm": 0.8296535611152649,
      "learning_rate": 0.0004245119929226658,
      "loss": 0.0971,
      "step": 34640
    },
    {
      "epoch": 10.155334114888628,
      "grad_norm": 0.8152122497558594,
      "learning_rate": 0.0004244281966350774,
      "loss": 0.0983,
      "step": 34650
    },
    {
      "epoch": 10.158264947245017,
      "grad_norm": 1.0326000452041626,
      "learning_rate": 0.000424344400347489,
      "loss": 0.1036,
      "step": 34660
    },
    {
      "epoch": 10.161195779601407,
      "grad_norm": 1.274598479270935,
      "learning_rate": 0.00042426060405990066,
      "loss": 0.0723,
      "step": 34670
    },
    {
      "epoch": 10.164126611957796,
      "grad_norm": 0.8246333003044128,
      "learning_rate": 0.00042417680777231227,
      "loss": 0.0918,
      "step": 34680
    },
    {
      "epoch": 10.167057444314185,
      "grad_norm": 1.7732373476028442,
      "learning_rate": 0.00042409301148472393,
      "loss": 0.1045,
      "step": 34690
    },
    {
      "epoch": 10.169988276670574,
      "grad_norm": 1.2191741466522217,
      "learning_rate": 0.00042400921519713554,
      "loss": 0.1045,
      "step": 34700
    },
    {
      "epoch": 10.172919109026964,
      "grad_norm": 0.9322351217269897,
      "learning_rate": 0.00042392541890954715,
      "loss": 0.1068,
      "step": 34710
    },
    {
      "epoch": 10.175849941383353,
      "grad_norm": 0.6420918107032776,
      "learning_rate": 0.0004238416226219588,
      "loss": 0.0785,
      "step": 34720
    },
    {
      "epoch": 10.178780773739742,
      "grad_norm": 0.848773181438446,
      "learning_rate": 0.0004237578263343704,
      "loss": 0.0696,
      "step": 34730
    },
    {
      "epoch": 10.181711606096131,
      "grad_norm": 1.250444769859314,
      "learning_rate": 0.0004236740300467821,
      "loss": 0.1122,
      "step": 34740
    },
    {
      "epoch": 10.18464243845252,
      "grad_norm": 0.44906049966812134,
      "learning_rate": 0.0004235902337591937,
      "loss": 0.0767,
      "step": 34750
    },
    {
      "epoch": 10.18757327080891,
      "grad_norm": 0.46748945116996765,
      "learning_rate": 0.00042350643747160536,
      "loss": 0.0941,
      "step": 34760
    },
    {
      "epoch": 10.1905041031653,
      "grad_norm": 1.8510551452636719,
      "learning_rate": 0.00042342264118401697,
      "loss": 0.0904,
      "step": 34770
    },
    {
      "epoch": 10.193434935521688,
      "grad_norm": 1.1104243993759155,
      "learning_rate": 0.0004233388448964286,
      "loss": 0.0954,
      "step": 34780
    },
    {
      "epoch": 10.196365767878078,
      "grad_norm": 0.6221134066581726,
      "learning_rate": 0.00042325504860884025,
      "loss": 0.063,
      "step": 34790
    },
    {
      "epoch": 10.199296600234467,
      "grad_norm": 0.17965078353881836,
      "learning_rate": 0.00042317125232125186,
      "loss": 0.0659,
      "step": 34800
    },
    {
      "epoch": 10.202227432590856,
      "grad_norm": 1.158794641494751,
      "learning_rate": 0.0004230874560336635,
      "loss": 0.0894,
      "step": 34810
    },
    {
      "epoch": 10.205158264947245,
      "grad_norm": 1.459837555885315,
      "learning_rate": 0.00042300365974607513,
      "loss": 0.0739,
      "step": 34820
    },
    {
      "epoch": 10.208089097303635,
      "grad_norm": 0.5153830647468567,
      "learning_rate": 0.00042291986345848674,
      "loss": 0.1025,
      "step": 34830
    },
    {
      "epoch": 10.211019929660024,
      "grad_norm": 0.6692390441894531,
      "learning_rate": 0.0004228360671708984,
      "loss": 0.0939,
      "step": 34840
    },
    {
      "epoch": 10.213950762016413,
      "grad_norm": 1.0837972164154053,
      "learning_rate": 0.00042275227088331,
      "loss": 0.0958,
      "step": 34850
    },
    {
      "epoch": 10.216881594372802,
      "grad_norm": 1.4220982789993286,
      "learning_rate": 0.00042266847459572173,
      "loss": 0.0893,
      "step": 34860
    },
    {
      "epoch": 10.219812426729192,
      "grad_norm": 1.3432635068893433,
      "learning_rate": 0.0004225846783081333,
      "loss": 0.0709,
      "step": 34870
    },
    {
      "epoch": 10.222743259085581,
      "grad_norm": 0.7646772265434265,
      "learning_rate": 0.0004225008820205449,
      "loss": 0.0861,
      "step": 34880
    },
    {
      "epoch": 10.22567409144197,
      "grad_norm": 0.8724179267883301,
      "learning_rate": 0.0004224170857329566,
      "loss": 0.1088,
      "step": 34890
    },
    {
      "epoch": 10.22860492379836,
      "grad_norm": 1.1389755010604858,
      "learning_rate": 0.00042233328944536817,
      "loss": 0.1027,
      "step": 34900
    },
    {
      "epoch": 10.231535756154749,
      "grad_norm": 0.8553501963615417,
      "learning_rate": 0.0004222494931577799,
      "loss": 0.1,
      "step": 34910
    },
    {
      "epoch": 10.234466588511138,
      "grad_norm": 0.712742805480957,
      "learning_rate": 0.00042216569687019144,
      "loss": 0.0897,
      "step": 34920
    },
    {
      "epoch": 10.237397420867527,
      "grad_norm": 0.889801025390625,
      "learning_rate": 0.00042208190058260316,
      "loss": 0.088,
      "step": 34930
    },
    {
      "epoch": 10.240328253223916,
      "grad_norm": 0.5989472270011902,
      "learning_rate": 0.00042199810429501477,
      "loss": 0.086,
      "step": 34940
    },
    {
      "epoch": 10.243259085580306,
      "grad_norm": 1.2341302633285522,
      "learning_rate": 0.0004219143080074263,
      "loss": 0.083,
      "step": 34950
    },
    {
      "epoch": 10.246189917936695,
      "grad_norm": 1.4379451274871826,
      "learning_rate": 0.00042183051171983804,
      "loss": 0.1013,
      "step": 34960
    },
    {
      "epoch": 10.249120750293084,
      "grad_norm": 0.9489368200302124,
      "learning_rate": 0.00042174671543224965,
      "loss": 0.1002,
      "step": 34970
    },
    {
      "epoch": 10.252051582649472,
      "grad_norm": 0.7646116614341736,
      "learning_rate": 0.0004216629191446613,
      "loss": 0.0771,
      "step": 34980
    },
    {
      "epoch": 10.25498241500586,
      "grad_norm": 0.47426310181617737,
      "learning_rate": 0.00042157912285707293,
      "loss": 0.0732,
      "step": 34990
    },
    {
      "epoch": 10.25791324736225,
      "grad_norm": 0.6198390126228333,
      "learning_rate": 0.00042149532656948454,
      "loss": 0.1014,
      "step": 35000
    },
    {
      "epoch": 10.26084407971864,
      "grad_norm": 1.1281787157058716,
      "learning_rate": 0.0004214115302818962,
      "loss": 0.0824,
      "step": 35010
    },
    {
      "epoch": 10.263774912075029,
      "grad_norm": 1.127615213394165,
      "learning_rate": 0.0004213277339943078,
      "loss": 0.0888,
      "step": 35020
    },
    {
      "epoch": 10.266705744431418,
      "grad_norm": 0.8672420978546143,
      "learning_rate": 0.0004212439377067195,
      "loss": 0.1005,
      "step": 35030
    },
    {
      "epoch": 10.269636576787807,
      "grad_norm": 0.8538544774055481,
      "learning_rate": 0.0004211601414191311,
      "loss": 0.0835,
      "step": 35040
    },
    {
      "epoch": 10.272567409144196,
      "grad_norm": 0.9700151681900024,
      "learning_rate": 0.0004210763451315427,
      "loss": 0.0981,
      "step": 35050
    },
    {
      "epoch": 10.275498241500586,
      "grad_norm": 1.0989431142807007,
      "learning_rate": 0.00042099254884395436,
      "loss": 0.0827,
      "step": 35060
    },
    {
      "epoch": 10.278429073856975,
      "grad_norm": 0.39460599422454834,
      "learning_rate": 0.00042090875255636597,
      "loss": 0.0945,
      "step": 35070
    },
    {
      "epoch": 10.281359906213364,
      "grad_norm": 0.7503011226654053,
      "learning_rate": 0.00042082495626877763,
      "loss": 0.0958,
      "step": 35080
    },
    {
      "epoch": 10.284290738569753,
      "grad_norm": 1.4744914770126343,
      "learning_rate": 0.00042074115998118924,
      "loss": 0.0924,
      "step": 35090
    },
    {
      "epoch": 10.287221570926143,
      "grad_norm": 1.2587685585021973,
      "learning_rate": 0.0004206573636936009,
      "loss": 0.1007,
      "step": 35100
    },
    {
      "epoch": 10.290152403282532,
      "grad_norm": 1.32193922996521,
      "learning_rate": 0.0004205735674060125,
      "loss": 0.0691,
      "step": 35110
    },
    {
      "epoch": 10.293083235638921,
      "grad_norm": 2.3930022716522217,
      "learning_rate": 0.0004204897711184241,
      "loss": 0.1126,
      "step": 35120
    },
    {
      "epoch": 10.29601406799531,
      "grad_norm": 1.060963749885559,
      "learning_rate": 0.0004204059748308358,
      "loss": 0.0774,
      "step": 35130
    },
    {
      "epoch": 10.2989449003517,
      "grad_norm": 1.0531036853790283,
      "learning_rate": 0.0004203221785432474,
      "loss": 0.095,
      "step": 35140
    },
    {
      "epoch": 10.301875732708089,
      "grad_norm": 0.6471096873283386,
      "learning_rate": 0.00042023838225565906,
      "loss": 0.0941,
      "step": 35150
    },
    {
      "epoch": 10.304806565064478,
      "grad_norm": 1.0216411352157593,
      "learning_rate": 0.00042015458596807067,
      "loss": 0.0974,
      "step": 35160
    },
    {
      "epoch": 10.307737397420867,
      "grad_norm": 1.1072674989700317,
      "learning_rate": 0.0004200707896804823,
      "loss": 0.0658,
      "step": 35170
    },
    {
      "epoch": 10.310668229777256,
      "grad_norm": 0.7067291140556335,
      "learning_rate": 0.00041998699339289395,
      "loss": 0.0812,
      "step": 35180
    },
    {
      "epoch": 10.313599062133646,
      "grad_norm": 1.0650514364242554,
      "learning_rate": 0.00041990319710530556,
      "loss": 0.0921,
      "step": 35190
    },
    {
      "epoch": 10.316529894490035,
      "grad_norm": 1.091317057609558,
      "learning_rate": 0.0004198194008177172,
      "loss": 0.0811,
      "step": 35200
    },
    {
      "epoch": 10.319460726846424,
      "grad_norm": 0.8096703290939331,
      "learning_rate": 0.00041973560453012883,
      "loss": 0.1088,
      "step": 35210
    },
    {
      "epoch": 10.322391559202813,
      "grad_norm": 1.0080657005310059,
      "learning_rate": 0.00041965180824254044,
      "loss": 0.0784,
      "step": 35220
    },
    {
      "epoch": 10.325322391559203,
      "grad_norm": 0.47115984559059143,
      "learning_rate": 0.0004195680119549521,
      "loss": 0.0701,
      "step": 35230
    },
    {
      "epoch": 10.328253223915592,
      "grad_norm": 1.2829546928405762,
      "learning_rate": 0.0004194842156673637,
      "loss": 0.0976,
      "step": 35240
    },
    {
      "epoch": 10.331184056271981,
      "grad_norm": 0.5929310321807861,
      "learning_rate": 0.0004194004193797754,
      "loss": 0.0742,
      "step": 35250
    },
    {
      "epoch": 10.33411488862837,
      "grad_norm": 0.6449202299118042,
      "learning_rate": 0.000419316623092187,
      "loss": 0.1088,
      "step": 35260
    },
    {
      "epoch": 10.33704572098476,
      "grad_norm": 1.8106036186218262,
      "learning_rate": 0.0004192328268045986,
      "loss": 0.0915,
      "step": 35270
    },
    {
      "epoch": 10.339976553341149,
      "grad_norm": 0.49307748675346375,
      "learning_rate": 0.00041914903051701026,
      "loss": 0.0863,
      "step": 35280
    },
    {
      "epoch": 10.342907385697538,
      "grad_norm": 0.6608580946922302,
      "learning_rate": 0.00041906523422942187,
      "loss": 0.0778,
      "step": 35290
    },
    {
      "epoch": 10.345838218053927,
      "grad_norm": 1.178433895111084,
      "learning_rate": 0.0004189814379418336,
      "loss": 0.103,
      "step": 35300
    },
    {
      "epoch": 10.348769050410317,
      "grad_norm": 1.0651692152023315,
      "learning_rate": 0.00041889764165424514,
      "loss": 0.0928,
      "step": 35310
    },
    {
      "epoch": 10.351699882766706,
      "grad_norm": 1.0164375305175781,
      "learning_rate": 0.00041881384536665686,
      "loss": 0.0703,
      "step": 35320
    },
    {
      "epoch": 10.354630715123095,
      "grad_norm": 1.579698085784912,
      "learning_rate": 0.00041873004907906847,
      "loss": 0.0882,
      "step": 35330
    },
    {
      "epoch": 10.357561547479484,
      "grad_norm": 0.8972679972648621,
      "learning_rate": 0.00041864625279148,
      "loss": 0.0754,
      "step": 35340
    },
    {
      "epoch": 10.360492379835874,
      "grad_norm": 0.669764518737793,
      "learning_rate": 0.00041856245650389174,
      "loss": 0.0832,
      "step": 35350
    },
    {
      "epoch": 10.363423212192263,
      "grad_norm": 1.3611340522766113,
      "learning_rate": 0.0004184786602163033,
      "loss": 0.0971,
      "step": 35360
    },
    {
      "epoch": 10.366354044548652,
      "grad_norm": 0.9332679510116577,
      "learning_rate": 0.000418394863928715,
      "loss": 0.0779,
      "step": 35370
    },
    {
      "epoch": 10.369284876905041,
      "grad_norm": 1.7461713552474976,
      "learning_rate": 0.00041831106764112663,
      "loss": 0.1067,
      "step": 35380
    },
    {
      "epoch": 10.37221570926143,
      "grad_norm": 0.4143581688404083,
      "learning_rate": 0.0004182272713535382,
      "loss": 0.078,
      "step": 35390
    },
    {
      "epoch": 10.37514654161782,
      "grad_norm": 1.1294044256210327,
      "learning_rate": 0.0004181434750659499,
      "loss": 0.0637,
      "step": 35400
    },
    {
      "epoch": 10.37807737397421,
      "grad_norm": 0.675704300403595,
      "learning_rate": 0.0004180596787783615,
      "loss": 0.0769,
      "step": 35410
    },
    {
      "epoch": 10.381008206330598,
      "grad_norm": 1.0016971826553345,
      "learning_rate": 0.0004179758824907732,
      "loss": 0.1145,
      "step": 35420
    },
    {
      "epoch": 10.383939038686988,
      "grad_norm": 0.8109431266784668,
      "learning_rate": 0.0004178920862031848,
      "loss": 0.1013,
      "step": 35430
    },
    {
      "epoch": 10.386869871043377,
      "grad_norm": 0.8712977766990662,
      "learning_rate": 0.0004178082899155964,
      "loss": 0.0972,
      "step": 35440
    },
    {
      "epoch": 10.389800703399766,
      "grad_norm": 0.5442768335342407,
      "learning_rate": 0.00041772449362800806,
      "loss": 0.086,
      "step": 35450
    },
    {
      "epoch": 10.392731535756155,
      "grad_norm": 0.4197365939617157,
      "learning_rate": 0.00041764069734041967,
      "loss": 0.0952,
      "step": 35460
    },
    {
      "epoch": 10.395662368112545,
      "grad_norm": 2.0791642665863037,
      "learning_rate": 0.00041755690105283133,
      "loss": 0.0707,
      "step": 35470
    },
    {
      "epoch": 10.398593200468934,
      "grad_norm": 0.40149927139282227,
      "learning_rate": 0.00041747310476524294,
      "loss": 0.0805,
      "step": 35480
    },
    {
      "epoch": 10.401524032825323,
      "grad_norm": 1.5070966482162476,
      "learning_rate": 0.0004173893084776546,
      "loss": 0.1615,
      "step": 35490
    },
    {
      "epoch": 10.404454865181712,
      "grad_norm": 0.957841157913208,
      "learning_rate": 0.0004173055121900662,
      "loss": 0.0651,
      "step": 35500
    },
    {
      "epoch": 10.407385697538102,
      "grad_norm": 0.660956859588623,
      "learning_rate": 0.0004172217159024778,
      "loss": 0.0562,
      "step": 35510
    },
    {
      "epoch": 10.41031652989449,
      "grad_norm": 0.45514294505119324,
      "learning_rate": 0.0004171379196148895,
      "loss": 0.0943,
      "step": 35520
    },
    {
      "epoch": 10.41324736225088,
      "grad_norm": 0.7084634304046631,
      "learning_rate": 0.0004170541233273011,
      "loss": 0.0961,
      "step": 35530
    },
    {
      "epoch": 10.41617819460727,
      "grad_norm": 1.613438367843628,
      "learning_rate": 0.00041697032703971276,
      "loss": 0.1128,
      "step": 35540
    },
    {
      "epoch": 10.419109026963657,
      "grad_norm": 1.2496119737625122,
      "learning_rate": 0.00041688653075212437,
      "loss": 0.0959,
      "step": 35550
    },
    {
      "epoch": 10.422039859320046,
      "grad_norm": 0.8991985321044922,
      "learning_rate": 0.000416802734464536,
      "loss": 0.0719,
      "step": 35560
    },
    {
      "epoch": 10.424970691676435,
      "grad_norm": 0.47696453332901,
      "learning_rate": 0.00041671893817694765,
      "loss": 0.0833,
      "step": 35570
    },
    {
      "epoch": 10.427901524032825,
      "grad_norm": 1.2405544519424438,
      "learning_rate": 0.00041663514188935926,
      "loss": 0.0723,
      "step": 35580
    },
    {
      "epoch": 10.430832356389214,
      "grad_norm": 1.5643211603164673,
      "learning_rate": 0.0004165513456017709,
      "loss": 0.1021,
      "step": 35590
    },
    {
      "epoch": 10.433763188745603,
      "grad_norm": 0.7130193114280701,
      "learning_rate": 0.00041646754931418253,
      "loss": 0.0882,
      "step": 35600
    },
    {
      "epoch": 10.436694021101992,
      "grad_norm": 1.0732165575027466,
      "learning_rate": 0.00041638375302659414,
      "loss": 0.0896,
      "step": 35610
    },
    {
      "epoch": 10.439624853458382,
      "grad_norm": 0.5990287065505981,
      "learning_rate": 0.0004162999567390058,
      "loss": 0.0754,
      "step": 35620
    },
    {
      "epoch": 10.44255568581477,
      "grad_norm": 0.9058422446250916,
      "learning_rate": 0.0004162161604514174,
      "loss": 0.1017,
      "step": 35630
    },
    {
      "epoch": 10.44548651817116,
      "grad_norm": 1.7293415069580078,
      "learning_rate": 0.0004161323641638291,
      "loss": 0.0966,
      "step": 35640
    },
    {
      "epoch": 10.44841735052755,
      "grad_norm": 0.8984959125518799,
      "learning_rate": 0.0004160485678762407,
      "loss": 0.0841,
      "step": 35650
    },
    {
      "epoch": 10.451348182883939,
      "grad_norm": 0.28613582253456116,
      "learning_rate": 0.00041596477158865235,
      "loss": 0.0899,
      "step": 35660
    },
    {
      "epoch": 10.454279015240328,
      "grad_norm": 0.7867265343666077,
      "learning_rate": 0.00041588097530106396,
      "loss": 0.0615,
      "step": 35670
    },
    {
      "epoch": 10.457209847596717,
      "grad_norm": 1.3250136375427246,
      "learning_rate": 0.00041579717901347557,
      "loss": 0.0784,
      "step": 35680
    },
    {
      "epoch": 10.460140679953106,
      "grad_norm": 1.3546242713928223,
      "learning_rate": 0.00041571338272588723,
      "loss": 0.0885,
      "step": 35690
    },
    {
      "epoch": 10.463071512309496,
      "grad_norm": 1.0846140384674072,
      "learning_rate": 0.00041562958643829884,
      "loss": 0.107,
      "step": 35700
    },
    {
      "epoch": 10.466002344665885,
      "grad_norm": 0.5524006485939026,
      "learning_rate": 0.00041554579015071056,
      "loss": 0.0819,
      "step": 35710
    },
    {
      "epoch": 10.468933177022274,
      "grad_norm": 0.664612889289856,
      "learning_rate": 0.0004154619938631221,
      "loss": 0.0727,
      "step": 35720
    },
    {
      "epoch": 10.471864009378663,
      "grad_norm": 1.1087093353271484,
      "learning_rate": 0.0004153781975755337,
      "loss": 0.0815,
      "step": 35730
    },
    {
      "epoch": 10.474794841735052,
      "grad_norm": 1.387723684310913,
      "learning_rate": 0.00041529440128794544,
      "loss": 0.1134,
      "step": 35740
    },
    {
      "epoch": 10.477725674091442,
      "grad_norm": 1.1554334163665771,
      "learning_rate": 0.000415210605000357,
      "loss": 0.0738,
      "step": 35750
    },
    {
      "epoch": 10.480656506447831,
      "grad_norm": 1.0744949579238892,
      "learning_rate": 0.0004151268087127687,
      "loss": 0.0882,
      "step": 35760
    },
    {
      "epoch": 10.48358733880422,
      "grad_norm": 0.7312902808189392,
      "learning_rate": 0.00041504301242518033,
      "loss": 0.0877,
      "step": 35770
    },
    {
      "epoch": 10.48651817116061,
      "grad_norm": 1.0228841304779053,
      "learning_rate": 0.0004149592161375919,
      "loss": 0.09,
      "step": 35780
    },
    {
      "epoch": 10.489449003516999,
      "grad_norm": 0.9647163152694702,
      "learning_rate": 0.0004148754198500036,
      "loss": 0.07,
      "step": 35790
    },
    {
      "epoch": 10.492379835873388,
      "grad_norm": 1.6099728345870972,
      "learning_rate": 0.00041479162356241516,
      "loss": 0.088,
      "step": 35800
    },
    {
      "epoch": 10.495310668229777,
      "grad_norm": 2.114096164703369,
      "learning_rate": 0.0004147078272748269,
      "loss": 0.0896,
      "step": 35810
    },
    {
      "epoch": 10.498241500586166,
      "grad_norm": 0.7434582114219666,
      "learning_rate": 0.0004146240309872385,
      "loss": 0.064,
      "step": 35820
    },
    {
      "epoch": 10.501172332942556,
      "grad_norm": 1.1744767427444458,
      "learning_rate": 0.00041454023469965015,
      "loss": 0.1011,
      "step": 35830
    },
    {
      "epoch": 10.504103165298945,
      "grad_norm": 0.6181514263153076,
      "learning_rate": 0.00041445643841206176,
      "loss": 0.0954,
      "step": 35840
    },
    {
      "epoch": 10.507033997655334,
      "grad_norm": 0.7605583667755127,
      "learning_rate": 0.00041437264212447337,
      "loss": 0.097,
      "step": 35850
    },
    {
      "epoch": 10.509964830011723,
      "grad_norm": 0.7388057708740234,
      "learning_rate": 0.00041428884583688503,
      "loss": 0.0858,
      "step": 35860
    },
    {
      "epoch": 10.512895662368113,
      "grad_norm": 0.6212084293365479,
      "learning_rate": 0.00041420504954929664,
      "loss": 0.1089,
      "step": 35870
    },
    {
      "epoch": 10.515826494724502,
      "grad_norm": 0.4099201261997223,
      "learning_rate": 0.0004141212532617083,
      "loss": 0.0613,
      "step": 35880
    },
    {
      "epoch": 10.518757327080891,
      "grad_norm": 0.7192084193229675,
      "learning_rate": 0.0004140374569741199,
      "loss": 0.0936,
      "step": 35890
    },
    {
      "epoch": 10.52168815943728,
      "grad_norm": 0.4416690170764923,
      "learning_rate": 0.0004139536606865315,
      "loss": 0.0753,
      "step": 35900
    },
    {
      "epoch": 10.52461899179367,
      "grad_norm": 1.285847783088684,
      "learning_rate": 0.0004138698643989432,
      "loss": 0.0858,
      "step": 35910
    },
    {
      "epoch": 10.527549824150059,
      "grad_norm": 0.8708050847053528,
      "learning_rate": 0.0004137860681113548,
      "loss": 0.0941,
      "step": 35920
    },
    {
      "epoch": 10.530480656506448,
      "grad_norm": 0.7883150577545166,
      "learning_rate": 0.00041370227182376646,
      "loss": 0.071,
      "step": 35930
    },
    {
      "epoch": 10.533411488862837,
      "grad_norm": 0.9645453095436096,
      "learning_rate": 0.00041361847553617807,
      "loss": 0.0936,
      "step": 35940
    },
    {
      "epoch": 10.536342321219227,
      "grad_norm": 0.5760372281074524,
      "learning_rate": 0.0004135346792485897,
      "loss": 0.0758,
      "step": 35950
    },
    {
      "epoch": 10.539273153575616,
      "grad_norm": 0.7608450055122375,
      "learning_rate": 0.00041345088296100135,
      "loss": 0.0803,
      "step": 35960
    },
    {
      "epoch": 10.542203985932005,
      "grad_norm": 0.29727301001548767,
      "learning_rate": 0.00041336708667341296,
      "loss": 0.0744,
      "step": 35970
    },
    {
      "epoch": 10.545134818288394,
      "grad_norm": 1.1823773384094238,
      "learning_rate": 0.0004132832903858246,
      "loss": 0.0845,
      "step": 35980
    },
    {
      "epoch": 10.548065650644784,
      "grad_norm": 0.3265252709388733,
      "learning_rate": 0.00041319949409823623,
      "loss": 0.0946,
      "step": 35990
    },
    {
      "epoch": 10.550996483001173,
      "grad_norm": 0.8758514523506165,
      "learning_rate": 0.0004131156978106479,
      "loss": 0.0818,
      "step": 36000
    },
    {
      "epoch": 10.553927315357562,
      "grad_norm": 1.1159356832504272,
      "learning_rate": 0.0004130319015230595,
      "loss": 0.0903,
      "step": 36010
    },
    {
      "epoch": 10.556858147713951,
      "grad_norm": 0.5037102699279785,
      "learning_rate": 0.0004129481052354711,
      "loss": 0.0896,
      "step": 36020
    },
    {
      "epoch": 10.55978898007034,
      "grad_norm": 1.8641963005065918,
      "learning_rate": 0.0004128643089478828,
      "loss": 0.0791,
      "step": 36030
    },
    {
      "epoch": 10.56271981242673,
      "grad_norm": 0.9659538865089417,
      "learning_rate": 0.0004127805126602944,
      "loss": 0.1041,
      "step": 36040
    },
    {
      "epoch": 10.565650644783119,
      "grad_norm": 0.6119762659072876,
      "learning_rate": 0.00041269671637270605,
      "loss": 0.0923,
      "step": 36050
    },
    {
      "epoch": 10.568581477139508,
      "grad_norm": 0.7835899591445923,
      "learning_rate": 0.00041261292008511766,
      "loss": 0.0935,
      "step": 36060
    },
    {
      "epoch": 10.571512309495898,
      "grad_norm": 1.034900426864624,
      "learning_rate": 0.00041252912379752927,
      "loss": 0.0985,
      "step": 36070
    },
    {
      "epoch": 10.574443141852287,
      "grad_norm": 0.9130374789237976,
      "learning_rate": 0.00041244532750994093,
      "loss": 0.0901,
      "step": 36080
    },
    {
      "epoch": 10.577373974208676,
      "grad_norm": 1.0964751243591309,
      "learning_rate": 0.00041236153122235254,
      "loss": 0.0971,
      "step": 36090
    },
    {
      "epoch": 10.580304806565064,
      "grad_norm": 0.6575194001197815,
      "learning_rate": 0.00041227773493476426,
      "loss": 0.0921,
      "step": 36100
    },
    {
      "epoch": 10.583235638921455,
      "grad_norm": 1.1046407222747803,
      "learning_rate": 0.0004121939386471758,
      "loss": 0.1163,
      "step": 36110
    },
    {
      "epoch": 10.586166471277842,
      "grad_norm": 1.51856529712677,
      "learning_rate": 0.0004121101423595874,
      "loss": 0.1012,
      "step": 36120
    },
    {
      "epoch": 10.589097303634231,
      "grad_norm": 0.8839870095252991,
      "learning_rate": 0.0004120263460719991,
      "loss": 0.1162,
      "step": 36130
    },
    {
      "epoch": 10.59202813599062,
      "grad_norm": 0.9068971872329712,
      "learning_rate": 0.0004119425497844107,
      "loss": 0.1002,
      "step": 36140
    },
    {
      "epoch": 10.59495896834701,
      "grad_norm": 1.0382617712020874,
      "learning_rate": 0.0004118587534968224,
      "loss": 0.0832,
      "step": 36150
    },
    {
      "epoch": 10.597889800703399,
      "grad_norm": 1.1009701490402222,
      "learning_rate": 0.000411774957209234,
      "loss": 0.0853,
      "step": 36160
    },
    {
      "epoch": 10.600820633059788,
      "grad_norm": 1.268627643585205,
      "learning_rate": 0.0004116911609216457,
      "loss": 0.0798,
      "step": 36170
    },
    {
      "epoch": 10.603751465416178,
      "grad_norm": 0.27602899074554443,
      "learning_rate": 0.0004116073646340573,
      "loss": 0.0799,
      "step": 36180
    },
    {
      "epoch": 10.606682297772567,
      "grad_norm": 0.9228836894035339,
      "learning_rate": 0.00041152356834646886,
      "loss": 0.067,
      "step": 36190
    },
    {
      "epoch": 10.609613130128956,
      "grad_norm": 0.43510204553604126,
      "learning_rate": 0.0004114397720588806,
      "loss": 0.0858,
      "step": 36200
    },
    {
      "epoch": 10.612543962485345,
      "grad_norm": 0.8145872354507446,
      "learning_rate": 0.0004113559757712922,
      "loss": 0.0913,
      "step": 36210
    },
    {
      "epoch": 10.615474794841735,
      "grad_norm": 0.51374751329422,
      "learning_rate": 0.00041127217948370385,
      "loss": 0.0974,
      "step": 36220
    },
    {
      "epoch": 10.618405627198124,
      "grad_norm": 0.8879350423812866,
      "learning_rate": 0.00041118838319611546,
      "loss": 0.079,
      "step": 36230
    },
    {
      "epoch": 10.621336459554513,
      "grad_norm": 0.8868109583854675,
      "learning_rate": 0.000411104586908527,
      "loss": 0.118,
      "step": 36240
    },
    {
      "epoch": 10.624267291910902,
      "grad_norm": 2.200598955154419,
      "learning_rate": 0.00041102079062093873,
      "loss": 0.1166,
      "step": 36250
    },
    {
      "epoch": 10.627198124267291,
      "grad_norm": 1.3496342897415161,
      "learning_rate": 0.00041093699433335034,
      "loss": 0.0846,
      "step": 36260
    },
    {
      "epoch": 10.63012895662368,
      "grad_norm": 0.5926579236984253,
      "learning_rate": 0.000410853198045762,
      "loss": 0.0961,
      "step": 36270
    },
    {
      "epoch": 10.63305978898007,
      "grad_norm": 0.7106102108955383,
      "learning_rate": 0.0004107694017581736,
      "loss": 0.1081,
      "step": 36280
    },
    {
      "epoch": 10.63599062133646,
      "grad_norm": 0.9088765382766724,
      "learning_rate": 0.0004106856054705852,
      "loss": 0.0861,
      "step": 36290
    },
    {
      "epoch": 10.638921453692848,
      "grad_norm": 1.0383998155593872,
      "learning_rate": 0.0004106018091829969,
      "loss": 0.095,
      "step": 36300
    },
    {
      "epoch": 10.641852286049238,
      "grad_norm": 1.4848076105117798,
      "learning_rate": 0.0004105180128954085,
      "loss": 0.1089,
      "step": 36310
    },
    {
      "epoch": 10.644783118405627,
      "grad_norm": 0.6538270711898804,
      "learning_rate": 0.00041043421660782016,
      "loss": 0.0909,
      "step": 36320
    },
    {
      "epoch": 10.647713950762016,
      "grad_norm": 1.0621070861816406,
      "learning_rate": 0.00041035042032023177,
      "loss": 0.0767,
      "step": 36330
    },
    {
      "epoch": 10.650644783118405,
      "grad_norm": 0.7328243851661682,
      "learning_rate": 0.0004102666240326434,
      "loss": 0.1114,
      "step": 36340
    },
    {
      "epoch": 10.653575615474795,
      "grad_norm": 1.0125547647476196,
      "learning_rate": 0.00041018282774505505,
      "loss": 0.0952,
      "step": 36350
    },
    {
      "epoch": 10.656506447831184,
      "grad_norm": 0.7479094862937927,
      "learning_rate": 0.00041009903145746665,
      "loss": 0.0957,
      "step": 36360
    },
    {
      "epoch": 10.659437280187573,
      "grad_norm": 1.8398300409317017,
      "learning_rate": 0.0004100152351698783,
      "loss": 0.0802,
      "step": 36370
    },
    {
      "epoch": 10.662368112543962,
      "grad_norm": 0.5874427556991577,
      "learning_rate": 0.00040993143888228993,
      "loss": 0.0625,
      "step": 36380
    },
    {
      "epoch": 10.665298944900352,
      "grad_norm": 1.1305687427520752,
      "learning_rate": 0.0004098476425947016,
      "loss": 0.0902,
      "step": 36390
    },
    {
      "epoch": 10.668229777256741,
      "grad_norm": 1.1979399919509888,
      "learning_rate": 0.0004097638463071132,
      "loss": 0.1107,
      "step": 36400
    },
    {
      "epoch": 10.67116060961313,
      "grad_norm": 1.0664055347442627,
      "learning_rate": 0.0004096800500195248,
      "loss": 0.0716,
      "step": 36410
    },
    {
      "epoch": 10.67409144196952,
      "grad_norm": 1.6475666761398315,
      "learning_rate": 0.0004095962537319365,
      "loss": 0.0805,
      "step": 36420
    },
    {
      "epoch": 10.677022274325909,
      "grad_norm": 1.1274815797805786,
      "learning_rate": 0.0004095124574443481,
      "loss": 0.0988,
      "step": 36430
    },
    {
      "epoch": 10.679953106682298,
      "grad_norm": 0.6117716431617737,
      "learning_rate": 0.00040942866115675975,
      "loss": 0.0809,
      "step": 36440
    },
    {
      "epoch": 10.682883939038687,
      "grad_norm": 0.48963263630867004,
      "learning_rate": 0.00040934486486917136,
      "loss": 0.0928,
      "step": 36450
    },
    {
      "epoch": 10.685814771395076,
      "grad_norm": 2.3630053997039795,
      "learning_rate": 0.00040926106858158297,
      "loss": 0.1133,
      "step": 36460
    },
    {
      "epoch": 10.688745603751466,
      "grad_norm": 1.1471242904663086,
      "learning_rate": 0.00040917727229399463,
      "loss": 0.0757,
      "step": 36470
    },
    {
      "epoch": 10.691676436107855,
      "grad_norm": 1.159365177154541,
      "learning_rate": 0.00040909347600640624,
      "loss": 0.0794,
      "step": 36480
    },
    {
      "epoch": 10.694607268464244,
      "grad_norm": 1.162336826324463,
      "learning_rate": 0.0004090096797188179,
      "loss": 0.0666,
      "step": 36490
    },
    {
      "epoch": 10.697538100820633,
      "grad_norm": 0.7089667916297913,
      "learning_rate": 0.0004089258834312295,
      "loss": 0.0885,
      "step": 36500
    },
    {
      "epoch": 10.700468933177023,
      "grad_norm": 0.8885058760643005,
      "learning_rate": 0.0004088420871436411,
      "loss": 0.0805,
      "step": 36510
    },
    {
      "epoch": 10.703399765533412,
      "grad_norm": 0.8135992884635925,
      "learning_rate": 0.0004087582908560528,
      "loss": 0.0865,
      "step": 36520
    },
    {
      "epoch": 10.706330597889801,
      "grad_norm": 1.3754843473434448,
      "learning_rate": 0.0004086744945684644,
      "loss": 0.0811,
      "step": 36530
    },
    {
      "epoch": 10.70926143024619,
      "grad_norm": 0.6114188432693481,
      "learning_rate": 0.0004085906982808761,
      "loss": 0.0759,
      "step": 36540
    },
    {
      "epoch": 10.71219226260258,
      "grad_norm": 0.8637414574623108,
      "learning_rate": 0.0004085069019932877,
      "loss": 0.0738,
      "step": 36550
    },
    {
      "epoch": 10.715123094958969,
      "grad_norm": 1.3523585796356201,
      "learning_rate": 0.0004084231057056994,
      "loss": 0.0602,
      "step": 36560
    },
    {
      "epoch": 10.718053927315358,
      "grad_norm": 1.167863130569458,
      "learning_rate": 0.00040833930941811095,
      "loss": 0.0901,
      "step": 36570
    },
    {
      "epoch": 10.720984759671747,
      "grad_norm": 0.4859054684638977,
      "learning_rate": 0.00040825551313052256,
      "loss": 0.0654,
      "step": 36580
    },
    {
      "epoch": 10.723915592028137,
      "grad_norm": 0.8424491882324219,
      "learning_rate": 0.0004081717168429343,
      "loss": 0.0928,
      "step": 36590
    },
    {
      "epoch": 10.726846424384526,
      "grad_norm": 1.4765362739562988,
      "learning_rate": 0.00040808792055534583,
      "loss": 0.1115,
      "step": 36600
    },
    {
      "epoch": 10.729777256740915,
      "grad_norm": 1.2143596410751343,
      "learning_rate": 0.00040800412426775755,
      "loss": 0.1108,
      "step": 36610
    },
    {
      "epoch": 10.732708089097304,
      "grad_norm": 0.6454131007194519,
      "learning_rate": 0.00040792032798016916,
      "loss": 0.085,
      "step": 36620
    },
    {
      "epoch": 10.735638921453694,
      "grad_norm": 0.614264965057373,
      "learning_rate": 0.0004078365316925807,
      "loss": 0.073,
      "step": 36630
    },
    {
      "epoch": 10.738569753810083,
      "grad_norm": 0.8564926981925964,
      "learning_rate": 0.00040775273540499243,
      "loss": 0.0703,
      "step": 36640
    },
    {
      "epoch": 10.741500586166472,
      "grad_norm": 0.6670345067977905,
      "learning_rate": 0.00040766893911740404,
      "loss": 0.085,
      "step": 36650
    },
    {
      "epoch": 10.744431418522861,
      "grad_norm": 0.7676089406013489,
      "learning_rate": 0.0004075851428298157,
      "loss": 0.0622,
      "step": 36660
    },
    {
      "epoch": 10.747362250879249,
      "grad_norm": 0.40195968747138977,
      "learning_rate": 0.0004075013465422273,
      "loss": 0.0859,
      "step": 36670
    },
    {
      "epoch": 10.75029308323564,
      "grad_norm": 1.5634326934814453,
      "learning_rate": 0.00040741755025463887,
      "loss": 0.0959,
      "step": 36680
    },
    {
      "epoch": 10.753223915592027,
      "grad_norm": 0.737164318561554,
      "learning_rate": 0.0004073337539670506,
      "loss": 0.1034,
      "step": 36690
    },
    {
      "epoch": 10.756154747948417,
      "grad_norm": 1.0312721729278564,
      "learning_rate": 0.0004072499576794622,
      "loss": 0.084,
      "step": 36700
    },
    {
      "epoch": 10.759085580304806,
      "grad_norm": 0.766668975353241,
      "learning_rate": 0.00040716616139187386,
      "loss": 0.0801,
      "step": 36710
    },
    {
      "epoch": 10.762016412661195,
      "grad_norm": 1.6702494621276855,
      "learning_rate": 0.00040708236510428547,
      "loss": 0.0741,
      "step": 36720
    },
    {
      "epoch": 10.764947245017584,
      "grad_norm": 0.8266003131866455,
      "learning_rate": 0.00040699856881669714,
      "loss": 0.0751,
      "step": 36730
    },
    {
      "epoch": 10.767878077373974,
      "grad_norm": 0.8690063953399658,
      "learning_rate": 0.00040691477252910875,
      "loss": 0.0852,
      "step": 36740
    },
    {
      "epoch": 10.770808909730363,
      "grad_norm": 1.8368347883224487,
      "learning_rate": 0.00040683097624152035,
      "loss": 0.0683,
      "step": 36750
    },
    {
      "epoch": 10.773739742086752,
      "grad_norm": 0.9474603533744812,
      "learning_rate": 0.000406747179953932,
      "loss": 0.0875,
      "step": 36760
    },
    {
      "epoch": 10.776670574443141,
      "grad_norm": 0.6713404059410095,
      "learning_rate": 0.00040666338366634363,
      "loss": 0.0838,
      "step": 36770
    },
    {
      "epoch": 10.77960140679953,
      "grad_norm": 0.49258193373680115,
      "learning_rate": 0.0004065795873787553,
      "loss": 0.0887,
      "step": 36780
    },
    {
      "epoch": 10.78253223915592,
      "grad_norm": 0.5072882771492004,
      "learning_rate": 0.0004064957910911669,
      "loss": 0.0771,
      "step": 36790
    },
    {
      "epoch": 10.785463071512309,
      "grad_norm": 0.9199073910713196,
      "learning_rate": 0.0004064119948035785,
      "loss": 0.0997,
      "step": 36800
    },
    {
      "epoch": 10.788393903868698,
      "grad_norm": 1.8420391082763672,
      "learning_rate": 0.0004063281985159902,
      "loss": 0.0907,
      "step": 36810
    },
    {
      "epoch": 10.791324736225087,
      "grad_norm": 0.9454635977745056,
      "learning_rate": 0.0004062444022284018,
      "loss": 0.0972,
      "step": 36820
    },
    {
      "epoch": 10.794255568581477,
      "grad_norm": 0.3667008578777313,
      "learning_rate": 0.00040616060594081345,
      "loss": 0.0683,
      "step": 36830
    },
    {
      "epoch": 10.797186400937866,
      "grad_norm": 2.5104873180389404,
      "learning_rate": 0.00040607680965322506,
      "loss": 0.0973,
      "step": 36840
    },
    {
      "epoch": 10.800117233294255,
      "grad_norm": 1.2779457569122314,
      "learning_rate": 0.00040599301336563667,
      "loss": 0.1096,
      "step": 36850
    },
    {
      "epoch": 10.803048065650644,
      "grad_norm": 1.3558812141418457,
      "learning_rate": 0.00040590921707804833,
      "loss": 0.0923,
      "step": 36860
    },
    {
      "epoch": 10.805978898007034,
      "grad_norm": 0.5204405188560486,
      "learning_rate": 0.00040582542079045994,
      "loss": 0.09,
      "step": 36870
    },
    {
      "epoch": 10.808909730363423,
      "grad_norm": 1.2259597778320312,
      "learning_rate": 0.0004057416245028716,
      "loss": 0.0886,
      "step": 36880
    },
    {
      "epoch": 10.811840562719812,
      "grad_norm": 0.5868403911590576,
      "learning_rate": 0.0004056578282152832,
      "loss": 0.1035,
      "step": 36890
    },
    {
      "epoch": 10.814771395076201,
      "grad_norm": 0.6439067721366882,
      "learning_rate": 0.0004055740319276949,
      "loss": 0.0937,
      "step": 36900
    },
    {
      "epoch": 10.81770222743259,
      "grad_norm": 0.647577166557312,
      "learning_rate": 0.0004054902356401065,
      "loss": 0.0996,
      "step": 36910
    },
    {
      "epoch": 10.82063305978898,
      "grad_norm": 0.8041674494743347,
      "learning_rate": 0.0004054064393525181,
      "loss": 0.073,
      "step": 36920
    },
    {
      "epoch": 10.82356389214537,
      "grad_norm": 1.1142878532409668,
      "learning_rate": 0.00040532264306492976,
      "loss": 0.0957,
      "step": 36930
    },
    {
      "epoch": 10.826494724501758,
      "grad_norm": 1.6020225286483765,
      "learning_rate": 0.0004052388467773414,
      "loss": 0.097,
      "step": 36940
    },
    {
      "epoch": 10.829425556858148,
      "grad_norm": 2.1370086669921875,
      "learning_rate": 0.0004051550504897531,
      "loss": 0.0855,
      "step": 36950
    },
    {
      "epoch": 10.832356389214537,
      "grad_norm": 0.6292535066604614,
      "learning_rate": 0.00040507125420216465,
      "loss": 0.0897,
      "step": 36960
    },
    {
      "epoch": 10.835287221570926,
      "grad_norm": 1.296220302581787,
      "learning_rate": 0.00040498745791457626,
      "loss": 0.088,
      "step": 36970
    },
    {
      "epoch": 10.838218053927315,
      "grad_norm": 1.477200984954834,
      "learning_rate": 0.000404903661626988,
      "loss": 0.0643,
      "step": 36980
    },
    {
      "epoch": 10.841148886283705,
      "grad_norm": 1.7559770345687866,
      "learning_rate": 0.00040481986533939953,
      "loss": 0.102,
      "step": 36990
    },
    {
      "epoch": 10.844079718640094,
      "grad_norm": 1.2690294981002808,
      "learning_rate": 0.00040473606905181125,
      "loss": 0.0883,
      "step": 37000
    },
    {
      "epoch": 10.847010550996483,
      "grad_norm": 1.1137365102767944,
      "learning_rate": 0.0004046522727642228,
      "loss": 0.088,
      "step": 37010
    },
    {
      "epoch": 10.849941383352872,
      "grad_norm": 0.6995149850845337,
      "learning_rate": 0.0004045684764766344,
      "loss": 0.0991,
      "step": 37020
    },
    {
      "epoch": 10.852872215709262,
      "grad_norm": 0.7438668608665466,
      "learning_rate": 0.00040448468018904613,
      "loss": 0.0786,
      "step": 37030
    },
    {
      "epoch": 10.85580304806565,
      "grad_norm": 0.6746074557304382,
      "learning_rate": 0.0004044008839014577,
      "loss": 0.0666,
      "step": 37040
    },
    {
      "epoch": 10.85873388042204,
      "grad_norm": 0.5473898649215698,
      "learning_rate": 0.0004043170876138694,
      "loss": 0.0908,
      "step": 37050
    },
    {
      "epoch": 10.86166471277843,
      "grad_norm": 1.077752709388733,
      "learning_rate": 0.000404233291326281,
      "loss": 0.0747,
      "step": 37060
    },
    {
      "epoch": 10.864595545134819,
      "grad_norm": 0.4647662937641144,
      "learning_rate": 0.0004041494950386927,
      "loss": 0.0915,
      "step": 37070
    },
    {
      "epoch": 10.867526377491208,
      "grad_norm": 1.1089286804199219,
      "learning_rate": 0.0004040656987511043,
      "loss": 0.0822,
      "step": 37080
    },
    {
      "epoch": 10.870457209847597,
      "grad_norm": 2.569652557373047,
      "learning_rate": 0.0004039819024635159,
      "loss": 0.0954,
      "step": 37090
    },
    {
      "epoch": 10.873388042203986,
      "grad_norm": 0.9253271222114563,
      "learning_rate": 0.00040389810617592756,
      "loss": 0.0931,
      "step": 37100
    },
    {
      "epoch": 10.876318874560376,
      "grad_norm": 2.4171688556671143,
      "learning_rate": 0.00040381430988833917,
      "loss": 0.1053,
      "step": 37110
    },
    {
      "epoch": 10.879249706916765,
      "grad_norm": 1.293809413909912,
      "learning_rate": 0.00040373051360075084,
      "loss": 0.0777,
      "step": 37120
    },
    {
      "epoch": 10.882180539273154,
      "grad_norm": 1.4784915447235107,
      "learning_rate": 0.00040364671731316245,
      "loss": 0.092,
      "step": 37130
    },
    {
      "epoch": 10.885111371629543,
      "grad_norm": 0.6882807016372681,
      "learning_rate": 0.00040356292102557405,
      "loss": 0.0744,
      "step": 37140
    },
    {
      "epoch": 10.888042203985933,
      "grad_norm": 0.49772071838378906,
      "learning_rate": 0.0004034791247379857,
      "loss": 0.1064,
      "step": 37150
    },
    {
      "epoch": 10.890973036342322,
      "grad_norm": 0.8833168148994446,
      "learning_rate": 0.00040339532845039733,
      "loss": 0.0819,
      "step": 37160
    },
    {
      "epoch": 10.893903868698711,
      "grad_norm": 1.0113223791122437,
      "learning_rate": 0.000403311532162809,
      "loss": 0.0906,
      "step": 37170
    },
    {
      "epoch": 10.8968347010551,
      "grad_norm": 1.2566132545471191,
      "learning_rate": 0.0004032277358752206,
      "loss": 0.0724,
      "step": 37180
    },
    {
      "epoch": 10.89976553341149,
      "grad_norm": 0.44551968574523926,
      "learning_rate": 0.0004031439395876322,
      "loss": 0.0996,
      "step": 37190
    },
    {
      "epoch": 10.902696365767879,
      "grad_norm": 0.5019394755363464,
      "learning_rate": 0.0004030601433000439,
      "loss": 0.0686,
      "step": 37200
    },
    {
      "epoch": 10.905627198124268,
      "grad_norm": 1.2150505781173706,
      "learning_rate": 0.0004029763470124555,
      "loss": 0.0692,
      "step": 37210
    },
    {
      "epoch": 10.908558030480657,
      "grad_norm": 0.3274182081222534,
      "learning_rate": 0.00040289255072486715,
      "loss": 0.0879,
      "step": 37220
    },
    {
      "epoch": 10.911488862837047,
      "grad_norm": 0.7369845509529114,
      "learning_rate": 0.00040280875443727876,
      "loss": 0.0677,
      "step": 37230
    },
    {
      "epoch": 10.914419695193434,
      "grad_norm": 0.4436667263507843,
      "learning_rate": 0.0004027249581496904,
      "loss": 0.0754,
      "step": 37240
    },
    {
      "epoch": 10.917350527549825,
      "grad_norm": 0.7567958235740662,
      "learning_rate": 0.00040264116186210203,
      "loss": 0.0605,
      "step": 37250
    },
    {
      "epoch": 10.920281359906213,
      "grad_norm": 0.5738034844398499,
      "learning_rate": 0.00040255736557451364,
      "loss": 0.0774,
      "step": 37260
    },
    {
      "epoch": 10.923212192262602,
      "grad_norm": 0.4340152144432068,
      "learning_rate": 0.0004024735692869253,
      "loss": 0.0678,
      "step": 37270
    },
    {
      "epoch": 10.926143024618991,
      "grad_norm": 1.3661378622055054,
      "learning_rate": 0.0004023897729993369,
      "loss": 0.0796,
      "step": 37280
    },
    {
      "epoch": 10.92907385697538,
      "grad_norm": 0.6158761978149414,
      "learning_rate": 0.0004023059767117486,
      "loss": 0.1097,
      "step": 37290
    },
    {
      "epoch": 10.93200468933177,
      "grad_norm": 0.43289491534233093,
      "learning_rate": 0.0004022221804241602,
      "loss": 0.0874,
      "step": 37300
    },
    {
      "epoch": 10.934935521688159,
      "grad_norm": 0.9464008212089539,
      "learning_rate": 0.0004021383841365718,
      "loss": 0.089,
      "step": 37310
    },
    {
      "epoch": 10.937866354044548,
      "grad_norm": 1.0252244472503662,
      "learning_rate": 0.00040205458784898346,
      "loss": 0.0703,
      "step": 37320
    },
    {
      "epoch": 10.940797186400937,
      "grad_norm": 1.1243902444839478,
      "learning_rate": 0.00040197079156139507,
      "loss": 0.1181,
      "step": 37330
    },
    {
      "epoch": 10.943728018757326,
      "grad_norm": 0.5184838771820068,
      "learning_rate": 0.00040188699527380674,
      "loss": 0.0754,
      "step": 37340
    },
    {
      "epoch": 10.946658851113716,
      "grad_norm": 0.9603688716888428,
      "learning_rate": 0.00040180319898621835,
      "loss": 0.0861,
      "step": 37350
    },
    {
      "epoch": 10.949589683470105,
      "grad_norm": 1.3543204069137573,
      "learning_rate": 0.00040171940269862996,
      "loss": 0.1104,
      "step": 37360
    },
    {
      "epoch": 10.952520515826494,
      "grad_norm": 0.9483499526977539,
      "learning_rate": 0.0004016356064110416,
      "loss": 0.1017,
      "step": 37370
    },
    {
      "epoch": 10.955451348182883,
      "grad_norm": 0.4135342240333557,
      "learning_rate": 0.00040155181012345323,
      "loss": 0.0811,
      "step": 37380
    },
    {
      "epoch": 10.958382180539273,
      "grad_norm": 0.8490735292434692,
      "learning_rate": 0.00040146801383586495,
      "loss": 0.0817,
      "step": 37390
    },
    {
      "epoch": 10.961313012895662,
      "grad_norm": 0.6567594408988953,
      "learning_rate": 0.0004013842175482765,
      "loss": 0.0753,
      "step": 37400
    },
    {
      "epoch": 10.964243845252051,
      "grad_norm": 0.9023543000221252,
      "learning_rate": 0.0004013004212606882,
      "loss": 0.0811,
      "step": 37410
    },
    {
      "epoch": 10.96717467760844,
      "grad_norm": 1.2780182361602783,
      "learning_rate": 0.00040121662497309983,
      "loss": 0.085,
      "step": 37420
    },
    {
      "epoch": 10.97010550996483,
      "grad_norm": 0.9485880136489868,
      "learning_rate": 0.0004011328286855114,
      "loss": 0.0876,
      "step": 37430
    },
    {
      "epoch": 10.973036342321219,
      "grad_norm": 0.5228697061538696,
      "learning_rate": 0.0004010490323979231,
      "loss": 0.0729,
      "step": 37440
    },
    {
      "epoch": 10.975967174677608,
      "grad_norm": 0.3481089174747467,
      "learning_rate": 0.00040096523611033466,
      "loss": 0.0746,
      "step": 37450
    },
    {
      "epoch": 10.978898007033997,
      "grad_norm": 1.0602508783340454,
      "learning_rate": 0.0004008814398227464,
      "loss": 0.0569,
      "step": 37460
    },
    {
      "epoch": 10.981828839390387,
      "grad_norm": 1.4083447456359863,
      "learning_rate": 0.000400797643535158,
      "loss": 0.1141,
      "step": 37470
    },
    {
      "epoch": 10.984759671746776,
      "grad_norm": 1.6440320014953613,
      "learning_rate": 0.00040071384724756954,
      "loss": 0.0577,
      "step": 37480
    },
    {
      "epoch": 10.987690504103165,
      "grad_norm": 2.1883866786956787,
      "learning_rate": 0.00040063005095998126,
      "loss": 0.0787,
      "step": 37490
    },
    {
      "epoch": 10.990621336459554,
      "grad_norm": 1.6084063053131104,
      "learning_rate": 0.00040054625467239287,
      "loss": 0.0789,
      "step": 37500
    },
    {
      "epoch": 10.993552168815944,
      "grad_norm": 1.1834726333618164,
      "learning_rate": 0.00040046245838480454,
      "loss": 0.1109,
      "step": 37510
    },
    {
      "epoch": 10.996483001172333,
      "grad_norm": 0.5789461731910706,
      "learning_rate": 0.00040037866209721615,
      "loss": 0.0885,
      "step": 37520
    },
    {
      "epoch": 10.999413833528722,
      "grad_norm": 0.8385745286941528,
      "learning_rate": 0.00040029486580962775,
      "loss": 0.079,
      "step": 37530
    },
    {
      "epoch": 11.0,
      "eval_accuracy": 0.6923837784371909,
      "eval_f1_macro": 0.7210061492899319,
      "eval_f1_micro": 0.7792045098653304,
      "eval_f1_weighted": 0.767370667177044,
      "eval_loss": 0.09476844221353531,
      "eval_roc_auc": 0.8568734231575871,
      "eval_runtime": 240.8935,
      "eval_samples_per_second": 12.591,
      "eval_steps_per_second": 1.577,
      "step": 37532
    },
    {
      "epoch": 11.002344665885111,
      "grad_norm": 0.49148544669151306,
      "learning_rate": 0.0004002110695220394,
      "loss": 0.0904,
      "step": 37540
    },
    {
      "epoch": 11.0052754982415,
      "grad_norm": 1.004435658454895,
      "learning_rate": 0.00040012727323445103,
      "loss": 0.0919,
      "step": 37550
    },
    {
      "epoch": 11.00820633059789,
      "grad_norm": 1.3059190511703491,
      "learning_rate": 0.0004000434769468627,
      "loss": 0.0951,
      "step": 37560
    },
    {
      "epoch": 11.01113716295428,
      "grad_norm": 0.863516092300415,
      "learning_rate": 0.0003999596806592743,
      "loss": 0.0791,
      "step": 37570
    },
    {
      "epoch": 11.014067995310668,
      "grad_norm": 0.5375223755836487,
      "learning_rate": 0.0003998758843716859,
      "loss": 0.0741,
      "step": 37580
    },
    {
      "epoch": 11.016998827667058,
      "grad_norm": 0.9429482817649841,
      "learning_rate": 0.0003997920880840976,
      "loss": 0.0616,
      "step": 37590
    },
    {
      "epoch": 11.019929660023447,
      "grad_norm": 0.7923570871353149,
      "learning_rate": 0.0003997082917965092,
      "loss": 0.0923,
      "step": 37600
    },
    {
      "epoch": 11.022860492379836,
      "grad_norm": 0.9878369569778442,
      "learning_rate": 0.00039962449550892085,
      "loss": 0.0751,
      "step": 37610
    },
    {
      "epoch": 11.025791324736225,
      "grad_norm": 1.2067220211029053,
      "learning_rate": 0.00039954069922133246,
      "loss": 0.1044,
      "step": 37620
    },
    {
      "epoch": 11.028722157092615,
      "grad_norm": 0.9043006896972656,
      "learning_rate": 0.0003994569029337441,
      "loss": 0.062,
      "step": 37630
    },
    {
      "epoch": 11.031652989449004,
      "grad_norm": 2.0273914337158203,
      "learning_rate": 0.00039937310664615573,
      "loss": 0.0779,
      "step": 37640
    },
    {
      "epoch": 11.034583821805393,
      "grad_norm": 0.8013270497322083,
      "learning_rate": 0.00039928931035856734,
      "loss": 0.0757,
      "step": 37650
    },
    {
      "epoch": 11.037514654161782,
      "grad_norm": 0.616129994392395,
      "learning_rate": 0.000399205514070979,
      "loss": 0.0932,
      "step": 37660
    },
    {
      "epoch": 11.040445486518172,
      "grad_norm": 0.9535722732543945,
      "learning_rate": 0.0003991217177833906,
      "loss": 0.0761,
      "step": 37670
    },
    {
      "epoch": 11.04337631887456,
      "grad_norm": 1.10526442527771,
      "learning_rate": 0.0003990379214958023,
      "loss": 0.0542,
      "step": 37680
    },
    {
      "epoch": 11.04630715123095,
      "grad_norm": 1.0851413011550903,
      "learning_rate": 0.0003989541252082139,
      "loss": 0.0945,
      "step": 37690
    },
    {
      "epoch": 11.04923798358734,
      "grad_norm": 0.4618675112724304,
      "learning_rate": 0.0003988703289206255,
      "loss": 0.0703,
      "step": 37700
    },
    {
      "epoch": 11.052168815943729,
      "grad_norm": 0.5142577886581421,
      "learning_rate": 0.00039878653263303716,
      "loss": 0.057,
      "step": 37710
    },
    {
      "epoch": 11.055099648300118,
      "grad_norm": 0.7714444398880005,
      "learning_rate": 0.00039870273634544877,
      "loss": 0.0832,
      "step": 37720
    },
    {
      "epoch": 11.058030480656507,
      "grad_norm": 2.4793105125427246,
      "learning_rate": 0.00039861894005786044,
      "loss": 0.0923,
      "step": 37730
    },
    {
      "epoch": 11.060961313012896,
      "grad_norm": 1.765127420425415,
      "learning_rate": 0.00039853514377027205,
      "loss": 0.0809,
      "step": 37740
    },
    {
      "epoch": 11.063892145369286,
      "grad_norm": 1.055330753326416,
      "learning_rate": 0.00039845134748268366,
      "loss": 0.0677,
      "step": 37750
    },
    {
      "epoch": 11.066822977725675,
      "grad_norm": 0.8670331239700317,
      "learning_rate": 0.0003983675511950953,
      "loss": 0.0518,
      "step": 37760
    },
    {
      "epoch": 11.069753810082064,
      "grad_norm": 2.2755110263824463,
      "learning_rate": 0.00039828375490750693,
      "loss": 0.0969,
      "step": 37770
    },
    {
      "epoch": 11.072684642438453,
      "grad_norm": 1.1368284225463867,
      "learning_rate": 0.0003981999586199186,
      "loss": 0.0886,
      "step": 37780
    },
    {
      "epoch": 11.075615474794843,
      "grad_norm": 0.14818039536476135,
      "learning_rate": 0.0003981161623323302,
      "loss": 0.0806,
      "step": 37790
    },
    {
      "epoch": 11.078546307151232,
      "grad_norm": 1.3090455532073975,
      "learning_rate": 0.0003980323660447419,
      "loss": 0.065,
      "step": 37800
    },
    {
      "epoch": 11.081477139507621,
      "grad_norm": 1.2957868576049805,
      "learning_rate": 0.0003979485697571535,
      "loss": 0.086,
      "step": 37810
    },
    {
      "epoch": 11.084407971864009,
      "grad_norm": 1.7901389598846436,
      "learning_rate": 0.0003978647734695651,
      "loss": 0.0846,
      "step": 37820
    },
    {
      "epoch": 11.087338804220398,
      "grad_norm": 1.2454296350479126,
      "learning_rate": 0.0003977809771819768,
      "loss": 0.0806,
      "step": 37830
    },
    {
      "epoch": 11.090269636576787,
      "grad_norm": 0.21420755982398987,
      "learning_rate": 0.00039769718089438836,
      "loss": 0.0855,
      "step": 37840
    },
    {
      "epoch": 11.093200468933176,
      "grad_norm": 0.9785597920417786,
      "learning_rate": 0.0003976133846068001,
      "loss": 0.0763,
      "step": 37850
    },
    {
      "epoch": 11.096131301289565,
      "grad_norm": 0.6275117993354797,
      "learning_rate": 0.0003975295883192117,
      "loss": 0.0737,
      "step": 37860
    },
    {
      "epoch": 11.099062133645955,
      "grad_norm": 0.7906033992767334,
      "learning_rate": 0.00039744579203162324,
      "loss": 0.0739,
      "step": 37870
    },
    {
      "epoch": 11.101992966002344,
      "grad_norm": 2.7894270420074463,
      "learning_rate": 0.00039736199574403496,
      "loss": 0.09,
      "step": 37880
    },
    {
      "epoch": 11.104923798358733,
      "grad_norm": 0.9119296073913574,
      "learning_rate": 0.00039727819945644657,
      "loss": 0.0731,
      "step": 37890
    },
    {
      "epoch": 11.107854630715122,
      "grad_norm": 1.0224279165267944,
      "learning_rate": 0.00039719440316885824,
      "loss": 0.0875,
      "step": 37900
    },
    {
      "epoch": 11.110785463071512,
      "grad_norm": 0.6236099004745483,
      "learning_rate": 0.00039711060688126984,
      "loss": 0.0879,
      "step": 37910
    },
    {
      "epoch": 11.113716295427901,
      "grad_norm": 0.9438164830207825,
      "learning_rate": 0.0003970268105936814,
      "loss": 0.0901,
      "step": 37920
    },
    {
      "epoch": 11.11664712778429,
      "grad_norm": 1.0683904886245728,
      "learning_rate": 0.0003969430143060931,
      "loss": 0.0547,
      "step": 37930
    },
    {
      "epoch": 11.11957796014068,
      "grad_norm": 0.8477734923362732,
      "learning_rate": 0.00039685921801850473,
      "loss": 0.0856,
      "step": 37940
    },
    {
      "epoch": 11.122508792497069,
      "grad_norm": 0.7129073739051819,
      "learning_rate": 0.0003967754217309164,
      "loss": 0.086,
      "step": 37950
    },
    {
      "epoch": 11.125439624853458,
      "grad_norm": 0.4378701150417328,
      "learning_rate": 0.000396691625443328,
      "loss": 0.0643,
      "step": 37960
    },
    {
      "epoch": 11.128370457209847,
      "grad_norm": 0.9369717836380005,
      "learning_rate": 0.00039660782915573967,
      "loss": 0.0684,
      "step": 37970
    },
    {
      "epoch": 11.131301289566236,
      "grad_norm": 0.8030492067337036,
      "learning_rate": 0.0003965240328681513,
      "loss": 0.0701,
      "step": 37980
    },
    {
      "epoch": 11.134232121922626,
      "grad_norm": 0.4408401548862457,
      "learning_rate": 0.0003964402365805629,
      "loss": 0.0734,
      "step": 37990
    },
    {
      "epoch": 11.137162954279015,
      "grad_norm": 0.9813576340675354,
      "learning_rate": 0.00039635644029297455,
      "loss": 0.0638,
      "step": 38000
    },
    {
      "epoch": 11.140093786635404,
      "grad_norm": 1.0175467729568481,
      "learning_rate": 0.00039627264400538616,
      "loss": 0.0693,
      "step": 38010
    },
    {
      "epoch": 11.143024618991793,
      "grad_norm": 0.8422455191612244,
      "learning_rate": 0.0003961888477177978,
      "loss": 0.0911,
      "step": 38020
    },
    {
      "epoch": 11.145955451348183,
      "grad_norm": 1.736733317375183,
      "learning_rate": 0.00039610505143020943,
      "loss": 0.0832,
      "step": 38030
    },
    {
      "epoch": 11.148886283704572,
      "grad_norm": 0.6980640888214111,
      "learning_rate": 0.00039602125514262104,
      "loss": 0.0837,
      "step": 38040
    },
    {
      "epoch": 11.151817116060961,
      "grad_norm": 1.6568201780319214,
      "learning_rate": 0.0003959374588550327,
      "loss": 0.0879,
      "step": 38050
    },
    {
      "epoch": 11.15474794841735,
      "grad_norm": 1.2067455053329468,
      "learning_rate": 0.0003958536625674443,
      "loss": 0.0765,
      "step": 38060
    },
    {
      "epoch": 11.15767878077374,
      "grad_norm": 0.6439945101737976,
      "learning_rate": 0.000395769866279856,
      "loss": 0.0854,
      "step": 38070
    },
    {
      "epoch": 11.160609613130129,
      "grad_norm": 1.8734502792358398,
      "learning_rate": 0.0003956860699922676,
      "loss": 0.0996,
      "step": 38080
    },
    {
      "epoch": 11.163540445486518,
      "grad_norm": 0.8530654311180115,
      "learning_rate": 0.0003956022737046792,
      "loss": 0.0936,
      "step": 38090
    },
    {
      "epoch": 11.166471277842907,
      "grad_norm": 0.8854520916938782,
      "learning_rate": 0.00039551847741709086,
      "loss": 0.0734,
      "step": 38100
    },
    {
      "epoch": 11.169402110199297,
      "grad_norm": 0.9421887993812561,
      "learning_rate": 0.00039543468112950247,
      "loss": 0.0932,
      "step": 38110
    },
    {
      "epoch": 11.172332942555686,
      "grad_norm": 0.6533418893814087,
      "learning_rate": 0.00039535088484191414,
      "loss": 0.0896,
      "step": 38120
    },
    {
      "epoch": 11.175263774912075,
      "grad_norm": 1.165095329284668,
      "learning_rate": 0.00039526708855432575,
      "loss": 0.0683,
      "step": 38130
    },
    {
      "epoch": 11.178194607268464,
      "grad_norm": 0.7593056559562683,
      "learning_rate": 0.0003951832922667374,
      "loss": 0.0831,
      "step": 38140
    },
    {
      "epoch": 11.181125439624854,
      "grad_norm": 1.1588584184646606,
      "learning_rate": 0.000395099495979149,
      "loss": 0.101,
      "step": 38150
    },
    {
      "epoch": 11.184056271981243,
      "grad_norm": 0.5634258985519409,
      "learning_rate": 0.00039501569969156063,
      "loss": 0.0597,
      "step": 38160
    },
    {
      "epoch": 11.186987104337632,
      "grad_norm": 0.9062870144844055,
      "learning_rate": 0.0003949319034039723,
      "loss": 0.1082,
      "step": 38170
    },
    {
      "epoch": 11.189917936694021,
      "grad_norm": 0.5965527892112732,
      "learning_rate": 0.0003948481071163839,
      "loss": 0.0855,
      "step": 38180
    },
    {
      "epoch": 11.19284876905041,
      "grad_norm": 0.47879844903945923,
      "learning_rate": 0.0003947643108287956,
      "loss": 0.0761,
      "step": 38190
    },
    {
      "epoch": 11.1957796014068,
      "grad_norm": 1.2479346990585327,
      "learning_rate": 0.0003946805145412072,
      "loss": 0.1094,
      "step": 38200
    },
    {
      "epoch": 11.198710433763189,
      "grad_norm": 0.5084792971611023,
      "learning_rate": 0.0003945967182536188,
      "loss": 0.0933,
      "step": 38210
    },
    {
      "epoch": 11.201641266119578,
      "grad_norm": 1.3634861707687378,
      "learning_rate": 0.00039451292196603045,
      "loss": 0.0726,
      "step": 38220
    },
    {
      "epoch": 11.204572098475968,
      "grad_norm": 1.1588307619094849,
      "learning_rate": 0.00039442912567844206,
      "loss": 0.0675,
      "step": 38230
    },
    {
      "epoch": 11.207502930832357,
      "grad_norm": 1.5192489624023438,
      "learning_rate": 0.0003943453293908538,
      "loss": 0.0774,
      "step": 38240
    },
    {
      "epoch": 11.210433763188746,
      "grad_norm": 0.7845795154571533,
      "learning_rate": 0.00039426153310326533,
      "loss": 0.0821,
      "step": 38250
    },
    {
      "epoch": 11.213364595545135,
      "grad_norm": 1.6509206295013428,
      "learning_rate": 0.00039417773681567694,
      "loss": 0.0908,
      "step": 38260
    },
    {
      "epoch": 11.216295427901525,
      "grad_norm": 0.9954352378845215,
      "learning_rate": 0.00039409394052808866,
      "loss": 0.0958,
      "step": 38270
    },
    {
      "epoch": 11.219226260257914,
      "grad_norm": 1.573956847190857,
      "learning_rate": 0.0003940101442405002,
      "loss": 0.0724,
      "step": 38280
    },
    {
      "epoch": 11.222157092614303,
      "grad_norm": 1.4425311088562012,
      "learning_rate": 0.00039392634795291194,
      "loss": 0.099,
      "step": 38290
    },
    {
      "epoch": 11.225087924970692,
      "grad_norm": 0.4580717086791992,
      "learning_rate": 0.00039384255166532354,
      "loss": 0.0846,
      "step": 38300
    },
    {
      "epoch": 11.228018757327082,
      "grad_norm": 0.8679991960525513,
      "learning_rate": 0.0003937587553777352,
      "loss": 0.0784,
      "step": 38310
    },
    {
      "epoch": 11.23094958968347,
      "grad_norm": 1.086379051208496,
      "learning_rate": 0.0003936749590901468,
      "loss": 0.0728,
      "step": 38320
    },
    {
      "epoch": 11.23388042203986,
      "grad_norm": 0.5892210006713867,
      "learning_rate": 0.00039359116280255843,
      "loss": 0.0861,
      "step": 38330
    },
    {
      "epoch": 11.23681125439625,
      "grad_norm": 1.0637120008468628,
      "learning_rate": 0.0003935073665149701,
      "loss": 0.071,
      "step": 38340
    },
    {
      "epoch": 11.239742086752639,
      "grad_norm": 0.9323022365570068,
      "learning_rate": 0.0003934235702273817,
      "loss": 0.0498,
      "step": 38350
    },
    {
      "epoch": 11.242672919109028,
      "grad_norm": 0.5910428166389465,
      "learning_rate": 0.00039333977393979337,
      "loss": 0.0693,
      "step": 38360
    },
    {
      "epoch": 11.245603751465417,
      "grad_norm": 1.0766570568084717,
      "learning_rate": 0.000393255977652205,
      "loss": 0.1074,
      "step": 38370
    },
    {
      "epoch": 11.248534583821806,
      "grad_norm": 0.3413923382759094,
      "learning_rate": 0.0003931721813646166,
      "loss": 0.079,
      "step": 38380
    },
    {
      "epoch": 11.251465416178196,
      "grad_norm": 1.0431665182113647,
      "learning_rate": 0.00039308838507702825,
      "loss": 0.0885,
      "step": 38390
    },
    {
      "epoch": 11.254396248534583,
      "grad_norm": 1.0494425296783447,
      "learning_rate": 0.00039300458878943986,
      "loss": 0.0752,
      "step": 38400
    },
    {
      "epoch": 11.257327080890972,
      "grad_norm": 0.9675718545913696,
      "learning_rate": 0.0003929207925018515,
      "loss": 0.1048,
      "step": 38410
    },
    {
      "epoch": 11.260257913247361,
      "grad_norm": 0.9242803454399109,
      "learning_rate": 0.00039283699621426313,
      "loss": 0.0854,
      "step": 38420
    },
    {
      "epoch": 11.26318874560375,
      "grad_norm": 0.9715592265129089,
      "learning_rate": 0.00039275319992667474,
      "loss": 0.0616,
      "step": 38430
    },
    {
      "epoch": 11.26611957796014,
      "grad_norm": 0.3734801411628723,
      "learning_rate": 0.0003926694036390864,
      "loss": 0.0635,
      "step": 38440
    },
    {
      "epoch": 11.26905041031653,
      "grad_norm": 1.0697511434555054,
      "learning_rate": 0.000392585607351498,
      "loss": 0.0915,
      "step": 38450
    },
    {
      "epoch": 11.271981242672918,
      "grad_norm": 1.263254165649414,
      "learning_rate": 0.0003925018110639097,
      "loss": 0.0772,
      "step": 38460
    },
    {
      "epoch": 11.274912075029308,
      "grad_norm": 0.9210697412490845,
      "learning_rate": 0.0003924180147763213,
      "loss": 0.0771,
      "step": 38470
    },
    {
      "epoch": 11.277842907385697,
      "grad_norm": 0.901623547077179,
      "learning_rate": 0.00039233421848873295,
      "loss": 0.1272,
      "step": 38480
    },
    {
      "epoch": 11.280773739742086,
      "grad_norm": 1.1137579679489136,
      "learning_rate": 0.00039225042220114456,
      "loss": 0.0755,
      "step": 38490
    },
    {
      "epoch": 11.283704572098475,
      "grad_norm": 0.7162273526191711,
      "learning_rate": 0.00039216662591355617,
      "loss": 0.0716,
      "step": 38500
    },
    {
      "epoch": 11.286635404454865,
      "grad_norm": 0.49122869968414307,
      "learning_rate": 0.00039208282962596784,
      "loss": 0.0811,
      "step": 38510
    },
    {
      "epoch": 11.289566236811254,
      "grad_norm": 1.0089486837387085,
      "learning_rate": 0.00039199903333837945,
      "loss": 0.0822,
      "step": 38520
    },
    {
      "epoch": 11.292497069167643,
      "grad_norm": 0.936021625995636,
      "learning_rate": 0.0003919152370507911,
      "loss": 0.082,
      "step": 38530
    },
    {
      "epoch": 11.295427901524032,
      "grad_norm": 1.2983719110488892,
      "learning_rate": 0.0003918314407632027,
      "loss": 0.1166,
      "step": 38540
    },
    {
      "epoch": 11.298358733880422,
      "grad_norm": 0.5935512185096741,
      "learning_rate": 0.00039174764447561433,
      "loss": 0.0813,
      "step": 38550
    },
    {
      "epoch": 11.301289566236811,
      "grad_norm": 0.7846982479095459,
      "learning_rate": 0.000391663848188026,
      "loss": 0.0947,
      "step": 38560
    },
    {
      "epoch": 11.3042203985932,
      "grad_norm": 0.6297056674957275,
      "learning_rate": 0.0003915800519004376,
      "loss": 0.0699,
      "step": 38570
    },
    {
      "epoch": 11.30715123094959,
      "grad_norm": 0.9146909713745117,
      "learning_rate": 0.00039149625561284927,
      "loss": 0.0783,
      "step": 38580
    },
    {
      "epoch": 11.310082063305979,
      "grad_norm": 1.0032399892807007,
      "learning_rate": 0.0003914124593252609,
      "loss": 0.0925,
      "step": 38590
    },
    {
      "epoch": 11.313012895662368,
      "grad_norm": 0.1729067713022232,
      "learning_rate": 0.0003913286630376725,
      "loss": 0.0843,
      "step": 38600
    },
    {
      "epoch": 11.315943728018757,
      "grad_norm": 1.2108222246170044,
      "learning_rate": 0.00039124486675008415,
      "loss": 0.1047,
      "step": 38610
    },
    {
      "epoch": 11.318874560375146,
      "grad_norm": 0.5665024518966675,
      "learning_rate": 0.00039116107046249576,
      "loss": 0.0868,
      "step": 38620
    },
    {
      "epoch": 11.321805392731536,
      "grad_norm": 1.9640140533447266,
      "learning_rate": 0.0003910772741749075,
      "loss": 0.0858,
      "step": 38630
    },
    {
      "epoch": 11.324736225087925,
      "grad_norm": 0.6972466111183167,
      "learning_rate": 0.00039099347788731903,
      "loss": 0.0694,
      "step": 38640
    },
    {
      "epoch": 11.327667057444314,
      "grad_norm": 1.042754888534546,
      "learning_rate": 0.00039090968159973064,
      "loss": 0.0981,
      "step": 38650
    },
    {
      "epoch": 11.330597889800703,
      "grad_norm": 0.6752983927726746,
      "learning_rate": 0.0003908258853121423,
      "loss": 0.0885,
      "step": 38660
    },
    {
      "epoch": 11.333528722157093,
      "grad_norm": 0.9261392951011658,
      "learning_rate": 0.0003907420890245539,
      "loss": 0.0817,
      "step": 38670
    },
    {
      "epoch": 11.336459554513482,
      "grad_norm": 0.7268784642219543,
      "learning_rate": 0.00039065829273696564,
      "loss": 0.072,
      "step": 38680
    },
    {
      "epoch": 11.339390386869871,
      "grad_norm": 1.0745285749435425,
      "learning_rate": 0.0003905744964493772,
      "loss": 0.0888,
      "step": 38690
    },
    {
      "epoch": 11.34232121922626,
      "grad_norm": 1.035726547241211,
      "learning_rate": 0.0003904907001617889,
      "loss": 0.096,
      "step": 38700
    },
    {
      "epoch": 11.34525205158265,
      "grad_norm": 1.08438241481781,
      "learning_rate": 0.0003904069038742005,
      "loss": 0.0705,
      "step": 38710
    },
    {
      "epoch": 11.348182883939039,
      "grad_norm": 1.0540963411331177,
      "learning_rate": 0.0003903231075866121,
      "loss": 0.0793,
      "step": 38720
    },
    {
      "epoch": 11.351113716295428,
      "grad_norm": 1.5088120698928833,
      "learning_rate": 0.0003902393112990238,
      "loss": 0.0606,
      "step": 38730
    },
    {
      "epoch": 11.354044548651817,
      "grad_norm": 1.996390461921692,
      "learning_rate": 0.0003901555150114354,
      "loss": 0.0631,
      "step": 38740
    },
    {
      "epoch": 11.356975381008207,
      "grad_norm": 0.6763195991516113,
      "learning_rate": 0.00039007171872384707,
      "loss": 0.0701,
      "step": 38750
    },
    {
      "epoch": 11.359906213364596,
      "grad_norm": 0.6871740818023682,
      "learning_rate": 0.0003899879224362587,
      "loss": 0.0728,
      "step": 38760
    },
    {
      "epoch": 11.362837045720985,
      "grad_norm": 0.8769595623016357,
      "learning_rate": 0.0003899041261486703,
      "loss": 0.0951,
      "step": 38770
    },
    {
      "epoch": 11.365767878077374,
      "grad_norm": 1.151829481124878,
      "learning_rate": 0.00038982032986108195,
      "loss": 0.071,
      "step": 38780
    },
    {
      "epoch": 11.368698710433764,
      "grad_norm": 0.963502049446106,
      "learning_rate": 0.00038973653357349356,
      "loss": 0.077,
      "step": 38790
    },
    {
      "epoch": 11.371629542790153,
      "grad_norm": 1.5544513463974,
      "learning_rate": 0.0003896527372859052,
      "loss": 0.0544,
      "step": 38800
    },
    {
      "epoch": 11.374560375146542,
      "grad_norm": 1.1801135540008545,
      "learning_rate": 0.00038956894099831683,
      "loss": 0.0877,
      "step": 38810
    },
    {
      "epoch": 11.377491207502931,
      "grad_norm": 1.504473328590393,
      "learning_rate": 0.00038948514471072844,
      "loss": 0.0826,
      "step": 38820
    },
    {
      "epoch": 11.38042203985932,
      "grad_norm": 1.4064165353775024,
      "learning_rate": 0.0003894013484231401,
      "loss": 0.0888,
      "step": 38830
    },
    {
      "epoch": 11.38335287221571,
      "grad_norm": 0.6845282912254333,
      "learning_rate": 0.0003893175521355517,
      "loss": 0.0942,
      "step": 38840
    },
    {
      "epoch": 11.386283704572099,
      "grad_norm": 0.8269486427307129,
      "learning_rate": 0.0003892337558479634,
      "loss": 0.0901,
      "step": 38850
    },
    {
      "epoch": 11.389214536928488,
      "grad_norm": 3.535881519317627,
      "learning_rate": 0.000389149959560375,
      "loss": 0.094,
      "step": 38860
    },
    {
      "epoch": 11.392145369284878,
      "grad_norm": 1.1808035373687744,
      "learning_rate": 0.00038906616327278665,
      "loss": 0.0912,
      "step": 38870
    },
    {
      "epoch": 11.395076201641267,
      "grad_norm": 0.8144117593765259,
      "learning_rate": 0.00038898236698519826,
      "loss": 0.0662,
      "step": 38880
    },
    {
      "epoch": 11.398007033997656,
      "grad_norm": 0.5795003175735474,
      "learning_rate": 0.00038889857069760987,
      "loss": 0.0584,
      "step": 38890
    },
    {
      "epoch": 11.400937866354045,
      "grad_norm": 0.8191671967506409,
      "learning_rate": 0.00038881477441002154,
      "loss": 0.0559,
      "step": 38900
    },
    {
      "epoch": 11.403868698710435,
      "grad_norm": 1.5224807262420654,
      "learning_rate": 0.00038873097812243315,
      "loss": 0.0944,
      "step": 38910
    },
    {
      "epoch": 11.406799531066824,
      "grad_norm": 0.9186599254608154,
      "learning_rate": 0.0003886471818348448,
      "loss": 0.102,
      "step": 38920
    },
    {
      "epoch": 11.409730363423213,
      "grad_norm": 1.6637502908706665,
      "learning_rate": 0.0003885633855472564,
      "loss": 0.0893,
      "step": 38930
    },
    {
      "epoch": 11.412661195779602,
      "grad_norm": 1.7901304960250854,
      "learning_rate": 0.00038847958925966803,
      "loss": 0.0719,
      "step": 38940
    },
    {
      "epoch": 11.41559202813599,
      "grad_norm": 1.020642876625061,
      "learning_rate": 0.0003883957929720797,
      "loss": 0.1325,
      "step": 38950
    },
    {
      "epoch": 11.41852286049238,
      "grad_norm": 0.5340962409973145,
      "learning_rate": 0.0003883119966844913,
      "loss": 0.088,
      "step": 38960
    },
    {
      "epoch": 11.421453692848768,
      "grad_norm": 0.826327383518219,
      "learning_rate": 0.00038822820039690297,
      "loss": 0.0818,
      "step": 38970
    },
    {
      "epoch": 11.424384525205157,
      "grad_norm": 0.5334193706512451,
      "learning_rate": 0.0003881444041093146,
      "loss": 0.0847,
      "step": 38980
    },
    {
      "epoch": 11.427315357561547,
      "grad_norm": 2.0456814765930176,
      "learning_rate": 0.0003880606078217262,
      "loss": 0.0936,
      "step": 38990
    },
    {
      "epoch": 11.430246189917936,
      "grad_norm": 0.4949718117713928,
      "learning_rate": 0.00038797681153413785,
      "loss": 0.0886,
      "step": 39000
    },
    {
      "epoch": 11.433177022274325,
      "grad_norm": 2.6299774646759033,
      "learning_rate": 0.00038789301524654946,
      "loss": 0.116,
      "step": 39010
    },
    {
      "epoch": 11.436107854630714,
      "grad_norm": 0.7425203323364258,
      "learning_rate": 0.0003878092189589611,
      "loss": 0.0821,
      "step": 39020
    },
    {
      "epoch": 11.439038686987104,
      "grad_norm": 1.023024559020996,
      "learning_rate": 0.00038772542267137273,
      "loss": 0.0796,
      "step": 39030
    },
    {
      "epoch": 11.441969519343493,
      "grad_norm": 1.4103407859802246,
      "learning_rate": 0.00038764162638378445,
      "loss": 0.1031,
      "step": 39040
    },
    {
      "epoch": 11.444900351699882,
      "grad_norm": 1.5675984621047974,
      "learning_rate": 0.000387557830096196,
      "loss": 0.0714,
      "step": 39050
    },
    {
      "epoch": 11.447831184056271,
      "grad_norm": 0.6361889839172363,
      "learning_rate": 0.0003874740338086076,
      "loss": 0.0656,
      "step": 39060
    },
    {
      "epoch": 11.45076201641266,
      "grad_norm": 0.24916990101337433,
      "learning_rate": 0.00038739023752101934,
      "loss": 0.0614,
      "step": 39070
    },
    {
      "epoch": 11.45369284876905,
      "grad_norm": 1.31227707862854,
      "learning_rate": 0.0003873064412334309,
      "loss": 0.1263,
      "step": 39080
    },
    {
      "epoch": 11.45662368112544,
      "grad_norm": 1.3008744716644287,
      "learning_rate": 0.0003872226449458426,
      "loss": 0.0751,
      "step": 39090
    },
    {
      "epoch": 11.459554513481828,
      "grad_norm": 0.6376798748970032,
      "learning_rate": 0.0003871388486582542,
      "loss": 0.0587,
      "step": 39100
    },
    {
      "epoch": 11.462485345838218,
      "grad_norm": 0.6123706698417664,
      "learning_rate": 0.0003870550523706658,
      "loss": 0.0801,
      "step": 39110
    },
    {
      "epoch": 11.465416178194607,
      "grad_norm": 1.1429457664489746,
      "learning_rate": 0.0003869712560830775,
      "loss": 0.0977,
      "step": 39120
    },
    {
      "epoch": 11.468347010550996,
      "grad_norm": 0.7021486759185791,
      "learning_rate": 0.00038688745979548905,
      "loss": 0.0896,
      "step": 39130
    },
    {
      "epoch": 11.471277842907385,
      "grad_norm": 0.2537488639354706,
      "learning_rate": 0.00038680366350790077,
      "loss": 0.0774,
      "step": 39140
    },
    {
      "epoch": 11.474208675263775,
      "grad_norm": 1.3804235458374023,
      "learning_rate": 0.0003867198672203124,
      "loss": 0.0815,
      "step": 39150
    },
    {
      "epoch": 11.477139507620164,
      "grad_norm": 1.3894683122634888,
      "learning_rate": 0.00038663607093272393,
      "loss": 0.0865,
      "step": 39160
    },
    {
      "epoch": 11.480070339976553,
      "grad_norm": 0.898287296295166,
      "learning_rate": 0.00038655227464513565,
      "loss": 0.0685,
      "step": 39170
    },
    {
      "epoch": 11.483001172332942,
      "grad_norm": 3.5310490131378174,
      "learning_rate": 0.00038646847835754726,
      "loss": 0.0851,
      "step": 39180
    },
    {
      "epoch": 11.485932004689332,
      "grad_norm": 0.9495692253112793,
      "learning_rate": 0.0003863846820699589,
      "loss": 0.0836,
      "step": 39190
    },
    {
      "epoch": 11.48886283704572,
      "grad_norm": 1.348558783531189,
      "learning_rate": 0.00038630088578237053,
      "loss": 0.0941,
      "step": 39200
    },
    {
      "epoch": 11.49179366940211,
      "grad_norm": 0.2757193446159363,
      "learning_rate": 0.0003862170894947822,
      "loss": 0.0774,
      "step": 39210
    },
    {
      "epoch": 11.4947245017585,
      "grad_norm": 0.9468430876731873,
      "learning_rate": 0.0003861332932071938,
      "loss": 0.062,
      "step": 39220
    },
    {
      "epoch": 11.497655334114889,
      "grad_norm": 0.9513434767723083,
      "learning_rate": 0.0003860494969196054,
      "loss": 0.0895,
      "step": 39230
    },
    {
      "epoch": 11.500586166471278,
      "grad_norm": 0.45984286069869995,
      "learning_rate": 0.0003859657006320171,
      "loss": 0.0653,
      "step": 39240
    },
    {
      "epoch": 11.503516998827667,
      "grad_norm": 1.0664362907409668,
      "learning_rate": 0.0003858819043444287,
      "loss": 0.0943,
      "step": 39250
    },
    {
      "epoch": 11.506447831184056,
      "grad_norm": 0.7363309860229492,
      "learning_rate": 0.00038579810805684035,
      "loss": 0.0854,
      "step": 39260
    },
    {
      "epoch": 11.509378663540446,
      "grad_norm": 1.61070716381073,
      "learning_rate": 0.00038571431176925196,
      "loss": 0.092,
      "step": 39270
    },
    {
      "epoch": 11.512309495896835,
      "grad_norm": 0.8911626935005188,
      "learning_rate": 0.00038563051548166357,
      "loss": 0.0854,
      "step": 39280
    },
    {
      "epoch": 11.515240328253224,
      "grad_norm": 1.274562120437622,
      "learning_rate": 0.00038554671919407524,
      "loss": 0.0849,
      "step": 39290
    },
    {
      "epoch": 11.518171160609613,
      "grad_norm": 0.8089597821235657,
      "learning_rate": 0.00038546292290648685,
      "loss": 0.0779,
      "step": 39300
    },
    {
      "epoch": 11.521101992966003,
      "grad_norm": 0.6387699842453003,
      "learning_rate": 0.0003853791266188985,
      "loss": 0.094,
      "step": 39310
    },
    {
      "epoch": 11.524032825322392,
      "grad_norm": 0.8093968629837036,
      "learning_rate": 0.0003852953303313101,
      "loss": 0.0937,
      "step": 39320
    },
    {
      "epoch": 11.526963657678781,
      "grad_norm": 1.274427056312561,
      "learning_rate": 0.00038521153404372173,
      "loss": 0.0572,
      "step": 39330
    },
    {
      "epoch": 11.52989449003517,
      "grad_norm": 0.6578637957572937,
      "learning_rate": 0.0003851277377561334,
      "loss": 0.1028,
      "step": 39340
    },
    {
      "epoch": 11.53282532239156,
      "grad_norm": 0.68369060754776,
      "learning_rate": 0.000385043941468545,
      "loss": 0.0743,
      "step": 39350
    },
    {
      "epoch": 11.535756154747949,
      "grad_norm": 0.4552434980869293,
      "learning_rate": 0.00038496014518095667,
      "loss": 0.0608,
      "step": 39360
    },
    {
      "epoch": 11.538686987104338,
      "grad_norm": 1.1888173818588257,
      "learning_rate": 0.0003848763488933683,
      "loss": 0.1033,
      "step": 39370
    },
    {
      "epoch": 11.541617819460727,
      "grad_norm": 0.565890371799469,
      "learning_rate": 0.00038479255260577994,
      "loss": 0.0757,
      "step": 39380
    },
    {
      "epoch": 11.544548651817117,
      "grad_norm": 1.2026551961898804,
      "learning_rate": 0.00038470875631819155,
      "loss": 0.1002,
      "step": 39390
    },
    {
      "epoch": 11.547479484173506,
      "grad_norm": 0.8171353340148926,
      "learning_rate": 0.00038462496003060316,
      "loss": 0.1133,
      "step": 39400
    },
    {
      "epoch": 11.550410316529895,
      "grad_norm": 0.32656052708625793,
      "learning_rate": 0.0003845411637430148,
      "loss": 0.0804,
      "step": 39410
    },
    {
      "epoch": 11.553341148886284,
      "grad_norm": 1.0718709230422974,
      "learning_rate": 0.00038445736745542643,
      "loss": 0.0904,
      "step": 39420
    },
    {
      "epoch": 11.556271981242674,
      "grad_norm": 0.5814430117607117,
      "learning_rate": 0.0003843735711678381,
      "loss": 0.0929,
      "step": 39430
    },
    {
      "epoch": 11.559202813599063,
      "grad_norm": 1.4704357385635376,
      "learning_rate": 0.0003842897748802497,
      "loss": 0.0655,
      "step": 39440
    },
    {
      "epoch": 11.562133645955452,
      "grad_norm": 0.5309626460075378,
      "learning_rate": 0.0003842059785926613,
      "loss": 0.0636,
      "step": 39450
    },
    {
      "epoch": 11.565064478311841,
      "grad_norm": 0.7742117047309875,
      "learning_rate": 0.000384122182305073,
      "loss": 0.0877,
      "step": 39460
    },
    {
      "epoch": 11.56799531066823,
      "grad_norm": 0.909949541091919,
      "learning_rate": 0.0003840383860174846,
      "loss": 0.1016,
      "step": 39470
    },
    {
      "epoch": 11.57092614302462,
      "grad_norm": 0.7160181403160095,
      "learning_rate": 0.0003839545897298963,
      "loss": 0.0799,
      "step": 39480
    },
    {
      "epoch": 11.573856975381009,
      "grad_norm": 1.1794722080230713,
      "learning_rate": 0.00038387079344230786,
      "loss": 0.0915,
      "step": 39490
    },
    {
      "epoch": 11.576787807737398,
      "grad_norm": 0.20388154685497284,
      "learning_rate": 0.0003837869971547195,
      "loss": 0.0723,
      "step": 39500
    },
    {
      "epoch": 11.579718640093787,
      "grad_norm": 0.9331590533256531,
      "learning_rate": 0.0003837032008671312,
      "loss": 0.0916,
      "step": 39510
    },
    {
      "epoch": 11.582649472450175,
      "grad_norm": 0.6056657433509827,
      "learning_rate": 0.00038361940457954275,
      "loss": 0.1107,
      "step": 39520
    },
    {
      "epoch": 11.585580304806566,
      "grad_norm": 1.2949260473251343,
      "learning_rate": 0.00038353560829195447,
      "loss": 0.0917,
      "step": 39530
    },
    {
      "epoch": 11.588511137162953,
      "grad_norm": 0.9157456159591675,
      "learning_rate": 0.0003834518120043661,
      "loss": 0.0884,
      "step": 39540
    },
    {
      "epoch": 11.591441969519343,
      "grad_norm": 0.8970163464546204,
      "learning_rate": 0.00038336801571677774,
      "loss": 0.0894,
      "step": 39550
    },
    {
      "epoch": 11.594372801875732,
      "grad_norm": 0.16131579875946045,
      "learning_rate": 0.00038328421942918935,
      "loss": 0.0846,
      "step": 39560
    },
    {
      "epoch": 11.597303634232121,
      "grad_norm": 1.1962566375732422,
      "learning_rate": 0.0003832004231416009,
      "loss": 0.0875,
      "step": 39570
    },
    {
      "epoch": 11.60023446658851,
      "grad_norm": 1.0450685024261475,
      "learning_rate": 0.0003831166268540126,
      "loss": 0.0943,
      "step": 39580
    },
    {
      "epoch": 11.6031652989449,
      "grad_norm": 1.3342136144638062,
      "learning_rate": 0.00038303283056642423,
      "loss": 0.0951,
      "step": 39590
    },
    {
      "epoch": 11.606096131301289,
      "grad_norm": 2.087096929550171,
      "learning_rate": 0.0003829490342788359,
      "loss": 0.0961,
      "step": 39600
    },
    {
      "epoch": 11.609026963657678,
      "grad_norm": 0.9841545224189758,
      "learning_rate": 0.0003828652379912475,
      "loss": 0.0556,
      "step": 39610
    },
    {
      "epoch": 11.611957796014067,
      "grad_norm": 1.3346025943756104,
      "learning_rate": 0.0003827814417036591,
      "loss": 0.092,
      "step": 39620
    },
    {
      "epoch": 11.614888628370457,
      "grad_norm": 0.636981725692749,
      "learning_rate": 0.0003826976454160708,
      "loss": 0.1026,
      "step": 39630
    },
    {
      "epoch": 11.617819460726846,
      "grad_norm": 0.8030888438224792,
      "learning_rate": 0.0003826138491284824,
      "loss": 0.0819,
      "step": 39640
    },
    {
      "epoch": 11.620750293083235,
      "grad_norm": 1.450803279876709,
      "learning_rate": 0.00038253005284089405,
      "loss": 0.0918,
      "step": 39650
    },
    {
      "epoch": 11.623681125439624,
      "grad_norm": 0.6650826930999756,
      "learning_rate": 0.00038244625655330566,
      "loss": 0.0929,
      "step": 39660
    },
    {
      "epoch": 11.626611957796014,
      "grad_norm": 0.8014441132545471,
      "learning_rate": 0.00038236246026571727,
      "loss": 0.0791,
      "step": 39670
    },
    {
      "epoch": 11.629542790152403,
      "grad_norm": 0.5457459092140198,
      "learning_rate": 0.00038227866397812894,
      "loss": 0.0743,
      "step": 39680
    },
    {
      "epoch": 11.632473622508792,
      "grad_norm": 2.7299931049346924,
      "learning_rate": 0.00038219486769054055,
      "loss": 0.1069,
      "step": 39690
    },
    {
      "epoch": 11.635404454865181,
      "grad_norm": 1.3336633443832397,
      "learning_rate": 0.0003821110714029522,
      "loss": 0.0888,
      "step": 39700
    },
    {
      "epoch": 11.63833528722157,
      "grad_norm": 0.8305339813232422,
      "learning_rate": 0.0003820272751153638,
      "loss": 0.0754,
      "step": 39710
    },
    {
      "epoch": 11.64126611957796,
      "grad_norm": 0.476499080657959,
      "learning_rate": 0.0003819434788277755,
      "loss": 0.0882,
      "step": 39720
    },
    {
      "epoch": 11.64419695193435,
      "grad_norm": 0.6402409076690674,
      "learning_rate": 0.0003818596825401871,
      "loss": 0.1026,
      "step": 39730
    },
    {
      "epoch": 11.647127784290738,
      "grad_norm": 0.7926609516143799,
      "learning_rate": 0.0003817758862525987,
      "loss": 0.0722,
      "step": 39740
    },
    {
      "epoch": 11.650058616647128,
      "grad_norm": 1.5263886451721191,
      "learning_rate": 0.00038169208996501037,
      "loss": 0.0789,
      "step": 39750
    },
    {
      "epoch": 11.652989449003517,
      "grad_norm": 0.6324566602706909,
      "learning_rate": 0.000381608293677422,
      "loss": 0.0966,
      "step": 39760
    },
    {
      "epoch": 11.655920281359906,
      "grad_norm": 0.655081570148468,
      "learning_rate": 0.00038152449738983364,
      "loss": 0.0829,
      "step": 39770
    },
    {
      "epoch": 11.658851113716295,
      "grad_norm": 0.9779746532440186,
      "learning_rate": 0.00038144070110224525,
      "loss": 0.0862,
      "step": 39780
    },
    {
      "epoch": 11.661781946072685,
      "grad_norm": 0.31212204694747925,
      "learning_rate": 0.00038135690481465686,
      "loss": 0.0966,
      "step": 39790
    },
    {
      "epoch": 11.664712778429074,
      "grad_norm": 1.1414878368377686,
      "learning_rate": 0.0003812731085270685,
      "loss": 0.0646,
      "step": 39800
    },
    {
      "epoch": 11.667643610785463,
      "grad_norm": 0.8960707783699036,
      "learning_rate": 0.00038118931223948013,
      "loss": 0.0985,
      "step": 39810
    },
    {
      "epoch": 11.670574443141852,
      "grad_norm": 1.4637889862060547,
      "learning_rate": 0.0003811055159518918,
      "loss": 0.0874,
      "step": 39820
    },
    {
      "epoch": 11.673505275498242,
      "grad_norm": 0.6620395183563232,
      "learning_rate": 0.0003810217196643034,
      "loss": 0.0854,
      "step": 39830
    },
    {
      "epoch": 11.67643610785463,
      "grad_norm": 1.0027532577514648,
      "learning_rate": 0.000380937923376715,
      "loss": 0.0844,
      "step": 39840
    },
    {
      "epoch": 11.67936694021102,
      "grad_norm": 0.5854695439338684,
      "learning_rate": 0.0003808541270891267,
      "loss": 0.0865,
      "step": 39850
    },
    {
      "epoch": 11.68229777256741,
      "grad_norm": 0.5036752820014954,
      "learning_rate": 0.0003807703308015383,
      "loss": 0.0844,
      "step": 39860
    },
    {
      "epoch": 11.685228604923799,
      "grad_norm": 0.22498813271522522,
      "learning_rate": 0.00038068653451395,
      "loss": 0.0787,
      "step": 39870
    },
    {
      "epoch": 11.688159437280188,
      "grad_norm": 0.35420340299606323,
      "learning_rate": 0.00038060273822636156,
      "loss": 0.0734,
      "step": 39880
    },
    {
      "epoch": 11.691090269636577,
      "grad_norm": 1.6224522590637207,
      "learning_rate": 0.0003805189419387732,
      "loss": 0.108,
      "step": 39890
    },
    {
      "epoch": 11.694021101992966,
      "grad_norm": 0.7692846059799194,
      "learning_rate": 0.00038043514565118484,
      "loss": 0.0764,
      "step": 39900
    },
    {
      "epoch": 11.696951934349356,
      "grad_norm": 1.3109829425811768,
      "learning_rate": 0.00038035134936359645,
      "loss": 0.0679,
      "step": 39910
    },
    {
      "epoch": 11.699882766705745,
      "grad_norm": 0.8751679062843323,
      "learning_rate": 0.00038026755307600817,
      "loss": 0.1005,
      "step": 39920
    },
    {
      "epoch": 11.702813599062134,
      "grad_norm": 1.0293893814086914,
      "learning_rate": 0.0003801837567884197,
      "loss": 0.0623,
      "step": 39930
    },
    {
      "epoch": 11.705744431418523,
      "grad_norm": 0.6142060160636902,
      "learning_rate": 0.00038009996050083144,
      "loss": 0.0588,
      "step": 39940
    },
    {
      "epoch": 11.708675263774913,
      "grad_norm": 1.3838434219360352,
      "learning_rate": 0.00038001616421324305,
      "loss": 0.0884,
      "step": 39950
    },
    {
      "epoch": 11.711606096131302,
      "grad_norm": 0.9782263040542603,
      "learning_rate": 0.0003799323679256546,
      "loss": 0.0876,
      "step": 39960
    },
    {
      "epoch": 11.714536928487691,
      "grad_norm": 0.6971427202224731,
      "learning_rate": 0.0003798485716380663,
      "loss": 0.0785,
      "step": 39970
    },
    {
      "epoch": 11.71746776084408,
      "grad_norm": 0.5541355609893799,
      "learning_rate": 0.00037976477535047793,
      "loss": 0.0829,
      "step": 39980
    },
    {
      "epoch": 11.72039859320047,
      "grad_norm": 1.0080796480178833,
      "learning_rate": 0.0003796809790628896,
      "loss": 0.0778,
      "step": 39990
    },
    {
      "epoch": 11.723329425556859,
      "grad_norm": 0.6726669669151306,
      "learning_rate": 0.0003795971827753012,
      "loss": 0.0924,
      "step": 40000
    },
    {
      "epoch": 11.726260257913248,
      "grad_norm": 0.8492518663406372,
      "learning_rate": 0.00037951338648771276,
      "loss": 0.0965,
      "step": 40010
    },
    {
      "epoch": 11.729191090269637,
      "grad_norm": 1.7464886903762817,
      "learning_rate": 0.0003794295902001245,
      "loss": 0.0703,
      "step": 40020
    },
    {
      "epoch": 11.732121922626026,
      "grad_norm": 0.5206696391105652,
      "learning_rate": 0.0003793457939125361,
      "loss": 0.0859,
      "step": 40030
    },
    {
      "epoch": 11.735052754982416,
      "grad_norm": 2.4840757846832275,
      "learning_rate": 0.00037926199762494775,
      "loss": 0.0885,
      "step": 40040
    },
    {
      "epoch": 11.737983587338805,
      "grad_norm": 0.8486203551292419,
      "learning_rate": 0.00037917820133735936,
      "loss": 0.0779,
      "step": 40050
    },
    {
      "epoch": 11.740914419695194,
      "grad_norm": 1.4208705425262451,
      "learning_rate": 0.00037909440504977097,
      "loss": 0.0866,
      "step": 40060
    },
    {
      "epoch": 11.743845252051583,
      "grad_norm": 0.9539158344268799,
      "learning_rate": 0.00037901060876218264,
      "loss": 0.051,
      "step": 40070
    },
    {
      "epoch": 11.746776084407973,
      "grad_norm": 0.4065465033054352,
      "learning_rate": 0.00037892681247459425,
      "loss": 0.1014,
      "step": 40080
    },
    {
      "epoch": 11.74970691676436,
      "grad_norm": 0.5313082933425903,
      "learning_rate": 0.0003788430161870059,
      "loss": 0.1061,
      "step": 40090
    },
    {
      "epoch": 11.752637749120751,
      "grad_norm": 1.6168848276138306,
      "learning_rate": 0.0003787592198994175,
      "loss": 0.0636,
      "step": 40100
    },
    {
      "epoch": 11.755568581477139,
      "grad_norm": 0.7671887278556824,
      "learning_rate": 0.0003786754236118292,
      "loss": 0.0733,
      "step": 40110
    },
    {
      "epoch": 11.758499413833528,
      "grad_norm": 1.1537108421325684,
      "learning_rate": 0.0003785916273242408,
      "loss": 0.0884,
      "step": 40120
    },
    {
      "epoch": 11.761430246189917,
      "grad_norm": 0.7800850868225098,
      "learning_rate": 0.0003785078310366524,
      "loss": 0.0893,
      "step": 40130
    },
    {
      "epoch": 11.764361078546306,
      "grad_norm": 1.1875171661376953,
      "learning_rate": 0.00037842403474906407,
      "loss": 0.0928,
      "step": 40140
    },
    {
      "epoch": 11.767291910902696,
      "grad_norm": 0.5475219488143921,
      "learning_rate": 0.0003783402384614757,
      "loss": 0.0933,
      "step": 40150
    },
    {
      "epoch": 11.770222743259085,
      "grad_norm": 0.5816083550453186,
      "learning_rate": 0.00037825644217388734,
      "loss": 0.0748,
      "step": 40160
    },
    {
      "epoch": 11.773153575615474,
      "grad_norm": 0.9677241444587708,
      "learning_rate": 0.00037817264588629895,
      "loss": 0.0941,
      "step": 40170
    },
    {
      "epoch": 11.776084407971863,
      "grad_norm": 0.29714882373809814,
      "learning_rate": 0.00037808884959871056,
      "loss": 0.0791,
      "step": 40180
    },
    {
      "epoch": 11.779015240328253,
      "grad_norm": 0.5141763687133789,
      "learning_rate": 0.0003780050533111222,
      "loss": 0.0544,
      "step": 40190
    },
    {
      "epoch": 11.781946072684642,
      "grad_norm": 1.0467416048049927,
      "learning_rate": 0.00037792125702353383,
      "loss": 0.0931,
      "step": 40200
    },
    {
      "epoch": 11.784876905041031,
      "grad_norm": 0.8606173992156982,
      "learning_rate": 0.0003778374607359455,
      "loss": 0.0847,
      "step": 40210
    },
    {
      "epoch": 11.78780773739742,
      "grad_norm": 0.7787474393844604,
      "learning_rate": 0.0003777536644483571,
      "loss": 0.0792,
      "step": 40220
    },
    {
      "epoch": 11.79073856975381,
      "grad_norm": 2.197489023208618,
      "learning_rate": 0.0003776698681607687,
      "loss": 0.0792,
      "step": 40230
    },
    {
      "epoch": 11.793669402110199,
      "grad_norm": 1.1758557558059692,
      "learning_rate": 0.0003775860718731804,
      "loss": 0.0741,
      "step": 40240
    },
    {
      "epoch": 11.796600234466588,
      "grad_norm": 0.8348797559738159,
      "learning_rate": 0.000377502275585592,
      "loss": 0.1182,
      "step": 40250
    },
    {
      "epoch": 11.799531066822977,
      "grad_norm": 1.1532849073410034,
      "learning_rate": 0.00037741847929800365,
      "loss": 0.0718,
      "step": 40260
    },
    {
      "epoch": 11.802461899179367,
      "grad_norm": 1.3141776323318481,
      "learning_rate": 0.00037733468301041526,
      "loss": 0.0732,
      "step": 40270
    },
    {
      "epoch": 11.805392731535756,
      "grad_norm": 1.4559924602508545,
      "learning_rate": 0.000377250886722827,
      "loss": 0.0897,
      "step": 40280
    },
    {
      "epoch": 11.808323563892145,
      "grad_norm": 0.7574300765991211,
      "learning_rate": 0.00037716709043523854,
      "loss": 0.1007,
      "step": 40290
    },
    {
      "epoch": 11.811254396248534,
      "grad_norm": 0.6472503542900085,
      "learning_rate": 0.00037708329414765015,
      "loss": 0.0623,
      "step": 40300
    },
    {
      "epoch": 11.814185228604924,
      "grad_norm": 0.5815964937210083,
      "learning_rate": 0.00037699949786006187,
      "loss": 0.0904,
      "step": 40310
    },
    {
      "epoch": 11.817116060961313,
      "grad_norm": 0.7531622052192688,
      "learning_rate": 0.0003769157015724734,
      "loss": 0.05,
      "step": 40320
    },
    {
      "epoch": 11.820046893317702,
      "grad_norm": 0.5411247611045837,
      "learning_rate": 0.00037683190528488514,
      "loss": 0.0765,
      "step": 40330
    },
    {
      "epoch": 11.822977725674091,
      "grad_norm": 2.8744566440582275,
      "learning_rate": 0.0003767481089972967,
      "loss": 0.0761,
      "step": 40340
    },
    {
      "epoch": 11.82590855803048,
      "grad_norm": 1.7218966484069824,
      "learning_rate": 0.0003766643127097083,
      "loss": 0.0735,
      "step": 40350
    },
    {
      "epoch": 11.82883939038687,
      "grad_norm": 0.9303174614906311,
      "learning_rate": 0.00037658051642212,
      "loss": 0.0967,
      "step": 40360
    },
    {
      "epoch": 11.831770222743259,
      "grad_norm": 1.0434328317642212,
      "learning_rate": 0.0003764967201345316,
      "loss": 0.1252,
      "step": 40370
    },
    {
      "epoch": 11.834701055099648,
      "grad_norm": 1.278348445892334,
      "learning_rate": 0.0003764129238469433,
      "loss": 0.0593,
      "step": 40380
    },
    {
      "epoch": 11.837631887456038,
      "grad_norm": 0.6459806561470032,
      "learning_rate": 0.0003763291275593549,
      "loss": 0.1018,
      "step": 40390
    },
    {
      "epoch": 11.840562719812427,
      "grad_norm": 0.5867520570755005,
      "learning_rate": 0.00037624533127176646,
      "loss": 0.0597,
      "step": 40400
    },
    {
      "epoch": 11.843493552168816,
      "grad_norm": 1.7441524267196655,
      "learning_rate": 0.0003761615349841782,
      "loss": 0.1117,
      "step": 40410
    },
    {
      "epoch": 11.846424384525205,
      "grad_norm": 0.4054938852787018,
      "learning_rate": 0.0003760777386965898,
      "loss": 0.0712,
      "step": 40420
    },
    {
      "epoch": 11.849355216881595,
      "grad_norm": 0.7049461007118225,
      "learning_rate": 0.00037599394240900145,
      "loss": 0.0743,
      "step": 40430
    },
    {
      "epoch": 11.852286049237984,
      "grad_norm": 0.6798675656318665,
      "learning_rate": 0.00037591014612141306,
      "loss": 0.0825,
      "step": 40440
    },
    {
      "epoch": 11.855216881594373,
      "grad_norm": 1.9420194625854492,
      "learning_rate": 0.0003758263498338247,
      "loss": 0.0793,
      "step": 40450
    },
    {
      "epoch": 11.858147713950762,
      "grad_norm": 1.6127393245697021,
      "learning_rate": 0.00037574255354623634,
      "loss": 0.0933,
      "step": 40460
    },
    {
      "epoch": 11.861078546307152,
      "grad_norm": 0.9102336764335632,
      "learning_rate": 0.00037565875725864795,
      "loss": 0.0662,
      "step": 40470
    },
    {
      "epoch": 11.86400937866354,
      "grad_norm": 1.4898288249969482,
      "learning_rate": 0.0003755749609710596,
      "loss": 0.1283,
      "step": 40480
    },
    {
      "epoch": 11.86694021101993,
      "grad_norm": 1.7335811853408813,
      "learning_rate": 0.0003754911646834712,
      "loss": 0.09,
      "step": 40490
    },
    {
      "epoch": 11.86987104337632,
      "grad_norm": 2.023230791091919,
      "learning_rate": 0.0003754073683958829,
      "loss": 0.0883,
      "step": 40500
    },
    {
      "epoch": 11.872801875732709,
      "grad_norm": 1.0983237028121948,
      "learning_rate": 0.0003753235721082945,
      "loss": 0.0778,
      "step": 40510
    },
    {
      "epoch": 11.875732708089098,
      "grad_norm": 0.9518341422080994,
      "learning_rate": 0.0003752397758207061,
      "loss": 0.0697,
      "step": 40520
    },
    {
      "epoch": 11.878663540445487,
      "grad_norm": 0.6885650157928467,
      "learning_rate": 0.00037515597953311777,
      "loss": 0.1036,
      "step": 40530
    },
    {
      "epoch": 11.881594372801876,
      "grad_norm": 0.8370291590690613,
      "learning_rate": 0.0003750721832455294,
      "loss": 0.0871,
      "step": 40540
    },
    {
      "epoch": 11.884525205158265,
      "grad_norm": 2.369356870651245,
      "learning_rate": 0.00037498838695794104,
      "loss": 0.0822,
      "step": 40550
    },
    {
      "epoch": 11.887456037514655,
      "grad_norm": 0.37824055552482605,
      "learning_rate": 0.00037490459067035265,
      "loss": 0.0861,
      "step": 40560
    },
    {
      "epoch": 11.890386869871044,
      "grad_norm": 2.037071704864502,
      "learning_rate": 0.00037482079438276426,
      "loss": 0.077,
      "step": 40570
    },
    {
      "epoch": 11.893317702227433,
      "grad_norm": 1.122592568397522,
      "learning_rate": 0.0003747369980951759,
      "loss": 0.083,
      "step": 40580
    },
    {
      "epoch": 11.896248534583822,
      "grad_norm": 0.8369296789169312,
      "learning_rate": 0.00037465320180758753,
      "loss": 0.0725,
      "step": 40590
    },
    {
      "epoch": 11.899179366940212,
      "grad_norm": 1.534077763557434,
      "learning_rate": 0.0003745694055199992,
      "loss": 0.0821,
      "step": 40600
    },
    {
      "epoch": 11.902110199296601,
      "grad_norm": 2.457280397415161,
      "learning_rate": 0.0003744856092324108,
      "loss": 0.0713,
      "step": 40610
    },
    {
      "epoch": 11.90504103165299,
      "grad_norm": 0.9762880206108093,
      "learning_rate": 0.00037440181294482247,
      "loss": 0.0835,
      "step": 40620
    },
    {
      "epoch": 11.90797186400938,
      "grad_norm": 0.7131158113479614,
      "learning_rate": 0.0003743180166572341,
      "loss": 0.0726,
      "step": 40630
    },
    {
      "epoch": 11.910902696365769,
      "grad_norm": 1.3049874305725098,
      "learning_rate": 0.0003742342203696457,
      "loss": 0.0984,
      "step": 40640
    },
    {
      "epoch": 11.913833528722158,
      "grad_norm": 0.9242156147956848,
      "learning_rate": 0.00037415042408205735,
      "loss": 0.0823,
      "step": 40650
    },
    {
      "epoch": 11.916764361078545,
      "grad_norm": 1.5066866874694824,
      "learning_rate": 0.00037406662779446896,
      "loss": 0.0738,
      "step": 40660
    },
    {
      "epoch": 11.919695193434936,
      "grad_norm": 1.099739909172058,
      "learning_rate": 0.00037398283150688063,
      "loss": 0.0729,
      "step": 40670
    },
    {
      "epoch": 11.922626025791324,
      "grad_norm": 0.7569246888160706,
      "learning_rate": 0.00037389903521929224,
      "loss": 0.0785,
      "step": 40680
    },
    {
      "epoch": 11.925556858147713,
      "grad_norm": 1.1100594997406006,
      "learning_rate": 0.00037381523893170385,
      "loss": 0.072,
      "step": 40690
    },
    {
      "epoch": 11.928487690504102,
      "grad_norm": 0.13407008349895477,
      "learning_rate": 0.0003737314426441155,
      "loss": 0.0736,
      "step": 40700
    },
    {
      "epoch": 11.931418522860492,
      "grad_norm": 1.6359128952026367,
      "learning_rate": 0.0003736476463565271,
      "loss": 0.0683,
      "step": 40710
    },
    {
      "epoch": 11.934349355216881,
      "grad_norm": 1.4970471858978271,
      "learning_rate": 0.00037356385006893884,
      "loss": 0.0804,
      "step": 40720
    },
    {
      "epoch": 11.93728018757327,
      "grad_norm": 0.5409656167030334,
      "learning_rate": 0.0003734800537813504,
      "loss": 0.0563,
      "step": 40730
    },
    {
      "epoch": 11.94021101992966,
      "grad_norm": 0.9342430233955383,
      "learning_rate": 0.000373396257493762,
      "loss": 0.0837,
      "step": 40740
    },
    {
      "epoch": 11.943141852286049,
      "grad_norm": 1.4898459911346436,
      "learning_rate": 0.0003733124612061737,
      "loss": 0.0844,
      "step": 40750
    },
    {
      "epoch": 11.946072684642438,
      "grad_norm": 1.1460055112838745,
      "learning_rate": 0.0003732286649185853,
      "loss": 0.063,
      "step": 40760
    },
    {
      "epoch": 11.949003516998827,
      "grad_norm": 0.8370513319969177,
      "learning_rate": 0.000373144868630997,
      "loss": 0.0766,
      "step": 40770
    },
    {
      "epoch": 11.951934349355216,
      "grad_norm": 1.3715523481369019,
      "learning_rate": 0.00037306107234340855,
      "loss": 0.0671,
      "step": 40780
    },
    {
      "epoch": 11.954865181711606,
      "grad_norm": 0.3763619065284729,
      "learning_rate": 0.00037297727605582027,
      "loss": 0.0679,
      "step": 40790
    },
    {
      "epoch": 11.957796014067995,
      "grad_norm": 0.8268985748291016,
      "learning_rate": 0.0003728934797682319,
      "loss": 0.0775,
      "step": 40800
    },
    {
      "epoch": 11.960726846424384,
      "grad_norm": 0.9327749013900757,
      "learning_rate": 0.00037280968348064343,
      "loss": 0.0857,
      "step": 40810
    },
    {
      "epoch": 11.963657678780773,
      "grad_norm": 1.1164183616638184,
      "learning_rate": 0.00037272588719305515,
      "loss": 0.0769,
      "step": 40820
    },
    {
      "epoch": 11.966588511137163,
      "grad_norm": 0.546905517578125,
      "learning_rate": 0.00037264209090546676,
      "loss": 0.0731,
      "step": 40830
    },
    {
      "epoch": 11.969519343493552,
      "grad_norm": 1.119910717010498,
      "learning_rate": 0.0003725582946178784,
      "loss": 0.1055,
      "step": 40840
    },
    {
      "epoch": 11.972450175849941,
      "grad_norm": 0.6810550093650818,
      "learning_rate": 0.00037247449833029004,
      "loss": 0.0607,
      "step": 40850
    },
    {
      "epoch": 11.97538100820633,
      "grad_norm": 0.8832672238349915,
      "learning_rate": 0.00037239070204270165,
      "loss": 0.0785,
      "step": 40860
    },
    {
      "epoch": 11.97831184056272,
      "grad_norm": 0.9846261739730835,
      "learning_rate": 0.0003723069057551133,
      "loss": 0.1019,
      "step": 40870
    },
    {
      "epoch": 11.981242672919109,
      "grad_norm": 1.3703688383102417,
      "learning_rate": 0.0003722231094675249,
      "loss": 0.0959,
      "step": 40880
    },
    {
      "epoch": 11.984173505275498,
      "grad_norm": 1.324971318244934,
      "learning_rate": 0.0003721393131799366,
      "loss": 0.0817,
      "step": 40890
    },
    {
      "epoch": 11.987104337631887,
      "grad_norm": 0.9961720705032349,
      "learning_rate": 0.0003720555168923482,
      "loss": 0.05,
      "step": 40900
    },
    {
      "epoch": 11.990035169988277,
      "grad_norm": 1.161679744720459,
      "learning_rate": 0.0003719717206047598,
      "loss": 0.097,
      "step": 40910
    },
    {
      "epoch": 11.992966002344666,
      "grad_norm": 0.9956310391426086,
      "learning_rate": 0.00037188792431717147,
      "loss": 0.1235,
      "step": 40920
    },
    {
      "epoch": 11.995896834701055,
      "grad_norm": 0.7878453731536865,
      "learning_rate": 0.0003718041280295831,
      "loss": 0.0569,
      "step": 40930
    },
    {
      "epoch": 11.998827667057444,
      "grad_norm": 0.6149082779884338,
      "learning_rate": 0.00037172033174199474,
      "loss": 0.0827,
      "step": 40940
    },
    {
      "epoch": 12.0,
      "eval_accuracy": 0.7045829212001319,
      "eval_f1_macro": 0.7255874013547279,
      "eval_f1_micro": 0.7814837905236908,
      "eval_f1_weighted": 0.7676709955056473,
      "eval_loss": 0.096104197204113,
      "eval_roc_auc": 0.8595038155707716,
      "eval_runtime": 241.5489,
      "eval_samples_per_second": 12.556,
      "eval_steps_per_second": 1.573,
      "step": 40944
    },
    {
      "epoch": 12.001758499413834,
      "grad_norm": 1.0070736408233643,
      "learning_rate": 0.00037163653545440635,
      "loss": 0.1048,
      "step": 40950
    },
    {
      "epoch": 12.004689331770223,
      "grad_norm": 0.6965979337692261,
      "learning_rate": 0.00037155273916681796,
      "loss": 0.0737,
      "step": 40960
    },
    {
      "epoch": 12.007620164126612,
      "grad_norm": 1.0938804149627686,
      "learning_rate": 0.0003714689428792296,
      "loss": 0.0969,
      "step": 40970
    },
    {
      "epoch": 12.010550996483001,
      "grad_norm": 0.4612523019313812,
      "learning_rate": 0.00037138514659164123,
      "loss": 0.0921,
      "step": 40980
    },
    {
      "epoch": 12.01348182883939,
      "grad_norm": 0.7769013047218323,
      "learning_rate": 0.0003713013503040529,
      "loss": 0.0906,
      "step": 40990
    },
    {
      "epoch": 12.01641266119578,
      "grad_norm": 0.5746021270751953,
      "learning_rate": 0.0003712175540164645,
      "loss": 0.0838,
      "step": 41000
    },
    {
      "epoch": 12.019343493552169,
      "grad_norm": 0.5177304744720459,
      "learning_rate": 0.00037113375772887617,
      "loss": 0.0566,
      "step": 41010
    },
    {
      "epoch": 12.022274325908558,
      "grad_norm": 0.8260195851325989,
      "learning_rate": 0.0003710499614412878,
      "loss": 0.0716,
      "step": 41020
    },
    {
      "epoch": 12.025205158264948,
      "grad_norm": 0.9891316294670105,
      "learning_rate": 0.0003709661651536994,
      "loss": 0.0784,
      "step": 41030
    },
    {
      "epoch": 12.028135990621337,
      "grad_norm": 1.2870121002197266,
      "learning_rate": 0.00037088236886611105,
      "loss": 0.0853,
      "step": 41040
    },
    {
      "epoch": 12.031066822977726,
      "grad_norm": 0.4328422248363495,
      "learning_rate": 0.00037079857257852266,
      "loss": 0.0914,
      "step": 41050
    },
    {
      "epoch": 12.033997655334115,
      "grad_norm": 0.9939775466918945,
      "learning_rate": 0.00037071477629093433,
      "loss": 0.061,
      "step": 41060
    },
    {
      "epoch": 12.036928487690504,
      "grad_norm": 0.8911651372909546,
      "learning_rate": 0.00037063098000334594,
      "loss": 0.0673,
      "step": 41070
    },
    {
      "epoch": 12.039859320046894,
      "grad_norm": 1.0259971618652344,
      "learning_rate": 0.00037054718371575755,
      "loss": 0.09,
      "step": 41080
    },
    {
      "epoch": 12.042790152403283,
      "grad_norm": 0.4438246488571167,
      "learning_rate": 0.0003704633874281692,
      "loss": 0.0802,
      "step": 41090
    },
    {
      "epoch": 12.045720984759672,
      "grad_norm": 0.9351861476898193,
      "learning_rate": 0.0003703795911405808,
      "loss": 0.0892,
      "step": 41100
    },
    {
      "epoch": 12.048651817116061,
      "grad_norm": 1.0485895872116089,
      "learning_rate": 0.0003702957948529925,
      "loss": 0.063,
      "step": 41110
    },
    {
      "epoch": 12.05158264947245,
      "grad_norm": 1.9756929874420166,
      "learning_rate": 0.0003702119985654041,
      "loss": 0.0634,
      "step": 41120
    },
    {
      "epoch": 12.05451348182884,
      "grad_norm": 1.2703937292099,
      "learning_rate": 0.0003701282022778157,
      "loss": 0.0547,
      "step": 41130
    },
    {
      "epoch": 12.05744431418523,
      "grad_norm": 1.0811560153961182,
      "learning_rate": 0.00037004440599022737,
      "loss": 0.0841,
      "step": 41140
    },
    {
      "epoch": 12.060375146541618,
      "grad_norm": 1.232881784439087,
      "learning_rate": 0.000369960609702639,
      "loss": 0.1228,
      "step": 41150
    },
    {
      "epoch": 12.063305978898008,
      "grad_norm": 1.359184741973877,
      "learning_rate": 0.0003698768134150507,
      "loss": 0.063,
      "step": 41160
    },
    {
      "epoch": 12.066236811254397,
      "grad_norm": 1.3533871173858643,
      "learning_rate": 0.00036979301712746225,
      "loss": 0.094,
      "step": 41170
    },
    {
      "epoch": 12.069167643610786,
      "grad_norm": 1.1431283950805664,
      "learning_rate": 0.00036970922083987397,
      "loss": 0.0806,
      "step": 41180
    },
    {
      "epoch": 12.072098475967175,
      "grad_norm": 0.20802606642246246,
      "learning_rate": 0.0003696254245522856,
      "loss": 0.069,
      "step": 41190
    },
    {
      "epoch": 12.075029308323565,
      "grad_norm": 0.537418782711029,
      "learning_rate": 0.00036954162826469713,
      "loss": 0.0681,
      "step": 41200
    },
    {
      "epoch": 12.077960140679954,
      "grad_norm": 0.7725688219070435,
      "learning_rate": 0.00036945783197710885,
      "loss": 0.0814,
      "step": 41210
    },
    {
      "epoch": 12.080890973036343,
      "grad_norm": 0.8224421739578247,
      "learning_rate": 0.0003693740356895204,
      "loss": 0.0565,
      "step": 41220
    },
    {
      "epoch": 12.08382180539273,
      "grad_norm": 0.791425883769989,
      "learning_rate": 0.0003692902394019321,
      "loss": 0.0598,
      "step": 41230
    },
    {
      "epoch": 12.08675263774912,
      "grad_norm": 0.8000580668449402,
      "learning_rate": 0.00036920644311434374,
      "loss": 0.0803,
      "step": 41240
    },
    {
      "epoch": 12.08968347010551,
      "grad_norm": 0.8032628297805786,
      "learning_rate": 0.0003691226468267553,
      "loss": 0.0966,
      "step": 41250
    },
    {
      "epoch": 12.092614302461898,
      "grad_norm": 0.5418322682380676,
      "learning_rate": 0.000369038850539167,
      "loss": 0.0872,
      "step": 41260
    },
    {
      "epoch": 12.095545134818288,
      "grad_norm": 0.6055595278739929,
      "learning_rate": 0.0003689550542515786,
      "loss": 0.0774,
      "step": 41270
    },
    {
      "epoch": 12.098475967174677,
      "grad_norm": 0.6959083676338196,
      "learning_rate": 0.0003688712579639903,
      "loss": 0.0624,
      "step": 41280
    },
    {
      "epoch": 12.101406799531066,
      "grad_norm": 0.5411456823348999,
      "learning_rate": 0.0003687874616764019,
      "loss": 0.0565,
      "step": 41290
    },
    {
      "epoch": 12.104337631887455,
      "grad_norm": 1.0767375230789185,
      "learning_rate": 0.0003687036653888135,
      "loss": 0.0988,
      "step": 41300
    },
    {
      "epoch": 12.107268464243845,
      "grad_norm": 0.8461548686027527,
      "learning_rate": 0.00036861986910122517,
      "loss": 0.0635,
      "step": 41310
    },
    {
      "epoch": 12.110199296600234,
      "grad_norm": 0.10152643918991089,
      "learning_rate": 0.0003685360728136368,
      "loss": 0.0472,
      "step": 41320
    },
    {
      "epoch": 12.113130128956623,
      "grad_norm": 1.6586579084396362,
      "learning_rate": 0.00036845227652604844,
      "loss": 0.0941,
      "step": 41330
    },
    {
      "epoch": 12.116060961313012,
      "grad_norm": 1.4253356456756592,
      "learning_rate": 0.00036836848023846005,
      "loss": 0.0833,
      "step": 41340
    },
    {
      "epoch": 12.118991793669402,
      "grad_norm": 2.03222918510437,
      "learning_rate": 0.0003682846839508717,
      "loss": 0.0918,
      "step": 41350
    },
    {
      "epoch": 12.12192262602579,
      "grad_norm": 0.8348592519760132,
      "learning_rate": 0.0003682008876632833,
      "loss": 0.0995,
      "step": 41360
    },
    {
      "epoch": 12.12485345838218,
      "grad_norm": 0.7380641102790833,
      "learning_rate": 0.00036811709137569493,
      "loss": 0.0522,
      "step": 41370
    },
    {
      "epoch": 12.12778429073857,
      "grad_norm": 0.6406126618385315,
      "learning_rate": 0.0003680332950881066,
      "loss": 0.0523,
      "step": 41380
    },
    {
      "epoch": 12.130715123094959,
      "grad_norm": 1.075445532798767,
      "learning_rate": 0.0003679494988005182,
      "loss": 0.0709,
      "step": 41390
    },
    {
      "epoch": 12.133645955451348,
      "grad_norm": 2.114906072616577,
      "learning_rate": 0.00036786570251292987,
      "loss": 0.0719,
      "step": 41400
    },
    {
      "epoch": 12.136576787807737,
      "grad_norm": 0.8964717388153076,
      "learning_rate": 0.0003677819062253415,
      "loss": 0.0952,
      "step": 41410
    },
    {
      "epoch": 12.139507620164126,
      "grad_norm": 0.9705002307891846,
      "learning_rate": 0.0003676981099377531,
      "loss": 0.0718,
      "step": 41420
    },
    {
      "epoch": 12.142438452520516,
      "grad_norm": 0.8203460574150085,
      "learning_rate": 0.00036761431365016475,
      "loss": 0.0593,
      "step": 41430
    },
    {
      "epoch": 12.145369284876905,
      "grad_norm": 0.8190548419952393,
      "learning_rate": 0.00036753051736257636,
      "loss": 0.0904,
      "step": 41440
    },
    {
      "epoch": 12.148300117233294,
      "grad_norm": 0.9874598383903503,
      "learning_rate": 0.00036744672107498803,
      "loss": 0.0669,
      "step": 41450
    },
    {
      "epoch": 12.151230949589683,
      "grad_norm": 0.7606928944587708,
      "learning_rate": 0.00036736292478739964,
      "loss": 0.0512,
      "step": 41460
    },
    {
      "epoch": 12.154161781946073,
      "grad_norm": 0.6298550963401794,
      "learning_rate": 0.00036727912849981125,
      "loss": 0.0824,
      "step": 41470
    },
    {
      "epoch": 12.157092614302462,
      "grad_norm": 1.0586276054382324,
      "learning_rate": 0.0003671953322122229,
      "loss": 0.0729,
      "step": 41480
    },
    {
      "epoch": 12.160023446658851,
      "grad_norm": 2.08359956741333,
      "learning_rate": 0.0003671115359246345,
      "loss": 0.0871,
      "step": 41490
    },
    {
      "epoch": 12.16295427901524,
      "grad_norm": 1.2614110708236694,
      "learning_rate": 0.0003670277396370462,
      "loss": 0.0793,
      "step": 41500
    },
    {
      "epoch": 12.16588511137163,
      "grad_norm": 1.0534298419952393,
      "learning_rate": 0.0003669439433494578,
      "loss": 0.0592,
      "step": 41510
    },
    {
      "epoch": 12.168815943728019,
      "grad_norm": 0.5118783712387085,
      "learning_rate": 0.0003668601470618695,
      "loss": 0.0641,
      "step": 41520
    },
    {
      "epoch": 12.171746776084408,
      "grad_norm": 1.3080711364746094,
      "learning_rate": 0.00036677635077428107,
      "loss": 0.0785,
      "step": 41530
    },
    {
      "epoch": 12.174677608440797,
      "grad_norm": 1.2974488735198975,
      "learning_rate": 0.0003666925544866927,
      "loss": 0.0716,
      "step": 41540
    },
    {
      "epoch": 12.177608440797187,
      "grad_norm": 0.8602575659751892,
      "learning_rate": 0.00036660875819910434,
      "loss": 0.0838,
      "step": 41550
    },
    {
      "epoch": 12.180539273153576,
      "grad_norm": 2.4740560054779053,
      "learning_rate": 0.00036652496191151595,
      "loss": 0.0903,
      "step": 41560
    },
    {
      "epoch": 12.183470105509965,
      "grad_norm": 0.688978374004364,
      "learning_rate": 0.00036644116562392767,
      "loss": 0.057,
      "step": 41570
    },
    {
      "epoch": 12.186400937866354,
      "grad_norm": 1.5922662019729614,
      "learning_rate": 0.0003663573693363392,
      "loss": 0.0803,
      "step": 41580
    },
    {
      "epoch": 12.189331770222744,
      "grad_norm": 1.1565320491790771,
      "learning_rate": 0.00036627357304875083,
      "loss": 0.0612,
      "step": 41590
    },
    {
      "epoch": 12.192262602579133,
      "grad_norm": 0.2612370550632477,
      "learning_rate": 0.00036618977676116255,
      "loss": 0.0835,
      "step": 41600
    },
    {
      "epoch": 12.195193434935522,
      "grad_norm": 0.7054997086524963,
      "learning_rate": 0.0003661059804735741,
      "loss": 0.0649,
      "step": 41610
    },
    {
      "epoch": 12.198124267291911,
      "grad_norm": 0.892187774181366,
      "learning_rate": 0.0003660221841859858,
      "loss": 0.0757,
      "step": 41620
    },
    {
      "epoch": 12.2010550996483,
      "grad_norm": 0.7699958086013794,
      "learning_rate": 0.00036593838789839744,
      "loss": 0.0831,
      "step": 41630
    },
    {
      "epoch": 12.20398593200469,
      "grad_norm": 1.1148799657821655,
      "learning_rate": 0.000365854591610809,
      "loss": 0.077,
      "step": 41640
    },
    {
      "epoch": 12.206916764361079,
      "grad_norm": 0.54477459192276,
      "learning_rate": 0.0003657707953232207,
      "loss": 0.0893,
      "step": 41650
    },
    {
      "epoch": 12.209847596717468,
      "grad_norm": 1.782918930053711,
      "learning_rate": 0.00036568699903563226,
      "loss": 0.0781,
      "step": 41660
    },
    {
      "epoch": 12.212778429073857,
      "grad_norm": 1.2299561500549316,
      "learning_rate": 0.000365603202748044,
      "loss": 0.0743,
      "step": 41670
    },
    {
      "epoch": 12.215709261430247,
      "grad_norm": 0.6456766128540039,
      "learning_rate": 0.0003655194064604556,
      "loss": 0.0904,
      "step": 41680
    },
    {
      "epoch": 12.218640093786636,
      "grad_norm": 1.417923092842102,
      "learning_rate": 0.00036543561017286726,
      "loss": 0.0776,
      "step": 41690
    },
    {
      "epoch": 12.221570926143025,
      "grad_norm": 0.20900312066078186,
      "learning_rate": 0.00036535181388527887,
      "loss": 0.0664,
      "step": 41700
    },
    {
      "epoch": 12.224501758499414,
      "grad_norm": 1.3511604070663452,
      "learning_rate": 0.0003652680175976905,
      "loss": 0.0685,
      "step": 41710
    },
    {
      "epoch": 12.227432590855804,
      "grad_norm": 0.45477956533432007,
      "learning_rate": 0.00036518422131010214,
      "loss": 0.0682,
      "step": 41720
    },
    {
      "epoch": 12.230363423212193,
      "grad_norm": 0.9028406739234924,
      "learning_rate": 0.00036510042502251375,
      "loss": 0.0654,
      "step": 41730
    },
    {
      "epoch": 12.233294255568582,
      "grad_norm": 0.5748477578163147,
      "learning_rate": 0.0003650166287349254,
      "loss": 0.0728,
      "step": 41740
    },
    {
      "epoch": 12.236225087924971,
      "grad_norm": 1.9798998832702637,
      "learning_rate": 0.000364932832447337,
      "loss": 0.0904,
      "step": 41750
    },
    {
      "epoch": 12.23915592028136,
      "grad_norm": 1.1032675504684448,
      "learning_rate": 0.00036484903615974863,
      "loss": 0.0628,
      "step": 41760
    },
    {
      "epoch": 12.24208675263775,
      "grad_norm": 0.6291766166687012,
      "learning_rate": 0.0003647652398721603,
      "loss": 0.0908,
      "step": 41770
    },
    {
      "epoch": 12.24501758499414,
      "grad_norm": 0.8443151712417603,
      "learning_rate": 0.0003646814435845719,
      "loss": 0.0873,
      "step": 41780
    },
    {
      "epoch": 12.247948417350528,
      "grad_norm": 1.2799707651138306,
      "learning_rate": 0.00036459764729698357,
      "loss": 0.0665,
      "step": 41790
    },
    {
      "epoch": 12.250879249706916,
      "grad_norm": 0.6531924605369568,
      "learning_rate": 0.0003645138510093952,
      "loss": 0.0713,
      "step": 41800
    },
    {
      "epoch": 12.253810082063305,
      "grad_norm": 0.9483030438423157,
      "learning_rate": 0.0003644300547218068,
      "loss": 0.089,
      "step": 41810
    },
    {
      "epoch": 12.256740914419694,
      "grad_norm": 0.7838935852050781,
      "learning_rate": 0.00036434625843421845,
      "loss": 0.085,
      "step": 41820
    },
    {
      "epoch": 12.259671746776084,
      "grad_norm": 1.7262146472930908,
      "learning_rate": 0.00036426246214663006,
      "loss": 0.0941,
      "step": 41830
    },
    {
      "epoch": 12.262602579132473,
      "grad_norm": 1.0854713916778564,
      "learning_rate": 0.00036417866585904173,
      "loss": 0.0789,
      "step": 41840
    },
    {
      "epoch": 12.265533411488862,
      "grad_norm": 1.1265357732772827,
      "learning_rate": 0.00036409486957145334,
      "loss": 0.0763,
      "step": 41850
    },
    {
      "epoch": 12.268464243845251,
      "grad_norm": 0.7614637017250061,
      "learning_rate": 0.000364011073283865,
      "loss": 0.0791,
      "step": 41860
    },
    {
      "epoch": 12.27139507620164,
      "grad_norm": 1.1358554363250732,
      "learning_rate": 0.0003639272769962766,
      "loss": 0.0814,
      "step": 41870
    },
    {
      "epoch": 12.27432590855803,
      "grad_norm": 0.27583202719688416,
      "learning_rate": 0.0003638434807086882,
      "loss": 0.0836,
      "step": 41880
    },
    {
      "epoch": 12.277256740914419,
      "grad_norm": 2.4268083572387695,
      "learning_rate": 0.0003637596844210999,
      "loss": 0.087,
      "step": 41890
    },
    {
      "epoch": 12.280187573270808,
      "grad_norm": 0.8036317825317383,
      "learning_rate": 0.0003636758881335115,
      "loss": 0.0711,
      "step": 41900
    },
    {
      "epoch": 12.283118405627198,
      "grad_norm": 0.43141716718673706,
      "learning_rate": 0.00036359209184592316,
      "loss": 0.0688,
      "step": 41910
    },
    {
      "epoch": 12.286049237983587,
      "grad_norm": 1.7056320905685425,
      "learning_rate": 0.00036350829555833477,
      "loss": 0.0904,
      "step": 41920
    },
    {
      "epoch": 12.288980070339976,
      "grad_norm": 1.364518404006958,
      "learning_rate": 0.0003634244992707464,
      "loss": 0.0799,
      "step": 41930
    },
    {
      "epoch": 12.291910902696365,
      "grad_norm": 0.8241984248161316,
      "learning_rate": 0.00036334070298315804,
      "loss": 0.0672,
      "step": 41940
    },
    {
      "epoch": 12.294841735052755,
      "grad_norm": 0.22661520540714264,
      "learning_rate": 0.00036325690669556965,
      "loss": 0.0709,
      "step": 41950
    },
    {
      "epoch": 12.297772567409144,
      "grad_norm": 1.4192097187042236,
      "learning_rate": 0.00036317311040798137,
      "loss": 0.0629,
      "step": 41960
    },
    {
      "epoch": 12.300703399765533,
      "grad_norm": 0.9126767516136169,
      "learning_rate": 0.0003630893141203929,
      "loss": 0.0773,
      "step": 41970
    },
    {
      "epoch": 12.303634232121922,
      "grad_norm": 1.6346904039382935,
      "learning_rate": 0.00036300551783280453,
      "loss": 0.084,
      "step": 41980
    },
    {
      "epoch": 12.306565064478312,
      "grad_norm": 0.7204248309135437,
      "learning_rate": 0.0003629217215452162,
      "loss": 0.0736,
      "step": 41990
    },
    {
      "epoch": 12.3094958968347,
      "grad_norm": 1.4591127634048462,
      "learning_rate": 0.0003628379252576278,
      "loss": 0.0924,
      "step": 42000
    },
    {
      "epoch": 12.31242672919109,
      "grad_norm": 1.4373104572296143,
      "learning_rate": 0.0003627541289700395,
      "loss": 0.0964,
      "step": 42010
    },
    {
      "epoch": 12.31535756154748,
      "grad_norm": 1.4825968742370605,
      "learning_rate": 0.0003626703326824511,
      "loss": 0.0676,
      "step": 42020
    },
    {
      "epoch": 12.318288393903869,
      "grad_norm": 1.1493364572525024,
      "learning_rate": 0.0003625865363948628,
      "loss": 0.0841,
      "step": 42030
    },
    {
      "epoch": 12.321219226260258,
      "grad_norm": 1.2514739036560059,
      "learning_rate": 0.0003625027401072744,
      "loss": 0.068,
      "step": 42040
    },
    {
      "epoch": 12.324150058616647,
      "grad_norm": 1.1157169342041016,
      "learning_rate": 0.00036241894381968596,
      "loss": 0.0762,
      "step": 42050
    },
    {
      "epoch": 12.327080890973036,
      "grad_norm": 1.1123658418655396,
      "learning_rate": 0.0003623351475320977,
      "loss": 0.0865,
      "step": 42060
    },
    {
      "epoch": 12.330011723329426,
      "grad_norm": 1.5023219585418701,
      "learning_rate": 0.0003622513512445093,
      "loss": 0.0805,
      "step": 42070
    },
    {
      "epoch": 12.332942555685815,
      "grad_norm": 0.9717211723327637,
      "learning_rate": 0.00036216755495692096,
      "loss": 0.0833,
      "step": 42080
    },
    {
      "epoch": 12.335873388042204,
      "grad_norm": 1.0611920356750488,
      "learning_rate": 0.00036208375866933257,
      "loss": 0.0958,
      "step": 42090
    },
    {
      "epoch": 12.338804220398593,
      "grad_norm": 1.4339174032211304,
      "learning_rate": 0.0003619999623817442,
      "loss": 0.104,
      "step": 42100
    },
    {
      "epoch": 12.341735052754983,
      "grad_norm": 1.4376442432403564,
      "learning_rate": 0.00036191616609415584,
      "loss": 0.0619,
      "step": 42110
    },
    {
      "epoch": 12.344665885111372,
      "grad_norm": 2.1534295082092285,
      "learning_rate": 0.00036183236980656745,
      "loss": 0.0659,
      "step": 42120
    },
    {
      "epoch": 12.347596717467761,
      "grad_norm": 0.7178403735160828,
      "learning_rate": 0.0003617485735189791,
      "loss": 0.0717,
      "step": 42130
    },
    {
      "epoch": 12.35052754982415,
      "grad_norm": 1.6479771137237549,
      "learning_rate": 0.0003616647772313907,
      "loss": 0.1126,
      "step": 42140
    },
    {
      "epoch": 12.35345838218054,
      "grad_norm": 0.6617552042007446,
      "learning_rate": 0.00036158098094380233,
      "loss": 0.0584,
      "step": 42150
    },
    {
      "epoch": 12.356389214536929,
      "grad_norm": 0.7862329483032227,
      "learning_rate": 0.000361497184656214,
      "loss": 0.0774,
      "step": 42160
    },
    {
      "epoch": 12.359320046893318,
      "grad_norm": 0.5400733351707458,
      "learning_rate": 0.0003614133883686256,
      "loss": 0.0703,
      "step": 42170
    },
    {
      "epoch": 12.362250879249707,
      "grad_norm": 0.4060472249984741,
      "learning_rate": 0.00036132959208103727,
      "loss": 0.0628,
      "step": 42180
    },
    {
      "epoch": 12.365181711606096,
      "grad_norm": 1.7755030393600464,
      "learning_rate": 0.0003612457957934489,
      "loss": 0.1025,
      "step": 42190
    },
    {
      "epoch": 12.368112543962486,
      "grad_norm": 1.2881782054901123,
      "learning_rate": 0.0003611619995058605,
      "loss": 0.0928,
      "step": 42200
    },
    {
      "epoch": 12.371043376318875,
      "grad_norm": 1.9108752012252808,
      "learning_rate": 0.00036107820321827215,
      "loss": 0.0725,
      "step": 42210
    },
    {
      "epoch": 12.373974208675264,
      "grad_norm": 0.8450276255607605,
      "learning_rate": 0.00036099440693068376,
      "loss": 0.0709,
      "step": 42220
    },
    {
      "epoch": 12.376905041031653,
      "grad_norm": 1.5274823904037476,
      "learning_rate": 0.00036091061064309543,
      "loss": 0.093,
      "step": 42230
    },
    {
      "epoch": 12.379835873388043,
      "grad_norm": 0.5354492664337158,
      "learning_rate": 0.00036082681435550704,
      "loss": 0.0575,
      "step": 42240
    },
    {
      "epoch": 12.382766705744432,
      "grad_norm": 1.030210018157959,
      "learning_rate": 0.0003607430180679187,
      "loss": 0.0633,
      "step": 42250
    },
    {
      "epoch": 12.385697538100821,
      "grad_norm": 0.7634880542755127,
      "learning_rate": 0.0003606592217803303,
      "loss": 0.0879,
      "step": 42260
    },
    {
      "epoch": 12.38862837045721,
      "grad_norm": 1.71654212474823,
      "learning_rate": 0.0003605754254927419,
      "loss": 0.0708,
      "step": 42270
    },
    {
      "epoch": 12.3915592028136,
      "grad_norm": 1.8885923624038696,
      "learning_rate": 0.0003604916292051536,
      "loss": 0.1026,
      "step": 42280
    },
    {
      "epoch": 12.394490035169989,
      "grad_norm": 0.17588631808757782,
      "learning_rate": 0.0003604078329175652,
      "loss": 0.0752,
      "step": 42290
    },
    {
      "epoch": 12.397420867526378,
      "grad_norm": 1.3308392763137817,
      "learning_rate": 0.00036032403662997686,
      "loss": 0.0955,
      "step": 42300
    },
    {
      "epoch": 12.400351699882767,
      "grad_norm": 0.6282235383987427,
      "learning_rate": 0.00036024024034238847,
      "loss": 0.0857,
      "step": 42310
    },
    {
      "epoch": 12.403282532239157,
      "grad_norm": 0.6723263263702393,
      "learning_rate": 0.0003601564440548001,
      "loss": 0.1016,
      "step": 42320
    },
    {
      "epoch": 12.406213364595546,
      "grad_norm": 0.5386276245117188,
      "learning_rate": 0.00036007264776721174,
      "loss": 0.09,
      "step": 42330
    },
    {
      "epoch": 12.409144196951935,
      "grad_norm": 1.1935135126113892,
      "learning_rate": 0.00035998885147962335,
      "loss": 0.0786,
      "step": 42340
    },
    {
      "epoch": 12.412075029308324,
      "grad_norm": 0.9539387822151184,
      "learning_rate": 0.000359905055192035,
      "loss": 0.0782,
      "step": 42350
    },
    {
      "epoch": 12.415005861664714,
      "grad_norm": 1.574603796005249,
      "learning_rate": 0.0003598212589044466,
      "loss": 0.0705,
      "step": 42360
    },
    {
      "epoch": 12.417936694021101,
      "grad_norm": 2.124171257019043,
      "learning_rate": 0.00035973746261685823,
      "loss": 0.0717,
      "step": 42370
    },
    {
      "epoch": 12.42086752637749,
      "grad_norm": 1.3494632244110107,
      "learning_rate": 0.0003596536663292699,
      "loss": 0.0756,
      "step": 42380
    },
    {
      "epoch": 12.42379835873388,
      "grad_norm": 0.8803706169128418,
      "learning_rate": 0.0003595698700416815,
      "loss": 0.1083,
      "step": 42390
    },
    {
      "epoch": 12.426729191090269,
      "grad_norm": 0.12106860429048538,
      "learning_rate": 0.0003594860737540932,
      "loss": 0.0663,
      "step": 42400
    },
    {
      "epoch": 12.429660023446658,
      "grad_norm": 1.8151596784591675,
      "learning_rate": 0.0003594022774665048,
      "loss": 0.0804,
      "step": 42410
    },
    {
      "epoch": 12.432590855803047,
      "grad_norm": 0.803385853767395,
      "learning_rate": 0.0003593184811789165,
      "loss": 0.0626,
      "step": 42420
    },
    {
      "epoch": 12.435521688159437,
      "grad_norm": 2.445939540863037,
      "learning_rate": 0.00035923468489132806,
      "loss": 0.0741,
      "step": 42430
    },
    {
      "epoch": 12.438452520515826,
      "grad_norm": 0.38475194573402405,
      "learning_rate": 0.00035915088860373966,
      "loss": 0.0709,
      "step": 42440
    },
    {
      "epoch": 12.441383352872215,
      "grad_norm": 0.9829689264297485,
      "learning_rate": 0.0003590670923161514,
      "loss": 0.0959,
      "step": 42450
    },
    {
      "epoch": 12.444314185228604,
      "grad_norm": 0.3234567940235138,
      "learning_rate": 0.00035898329602856294,
      "loss": 0.0867,
      "step": 42460
    },
    {
      "epoch": 12.447245017584994,
      "grad_norm": 0.7651566863059998,
      "learning_rate": 0.00035889949974097466,
      "loss": 0.0575,
      "step": 42470
    },
    {
      "epoch": 12.450175849941383,
      "grad_norm": 0.8611485362052917,
      "learning_rate": 0.00035881570345338627,
      "loss": 0.0716,
      "step": 42480
    },
    {
      "epoch": 12.453106682297772,
      "grad_norm": 1.320305585861206,
      "learning_rate": 0.0003587319071657978,
      "loss": 0.0731,
      "step": 42490
    },
    {
      "epoch": 12.456037514654161,
      "grad_norm": 1.0977163314819336,
      "learning_rate": 0.00035864811087820954,
      "loss": 0.0809,
      "step": 42500
    },
    {
      "epoch": 12.45896834701055,
      "grad_norm": 0.07222341001033783,
      "learning_rate": 0.00035856431459062115,
      "loss": 0.0773,
      "step": 42510
    },
    {
      "epoch": 12.46189917936694,
      "grad_norm": 0.5598056316375732,
      "learning_rate": 0.0003584805183030328,
      "loss": 0.1156,
      "step": 42520
    },
    {
      "epoch": 12.464830011723329,
      "grad_norm": 0.4203856587409973,
      "learning_rate": 0.0003583967220154444,
      "loss": 0.0915,
      "step": 42530
    },
    {
      "epoch": 12.467760844079718,
      "grad_norm": 0.6909830570220947,
      "learning_rate": 0.00035831292572785603,
      "loss": 0.061,
      "step": 42540
    },
    {
      "epoch": 12.470691676436108,
      "grad_norm": 0.737818717956543,
      "learning_rate": 0.0003582291294402677,
      "loss": 0.0715,
      "step": 42550
    },
    {
      "epoch": 12.473622508792497,
      "grad_norm": 1.5578877925872803,
      "learning_rate": 0.0003581453331526793,
      "loss": 0.056,
      "step": 42560
    },
    {
      "epoch": 12.476553341148886,
      "grad_norm": 1.7010953426361084,
      "learning_rate": 0.00035806153686509097,
      "loss": 0.0781,
      "step": 42570
    },
    {
      "epoch": 12.479484173505275,
      "grad_norm": 0.7005839943885803,
      "learning_rate": 0.0003579777405775026,
      "loss": 0.0571,
      "step": 42580
    },
    {
      "epoch": 12.482415005861665,
      "grad_norm": 0.23311465978622437,
      "learning_rate": 0.00035789394428991424,
      "loss": 0.0754,
      "step": 42590
    },
    {
      "epoch": 12.485345838218054,
      "grad_norm": 0.7904546856880188,
      "learning_rate": 0.00035781014800232585,
      "loss": 0.0758,
      "step": 42600
    },
    {
      "epoch": 12.488276670574443,
      "grad_norm": 0.6011090278625488,
      "learning_rate": 0.00035772635171473746,
      "loss": 0.0552,
      "step": 42610
    },
    {
      "epoch": 12.491207502930832,
      "grad_norm": 0.8238677382469177,
      "learning_rate": 0.00035764255542714913,
      "loss": 0.1059,
      "step": 42620
    },
    {
      "epoch": 12.494138335287222,
      "grad_norm": 0.8474045395851135,
      "learning_rate": 0.00035755875913956074,
      "loss": 0.1105,
      "step": 42630
    },
    {
      "epoch": 12.49706916764361,
      "grad_norm": 0.8055356740951538,
      "learning_rate": 0.0003574749628519724,
      "loss": 0.0804,
      "step": 42640
    },
    {
      "epoch": 12.5,
      "grad_norm": 1.7909932136535645,
      "learning_rate": 0.000357391166564384,
      "loss": 0.0518,
      "step": 42650
    },
    {
      "epoch": 12.50293083235639,
      "grad_norm": 2.4310550689697266,
      "learning_rate": 0.0003573073702767957,
      "loss": 0.0695,
      "step": 42660
    },
    {
      "epoch": 12.505861664712778,
      "grad_norm": 1.8193943500518799,
      "learning_rate": 0.0003572235739892073,
      "loss": 0.095,
      "step": 42670
    },
    {
      "epoch": 12.508792497069168,
      "grad_norm": 0.7320311665534973,
      "learning_rate": 0.0003571397777016189,
      "loss": 0.068,
      "step": 42680
    },
    {
      "epoch": 12.511723329425557,
      "grad_norm": 0.9810828566551208,
      "learning_rate": 0.00035705598141403056,
      "loss": 0.0587,
      "step": 42690
    },
    {
      "epoch": 12.514654161781946,
      "grad_norm": 1.1955451965332031,
      "learning_rate": 0.00035697218512644217,
      "loss": 0.0815,
      "step": 42700
    },
    {
      "epoch": 12.517584994138335,
      "grad_norm": 0.9573879837989807,
      "learning_rate": 0.00035688838883885383,
      "loss": 0.0839,
      "step": 42710
    },
    {
      "epoch": 12.520515826494725,
      "grad_norm": 0.657532274723053,
      "learning_rate": 0.00035680459255126544,
      "loss": 0.0857,
      "step": 42720
    },
    {
      "epoch": 12.523446658851114,
      "grad_norm": 1.3789361715316772,
      "learning_rate": 0.0003567207962636771,
      "loss": 0.0823,
      "step": 42730
    },
    {
      "epoch": 12.526377491207503,
      "grad_norm": 1.1881572008132935,
      "learning_rate": 0.0003566369999760887,
      "loss": 0.0737,
      "step": 42740
    },
    {
      "epoch": 12.529308323563892,
      "grad_norm": 1.357045292854309,
      "learning_rate": 0.0003565532036885003,
      "loss": 0.0737,
      "step": 42750
    },
    {
      "epoch": 12.532239155920282,
      "grad_norm": 1.6202281713485718,
      "learning_rate": 0.000356469407400912,
      "loss": 0.0728,
      "step": 42760
    },
    {
      "epoch": 12.535169988276671,
      "grad_norm": 0.9486150741577148,
      "learning_rate": 0.0003563856111133236,
      "loss": 0.0866,
      "step": 42770
    },
    {
      "epoch": 12.53810082063306,
      "grad_norm": 1.2421332597732544,
      "learning_rate": 0.00035630181482573526,
      "loss": 0.0942,
      "step": 42780
    },
    {
      "epoch": 12.54103165298945,
      "grad_norm": 0.6824812889099121,
      "learning_rate": 0.00035621801853814687,
      "loss": 0.054,
      "step": 42790
    },
    {
      "epoch": 12.543962485345839,
      "grad_norm": 1.071167230606079,
      "learning_rate": 0.0003561342222505585,
      "loss": 0.0641,
      "step": 42800
    },
    {
      "epoch": 12.546893317702228,
      "grad_norm": 0.4251355826854706,
      "learning_rate": 0.00035605042596297015,
      "loss": 0.0597,
      "step": 42810
    },
    {
      "epoch": 12.549824150058617,
      "grad_norm": 0.31955304741859436,
      "learning_rate": 0.00035596662967538175,
      "loss": 0.0673,
      "step": 42820
    },
    {
      "epoch": 12.552754982415006,
      "grad_norm": 0.9012231230735779,
      "learning_rate": 0.0003558828333877934,
      "loss": 0.0738,
      "step": 42830
    },
    {
      "epoch": 12.555685814771396,
      "grad_norm": 0.610735297203064,
      "learning_rate": 0.0003557990371002051,
      "loss": 0.0888,
      "step": 42840
    },
    {
      "epoch": 12.558616647127785,
      "grad_norm": 1.041164517402649,
      "learning_rate": 0.00035571524081261664,
      "loss": 0.0804,
      "step": 42850
    },
    {
      "epoch": 12.561547479484174,
      "grad_norm": 0.8479171395301819,
      "learning_rate": 0.0003556314445250283,
      "loss": 0.0777,
      "step": 42860
    },
    {
      "epoch": 12.564478311840563,
      "grad_norm": 1.103052020072937,
      "learning_rate": 0.00035554764823743997,
      "loss": 0.0797,
      "step": 42870
    },
    {
      "epoch": 12.567409144196953,
      "grad_norm": 0.667830765247345,
      "learning_rate": 0.0003554638519498516,
      "loss": 0.0612,
      "step": 42880
    },
    {
      "epoch": 12.570339976553342,
      "grad_norm": 0.7568936347961426,
      "learning_rate": 0.00035538005566226324,
      "loss": 0.1033,
      "step": 42890
    },
    {
      "epoch": 12.573270808909731,
      "grad_norm": 0.8894203305244446,
      "learning_rate": 0.00035529625937467485,
      "loss": 0.0795,
      "step": 42900
    },
    {
      "epoch": 12.57620164126612,
      "grad_norm": 1.3671215772628784,
      "learning_rate": 0.00035521246308708646,
      "loss": 0.0912,
      "step": 42910
    },
    {
      "epoch": 12.579132473622508,
      "grad_norm": 1.9982836246490479,
      "learning_rate": 0.0003551286667994981,
      "loss": 0.0905,
      "step": 42920
    },
    {
      "epoch": 12.582063305978899,
      "grad_norm": 0.7659293413162231,
      "learning_rate": 0.00035504487051190973,
      "loss": 0.0769,
      "step": 42930
    },
    {
      "epoch": 12.584994138335286,
      "grad_norm": 0.4416302740573883,
      "learning_rate": 0.0003549610742243214,
      "loss": 0.0653,
      "step": 42940
    },
    {
      "epoch": 12.587924970691677,
      "grad_norm": 0.32279321551322937,
      "learning_rate": 0.000354877277936733,
      "loss": 0.0631,
      "step": 42950
    },
    {
      "epoch": 12.590855803048065,
      "grad_norm": 1.051902413368225,
      "learning_rate": 0.0003547934816491446,
      "loss": 0.0578,
      "step": 42960
    },
    {
      "epoch": 12.593786635404454,
      "grad_norm": 0.5223501920700073,
      "learning_rate": 0.0003547096853615563,
      "loss": 0.0773,
      "step": 42970
    },
    {
      "epoch": 12.596717467760843,
      "grad_norm": 1.6405123472213745,
      "learning_rate": 0.0003546258890739679,
      "loss": 0.1253,
      "step": 42980
    },
    {
      "epoch": 12.599648300117233,
      "grad_norm": 0.7586284279823303,
      "learning_rate": 0.00035454209278637955,
      "loss": 0.0706,
      "step": 42990
    },
    {
      "epoch": 12.602579132473622,
      "grad_norm": 0.7813575863838196,
      "learning_rate": 0.00035445829649879116,
      "loss": 0.0752,
      "step": 43000
    },
    {
      "epoch": 12.605509964830011,
      "grad_norm": 0.5796694755554199,
      "learning_rate": 0.00035437450021120283,
      "loss": 0.062,
      "step": 43010
    },
    {
      "epoch": 12.6084407971864,
      "grad_norm": 0.3656146824359894,
      "learning_rate": 0.00035429070392361444,
      "loss": 0.0609,
      "step": 43020
    },
    {
      "epoch": 12.61137162954279,
      "grad_norm": 1.727433443069458,
      "learning_rate": 0.00035420690763602605,
      "loss": 0.0812,
      "step": 43030
    },
    {
      "epoch": 12.614302461899179,
      "grad_norm": 1.298703670501709,
      "learning_rate": 0.0003541231113484377,
      "loss": 0.0906,
      "step": 43040
    },
    {
      "epoch": 12.617233294255568,
      "grad_norm": 0.8416727185249329,
      "learning_rate": 0.0003540393150608493,
      "loss": 0.0782,
      "step": 43050
    },
    {
      "epoch": 12.620164126611957,
      "grad_norm": 0.9142130613327026,
      "learning_rate": 0.000353955518773261,
      "loss": 0.0574,
      "step": 43060
    },
    {
      "epoch": 12.623094958968347,
      "grad_norm": 1.5072569847106934,
      "learning_rate": 0.00035387172248567265,
      "loss": 0.0896,
      "step": 43070
    },
    {
      "epoch": 12.626025791324736,
      "grad_norm": 1.0525985956192017,
      "learning_rate": 0.0003537879261980842,
      "loss": 0.0775,
      "step": 43080
    },
    {
      "epoch": 12.628956623681125,
      "grad_norm": 0.93462735414505,
      "learning_rate": 0.00035370412991049587,
      "loss": 0.0661,
      "step": 43090
    },
    {
      "epoch": 12.631887456037514,
      "grad_norm": 0.8089906573295593,
      "learning_rate": 0.00035362033362290753,
      "loss": 0.0706,
      "step": 43100
    },
    {
      "epoch": 12.634818288393904,
      "grad_norm": 1.015043020248413,
      "learning_rate": 0.00035353653733531914,
      "loss": 0.1071,
      "step": 43110
    },
    {
      "epoch": 12.637749120750293,
      "grad_norm": 0.9617703557014465,
      "learning_rate": 0.0003534527410477308,
      "loss": 0.0962,
      "step": 43120
    },
    {
      "epoch": 12.640679953106682,
      "grad_norm": 0.5403134822845459,
      "learning_rate": 0.0003533689447601424,
      "loss": 0.0859,
      "step": 43130
    },
    {
      "epoch": 12.643610785463071,
      "grad_norm": 0.5765175819396973,
      "learning_rate": 0.000353285148472554,
      "loss": 0.0564,
      "step": 43140
    },
    {
      "epoch": 12.64654161781946,
      "grad_norm": 0.534092903137207,
      "learning_rate": 0.0003532013521849657,
      "loss": 0.0786,
      "step": 43150
    },
    {
      "epoch": 12.64947245017585,
      "grad_norm": 2.0601625442504883,
      "learning_rate": 0.0003531175558973773,
      "loss": 0.0883,
      "step": 43160
    },
    {
      "epoch": 12.652403282532239,
      "grad_norm": 0.659863293170929,
      "learning_rate": 0.00035303375960978896,
      "loss": 0.0613,
      "step": 43170
    },
    {
      "epoch": 12.655334114888628,
      "grad_norm": 0.8455711603164673,
      "learning_rate": 0.00035294996332220057,
      "loss": 0.0757,
      "step": 43180
    },
    {
      "epoch": 12.658264947245017,
      "grad_norm": 1.3615379333496094,
      "learning_rate": 0.0003528661670346122,
      "loss": 0.0936,
      "step": 43190
    },
    {
      "epoch": 12.661195779601407,
      "grad_norm": 0.6015743613243103,
      "learning_rate": 0.00035278237074702385,
      "loss": 0.0654,
      "step": 43200
    },
    {
      "epoch": 12.664126611957796,
      "grad_norm": 0.6626682877540588,
      "learning_rate": 0.00035269857445943545,
      "loss": 0.0651,
      "step": 43210
    },
    {
      "epoch": 12.667057444314185,
      "grad_norm": 0.771621584892273,
      "learning_rate": 0.0003526147781718471,
      "loss": 0.0947,
      "step": 43220
    },
    {
      "epoch": 12.669988276670574,
      "grad_norm": 0.2540818452835083,
      "learning_rate": 0.00035253098188425873,
      "loss": 0.0572,
      "step": 43230
    },
    {
      "epoch": 12.672919109026964,
      "grad_norm": 0.852373480796814,
      "learning_rate": 0.0003524471855966704,
      "loss": 0.1106,
      "step": 43240
    },
    {
      "epoch": 12.675849941383353,
      "grad_norm": 0.6975613236427307,
      "learning_rate": 0.000352363389309082,
      "loss": 0.0747,
      "step": 43250
    },
    {
      "epoch": 12.678780773739742,
      "grad_norm": 0.6254844665527344,
      "learning_rate": 0.0003522795930214936,
      "loss": 0.0962,
      "step": 43260
    },
    {
      "epoch": 12.681711606096131,
      "grad_norm": 0.7318698763847351,
      "learning_rate": 0.0003521957967339053,
      "loss": 0.087,
      "step": 43270
    },
    {
      "epoch": 12.68464243845252,
      "grad_norm": 0.5129622220993042,
      "learning_rate": 0.00035211200044631694,
      "loss": 0.0519,
      "step": 43280
    },
    {
      "epoch": 12.68757327080891,
      "grad_norm": 1.5182340145111084,
      "learning_rate": 0.00035202820415872855,
      "loss": 0.0918,
      "step": 43290
    },
    {
      "epoch": 12.6905041031653,
      "grad_norm": 1.3289968967437744,
      "learning_rate": 0.00035194440787114016,
      "loss": 0.0927,
      "step": 43300
    },
    {
      "epoch": 12.693434935521688,
      "grad_norm": 1.2332944869995117,
      "learning_rate": 0.0003518606115835518,
      "loss": 0.0929,
      "step": 43310
    },
    {
      "epoch": 12.696365767878078,
      "grad_norm": 2.057727813720703,
      "learning_rate": 0.00035177681529596343,
      "loss": 0.1155,
      "step": 43320
    },
    {
      "epoch": 12.699296600234467,
      "grad_norm": 0.8559767007827759,
      "learning_rate": 0.0003516930190083751,
      "loss": 0.0901,
      "step": 43330
    },
    {
      "epoch": 12.702227432590856,
      "grad_norm": 0.7070303559303284,
      "learning_rate": 0.0003516092227207867,
      "loss": 0.0814,
      "step": 43340
    },
    {
      "epoch": 12.705158264947245,
      "grad_norm": 1.053214192390442,
      "learning_rate": 0.00035152542643319837,
      "loss": 0.0673,
      "step": 43350
    },
    {
      "epoch": 12.708089097303635,
      "grad_norm": 1.3467180728912354,
      "learning_rate": 0.00035144163014561,
      "loss": 0.0934,
      "step": 43360
    },
    {
      "epoch": 12.711019929660024,
      "grad_norm": 0.9564103484153748,
      "learning_rate": 0.0003513578338580216,
      "loss": 0.1101,
      "step": 43370
    },
    {
      "epoch": 12.713950762016413,
      "grad_norm": 0.6687820553779602,
      "learning_rate": 0.00035127403757043325,
      "loss": 0.0891,
      "step": 43380
    },
    {
      "epoch": 12.716881594372802,
      "grad_norm": 1.0962917804718018,
      "learning_rate": 0.00035119024128284486,
      "loss": 0.092,
      "step": 43390
    },
    {
      "epoch": 12.719812426729192,
      "grad_norm": 0.575265645980835,
      "learning_rate": 0.00035110644499525653,
      "loss": 0.07,
      "step": 43400
    },
    {
      "epoch": 12.722743259085581,
      "grad_norm": 0.6464748978614807,
      "learning_rate": 0.00035102264870766814,
      "loss": 0.0673,
      "step": 43410
    },
    {
      "epoch": 12.72567409144197,
      "grad_norm": 0.8128575682640076,
      "learning_rate": 0.00035093885242007975,
      "loss": 0.065,
      "step": 43420
    },
    {
      "epoch": 12.72860492379836,
      "grad_norm": 1.0423249006271362,
      "learning_rate": 0.0003508550561324914,
      "loss": 0.0575,
      "step": 43430
    },
    {
      "epoch": 12.731535756154749,
      "grad_norm": 1.183151125907898,
      "learning_rate": 0.000350771259844903,
      "loss": 0.0554,
      "step": 43440
    },
    {
      "epoch": 12.734466588511138,
      "grad_norm": 1.1221896409988403,
      "learning_rate": 0.0003506874635573147,
      "loss": 0.0656,
      "step": 43450
    },
    {
      "epoch": 12.737397420867527,
      "grad_norm": 0.5291519165039062,
      "learning_rate": 0.00035060366726972635,
      "loss": 0.0702,
      "step": 43460
    },
    {
      "epoch": 12.740328253223916,
      "grad_norm": 1.6431643962860107,
      "learning_rate": 0.0003505198709821379,
      "loss": 0.0807,
      "step": 43470
    },
    {
      "epoch": 12.743259085580306,
      "grad_norm": 0.73836749792099,
      "learning_rate": 0.00035043607469454957,
      "loss": 0.0784,
      "step": 43480
    },
    {
      "epoch": 12.746189917936693,
      "grad_norm": 2.145949125289917,
      "learning_rate": 0.0003503522784069612,
      "loss": 0.0976,
      "step": 43490
    },
    {
      "epoch": 12.749120750293084,
      "grad_norm": 1.4839972257614136,
      "learning_rate": 0.00035026848211937284,
      "loss": 0.0623,
      "step": 43500
    },
    {
      "epoch": 12.752051582649472,
      "grad_norm": 1.1596897840499878,
      "learning_rate": 0.0003501846858317845,
      "loss": 0.0812,
      "step": 43510
    },
    {
      "epoch": 12.754982415005863,
      "grad_norm": 1.7642474174499512,
      "learning_rate": 0.0003501008895441961,
      "loss": 0.0855,
      "step": 43520
    },
    {
      "epoch": 12.75791324736225,
      "grad_norm": 2.22771954536438,
      "learning_rate": 0.0003500170932566077,
      "loss": 0.0774,
      "step": 43530
    },
    {
      "epoch": 12.76084407971864,
      "grad_norm": 1.0069869756698608,
      "learning_rate": 0.0003499332969690194,
      "loss": 0.0721,
      "step": 43540
    },
    {
      "epoch": 12.763774912075029,
      "grad_norm": 0.6348574757575989,
      "learning_rate": 0.000349849500681431,
      "loss": 0.0954,
      "step": 43550
    },
    {
      "epoch": 12.766705744431418,
      "grad_norm": 0.3313511908054352,
      "learning_rate": 0.00034976570439384266,
      "loss": 0.066,
      "step": 43560
    },
    {
      "epoch": 12.769636576787807,
      "grad_norm": 0.9799715876579285,
      "learning_rate": 0.00034968190810625427,
      "loss": 0.0826,
      "step": 43570
    },
    {
      "epoch": 12.772567409144196,
      "grad_norm": 0.8417664170265198,
      "learning_rate": 0.0003495981118186659,
      "loss": 0.0634,
      "step": 43580
    },
    {
      "epoch": 12.775498241500586,
      "grad_norm": 1.2227684259414673,
      "learning_rate": 0.00034951431553107755,
      "loss": 0.0725,
      "step": 43590
    },
    {
      "epoch": 12.778429073856975,
      "grad_norm": 0.8725754022598267,
      "learning_rate": 0.00034943051924348915,
      "loss": 0.0786,
      "step": 43600
    },
    {
      "epoch": 12.781359906213364,
      "grad_norm": 1.1691510677337646,
      "learning_rate": 0.0003493467229559008,
      "loss": 0.0902,
      "step": 43610
    },
    {
      "epoch": 12.784290738569753,
      "grad_norm": 0.928285539150238,
      "learning_rate": 0.00034926292666831243,
      "loss": 0.0592,
      "step": 43620
    },
    {
      "epoch": 12.787221570926143,
      "grad_norm": 0.4653814733028412,
      "learning_rate": 0.0003491791303807241,
      "loss": 0.0814,
      "step": 43630
    },
    {
      "epoch": 12.790152403282532,
      "grad_norm": 0.6119604110717773,
      "learning_rate": 0.0003490953340931357,
      "loss": 0.0519,
      "step": 43640
    },
    {
      "epoch": 12.793083235638921,
      "grad_norm": 0.755944550037384,
      "learning_rate": 0.0003490115378055473,
      "loss": 0.063,
      "step": 43650
    },
    {
      "epoch": 12.79601406799531,
      "grad_norm": 0.5235110521316528,
      "learning_rate": 0.000348927741517959,
      "loss": 0.0754,
      "step": 43660
    },
    {
      "epoch": 12.7989449003517,
      "grad_norm": 0.7372724413871765,
      "learning_rate": 0.0003488439452303706,
      "loss": 0.0959,
      "step": 43670
    },
    {
      "epoch": 12.801875732708089,
      "grad_norm": 1.4432570934295654,
      "learning_rate": 0.00034876014894278225,
      "loss": 0.0817,
      "step": 43680
    },
    {
      "epoch": 12.804806565064478,
      "grad_norm": 0.3577542304992676,
      "learning_rate": 0.0003486763526551939,
      "loss": 0.0668,
      "step": 43690
    },
    {
      "epoch": 12.807737397420867,
      "grad_norm": 0.7141146063804626,
      "learning_rate": 0.00034859255636760547,
      "loss": 0.0634,
      "step": 43700
    },
    {
      "epoch": 12.810668229777256,
      "grad_norm": 1.5585267543792725,
      "learning_rate": 0.00034850876008001713,
      "loss": 0.0783,
      "step": 43710
    },
    {
      "epoch": 12.813599062133646,
      "grad_norm": 0.5116807222366333,
      "learning_rate": 0.0003484249637924288,
      "loss": 0.0909,
      "step": 43720
    },
    {
      "epoch": 12.816529894490035,
      "grad_norm": 0.5654160380363464,
      "learning_rate": 0.0003483411675048404,
      "loss": 0.0686,
      "step": 43730
    },
    {
      "epoch": 12.819460726846424,
      "grad_norm": 0.6027914881706238,
      "learning_rate": 0.00034825737121725207,
      "loss": 0.0952,
      "step": 43740
    },
    {
      "epoch": 12.822391559202813,
      "grad_norm": 0.4833977520465851,
      "learning_rate": 0.0003481735749296637,
      "loss": 0.0797,
      "step": 43750
    },
    {
      "epoch": 12.825322391559203,
      "grad_norm": 0.33703532814979553,
      "learning_rate": 0.0003480897786420753,
      "loss": 0.0681,
      "step": 43760
    },
    {
      "epoch": 12.828253223915592,
      "grad_norm": 0.7576434016227722,
      "learning_rate": 0.00034800598235448695,
      "loss": 0.0893,
      "step": 43770
    },
    {
      "epoch": 12.831184056271981,
      "grad_norm": 0.774815559387207,
      "learning_rate": 0.00034792218606689856,
      "loss": 0.076,
      "step": 43780
    },
    {
      "epoch": 12.83411488862837,
      "grad_norm": 1.4697896242141724,
      "learning_rate": 0.0003478383897793102,
      "loss": 0.0792,
      "step": 43790
    },
    {
      "epoch": 12.83704572098476,
      "grad_norm": 0.42297694087028503,
      "learning_rate": 0.00034775459349172184,
      "loss": 0.0908,
      "step": 43800
    },
    {
      "epoch": 12.839976553341149,
      "grad_norm": 0.9990189671516418,
      "learning_rate": 0.00034767079720413345,
      "loss": 0.073,
      "step": 43810
    },
    {
      "epoch": 12.842907385697538,
      "grad_norm": 1.0745946168899536,
      "learning_rate": 0.0003475870009165451,
      "loss": 0.0615,
      "step": 43820
    },
    {
      "epoch": 12.845838218053927,
      "grad_norm": 0.7244387269020081,
      "learning_rate": 0.0003475032046289567,
      "loss": 0.061,
      "step": 43830
    },
    {
      "epoch": 12.848769050410317,
      "grad_norm": 0.8938272595405579,
      "learning_rate": 0.0003474194083413684,
      "loss": 0.0737,
      "step": 43840
    },
    {
      "epoch": 12.851699882766706,
      "grad_norm": 0.9404486417770386,
      "learning_rate": 0.00034733561205378,
      "loss": 0.0755,
      "step": 43850
    },
    {
      "epoch": 12.854630715123095,
      "grad_norm": 0.9478761553764343,
      "learning_rate": 0.00034725181576619166,
      "loss": 0.0844,
      "step": 43860
    },
    {
      "epoch": 12.857561547479484,
      "grad_norm": 0.5123778581619263,
      "learning_rate": 0.00034716801947860327,
      "loss": 0.0624,
      "step": 43870
    },
    {
      "epoch": 12.860492379835874,
      "grad_norm": 0.7774444818496704,
      "learning_rate": 0.0003470842231910149,
      "loss": 0.0784,
      "step": 43880
    },
    {
      "epoch": 12.863423212192263,
      "grad_norm": 1.53716242313385,
      "learning_rate": 0.00034700042690342654,
      "loss": 0.0761,
      "step": 43890
    },
    {
      "epoch": 12.866354044548652,
      "grad_norm": 0.7467849850654602,
      "learning_rate": 0.0003469166306158382,
      "loss": 0.0648,
      "step": 43900
    },
    {
      "epoch": 12.869284876905041,
      "grad_norm": 0.4997815787792206,
      "learning_rate": 0.0003468328343282498,
      "loss": 0.0832,
      "step": 43910
    },
    {
      "epoch": 12.87221570926143,
      "grad_norm": 1.5505622625350952,
      "learning_rate": 0.0003467490380406614,
      "loss": 0.082,
      "step": 43920
    },
    {
      "epoch": 12.87514654161782,
      "grad_norm": 0.47638818621635437,
      "learning_rate": 0.00034666524175307303,
      "loss": 0.0545,
      "step": 43930
    },
    {
      "epoch": 12.87807737397421,
      "grad_norm": 1.1138197183609009,
      "learning_rate": 0.0003465814454654847,
      "loss": 0.0737,
      "step": 43940
    },
    {
      "epoch": 12.881008206330598,
      "grad_norm": 2.131746292114258,
      "learning_rate": 0.00034649764917789636,
      "loss": 0.0717,
      "step": 43950
    },
    {
      "epoch": 12.883939038686988,
      "grad_norm": 0.776146411895752,
      "learning_rate": 0.00034641385289030797,
      "loss": 0.0888,
      "step": 43960
    },
    {
      "epoch": 12.886869871043377,
      "grad_norm": 0.7979655861854553,
      "learning_rate": 0.00034633005660271964,
      "loss": 0.0846,
      "step": 43970
    },
    {
      "epoch": 12.889800703399766,
      "grad_norm": 0.43234384059906006,
      "learning_rate": 0.00034624626031513125,
      "loss": 0.0813,
      "step": 43980
    },
    {
      "epoch": 12.892731535756155,
      "grad_norm": 0.7469475269317627,
      "learning_rate": 0.00034616246402754285,
      "loss": 0.0679,
      "step": 43990
    },
    {
      "epoch": 12.895662368112545,
      "grad_norm": 0.5008536577224731,
      "learning_rate": 0.0003460786677399545,
      "loss": 0.0599,
      "step": 44000
    },
    {
      "epoch": 12.898593200468934,
      "grad_norm": 1.3399065732955933,
      "learning_rate": 0.00034599487145236613,
      "loss": 0.0807,
      "step": 44010
    },
    {
      "epoch": 12.901524032825323,
      "grad_norm": 1.48995840549469,
      "learning_rate": 0.0003459110751647778,
      "loss": 0.0952,
      "step": 44020
    },
    {
      "epoch": 12.904454865181712,
      "grad_norm": 0.08530817180871964,
      "learning_rate": 0.0003458272788771894,
      "loss": 0.067,
      "step": 44030
    },
    {
      "epoch": 12.907385697538102,
      "grad_norm": 0.7047861218452454,
      "learning_rate": 0.000345743482589601,
      "loss": 0.06,
      "step": 44040
    },
    {
      "epoch": 12.91031652989449,
      "grad_norm": 0.911841630935669,
      "learning_rate": 0.0003456596863020127,
      "loss": 0.0771,
      "step": 44050
    },
    {
      "epoch": 12.913247362250878,
      "grad_norm": 0.7400513887405396,
      "learning_rate": 0.0003455758900144243,
      "loss": 0.0825,
      "step": 44060
    },
    {
      "epoch": 12.91617819460727,
      "grad_norm": 0.9945361018180847,
      "learning_rate": 0.00034549209372683595,
      "loss": 0.0655,
      "step": 44070
    },
    {
      "epoch": 12.919109026963657,
      "grad_norm": 0.7107875943183899,
      "learning_rate": 0.0003454082974392476,
      "loss": 0.0794,
      "step": 44080
    },
    {
      "epoch": 12.922039859320046,
      "grad_norm": 1.843491792678833,
      "learning_rate": 0.00034532450115165917,
      "loss": 0.1004,
      "step": 44090
    },
    {
      "epoch": 12.924970691676435,
      "grad_norm": 1.6398005485534668,
      "learning_rate": 0.00034524070486407083,
      "loss": 0.0815,
      "step": 44100
    },
    {
      "epoch": 12.927901524032825,
      "grad_norm": 0.4100860357284546,
      "learning_rate": 0.00034515690857648244,
      "loss": 0.0755,
      "step": 44110
    },
    {
      "epoch": 12.930832356389214,
      "grad_norm": 0.8249718546867371,
      "learning_rate": 0.0003450731122888941,
      "loss": 0.0823,
      "step": 44120
    },
    {
      "epoch": 12.933763188745603,
      "grad_norm": 0.3614839017391205,
      "learning_rate": 0.00034498931600130577,
      "loss": 0.1013,
      "step": 44130
    },
    {
      "epoch": 12.936694021101992,
      "grad_norm": 0.5780599117279053,
      "learning_rate": 0.0003449055197137174,
      "loss": 0.065,
      "step": 44140
    },
    {
      "epoch": 12.939624853458382,
      "grad_norm": 1.1701740026474,
      "learning_rate": 0.000344821723426129,
      "loss": 0.0691,
      "step": 44150
    },
    {
      "epoch": 12.94255568581477,
      "grad_norm": 0.6776959896087646,
      "learning_rate": 0.00034473792713854065,
      "loss": 0.0608,
      "step": 44160
    },
    {
      "epoch": 12.94548651817116,
      "grad_norm": 1.7619351148605347,
      "learning_rate": 0.00034465413085095226,
      "loss": 0.0781,
      "step": 44170
    },
    {
      "epoch": 12.94841735052755,
      "grad_norm": 0.8782370090484619,
      "learning_rate": 0.0003445703345633639,
      "loss": 0.0764,
      "step": 44180
    },
    {
      "epoch": 12.951348182883939,
      "grad_norm": 1.2131307125091553,
      "learning_rate": 0.00034448653827577554,
      "loss": 0.0713,
      "step": 44190
    },
    {
      "epoch": 12.954279015240328,
      "grad_norm": 0.8379723429679871,
      "learning_rate": 0.00034440274198818715,
      "loss": 0.0643,
      "step": 44200
    },
    {
      "epoch": 12.957209847596717,
      "grad_norm": 0.7988313436508179,
      "learning_rate": 0.0003443189457005988,
      "loss": 0.08,
      "step": 44210
    },
    {
      "epoch": 12.960140679953106,
      "grad_norm": 0.8069653511047363,
      "learning_rate": 0.0003442351494130104,
      "loss": 0.0535,
      "step": 44220
    },
    {
      "epoch": 12.963071512309496,
      "grad_norm": 1.1370418071746826,
      "learning_rate": 0.0003441513531254221,
      "loss": 0.0817,
      "step": 44230
    },
    {
      "epoch": 12.966002344665885,
      "grad_norm": 0.7412020564079285,
      "learning_rate": 0.0003440675568378337,
      "loss": 0.0627,
      "step": 44240
    },
    {
      "epoch": 12.968933177022274,
      "grad_norm": 0.9013175964355469,
      "learning_rate": 0.00034398376055024536,
      "loss": 0.0781,
      "step": 44250
    },
    {
      "epoch": 12.971864009378663,
      "grad_norm": 0.29473191499710083,
      "learning_rate": 0.00034389996426265697,
      "loss": 0.0514,
      "step": 44260
    },
    {
      "epoch": 12.974794841735052,
      "grad_norm": 1.6401036977767944,
      "learning_rate": 0.0003438161679750686,
      "loss": 0.0724,
      "step": 44270
    },
    {
      "epoch": 12.977725674091442,
      "grad_norm": 1.1822974681854248,
      "learning_rate": 0.00034373237168748024,
      "loss": 0.0871,
      "step": 44280
    },
    {
      "epoch": 12.980656506447831,
      "grad_norm": 1.0099267959594727,
      "learning_rate": 0.00034364857539989185,
      "loss": 0.085,
      "step": 44290
    },
    {
      "epoch": 12.98358733880422,
      "grad_norm": 0.9275400638580322,
      "learning_rate": 0.0003435647791123035,
      "loss": 0.0897,
      "step": 44300
    },
    {
      "epoch": 12.98651817116061,
      "grad_norm": 0.28784337639808655,
      "learning_rate": 0.0003434809828247152,
      "loss": 0.0636,
      "step": 44310
    },
    {
      "epoch": 12.989449003516999,
      "grad_norm": 0.4893260896205902,
      "learning_rate": 0.00034339718653712673,
      "loss": 0.062,
      "step": 44320
    },
    {
      "epoch": 12.992379835873388,
      "grad_norm": 1.0353703498840332,
      "learning_rate": 0.0003433133902495384,
      "loss": 0.086,
      "step": 44330
    },
    {
      "epoch": 12.995310668229777,
      "grad_norm": 2.667273998260498,
      "learning_rate": 0.00034322959396195006,
      "loss": 0.0904,
      "step": 44340
    },
    {
      "epoch": 12.998241500586166,
      "grad_norm": 0.9169719815254211,
      "learning_rate": 0.00034314579767436167,
      "loss": 0.0663,
      "step": 44350
    },
    {
      "epoch": 13.0,
      "eval_accuracy": 0.7408506429277942,
      "eval_f1_macro": 0.778378643783092,
      "eval_f1_micro": 0.8189655172413793,
      "eval_f1_weighted": 0.8126817774352946,
      "eval_loss": 0.07879258692264557,
      "eval_roc_auc": 0.882755117176666,
      "eval_runtime": 240.3662,
      "eval_samples_per_second": 12.618,
      "eval_steps_per_second": 1.581,
      "step": 44356
    },
    {
      "epoch": 13.001172332942556,
      "grad_norm": 0.6986812353134155,
      "learning_rate": 0.00034306200138677334,
      "loss": 0.0676,
      "step": 44360
    },
    {
      "epoch": 13.004103165298945,
      "grad_norm": 2.6510860919952393,
      "learning_rate": 0.00034297820509918494,
      "loss": 0.0651,
      "step": 44370
    },
    {
      "epoch": 13.007033997655334,
      "grad_norm": 0.8662214279174805,
      "learning_rate": 0.00034289440881159655,
      "loss": 0.0645,
      "step": 44380
    },
    {
      "epoch": 13.009964830011723,
      "grad_norm": 1.3292282819747925,
      "learning_rate": 0.0003428106125240082,
      "loss": 0.0722,
      "step": 44390
    },
    {
      "epoch": 13.012895662368113,
      "grad_norm": 1.4367843866348267,
      "learning_rate": 0.00034272681623641983,
      "loss": 0.0666,
      "step": 44400
    },
    {
      "epoch": 13.015826494724502,
      "grad_norm": 0.5877256989479065,
      "learning_rate": 0.0003426430199488315,
      "loss": 0.0468,
      "step": 44410
    },
    {
      "epoch": 13.018757327080891,
      "grad_norm": 0.7404069304466248,
      "learning_rate": 0.0003425592236612431,
      "loss": 0.0318,
      "step": 44420
    },
    {
      "epoch": 13.02168815943728,
      "grad_norm": 0.790276825428009,
      "learning_rate": 0.0003424754273736547,
      "loss": 0.0887,
      "step": 44430
    },
    {
      "epoch": 13.02461899179367,
      "grad_norm": 3.4030675888061523,
      "learning_rate": 0.0003423916310860664,
      "loss": 0.0693,
      "step": 44440
    },
    {
      "epoch": 13.027549824150059,
      "grad_norm": 0.8564105033874512,
      "learning_rate": 0.000342307834798478,
      "loss": 0.0981,
      "step": 44450
    },
    {
      "epoch": 13.030480656506448,
      "grad_norm": 1.2713662385940552,
      "learning_rate": 0.00034222403851088965,
      "loss": 0.0908,
      "step": 44460
    },
    {
      "epoch": 13.033411488862837,
      "grad_norm": 0.8010669946670532,
      "learning_rate": 0.00034214024222330126,
      "loss": 0.051,
      "step": 44470
    },
    {
      "epoch": 13.036342321219227,
      "grad_norm": 1.1040339469909668,
      "learning_rate": 0.0003420564459357129,
      "loss": 0.0525,
      "step": 44480
    },
    {
      "epoch": 13.039273153575616,
      "grad_norm": 1.5020811557769775,
      "learning_rate": 0.00034197264964812453,
      "loss": 0.0538,
      "step": 44490
    },
    {
      "epoch": 13.042203985932005,
      "grad_norm": 0.8125133514404297,
      "learning_rate": 0.00034188885336053614,
      "loss": 0.0635,
      "step": 44500
    },
    {
      "epoch": 13.045134818288394,
      "grad_norm": 0.7717294692993164,
      "learning_rate": 0.0003418050570729478,
      "loss": 0.0594,
      "step": 44510
    },
    {
      "epoch": 13.048065650644784,
      "grad_norm": 1.1119251251220703,
      "learning_rate": 0.00034172126078535947,
      "loss": 0.1025,
      "step": 44520
    },
    {
      "epoch": 13.050996483001173,
      "grad_norm": 0.7076327204704285,
      "learning_rate": 0.0003416374644977711,
      "loss": 0.0599,
      "step": 44530
    },
    {
      "epoch": 13.053927315357562,
      "grad_norm": 1.8089454174041748,
      "learning_rate": 0.0003415536682101827,
      "loss": 0.0809,
      "step": 44540
    },
    {
      "epoch": 13.056858147713951,
      "grad_norm": 1.1080337762832642,
      "learning_rate": 0.0003414698719225943,
      "loss": 0.0869,
      "step": 44550
    },
    {
      "epoch": 13.05978898007034,
      "grad_norm": 0.9903318285942078,
      "learning_rate": 0.00034138607563500596,
      "loss": 0.0669,
      "step": 44560
    },
    {
      "epoch": 13.06271981242673,
      "grad_norm": 0.6235763430595398,
      "learning_rate": 0.0003413022793474176,
      "loss": 0.0783,
      "step": 44570
    },
    {
      "epoch": 13.065650644783119,
      "grad_norm": 0.5905886888504028,
      "learning_rate": 0.00034121848305982924,
      "loss": 0.0578,
      "step": 44580
    },
    {
      "epoch": 13.068581477139508,
      "grad_norm": 2.102829933166504,
      "learning_rate": 0.0003411346867722409,
      "loss": 0.0763,
      "step": 44590
    },
    {
      "epoch": 13.071512309495898,
      "grad_norm": 0.3766554296016693,
      "learning_rate": 0.0003410508904846525,
      "loss": 0.0612,
      "step": 44600
    },
    {
      "epoch": 13.074443141852287,
      "grad_norm": 1.7947065830230713,
      "learning_rate": 0.0003409670941970641,
      "loss": 0.0695,
      "step": 44610
    },
    {
      "epoch": 13.077373974208676,
      "grad_norm": 1.423750400543213,
      "learning_rate": 0.0003408832979094758,
      "loss": 0.1029,
      "step": 44620
    },
    {
      "epoch": 13.080304806565065,
      "grad_norm": 1.0277420282363892,
      "learning_rate": 0.0003407995016218874,
      "loss": 0.0736,
      "step": 44630
    },
    {
      "epoch": 13.083235638921455,
      "grad_norm": 0.4127768874168396,
      "learning_rate": 0.00034071570533429906,
      "loss": 0.0471,
      "step": 44640
    },
    {
      "epoch": 13.086166471277842,
      "grad_norm": 0.12648653984069824,
      "learning_rate": 0.00034063190904671067,
      "loss": 0.0562,
      "step": 44650
    },
    {
      "epoch": 13.089097303634231,
      "grad_norm": 0.1656109094619751,
      "learning_rate": 0.0003405481127591223,
      "loss": 0.0435,
      "step": 44660
    },
    {
      "epoch": 13.09202813599062,
      "grad_norm": 0.7106293439865112,
      "learning_rate": 0.00034046431647153394,
      "loss": 0.0539,
      "step": 44670
    },
    {
      "epoch": 13.09495896834701,
      "grad_norm": 1.0664491653442383,
      "learning_rate": 0.00034038052018394555,
      "loss": 0.0682,
      "step": 44680
    },
    {
      "epoch": 13.097889800703399,
      "grad_norm": 0.9536558985710144,
      "learning_rate": 0.0003402967238963572,
      "loss": 0.0696,
      "step": 44690
    },
    {
      "epoch": 13.100820633059788,
      "grad_norm": 1.2012666463851929,
      "learning_rate": 0.0003402129276087688,
      "loss": 0.0657,
      "step": 44700
    },
    {
      "epoch": 13.103751465416178,
      "grad_norm": 0.9907760620117188,
      "learning_rate": 0.00034012913132118043,
      "loss": 0.0908,
      "step": 44710
    },
    {
      "epoch": 13.106682297772567,
      "grad_norm": 0.7434561252593994,
      "learning_rate": 0.0003400453350335921,
      "loss": 0.055,
      "step": 44720
    },
    {
      "epoch": 13.109613130128956,
      "grad_norm": 0.40574073791503906,
      "learning_rate": 0.0003399615387460037,
      "loss": 0.0609,
      "step": 44730
    },
    {
      "epoch": 13.112543962485345,
      "grad_norm": 1.8455623388290405,
      "learning_rate": 0.00033987774245841537,
      "loss": 0.076,
      "step": 44740
    },
    {
      "epoch": 13.115474794841735,
      "grad_norm": 1.4033403396606445,
      "learning_rate": 0.00033979394617082704,
      "loss": 0.0714,
      "step": 44750
    },
    {
      "epoch": 13.118405627198124,
      "grad_norm": 1.1882855892181396,
      "learning_rate": 0.00033971014988323864,
      "loss": 0.0523,
      "step": 44760
    },
    {
      "epoch": 13.121336459554513,
      "grad_norm": 1.985156774520874,
      "learning_rate": 0.00033962635359565025,
      "loss": 0.0771,
      "step": 44770
    },
    {
      "epoch": 13.124267291910902,
      "grad_norm": 0.9163080453872681,
      "learning_rate": 0.0003395425573080619,
      "loss": 0.0712,
      "step": 44780
    },
    {
      "epoch": 13.127198124267291,
      "grad_norm": 1.0592560768127441,
      "learning_rate": 0.00033945876102047353,
      "loss": 0.0772,
      "step": 44790
    },
    {
      "epoch": 13.13012895662368,
      "grad_norm": 0.7162085175514221,
      "learning_rate": 0.0003393749647328852,
      "loss": 0.0776,
      "step": 44800
    },
    {
      "epoch": 13.13305978898007,
      "grad_norm": 0.6888065338134766,
      "learning_rate": 0.0003392911684452968,
      "loss": 0.0798,
      "step": 44810
    },
    {
      "epoch": 13.13599062133646,
      "grad_norm": 0.7299715876579285,
      "learning_rate": 0.0003392073721577084,
      "loss": 0.0931,
      "step": 44820
    },
    {
      "epoch": 13.138921453692848,
      "grad_norm": 1.8149288892745972,
      "learning_rate": 0.0003391235758701201,
      "loss": 0.0666,
      "step": 44830
    },
    {
      "epoch": 13.141852286049238,
      "grad_norm": 0.2614108622074127,
      "learning_rate": 0.0003390397795825317,
      "loss": 0.0514,
      "step": 44840
    },
    {
      "epoch": 13.144783118405627,
      "grad_norm": 0.4576561152935028,
      "learning_rate": 0.00033895598329494335,
      "loss": 0.0561,
      "step": 44850
    },
    {
      "epoch": 13.147713950762016,
      "grad_norm": 0.2062816470861435,
      "learning_rate": 0.00033887218700735496,
      "loss": 0.0514,
      "step": 44860
    },
    {
      "epoch": 13.150644783118405,
      "grad_norm": 1.0116240978240967,
      "learning_rate": 0.0003387883907197666,
      "loss": 0.071,
      "step": 44870
    },
    {
      "epoch": 13.153575615474795,
      "grad_norm": 0.5884776711463928,
      "learning_rate": 0.00033870459443217823,
      "loss": 0.0642,
      "step": 44880
    },
    {
      "epoch": 13.156506447831184,
      "grad_norm": 1.2027969360351562,
      "learning_rate": 0.00033862079814458984,
      "loss": 0.0985,
      "step": 44890
    },
    {
      "epoch": 13.159437280187573,
      "grad_norm": 0.7487442493438721,
      "learning_rate": 0.0003385370018570015,
      "loss": 0.0756,
      "step": 44900
    },
    {
      "epoch": 13.162368112543962,
      "grad_norm": 1.1221612691879272,
      "learning_rate": 0.0003384532055694131,
      "loss": 0.1147,
      "step": 44910
    },
    {
      "epoch": 13.165298944900352,
      "grad_norm": 1.1798336505889893,
      "learning_rate": 0.0003383694092818248,
      "loss": 0.0748,
      "step": 44920
    },
    {
      "epoch": 13.168229777256741,
      "grad_norm": 1.5658414363861084,
      "learning_rate": 0.00033828561299423644,
      "loss": 0.051,
      "step": 44930
    },
    {
      "epoch": 13.17116060961313,
      "grad_norm": 0.7484583258628845,
      "learning_rate": 0.000338201816706648,
      "loss": 0.0591,
      "step": 44940
    },
    {
      "epoch": 13.17409144196952,
      "grad_norm": 0.5180429816246033,
      "learning_rate": 0.00033811802041905966,
      "loss": 0.0516,
      "step": 44950
    },
    {
      "epoch": 13.177022274325909,
      "grad_norm": 0.08410704135894775,
      "learning_rate": 0.0003380342241314713,
      "loss": 0.0762,
      "step": 44960
    },
    {
      "epoch": 13.179953106682298,
      "grad_norm": 0.8377295732498169,
      "learning_rate": 0.00033795042784388294,
      "loss": 0.0737,
      "step": 44970
    },
    {
      "epoch": 13.182883939038687,
      "grad_norm": 1.1883126497268677,
      "learning_rate": 0.0003378666315562946,
      "loss": 0.0565,
      "step": 44980
    },
    {
      "epoch": 13.185814771395076,
      "grad_norm": 1.6451345682144165,
      "learning_rate": 0.00033778283526870616,
      "loss": 0.0627,
      "step": 44990
    },
    {
      "epoch": 13.188745603751466,
      "grad_norm": 1.5991146564483643,
      "learning_rate": 0.0003376990389811178,
      "loss": 0.0908,
      "step": 45000
    },
    {
      "epoch": 13.191676436107855,
      "grad_norm": 0.778937041759491,
      "learning_rate": 0.0003376152426935295,
      "loss": 0.072,
      "step": 45010
    },
    {
      "epoch": 13.194607268464244,
      "grad_norm": 0.8036283850669861,
      "learning_rate": 0.0003375314464059411,
      "loss": 0.0834,
      "step": 45020
    },
    {
      "epoch": 13.197538100820633,
      "grad_norm": 0.7586517930030823,
      "learning_rate": 0.00033744765011835276,
      "loss": 0.0605,
      "step": 45030
    },
    {
      "epoch": 13.200468933177023,
      "grad_norm": 1.6976255178451538,
      "learning_rate": 0.00033736385383076437,
      "loss": 0.0952,
      "step": 45040
    },
    {
      "epoch": 13.203399765533412,
      "grad_norm": 1.4016478061676025,
      "learning_rate": 0.000337280057543176,
      "loss": 0.0839,
      "step": 45050
    },
    {
      "epoch": 13.206330597889801,
      "grad_norm": 0.9285971522331238,
      "learning_rate": 0.00033719626125558764,
      "loss": 0.0467,
      "step": 45060
    },
    {
      "epoch": 13.20926143024619,
      "grad_norm": 1.0565681457519531,
      "learning_rate": 0.00033711246496799925,
      "loss": 0.0631,
      "step": 45070
    },
    {
      "epoch": 13.21219226260258,
      "grad_norm": 1.6524797677993774,
      "learning_rate": 0.0003370286686804109,
      "loss": 0.094,
      "step": 45080
    },
    {
      "epoch": 13.215123094958969,
      "grad_norm": 0.6909682750701904,
      "learning_rate": 0.0003369448723928225,
      "loss": 0.0826,
      "step": 45090
    },
    {
      "epoch": 13.218053927315358,
      "grad_norm": 0.649863600730896,
      "learning_rate": 0.0003368610761052342,
      "loss": 0.058,
      "step": 45100
    },
    {
      "epoch": 13.220984759671747,
      "grad_norm": 0.6032936573028564,
      "learning_rate": 0.0003367772798176458,
      "loss": 0.0697,
      "step": 45110
    },
    {
      "epoch": 13.223915592028137,
      "grad_norm": 1.2294644117355347,
      "learning_rate": 0.0003366934835300574,
      "loss": 0.0724,
      "step": 45120
    },
    {
      "epoch": 13.226846424384526,
      "grad_norm": 0.4943911135196686,
      "learning_rate": 0.00033660968724246907,
      "loss": 0.0796,
      "step": 45130
    },
    {
      "epoch": 13.229777256740915,
      "grad_norm": 0.6467888951301575,
      "learning_rate": 0.00033652589095488074,
      "loss": 0.0433,
      "step": 45140
    },
    {
      "epoch": 13.232708089097304,
      "grad_norm": 0.8356146812438965,
      "learning_rate": 0.00033644209466729234,
      "loss": 0.0686,
      "step": 45150
    },
    {
      "epoch": 13.235638921453694,
      "grad_norm": 0.9454203248023987,
      "learning_rate": 0.00033635829837970395,
      "loss": 0.064,
      "step": 45160
    },
    {
      "epoch": 13.238569753810083,
      "grad_norm": 0.7741177678108215,
      "learning_rate": 0.00033627450209211556,
      "loss": 0.0629,
      "step": 45170
    },
    {
      "epoch": 13.241500586166472,
      "grad_norm": 1.133188009262085,
      "learning_rate": 0.00033619070580452723,
      "loss": 0.0898,
      "step": 45180
    },
    {
      "epoch": 13.244431418522861,
      "grad_norm": 0.5417998433113098,
      "learning_rate": 0.0003361069095169389,
      "loss": 0.0853,
      "step": 45190
    },
    {
      "epoch": 13.24736225087925,
      "grad_norm": 0.2857396900653839,
      "learning_rate": 0.0003360231132293505,
      "loss": 0.0667,
      "step": 45200
    },
    {
      "epoch": 13.25029308323564,
      "grad_norm": 2.001070976257324,
      "learning_rate": 0.00033593931694176217,
      "loss": 0.0653,
      "step": 45210
    },
    {
      "epoch": 13.253223915592027,
      "grad_norm": 1.4089641571044922,
      "learning_rate": 0.0003358555206541738,
      "loss": 0.0383,
      "step": 45220
    },
    {
      "epoch": 13.256154747948417,
      "grad_norm": 1.564452052116394,
      "learning_rate": 0.0003357717243665854,
      "loss": 0.0795,
      "step": 45230
    },
    {
      "epoch": 13.259085580304806,
      "grad_norm": 1.9468334913253784,
      "learning_rate": 0.00033568792807899705,
      "loss": 0.069,
      "step": 45240
    },
    {
      "epoch": 13.262016412661195,
      "grad_norm": 0.7450152635574341,
      "learning_rate": 0.00033560413179140866,
      "loss": 0.0574,
      "step": 45250
    },
    {
      "epoch": 13.264947245017584,
      "grad_norm": 0.6854372620582581,
      "learning_rate": 0.0003355203355038203,
      "loss": 0.0723,
      "step": 45260
    },
    {
      "epoch": 13.267878077373974,
      "grad_norm": 0.9462814331054688,
      "learning_rate": 0.00033543653921623193,
      "loss": 0.0748,
      "step": 45270
    },
    {
      "epoch": 13.270808909730363,
      "grad_norm": 0.37311404943466187,
      "learning_rate": 0.00033535274292864354,
      "loss": 0.0576,
      "step": 45280
    },
    {
      "epoch": 13.273739742086752,
      "grad_norm": 1.361194372177124,
      "learning_rate": 0.0003352689466410552,
      "loss": 0.0603,
      "step": 45290
    },
    {
      "epoch": 13.276670574443141,
      "grad_norm": 0.7444778084754944,
      "learning_rate": 0.0003351851503534668,
      "loss": 0.0695,
      "step": 45300
    },
    {
      "epoch": 13.27960140679953,
      "grad_norm": 0.7025447487831116,
      "learning_rate": 0.0003351013540658785,
      "loss": 0.0538,
      "step": 45310
    },
    {
      "epoch": 13.28253223915592,
      "grad_norm": 2.668813705444336,
      "learning_rate": 0.0003350175577782901,
      "loss": 0.0688,
      "step": 45320
    },
    {
      "epoch": 13.285463071512309,
      "grad_norm": 0.8062412142753601,
      "learning_rate": 0.0003349337614907017,
      "loss": 0.0682,
      "step": 45330
    },
    {
      "epoch": 13.288393903868698,
      "grad_norm": 0.1027398556470871,
      "learning_rate": 0.00033484996520311336,
      "loss": 0.0607,
      "step": 45340
    },
    {
      "epoch": 13.291324736225087,
      "grad_norm": 0.8820978999137878,
      "learning_rate": 0.00033476616891552497,
      "loss": 0.0773,
      "step": 45350
    },
    {
      "epoch": 13.294255568581477,
      "grad_norm": 0.48104989528656006,
      "learning_rate": 0.00033468237262793664,
      "loss": 0.0617,
      "step": 45360
    },
    {
      "epoch": 13.297186400937866,
      "grad_norm": 0.17026329040527344,
      "learning_rate": 0.0003345985763403483,
      "loss": 0.0646,
      "step": 45370
    },
    {
      "epoch": 13.300117233294255,
      "grad_norm": 0.9339527487754822,
      "learning_rate": 0.0003345147800527599,
      "loss": 0.0734,
      "step": 45380
    },
    {
      "epoch": 13.303048065650644,
      "grad_norm": 1.0080552101135254,
      "learning_rate": 0.0003344309837651715,
      "loss": 0.0723,
      "step": 45390
    },
    {
      "epoch": 13.305978898007034,
      "grad_norm": 1.112899661064148,
      "learning_rate": 0.0003343471874775832,
      "loss": 0.0805,
      "step": 45400
    },
    {
      "epoch": 13.308909730363423,
      "grad_norm": 0.7153935432434082,
      "learning_rate": 0.0003342633911899948,
      "loss": 0.0751,
      "step": 45410
    },
    {
      "epoch": 13.311840562719812,
      "grad_norm": 2.167780637741089,
      "learning_rate": 0.00033417959490240646,
      "loss": 0.0897,
      "step": 45420
    },
    {
      "epoch": 13.314771395076201,
      "grad_norm": 0.5209752917289734,
      "learning_rate": 0.00033409579861481807,
      "loss": 0.0434,
      "step": 45430
    },
    {
      "epoch": 13.31770222743259,
      "grad_norm": 3.0489118099212646,
      "learning_rate": 0.0003340120023272297,
      "loss": 0.0744,
      "step": 45440
    },
    {
      "epoch": 13.32063305978898,
      "grad_norm": 0.7058754563331604,
      "learning_rate": 0.00033392820603964134,
      "loss": 0.0585,
      "step": 45450
    },
    {
      "epoch": 13.32356389214537,
      "grad_norm": 0.6425006985664368,
      "learning_rate": 0.00033384440975205295,
      "loss": 0.0796,
      "step": 45460
    },
    {
      "epoch": 13.326494724501758,
      "grad_norm": 2.2559878826141357,
      "learning_rate": 0.0003337606134644646,
      "loss": 0.0564,
      "step": 45470
    },
    {
      "epoch": 13.329425556858148,
      "grad_norm": 1.3540382385253906,
      "learning_rate": 0.0003336768171768762,
      "loss": 0.0625,
      "step": 45480
    },
    {
      "epoch": 13.332356389214537,
      "grad_norm": 0.8397724628448486,
      "learning_rate": 0.0003335930208892879,
      "loss": 0.0592,
      "step": 45490
    },
    {
      "epoch": 13.335287221570926,
      "grad_norm": 1.2955846786499023,
      "learning_rate": 0.0003335092246016995,
      "loss": 0.065,
      "step": 45500
    },
    {
      "epoch": 13.338218053927315,
      "grad_norm": 2.417877674102783,
      "learning_rate": 0.0003334254283141111,
      "loss": 0.0811,
      "step": 45510
    },
    {
      "epoch": 13.341148886283705,
      "grad_norm": 0.9127000570297241,
      "learning_rate": 0.00033334163202652277,
      "loss": 0.077,
      "step": 45520
    },
    {
      "epoch": 13.344079718640094,
      "grad_norm": 1.5875943899154663,
      "learning_rate": 0.0003332578357389344,
      "loss": 0.0521,
      "step": 45530
    },
    {
      "epoch": 13.347010550996483,
      "grad_norm": 1.2267714738845825,
      "learning_rate": 0.00033317403945134604,
      "loss": 0.078,
      "step": 45540
    },
    {
      "epoch": 13.349941383352872,
      "grad_norm": 1.1422067880630493,
      "learning_rate": 0.0003330902431637577,
      "loss": 0.074,
      "step": 45550
    },
    {
      "epoch": 13.352872215709262,
      "grad_norm": 0.6435608863830566,
      "learning_rate": 0.00033300644687616926,
      "loss": 0.0577,
      "step": 45560
    },
    {
      "epoch": 13.35580304806565,
      "grad_norm": 0.16482973098754883,
      "learning_rate": 0.00033292265058858093,
      "loss": 0.0943,
      "step": 45570
    },
    {
      "epoch": 13.35873388042204,
      "grad_norm": 1.7364708185195923,
      "learning_rate": 0.0003328388543009926,
      "loss": 0.0784,
      "step": 45580
    },
    {
      "epoch": 13.36166471277843,
      "grad_norm": 0.9256892204284668,
      "learning_rate": 0.0003327550580134042,
      "loss": 0.0789,
      "step": 45590
    },
    {
      "epoch": 13.364595545134819,
      "grad_norm": 1.7187236547470093,
      "learning_rate": 0.00033267126172581587,
      "loss": 0.0766,
      "step": 45600
    },
    {
      "epoch": 13.367526377491208,
      "grad_norm": 0.5058872103691101,
      "learning_rate": 0.0003325874654382274,
      "loss": 0.0593,
      "step": 45610
    },
    {
      "epoch": 13.370457209847597,
      "grad_norm": 1.11001455783844,
      "learning_rate": 0.0003325036691506391,
      "loss": 0.0646,
      "step": 45620
    },
    {
      "epoch": 13.373388042203986,
      "grad_norm": 1.2351716756820679,
      "learning_rate": 0.00033241987286305075,
      "loss": 0.0855,
      "step": 45630
    },
    {
      "epoch": 13.376318874560376,
      "grad_norm": 1.2720563411712646,
      "learning_rate": 0.00033233607657546236,
      "loss": 0.0836,
      "step": 45640
    },
    {
      "epoch": 13.379249706916765,
      "grad_norm": 1.6134631633758545,
      "learning_rate": 0.000332252280287874,
      "loss": 0.0735,
      "step": 45650
    },
    {
      "epoch": 13.382180539273154,
      "grad_norm": 1.0703636407852173,
      "learning_rate": 0.00033216848400028563,
      "loss": 0.089,
      "step": 45660
    },
    {
      "epoch": 13.385111371629543,
      "grad_norm": 0.991510272026062,
      "learning_rate": 0.00033208468771269724,
      "loss": 0.0555,
      "step": 45670
    },
    {
      "epoch": 13.388042203985933,
      "grad_norm": 0.5824469327926636,
      "learning_rate": 0.0003320008914251089,
      "loss": 0.0485,
      "step": 45680
    },
    {
      "epoch": 13.390973036342322,
      "grad_norm": 1.10621976852417,
      "learning_rate": 0.0003319170951375205,
      "loss": 0.0912,
      "step": 45690
    },
    {
      "epoch": 13.393903868698711,
      "grad_norm": 0.5505573749542236,
      "learning_rate": 0.0003318332988499322,
      "loss": 0.0595,
      "step": 45700
    },
    {
      "epoch": 13.3968347010551,
      "grad_norm": 0.5450443625450134,
      "learning_rate": 0.0003317495025623438,
      "loss": 0.0767,
      "step": 45710
    },
    {
      "epoch": 13.39976553341149,
      "grad_norm": 0.5334576964378357,
      "learning_rate": 0.00033166570627475545,
      "loss": 0.0566,
      "step": 45720
    },
    {
      "epoch": 13.402696365767879,
      "grad_norm": 0.9763235449790955,
      "learning_rate": 0.00033158190998716706,
      "loss": 0.0489,
      "step": 45730
    },
    {
      "epoch": 13.405627198124268,
      "grad_norm": 0.8153928518295288,
      "learning_rate": 0.00033149811369957867,
      "loss": 0.0663,
      "step": 45740
    },
    {
      "epoch": 13.408558030480657,
      "grad_norm": 1.2063902616500854,
      "learning_rate": 0.00033141431741199034,
      "loss": 0.0577,
      "step": 45750
    },
    {
      "epoch": 13.411488862837047,
      "grad_norm": 1.5919221639633179,
      "learning_rate": 0.00033133052112440195,
      "loss": 0.0572,
      "step": 45760
    },
    {
      "epoch": 13.414419695193434,
      "grad_norm": 0.9073792099952698,
      "learning_rate": 0.0003312467248368136,
      "loss": 0.0642,
      "step": 45770
    },
    {
      "epoch": 13.417350527549825,
      "grad_norm": 0.07879754155874252,
      "learning_rate": 0.0003311629285492252,
      "loss": 0.0845,
      "step": 45780
    },
    {
      "epoch": 13.420281359906213,
      "grad_norm": 0.5260370373725891,
      "learning_rate": 0.00033107913226163683,
      "loss": 0.0764,
      "step": 45790
    },
    {
      "epoch": 13.423212192262602,
      "grad_norm": 1.2437580823898315,
      "learning_rate": 0.0003309953359740485,
      "loss": 0.0751,
      "step": 45800
    },
    {
      "epoch": 13.426143024618991,
      "grad_norm": 0.869541585445404,
      "learning_rate": 0.00033091153968646016,
      "loss": 0.0942,
      "step": 45810
    },
    {
      "epoch": 13.42907385697538,
      "grad_norm": 1.4785916805267334,
      "learning_rate": 0.00033082774339887177,
      "loss": 0.0792,
      "step": 45820
    },
    {
      "epoch": 13.43200468933177,
      "grad_norm": 0.8495835661888123,
      "learning_rate": 0.00033074394711128343,
      "loss": 0.0634,
      "step": 45830
    },
    {
      "epoch": 13.434935521688159,
      "grad_norm": 0.6667369604110718,
      "learning_rate": 0.00033066015082369504,
      "loss": 0.0384,
      "step": 45840
    },
    {
      "epoch": 13.437866354044548,
      "grad_norm": 0.6823145747184753,
      "learning_rate": 0.00033057635453610665,
      "loss": 0.0604,
      "step": 45850
    },
    {
      "epoch": 13.440797186400937,
      "grad_norm": 1.2275787591934204,
      "learning_rate": 0.0003304925582485183,
      "loss": 0.0794,
      "step": 45860
    },
    {
      "epoch": 13.443728018757326,
      "grad_norm": 0.7134283781051636,
      "learning_rate": 0.0003304087619609299,
      "loss": 0.0803,
      "step": 45870
    },
    {
      "epoch": 13.446658851113716,
      "grad_norm": 1.2899640798568726,
      "learning_rate": 0.0003303249656733416,
      "loss": 0.0736,
      "step": 45880
    },
    {
      "epoch": 13.449589683470105,
      "grad_norm": 1.4413219690322876,
      "learning_rate": 0.0003302411693857532,
      "loss": 0.0669,
      "step": 45890
    },
    {
      "epoch": 13.452520515826494,
      "grad_norm": 2.2464230060577393,
      "learning_rate": 0.0003301573730981648,
      "loss": 0.0699,
      "step": 45900
    },
    {
      "epoch": 13.455451348182883,
      "grad_norm": 0.6906107068061829,
      "learning_rate": 0.00033007357681057647,
      "loss": 0.0644,
      "step": 45910
    },
    {
      "epoch": 13.458382180539273,
      "grad_norm": 1.049953818321228,
      "learning_rate": 0.0003299897805229881,
      "loss": 0.0814,
      "step": 45920
    },
    {
      "epoch": 13.461313012895662,
      "grad_norm": 1.0061935186386108,
      "learning_rate": 0.00032990598423539974,
      "loss": 0.08,
      "step": 45930
    },
    {
      "epoch": 13.464243845252051,
      "grad_norm": 0.45915406942367554,
      "learning_rate": 0.00032982218794781135,
      "loss": 0.0911,
      "step": 45940
    },
    {
      "epoch": 13.46717467760844,
      "grad_norm": 1.1529394388198853,
      "learning_rate": 0.00032973839166022296,
      "loss": 0.0748,
      "step": 45950
    },
    {
      "epoch": 13.47010550996483,
      "grad_norm": 0.7776064276695251,
      "learning_rate": 0.00032965459537263463,
      "loss": 0.069,
      "step": 45960
    },
    {
      "epoch": 13.473036342321219,
      "grad_norm": 0.4888046979904175,
      "learning_rate": 0.00032957079908504624,
      "loss": 0.061,
      "step": 45970
    },
    {
      "epoch": 13.475967174677608,
      "grad_norm": 0.9833450317382812,
      "learning_rate": 0.0003294870027974579,
      "loss": 0.0672,
      "step": 45980
    },
    {
      "epoch": 13.478898007033997,
      "grad_norm": 1.8711422681808472,
      "learning_rate": 0.00032940320650986957,
      "loss": 0.0901,
      "step": 45990
    },
    {
      "epoch": 13.481828839390387,
      "grad_norm": 1.0599446296691895,
      "learning_rate": 0.0003293194102222812,
      "loss": 0.0678,
      "step": 46000
    },
    {
      "epoch": 13.484759671746776,
      "grad_norm": 0.8617553114891052,
      "learning_rate": 0.0003292356139346928,
      "loss": 0.0636,
      "step": 46010
    },
    {
      "epoch": 13.487690504103165,
      "grad_norm": 0.33775296807289124,
      "learning_rate": 0.00032915181764710445,
      "loss": 0.0878,
      "step": 46020
    },
    {
      "epoch": 13.490621336459554,
      "grad_norm": 0.4998425245285034,
      "learning_rate": 0.00032906802135951606,
      "loss": 0.0727,
      "step": 46030
    },
    {
      "epoch": 13.493552168815944,
      "grad_norm": 0.6380739212036133,
      "learning_rate": 0.0003289842250719277,
      "loss": 0.0613,
      "step": 46040
    },
    {
      "epoch": 13.496483001172333,
      "grad_norm": 1.0325924158096313,
      "learning_rate": 0.00032890042878433933,
      "loss": 0.0692,
      "step": 46050
    },
    {
      "epoch": 13.499413833528722,
      "grad_norm": 1.6307138204574585,
      "learning_rate": 0.00032881663249675094,
      "loss": 0.0681,
      "step": 46060
    },
    {
      "epoch": 13.502344665885111,
      "grad_norm": 1.953148365020752,
      "learning_rate": 0.0003287328362091626,
      "loss": 0.1042,
      "step": 46070
    },
    {
      "epoch": 13.5052754982415,
      "grad_norm": 1.186823844909668,
      "learning_rate": 0.0003286490399215742,
      "loss": 0.0858,
      "step": 46080
    },
    {
      "epoch": 13.50820633059789,
      "grad_norm": 0.908145010471344,
      "learning_rate": 0.0003285652436339859,
      "loss": 0.0472,
      "step": 46090
    },
    {
      "epoch": 13.51113716295428,
      "grad_norm": 1.300933599472046,
      "learning_rate": 0.0003284814473463975,
      "loss": 0.0493,
      "step": 46100
    },
    {
      "epoch": 13.514067995310668,
      "grad_norm": 0.409210741519928,
      "learning_rate": 0.00032839765105880915,
      "loss": 0.0568,
      "step": 46110
    },
    {
      "epoch": 13.516998827667058,
      "grad_norm": 1.6360540390014648,
      "learning_rate": 0.00032831385477122076,
      "loss": 0.0675,
      "step": 46120
    },
    {
      "epoch": 13.519929660023447,
      "grad_norm": 0.9485906362533569,
      "learning_rate": 0.00032823005848363237,
      "loss": 0.0802,
      "step": 46130
    },
    {
      "epoch": 13.522860492379836,
      "grad_norm": 0.6817163825035095,
      "learning_rate": 0.00032814626219604404,
      "loss": 0.0708,
      "step": 46140
    },
    {
      "epoch": 13.525791324736225,
      "grad_norm": 1.1406123638153076,
      "learning_rate": 0.00032806246590845565,
      "loss": 0.0879,
      "step": 46150
    },
    {
      "epoch": 13.528722157092615,
      "grad_norm": 0.8427984714508057,
      "learning_rate": 0.0003279786696208673,
      "loss": 0.0583,
      "step": 46160
    },
    {
      "epoch": 13.531652989449004,
      "grad_norm": 1.4013301134109497,
      "learning_rate": 0.000327894873333279,
      "loss": 0.0658,
      "step": 46170
    },
    {
      "epoch": 13.534583821805393,
      "grad_norm": 1.1702203750610352,
      "learning_rate": 0.00032781107704569053,
      "loss": 0.0639,
      "step": 46180
    },
    {
      "epoch": 13.537514654161782,
      "grad_norm": 0.4102054536342621,
      "learning_rate": 0.0003277272807581022,
      "loss": 0.0969,
      "step": 46190
    },
    {
      "epoch": 13.540445486518172,
      "grad_norm": 0.6737799644470215,
      "learning_rate": 0.0003276434844705138,
      "loss": 0.0678,
      "step": 46200
    },
    {
      "epoch": 13.54337631887456,
      "grad_norm": 1.8476064205169678,
      "learning_rate": 0.00032755968818292547,
      "loss": 0.0848,
      "step": 46210
    },
    {
      "epoch": 13.54630715123095,
      "grad_norm": 0.42604491114616394,
      "learning_rate": 0.00032747589189533713,
      "loss": 0.0694,
      "step": 46220
    },
    {
      "epoch": 13.54923798358734,
      "grad_norm": 0.5321105718612671,
      "learning_rate": 0.0003273920956077487,
      "loss": 0.0688,
      "step": 46230
    },
    {
      "epoch": 13.552168815943729,
      "grad_norm": 1.2999097108840942,
      "learning_rate": 0.00032730829932016035,
      "loss": 0.0694,
      "step": 46240
    },
    {
      "epoch": 13.555099648300118,
      "grad_norm": 2.4443557262420654,
      "learning_rate": 0.000327224503032572,
      "loss": 0.0768,
      "step": 46250
    },
    {
      "epoch": 13.558030480656507,
      "grad_norm": 0.9343171119689941,
      "learning_rate": 0.0003271407067449836,
      "loss": 0.0608,
      "step": 46260
    },
    {
      "epoch": 13.560961313012896,
      "grad_norm": 0.5519894361495972,
      "learning_rate": 0.0003270569104573953,
      "loss": 0.0776,
      "step": 46270
    },
    {
      "epoch": 13.563892145369286,
      "grad_norm": 0.4647933840751648,
      "learning_rate": 0.0003269731141698069,
      "loss": 0.061,
      "step": 46280
    },
    {
      "epoch": 13.566822977725675,
      "grad_norm": 0.5376819968223572,
      "learning_rate": 0.0003268893178822185,
      "loss": 0.0805,
      "step": 46290
    },
    {
      "epoch": 13.569753810082064,
      "grad_norm": 2.1560134887695312,
      "learning_rate": 0.00032680552159463017,
      "loss": 0.0647,
      "step": 46300
    },
    {
      "epoch": 13.572684642438453,
      "grad_norm": 0.8143362998962402,
      "learning_rate": 0.0003267217253070418,
      "loss": 0.0566,
      "step": 46310
    },
    {
      "epoch": 13.575615474794843,
      "grad_norm": 1.5270875692367554,
      "learning_rate": 0.00032663792901945344,
      "loss": 0.0733,
      "step": 46320
    },
    {
      "epoch": 13.578546307151232,
      "grad_norm": 1.0475102663040161,
      "learning_rate": 0.00032655413273186505,
      "loss": 0.0607,
      "step": 46330
    },
    {
      "epoch": 13.58147713950762,
      "grad_norm": 1.1907836198806763,
      "learning_rate": 0.0003264703364442767,
      "loss": 0.0633,
      "step": 46340
    },
    {
      "epoch": 13.58440797186401,
      "grad_norm": 0.5974340438842773,
      "learning_rate": 0.00032638654015668833,
      "loss": 0.077,
      "step": 46350
    },
    {
      "epoch": 13.587338804220398,
      "grad_norm": 1.6026419401168823,
      "learning_rate": 0.00032630274386909994,
      "loss": 0.0764,
      "step": 46360
    },
    {
      "epoch": 13.590269636576787,
      "grad_norm": 2.911198377609253,
      "learning_rate": 0.0003262189475815116,
      "loss": 0.0709,
      "step": 46370
    },
    {
      "epoch": 13.593200468933176,
      "grad_norm": 0.7372017502784729,
      "learning_rate": 0.0003261351512939232,
      "loss": 0.0746,
      "step": 46380
    },
    {
      "epoch": 13.596131301289565,
      "grad_norm": 1.5507484674453735,
      "learning_rate": 0.0003260513550063349,
      "loss": 0.0777,
      "step": 46390
    },
    {
      "epoch": 13.599062133645955,
      "grad_norm": 0.9345232248306274,
      "learning_rate": 0.0003259675587187465,
      "loss": 0.0675,
      "step": 46400
    },
    {
      "epoch": 13.601992966002344,
      "grad_norm": 1.067745327949524,
      "learning_rate": 0.0003258837624311581,
      "loss": 0.0705,
      "step": 46410
    },
    {
      "epoch": 13.604923798358733,
      "grad_norm": 1.628232717514038,
      "learning_rate": 0.00032579996614356976,
      "loss": 0.0538,
      "step": 46420
    },
    {
      "epoch": 13.607854630715122,
      "grad_norm": 1.2706867456436157,
      "learning_rate": 0.0003257161698559814,
      "loss": 0.0915,
      "step": 46430
    },
    {
      "epoch": 13.610785463071512,
      "grad_norm": 0.7234078049659729,
      "learning_rate": 0.00032563237356839303,
      "loss": 0.0592,
      "step": 46440
    },
    {
      "epoch": 13.613716295427901,
      "grad_norm": 1.0141130685806274,
      "learning_rate": 0.0003255485772808047,
      "loss": 0.0809,
      "step": 46450
    },
    {
      "epoch": 13.61664712778429,
      "grad_norm": 0.8579780459403992,
      "learning_rate": 0.0003254647809932163,
      "loss": 0.0759,
      "step": 46460
    },
    {
      "epoch": 13.61957796014068,
      "grad_norm": 0.7349904775619507,
      "learning_rate": 0.0003253809847056279,
      "loss": 0.0834,
      "step": 46470
    },
    {
      "epoch": 13.622508792497069,
      "grad_norm": 0.6015307307243347,
      "learning_rate": 0.0003252971884180396,
      "loss": 0.0705,
      "step": 46480
    },
    {
      "epoch": 13.625439624853458,
      "grad_norm": 1.3581359386444092,
      "learning_rate": 0.0003252133921304512,
      "loss": 0.0662,
      "step": 46490
    },
    {
      "epoch": 13.628370457209847,
      "grad_norm": 0.9019427299499512,
      "learning_rate": 0.00032512959584286285,
      "loss": 0.0638,
      "step": 46500
    },
    {
      "epoch": 13.631301289566236,
      "grad_norm": 0.4830441176891327,
      "learning_rate": 0.00032504579955527446,
      "loss": 0.0444,
      "step": 46510
    },
    {
      "epoch": 13.634232121922626,
      "grad_norm": 0.4349961280822754,
      "learning_rate": 0.00032496200326768607,
      "loss": 0.0554,
      "step": 46520
    },
    {
      "epoch": 13.637162954279015,
      "grad_norm": 0.8856682777404785,
      "learning_rate": 0.00032487820698009774,
      "loss": 0.0751,
      "step": 46530
    },
    {
      "epoch": 13.640093786635404,
      "grad_norm": 1.1016160249710083,
      "learning_rate": 0.00032479441069250935,
      "loss": 0.071,
      "step": 46540
    },
    {
      "epoch": 13.643024618991793,
      "grad_norm": 1.130598545074463,
      "learning_rate": 0.000324710614404921,
      "loss": 0.1048,
      "step": 46550
    },
    {
      "epoch": 13.645955451348183,
      "grad_norm": 1.0354866981506348,
      "learning_rate": 0.0003246268181173326,
      "loss": 0.0592,
      "step": 46560
    },
    {
      "epoch": 13.648886283704572,
      "grad_norm": 1.0045069456100464,
      "learning_rate": 0.00032454302182974423,
      "loss": 0.0554,
      "step": 46570
    },
    {
      "epoch": 13.651817116060961,
      "grad_norm": 0.4322351813316345,
      "learning_rate": 0.0003244592255421559,
      "loss": 0.0806,
      "step": 46580
    },
    {
      "epoch": 13.65474794841735,
      "grad_norm": 0.9838120341300964,
      "learning_rate": 0.0003243754292545675,
      "loss": 0.0864,
      "step": 46590
    },
    {
      "epoch": 13.65767878077374,
      "grad_norm": 0.7380859851837158,
      "learning_rate": 0.00032429163296697917,
      "loss": 0.0461,
      "step": 46600
    },
    {
      "epoch": 13.660609613130129,
      "grad_norm": 0.7718361616134644,
      "learning_rate": 0.00032420783667939083,
      "loss": 0.0933,
      "step": 46610
    },
    {
      "epoch": 13.663540445486518,
      "grad_norm": 1.5706714391708374,
      "learning_rate": 0.00032412404039180244,
      "loss": 0.0973,
      "step": 46620
    },
    {
      "epoch": 13.666471277842907,
      "grad_norm": 0.8564587235450745,
      "learning_rate": 0.00032404024410421405,
      "loss": 0.0488,
      "step": 46630
    },
    {
      "epoch": 13.669402110199297,
      "grad_norm": 1.3307151794433594,
      "learning_rate": 0.0003239564478166257,
      "loss": 0.0928,
      "step": 46640
    },
    {
      "epoch": 13.672332942555686,
      "grad_norm": 1.0725507736206055,
      "learning_rate": 0.0003238726515290373,
      "loss": 0.0618,
      "step": 46650
    },
    {
      "epoch": 13.675263774912075,
      "grad_norm": 3.373110055923462,
      "learning_rate": 0.000323788855241449,
      "loss": 0.0634,
      "step": 46660
    },
    {
      "epoch": 13.678194607268464,
      "grad_norm": 1.0367780923843384,
      "learning_rate": 0.0003237050589538606,
      "loss": 0.075,
      "step": 46670
    },
    {
      "epoch": 13.681125439624854,
      "grad_norm": 0.3644323945045471,
      "learning_rate": 0.0003236212626662722,
      "loss": 0.0589,
      "step": 46680
    },
    {
      "epoch": 13.684056271981243,
      "grad_norm": 0.623015284538269,
      "learning_rate": 0.00032353746637868387,
      "loss": 0.0661,
      "step": 46690
    },
    {
      "epoch": 13.686987104337632,
      "grad_norm": 0.9566037058830261,
      "learning_rate": 0.0003234536700910955,
      "loss": 0.0476,
      "step": 46700
    },
    {
      "epoch": 13.689917936694021,
      "grad_norm": 0.8730467557907104,
      "learning_rate": 0.00032336987380350714,
      "loss": 0.0815,
      "step": 46710
    },
    {
      "epoch": 13.69284876905041,
      "grad_norm": 3.2287662029266357,
      "learning_rate": 0.00032328607751591875,
      "loss": 0.0987,
      "step": 46720
    },
    {
      "epoch": 13.6957796014068,
      "grad_norm": 0.825872004032135,
      "learning_rate": 0.0003232022812283304,
      "loss": 0.069,
      "step": 46730
    },
    {
      "epoch": 13.698710433763189,
      "grad_norm": 0.8002133369445801,
      "learning_rate": 0.00032311848494074203,
      "loss": 0.0694,
      "step": 46740
    },
    {
      "epoch": 13.701641266119578,
      "grad_norm": 1.1058359146118164,
      "learning_rate": 0.00032303468865315364,
      "loss": 0.0803,
      "step": 46750
    },
    {
      "epoch": 13.704572098475968,
      "grad_norm": 1.2479766607284546,
      "learning_rate": 0.0003229508923655653,
      "loss": 0.0759,
      "step": 46760
    },
    {
      "epoch": 13.707502930832357,
      "grad_norm": 0.6858299970626831,
      "learning_rate": 0.0003228670960779769,
      "loss": 0.0487,
      "step": 46770
    },
    {
      "epoch": 13.710433763188746,
      "grad_norm": 1.851815104484558,
      "learning_rate": 0.0003227832997903886,
      "loss": 0.0888,
      "step": 46780
    },
    {
      "epoch": 13.713364595545135,
      "grad_norm": 3.250523090362549,
      "learning_rate": 0.00032269950350280024,
      "loss": 0.0867,
      "step": 46790
    },
    {
      "epoch": 13.716295427901525,
      "grad_norm": 2.076129913330078,
      "learning_rate": 0.0003226157072152118,
      "loss": 0.0863,
      "step": 46800
    },
    {
      "epoch": 13.719226260257914,
      "grad_norm": 0.3758467733860016,
      "learning_rate": 0.00032253191092762346,
      "loss": 0.095,
      "step": 46810
    },
    {
      "epoch": 13.722157092614303,
      "grad_norm": 0.2720910310745239,
      "learning_rate": 0.00032244811464003507,
      "loss": 0.044,
      "step": 46820
    },
    {
      "epoch": 13.725087924970692,
      "grad_norm": 0.4663117825984955,
      "learning_rate": 0.00032236431835244673,
      "loss": 0.0748,
      "step": 46830
    },
    {
      "epoch": 13.728018757327082,
      "grad_norm": 0.858032763004303,
      "learning_rate": 0.0003222805220648584,
      "loss": 0.0928,
      "step": 46840
    },
    {
      "epoch": 13.73094958968347,
      "grad_norm": 0.6001263856887817,
      "learning_rate": 0.00032219672577726995,
      "loss": 0.0559,
      "step": 46850
    },
    {
      "epoch": 13.73388042203986,
      "grad_norm": 0.20943035185337067,
      "learning_rate": 0.0003221129294896816,
      "loss": 0.0893,
      "step": 46860
    },
    {
      "epoch": 13.73681125439625,
      "grad_norm": 0.4168221652507782,
      "learning_rate": 0.0003220291332020933,
      "loss": 0.0884,
      "step": 46870
    },
    {
      "epoch": 13.739742086752639,
      "grad_norm": 0.8690727353096008,
      "learning_rate": 0.0003219453369145049,
      "loss": 0.0759,
      "step": 46880
    },
    {
      "epoch": 13.742672919109028,
      "grad_norm": 0.5752180814743042,
      "learning_rate": 0.00032186154062691655,
      "loss": 0.0534,
      "step": 46890
    },
    {
      "epoch": 13.745603751465417,
      "grad_norm": 0.490747332572937,
      "learning_rate": 0.00032177774433932816,
      "loss": 0.0561,
      "step": 46900
    },
    {
      "epoch": 13.748534583821804,
      "grad_norm": 0.9694390296936035,
      "learning_rate": 0.00032169394805173977,
      "loss": 0.068,
      "step": 46910
    },
    {
      "epoch": 13.751465416178196,
      "grad_norm": 2.0333971977233887,
      "learning_rate": 0.00032161015176415144,
      "loss": 0.0754,
      "step": 46920
    },
    {
      "epoch": 13.754396248534583,
      "grad_norm": 0.8098629713058472,
      "learning_rate": 0.00032152635547656305,
      "loss": 0.0677,
      "step": 46930
    },
    {
      "epoch": 13.757327080890972,
      "grad_norm": 1.191685676574707,
      "learning_rate": 0.0003214425591889747,
      "loss": 0.0818,
      "step": 46940
    },
    {
      "epoch": 13.760257913247361,
      "grad_norm": 0.8355265259742737,
      "learning_rate": 0.0003213587629013863,
      "loss": 0.063,
      "step": 46950
    },
    {
      "epoch": 13.76318874560375,
      "grad_norm": 1.0669711828231812,
      "learning_rate": 0.000321274966613798,
      "loss": 0.0819,
      "step": 46960
    },
    {
      "epoch": 13.76611957796014,
      "grad_norm": 1.659084677696228,
      "learning_rate": 0.0003211911703262096,
      "loss": 0.0651,
      "step": 46970
    },
    {
      "epoch": 13.76905041031653,
      "grad_norm": 0.40743693709373474,
      "learning_rate": 0.0003211073740386212,
      "loss": 0.0751,
      "step": 46980
    },
    {
      "epoch": 13.771981242672918,
      "grad_norm": 0.447537362575531,
      "learning_rate": 0.00032102357775103287,
      "loss": 0.0599,
      "step": 46990
    },
    {
      "epoch": 13.774912075029308,
      "grad_norm": 1.6565775871276855,
      "learning_rate": 0.0003209397814634445,
      "loss": 0.0847,
      "step": 47000
    },
    {
      "epoch": 13.777842907385697,
      "grad_norm": 1.1704065799713135,
      "learning_rate": 0.00032085598517585614,
      "loss": 0.0685,
      "step": 47010
    },
    {
      "epoch": 13.780773739742086,
      "grad_norm": 2.3582746982574463,
      "learning_rate": 0.00032077218888826775,
      "loss": 0.0834,
      "step": 47020
    },
    {
      "epoch": 13.783704572098475,
      "grad_norm": 0.5429075956344604,
      "learning_rate": 0.00032068839260067936,
      "loss": 0.0593,
      "step": 47030
    },
    {
      "epoch": 13.786635404454865,
      "grad_norm": 0.31528353691101074,
      "learning_rate": 0.000320604596313091,
      "loss": 0.0814,
      "step": 47040
    },
    {
      "epoch": 13.789566236811254,
      "grad_norm": 1.6243749856948853,
      "learning_rate": 0.0003205208000255027,
      "loss": 0.0649,
      "step": 47050
    },
    {
      "epoch": 13.792497069167643,
      "grad_norm": 0.3168543875217438,
      "learning_rate": 0.0003204370037379143,
      "loss": 0.0549,
      "step": 47060
    },
    {
      "epoch": 13.795427901524032,
      "grad_norm": 0.6558542251586914,
      "learning_rate": 0.00032035320745032596,
      "loss": 0.0482,
      "step": 47070
    },
    {
      "epoch": 13.798358733880422,
      "grad_norm": 2.1533374786376953,
      "learning_rate": 0.00032026941116273757,
      "loss": 0.0685,
      "step": 47080
    },
    {
      "epoch": 13.801289566236811,
      "grad_norm": 1.2033958435058594,
      "learning_rate": 0.0003201856148751492,
      "loss": 0.0664,
      "step": 47090
    },
    {
      "epoch": 13.8042203985932,
      "grad_norm": 0.6185891032218933,
      "learning_rate": 0.00032010181858756084,
      "loss": 0.0684,
      "step": 47100
    },
    {
      "epoch": 13.80715123094959,
      "grad_norm": 1.6638717651367188,
      "learning_rate": 0.00032001802229997245,
      "loss": 0.0872,
      "step": 47110
    },
    {
      "epoch": 13.810082063305979,
      "grad_norm": 0.8989068269729614,
      "learning_rate": 0.0003199342260123841,
      "loss": 0.0728,
      "step": 47120
    },
    {
      "epoch": 13.813012895662368,
      "grad_norm": 1.0968502759933472,
      "learning_rate": 0.00031985042972479573,
      "loss": 0.0695,
      "step": 47130
    },
    {
      "epoch": 13.815943728018757,
      "grad_norm": 1.7395004034042358,
      "learning_rate": 0.00031976663343720734,
      "loss": 0.0621,
      "step": 47140
    },
    {
      "epoch": 13.818874560375146,
      "grad_norm": 0.730476975440979,
      "learning_rate": 0.000319682837149619,
      "loss": 0.0685,
      "step": 47150
    },
    {
      "epoch": 13.821805392731536,
      "grad_norm": 0.9633534550666809,
      "learning_rate": 0.0003195990408620306,
      "loss": 0.0865,
      "step": 47160
    },
    {
      "epoch": 13.824736225087925,
      "grad_norm": 0.35488682985305786,
      "learning_rate": 0.0003195152445744423,
      "loss": 0.0632,
      "step": 47170
    },
    {
      "epoch": 13.827667057444314,
      "grad_norm": 1.5379289388656616,
      "learning_rate": 0.0003194314482868539,
      "loss": 0.0887,
      "step": 47180
    },
    {
      "epoch": 13.830597889800703,
      "grad_norm": 0.6258271932601929,
      "learning_rate": 0.0003193476519992655,
      "loss": 0.0466,
      "step": 47190
    },
    {
      "epoch": 13.833528722157093,
      "grad_norm": 0.8746689558029175,
      "learning_rate": 0.00031926385571167716,
      "loss": 0.1165,
      "step": 47200
    },
    {
      "epoch": 13.836459554513482,
      "grad_norm": 1.1247313022613525,
      "learning_rate": 0.00031918005942408877,
      "loss": 0.0499,
      "step": 47210
    },
    {
      "epoch": 13.839390386869871,
      "grad_norm": 0.4940623641014099,
      "learning_rate": 0.00031909626313650043,
      "loss": 0.0736,
      "step": 47220
    },
    {
      "epoch": 13.84232121922626,
      "grad_norm": 1.7167147397994995,
      "learning_rate": 0.0003190124668489121,
      "loss": 0.0852,
      "step": 47230
    },
    {
      "epoch": 13.84525205158265,
      "grad_norm": 1.155647873878479,
      "learning_rate": 0.0003189286705613237,
      "loss": 0.0705,
      "step": 47240
    },
    {
      "epoch": 13.848182883939039,
      "grad_norm": 1.4702438116073608,
      "learning_rate": 0.0003188448742737353,
      "loss": 0.0854,
      "step": 47250
    },
    {
      "epoch": 13.851113716295428,
      "grad_norm": 1.6080340147018433,
      "learning_rate": 0.0003187610779861469,
      "loss": 0.079,
      "step": 47260
    },
    {
      "epoch": 13.854044548651817,
      "grad_norm": 1.04106867313385,
      "learning_rate": 0.0003186772816985586,
      "loss": 0.0786,
      "step": 47270
    },
    {
      "epoch": 13.856975381008207,
      "grad_norm": 1.2175815105438232,
      "learning_rate": 0.00031859348541097025,
      "loss": 0.083,
      "step": 47280
    },
    {
      "epoch": 13.859906213364596,
      "grad_norm": 1.6900211572647095,
      "learning_rate": 0.00031850968912338186,
      "loss": 0.073,
      "step": 47290
    },
    {
      "epoch": 13.862837045720985,
      "grad_norm": 0.6890630125999451,
      "learning_rate": 0.00031842589283579347,
      "loss": 0.0962,
      "step": 47300
    },
    {
      "epoch": 13.865767878077374,
      "grad_norm": 1.0715526342391968,
      "learning_rate": 0.00031834209654820514,
      "loss": 0.0683,
      "step": 47310
    },
    {
      "epoch": 13.868698710433764,
      "grad_norm": 1.263639211654663,
      "learning_rate": 0.00031825830026061675,
      "loss": 0.0916,
      "step": 47320
    },
    {
      "epoch": 13.871629542790153,
      "grad_norm": 0.8624362945556641,
      "learning_rate": 0.0003181745039730284,
      "loss": 0.0553,
      "step": 47330
    },
    {
      "epoch": 13.874560375146542,
      "grad_norm": 0.8203526735305786,
      "learning_rate": 0.00031809070768544,
      "loss": 0.0836,
      "step": 47340
    },
    {
      "epoch": 13.877491207502931,
      "grad_norm": 0.417432963848114,
      "learning_rate": 0.0003180069113978517,
      "loss": 0.0671,
      "step": 47350
    },
    {
      "epoch": 13.88042203985932,
      "grad_norm": 0.40030768513679504,
      "learning_rate": 0.0003179231151102633,
      "loss": 0.0754,
      "step": 47360
    },
    {
      "epoch": 13.88335287221571,
      "grad_norm": 0.8768587708473206,
      "learning_rate": 0.0003178393188226749,
      "loss": 0.0851,
      "step": 47370
    },
    {
      "epoch": 13.886283704572099,
      "grad_norm": 0.8776516914367676,
      "learning_rate": 0.00031775552253508657,
      "loss": 0.0811,
      "step": 47380
    },
    {
      "epoch": 13.889214536928488,
      "grad_norm": 0.42510145902633667,
      "learning_rate": 0.0003176717262474982,
      "loss": 0.0492,
      "step": 47390
    },
    {
      "epoch": 13.892145369284878,
      "grad_norm": 0.6812466382980347,
      "learning_rate": 0.00031758792995990984,
      "loss": 0.0551,
      "step": 47400
    },
    {
      "epoch": 13.895076201641267,
      "grad_norm": 0.8462878465652466,
      "learning_rate": 0.00031750413367232145,
      "loss": 0.0889,
      "step": 47410
    },
    {
      "epoch": 13.898007033997656,
      "grad_norm": 2.5582127571105957,
      "learning_rate": 0.00031742033738473306,
      "loss": 0.0823,
      "step": 47420
    },
    {
      "epoch": 13.900937866354045,
      "grad_norm": 0.5421364903450012,
      "learning_rate": 0.0003173365410971447,
      "loss": 0.0703,
      "step": 47430
    },
    {
      "epoch": 13.903868698710435,
      "grad_norm": 1.6928859949111938,
      "learning_rate": 0.00031725274480955633,
      "loss": 0.0711,
      "step": 47440
    },
    {
      "epoch": 13.906799531066824,
      "grad_norm": 1.0463900566101074,
      "learning_rate": 0.000317168948521968,
      "loss": 0.0556,
      "step": 47450
    },
    {
      "epoch": 13.909730363423213,
      "grad_norm": 1.0575143098831177,
      "learning_rate": 0.00031708515223437966,
      "loss": 0.0899,
      "step": 47460
    },
    {
      "epoch": 13.912661195779602,
      "grad_norm": 0.670680820941925,
      "learning_rate": 0.0003170013559467912,
      "loss": 0.0523,
      "step": 47470
    },
    {
      "epoch": 13.91559202813599,
      "grad_norm": 0.533785343170166,
      "learning_rate": 0.0003169175596592029,
      "loss": 0.0653,
      "step": 47480
    },
    {
      "epoch": 13.91852286049238,
      "grad_norm": 0.7632532715797424,
      "learning_rate": 0.00031683376337161454,
      "loss": 0.045,
      "step": 47490
    },
    {
      "epoch": 13.921453692848768,
      "grad_norm": 2.2112526893615723,
      "learning_rate": 0.00031674996708402615,
      "loss": 0.0792,
      "step": 47500
    },
    {
      "epoch": 13.924384525205157,
      "grad_norm": 0.5342475175857544,
      "learning_rate": 0.0003166661707964378,
      "loss": 0.0426,
      "step": 47510
    },
    {
      "epoch": 13.927315357561547,
      "grad_norm": 0.4592255651950836,
      "learning_rate": 0.00031658237450884943,
      "loss": 0.0608,
      "step": 47520
    },
    {
      "epoch": 13.930246189917936,
      "grad_norm": 2.843414545059204,
      "learning_rate": 0.00031649857822126104,
      "loss": 0.0516,
      "step": 47530
    },
    {
      "epoch": 13.933177022274325,
      "grad_norm": 1.2872741222381592,
      "learning_rate": 0.0003164147819336727,
      "loss": 0.0755,
      "step": 47540
    },
    {
      "epoch": 13.936107854630714,
      "grad_norm": 0.7168035507202148,
      "learning_rate": 0.0003163309856460843,
      "loss": 0.0476,
      "step": 47550
    },
    {
      "epoch": 13.939038686987104,
      "grad_norm": 1.505082607269287,
      "learning_rate": 0.000316247189358496,
      "loss": 0.0622,
      "step": 47560
    },
    {
      "epoch": 13.941969519343493,
      "grad_norm": 0.9843716025352478,
      "learning_rate": 0.0003161633930709076,
      "loss": 0.0852,
      "step": 47570
    },
    {
      "epoch": 13.944900351699882,
      "grad_norm": 0.8896812200546265,
      "learning_rate": 0.0003160795967833192,
      "loss": 0.0665,
      "step": 47580
    },
    {
      "epoch": 13.947831184056271,
      "grad_norm": 1.180330514907837,
      "learning_rate": 0.00031599580049573086,
      "loss": 0.0864,
      "step": 47590
    },
    {
      "epoch": 13.95076201641266,
      "grad_norm": 0.8395146727561951,
      "learning_rate": 0.00031591200420814247,
      "loss": 0.0829,
      "step": 47600
    },
    {
      "epoch": 13.95369284876905,
      "grad_norm": 0.8351894021034241,
      "learning_rate": 0.00031582820792055413,
      "loss": 0.0854,
      "step": 47610
    },
    {
      "epoch": 13.95662368112544,
      "grad_norm": 0.8355394005775452,
      "learning_rate": 0.00031574441163296574,
      "loss": 0.0849,
      "step": 47620
    },
    {
      "epoch": 13.959554513481828,
      "grad_norm": 0.5025179386138916,
      "learning_rate": 0.0003156606153453774,
      "loss": 0.0529,
      "step": 47630
    },
    {
      "epoch": 13.962485345838218,
      "grad_norm": 0.8171128630638123,
      "learning_rate": 0.000315576819057789,
      "loss": 0.0774,
      "step": 47640
    },
    {
      "epoch": 13.965416178194607,
      "grad_norm": 0.9681593179702759,
      "learning_rate": 0.0003154930227702006,
      "loss": 0.0553,
      "step": 47650
    },
    {
      "epoch": 13.968347010550996,
      "grad_norm": 1.6090751886367798,
      "learning_rate": 0.0003154092264826123,
      "loss": 0.0927,
      "step": 47660
    },
    {
      "epoch": 13.971277842907385,
      "grad_norm": 0.4518037438392639,
      "learning_rate": 0.00031532543019502395,
      "loss": 0.0423,
      "step": 47670
    },
    {
      "epoch": 13.974208675263775,
      "grad_norm": 0.371535986661911,
      "learning_rate": 0.00031524163390743556,
      "loss": 0.0634,
      "step": 47680
    },
    {
      "epoch": 13.977139507620164,
      "grad_norm": 0.4788283705711365,
      "learning_rate": 0.0003151578376198472,
      "loss": 0.0709,
      "step": 47690
    },
    {
      "epoch": 13.980070339976553,
      "grad_norm": 1.053898811340332,
      "learning_rate": 0.0003150740413322588,
      "loss": 0.0604,
      "step": 47700
    },
    {
      "epoch": 13.983001172332942,
      "grad_norm": 0.9635866284370422,
      "learning_rate": 0.00031499024504467045,
      "loss": 0.0847,
      "step": 47710
    },
    {
      "epoch": 13.985932004689332,
      "grad_norm": 1.504164218902588,
      "learning_rate": 0.0003149064487570821,
      "loss": 0.083,
      "step": 47720
    },
    {
      "epoch": 13.98886283704572,
      "grad_norm": 0.8016844987869263,
      "learning_rate": 0.0003148226524694937,
      "loss": 0.073,
      "step": 47730
    },
    {
      "epoch": 13.99179366940211,
      "grad_norm": 0.8849840760231018,
      "learning_rate": 0.0003147388561819054,
      "loss": 0.0509,
      "step": 47740
    },
    {
      "epoch": 13.9947245017585,
      "grad_norm": 2.1257004737854004,
      "learning_rate": 0.000314655059894317,
      "loss": 0.0616,
      "step": 47750
    },
    {
      "epoch": 13.997655334114889,
      "grad_norm": 1.8312581777572632,
      "learning_rate": 0.0003145712636067286,
      "loss": 0.0766,
      "step": 47760
    },
    {
      "epoch": 14.0,
      "eval_accuracy": 0.7411803494889548,
      "eval_f1_macro": 0.763096600171475,
      "eval_f1_micro": 0.8078395345276375,
      "eval_f1_weighted": 0.804420869052155,
      "eval_loss": 0.08449528366327286,
      "eval_roc_auc": 0.8788161163271537,
      "eval_runtime": 239.4716,
      "eval_samples_per_second": 12.665,
      "eval_steps_per_second": 1.587,
      "step": 47768
    },
    {
      "epoch": 14.000586166471278,
      "grad_norm": 0.7829017043113708,
      "learning_rate": 0.00031448746731914027,
      "loss": 0.0554,
      "step": 47770
    },
    {
      "epoch": 14.003516998827667,
      "grad_norm": 1.2578095197677612,
      "learning_rate": 0.0003144036710315519,
      "loss": 0.0497,
      "step": 47780
    },
    {
      "epoch": 14.006447831184056,
      "grad_norm": 0.6673850417137146,
      "learning_rate": 0.00031431987474396354,
      "loss": 0.0769,
      "step": 47790
    },
    {
      "epoch": 14.009378663540446,
      "grad_norm": 0.7205002307891846,
      "learning_rate": 0.00031423607845637515,
      "loss": 0.0831,
      "step": 47800
    },
    {
      "epoch": 14.012309495896835,
      "grad_norm": 1.2896440029144287,
      "learning_rate": 0.00031415228216878676,
      "loss": 0.084,
      "step": 47810
    },
    {
      "epoch": 14.015240328253224,
      "grad_norm": 0.6859326958656311,
      "learning_rate": 0.0003140684858811984,
      "loss": 0.0619,
      "step": 47820
    },
    {
      "epoch": 14.018171160609613,
      "grad_norm": 1.0592797994613647,
      "learning_rate": 0.00031398468959361003,
      "loss": 0.0802,
      "step": 47830
    },
    {
      "epoch": 14.021101992966003,
      "grad_norm": 0.5100420713424683,
      "learning_rate": 0.0003139008933060217,
      "loss": 0.079,
      "step": 47840
    },
    {
      "epoch": 14.024032825322392,
      "grad_norm": 1.6173306703567505,
      "learning_rate": 0.00031381709701843336,
      "loss": 0.0487,
      "step": 47850
    },
    {
      "epoch": 14.026963657678781,
      "grad_norm": 1.1168444156646729,
      "learning_rate": 0.00031373330073084497,
      "loss": 0.0588,
      "step": 47860
    },
    {
      "epoch": 14.02989449003517,
      "grad_norm": 0.5728568434715271,
      "learning_rate": 0.0003136495044432566,
      "loss": 0.0567,
      "step": 47870
    },
    {
      "epoch": 14.03282532239156,
      "grad_norm": 1.5435268878936768,
      "learning_rate": 0.0003135657081556682,
      "loss": 0.0634,
      "step": 47880
    },
    {
      "epoch": 14.035756154747949,
      "grad_norm": 1.034462809562683,
      "learning_rate": 0.00031348191186807985,
      "loss": 0.0639,
      "step": 47890
    },
    {
      "epoch": 14.038686987104338,
      "grad_norm": 0.9385395050048828,
      "learning_rate": 0.0003133981155804915,
      "loss": 0.0712,
      "step": 47900
    },
    {
      "epoch": 14.041617819460727,
      "grad_norm": 0.9843151569366455,
      "learning_rate": 0.00031331431929290313,
      "loss": 0.0473,
      "step": 47910
    },
    {
      "epoch": 14.044548651817117,
      "grad_norm": 2.463671922683716,
      "learning_rate": 0.00031323052300531474,
      "loss": 0.0752,
      "step": 47920
    },
    {
      "epoch": 14.047479484173506,
      "grad_norm": 0.6677550673484802,
      "learning_rate": 0.0003131467267177264,
      "loss": 0.0789,
      "step": 47930
    },
    {
      "epoch": 14.050410316529895,
      "grad_norm": 0.8999682068824768,
      "learning_rate": 0.000313062930430138,
      "loss": 0.0497,
      "step": 47940
    },
    {
      "epoch": 14.053341148886284,
      "grad_norm": 0.8327671885490417,
      "learning_rate": 0.0003129791341425497,
      "loss": 0.0683,
      "step": 47950
    },
    {
      "epoch": 14.056271981242674,
      "grad_norm": 1.0200635194778442,
      "learning_rate": 0.0003128953378549613,
      "loss": 0.0715,
      "step": 47960
    },
    {
      "epoch": 14.059202813599063,
      "grad_norm": 0.6410332322120667,
      "learning_rate": 0.00031281154156737295,
      "loss": 0.0438,
      "step": 47970
    },
    {
      "epoch": 14.062133645955452,
      "grad_norm": 0.6620146036148071,
      "learning_rate": 0.00031272774527978456,
      "loss": 0.0498,
      "step": 47980
    },
    {
      "epoch": 14.065064478311841,
      "grad_norm": 0.25511062145233154,
      "learning_rate": 0.00031264394899219617,
      "loss": 0.0417,
      "step": 47990
    },
    {
      "epoch": 14.06799531066823,
      "grad_norm": 0.36748918890953064,
      "learning_rate": 0.00031256015270460783,
      "loss": 0.0724,
      "step": 48000
    },
    {
      "epoch": 14.07092614302462,
      "grad_norm": 0.9719218611717224,
      "learning_rate": 0.00031247635641701944,
      "loss": 0.0587,
      "step": 48010
    },
    {
      "epoch": 14.073856975381009,
      "grad_norm": 0.6212009787559509,
      "learning_rate": 0.0003123925601294311,
      "loss": 0.062,
      "step": 48020
    },
    {
      "epoch": 14.076787807737398,
      "grad_norm": 0.512904703617096,
      "learning_rate": 0.0003123087638418427,
      "loss": 0.0521,
      "step": 48030
    },
    {
      "epoch": 14.079718640093787,
      "grad_norm": 0.6768299341201782,
      "learning_rate": 0.0003122249675542543,
      "loss": 0.04,
      "step": 48040
    },
    {
      "epoch": 14.082649472450177,
      "grad_norm": 0.7851716876029968,
      "learning_rate": 0.000312141171266666,
      "loss": 0.0572,
      "step": 48050
    },
    {
      "epoch": 14.085580304806564,
      "grad_norm": 0.7092861533164978,
      "learning_rate": 0.0003120573749790776,
      "loss": 0.0719,
      "step": 48060
    },
    {
      "epoch": 14.088511137162953,
      "grad_norm": 0.9879704117774963,
      "learning_rate": 0.00031197357869148926,
      "loss": 0.0701,
      "step": 48070
    },
    {
      "epoch": 14.091441969519343,
      "grad_norm": 1.174210786819458,
      "learning_rate": 0.0003118897824039009,
      "loss": 0.0535,
      "step": 48080
    },
    {
      "epoch": 14.094372801875732,
      "grad_norm": 1.4941279888153076,
      "learning_rate": 0.0003118059861163125,
      "loss": 0.0801,
      "step": 48090
    },
    {
      "epoch": 14.097303634232121,
      "grad_norm": 1.7050684690475464,
      "learning_rate": 0.00031172218982872415,
      "loss": 0.1067,
      "step": 48100
    },
    {
      "epoch": 14.10023446658851,
      "grad_norm": 0.22614786028862,
      "learning_rate": 0.0003116383935411358,
      "loss": 0.0674,
      "step": 48110
    },
    {
      "epoch": 14.1031652989449,
      "grad_norm": 1.8914958238601685,
      "learning_rate": 0.0003115545972535474,
      "loss": 0.0587,
      "step": 48120
    },
    {
      "epoch": 14.106096131301289,
      "grad_norm": 1.0215177536010742,
      "learning_rate": 0.0003114708009659591,
      "loss": 0.0805,
      "step": 48130
    },
    {
      "epoch": 14.109026963657678,
      "grad_norm": 0.9091354608535767,
      "learning_rate": 0.0003113870046783707,
      "loss": 0.0556,
      "step": 48140
    },
    {
      "epoch": 14.111957796014067,
      "grad_norm": 1.4345391988754272,
      "learning_rate": 0.0003113032083907823,
      "loss": 0.0433,
      "step": 48150
    },
    {
      "epoch": 14.114888628370457,
      "grad_norm": 0.7755725383758545,
      "learning_rate": 0.00031121941210319397,
      "loss": 0.0566,
      "step": 48160
    },
    {
      "epoch": 14.117819460726846,
      "grad_norm": 0.6414567232131958,
      "learning_rate": 0.0003111356158156056,
      "loss": 0.0475,
      "step": 48170
    },
    {
      "epoch": 14.120750293083235,
      "grad_norm": 1.7396066188812256,
      "learning_rate": 0.00031105181952801724,
      "loss": 0.0712,
      "step": 48180
    },
    {
      "epoch": 14.123681125439624,
      "grad_norm": 0.19253021478652954,
      "learning_rate": 0.00031096802324042885,
      "loss": 0.0675,
      "step": 48190
    },
    {
      "epoch": 14.126611957796014,
      "grad_norm": 0.7762647867202759,
      "learning_rate": 0.00031088422695284046,
      "loss": 0.0795,
      "step": 48200
    },
    {
      "epoch": 14.129542790152403,
      "grad_norm": 0.605957567691803,
      "learning_rate": 0.0003108004306652521,
      "loss": 0.0697,
      "step": 48210
    },
    {
      "epoch": 14.132473622508792,
      "grad_norm": 0.9987719655036926,
      "learning_rate": 0.00031071663437766373,
      "loss": 0.0535,
      "step": 48220
    },
    {
      "epoch": 14.135404454865181,
      "grad_norm": 0.8920440673828125,
      "learning_rate": 0.0003106328380900754,
      "loss": 0.0602,
      "step": 48230
    },
    {
      "epoch": 14.13833528722157,
      "grad_norm": 1.7345713376998901,
      "learning_rate": 0.000310549041802487,
      "loss": 0.079,
      "step": 48240
    },
    {
      "epoch": 14.14126611957796,
      "grad_norm": 2.7200605869293213,
      "learning_rate": 0.00031046524551489867,
      "loss": 0.0432,
      "step": 48250
    },
    {
      "epoch": 14.14419695193435,
      "grad_norm": 1.1143779754638672,
      "learning_rate": 0.0003103814492273103,
      "loss": 0.0544,
      "step": 48260
    },
    {
      "epoch": 14.147127784290738,
      "grad_norm": 1.84089195728302,
      "learning_rate": 0.0003102976529397219,
      "loss": 0.0711,
      "step": 48270
    },
    {
      "epoch": 14.150058616647128,
      "grad_norm": 0.3981284499168396,
      "learning_rate": 0.00031021385665213355,
      "loss": 0.0686,
      "step": 48280
    },
    {
      "epoch": 14.152989449003517,
      "grad_norm": 1.1237646341323853,
      "learning_rate": 0.0003101300603645452,
      "loss": 0.0645,
      "step": 48290
    },
    {
      "epoch": 14.155920281359906,
      "grad_norm": 1.0701247453689575,
      "learning_rate": 0.00031004626407695683,
      "loss": 0.0753,
      "step": 48300
    },
    {
      "epoch": 14.158851113716295,
      "grad_norm": 1.6213124990463257,
      "learning_rate": 0.0003099624677893685,
      "loss": 0.0786,
      "step": 48310
    },
    {
      "epoch": 14.161781946072685,
      "grad_norm": 0.8778699636459351,
      "learning_rate": 0.00030987867150178005,
      "loss": 0.0852,
      "step": 48320
    },
    {
      "epoch": 14.164712778429074,
      "grad_norm": 0.5417674779891968,
      "learning_rate": 0.0003097948752141917,
      "loss": 0.0671,
      "step": 48330
    },
    {
      "epoch": 14.167643610785463,
      "grad_norm": 1.3802708387374878,
      "learning_rate": 0.0003097110789266034,
      "loss": 0.0808,
      "step": 48340
    },
    {
      "epoch": 14.170574443141852,
      "grad_norm": 0.39131680130958557,
      "learning_rate": 0.000309627282639015,
      "loss": 0.0505,
      "step": 48350
    },
    {
      "epoch": 14.173505275498242,
      "grad_norm": 1.5093464851379395,
      "learning_rate": 0.00030954348635142665,
      "loss": 0.0686,
      "step": 48360
    },
    {
      "epoch": 14.17643610785463,
      "grad_norm": 0.5424200892448425,
      "learning_rate": 0.00030945969006383826,
      "loss": 0.0699,
      "step": 48370
    },
    {
      "epoch": 14.17936694021102,
      "grad_norm": 0.18828968703746796,
      "learning_rate": 0.00030937589377624987,
      "loss": 0.0698,
      "step": 48380
    },
    {
      "epoch": 14.18229777256741,
      "grad_norm": 2.982793092727661,
      "learning_rate": 0.00030929209748866153,
      "loss": 0.0456,
      "step": 48390
    },
    {
      "epoch": 14.185228604923799,
      "grad_norm": 1.9450865983963013,
      "learning_rate": 0.00030920830120107314,
      "loss": 0.0776,
      "step": 48400
    },
    {
      "epoch": 14.188159437280188,
      "grad_norm": 0.2659721076488495,
      "learning_rate": 0.0003091245049134848,
      "loss": 0.0585,
      "step": 48410
    },
    {
      "epoch": 14.191090269636577,
      "grad_norm": 3.379568576812744,
      "learning_rate": 0.0003090407086258964,
      "loss": 0.0546,
      "step": 48420
    },
    {
      "epoch": 14.194021101992966,
      "grad_norm": 0.8525252342224121,
      "learning_rate": 0.000308956912338308,
      "loss": 0.0822,
      "step": 48430
    },
    {
      "epoch": 14.196951934349356,
      "grad_norm": 0.7874976396560669,
      "learning_rate": 0.0003088731160507197,
      "loss": 0.0804,
      "step": 48440
    },
    {
      "epoch": 14.199882766705745,
      "grad_norm": 0.8663644194602966,
      "learning_rate": 0.0003087893197631313,
      "loss": 0.0828,
      "step": 48450
    },
    {
      "epoch": 14.202813599062134,
      "grad_norm": 0.429870069026947,
      "learning_rate": 0.00030870552347554296,
      "loss": 0.0647,
      "step": 48460
    },
    {
      "epoch": 14.205744431418523,
      "grad_norm": 1.6205686330795288,
      "learning_rate": 0.00030862172718795457,
      "loss": 0.0663,
      "step": 48470
    },
    {
      "epoch": 14.208675263774913,
      "grad_norm": 0.6447229385375977,
      "learning_rate": 0.00030853793090036624,
      "loss": 0.078,
      "step": 48480
    },
    {
      "epoch": 14.211606096131302,
      "grad_norm": 1.3079044818878174,
      "learning_rate": 0.00030845413461277785,
      "loss": 0.0611,
      "step": 48490
    },
    {
      "epoch": 14.214536928487691,
      "grad_norm": 0.683259129524231,
      "learning_rate": 0.00030837033832518946,
      "loss": 0.0744,
      "step": 48500
    },
    {
      "epoch": 14.21746776084408,
      "grad_norm": 0.8967839479446411,
      "learning_rate": 0.0003082865420376011,
      "loss": 0.0599,
      "step": 48510
    },
    {
      "epoch": 14.22039859320047,
      "grad_norm": 1.9934923648834229,
      "learning_rate": 0.0003082027457500128,
      "loss": 0.0519,
      "step": 48520
    },
    {
      "epoch": 14.223329425556859,
      "grad_norm": 1.5193957090377808,
      "learning_rate": 0.0003081189494624244,
      "loss": 0.0779,
      "step": 48530
    },
    {
      "epoch": 14.226260257913248,
      "grad_norm": 1.2639036178588867,
      "learning_rate": 0.000308035153174836,
      "loss": 0.0563,
      "step": 48540
    },
    {
      "epoch": 14.229191090269637,
      "grad_norm": 0.7182616591453552,
      "learning_rate": 0.00030795135688724767,
      "loss": 0.067,
      "step": 48550
    },
    {
      "epoch": 14.232121922626026,
      "grad_norm": 1.3828985691070557,
      "learning_rate": 0.0003078675605996593,
      "loss": 0.0596,
      "step": 48560
    },
    {
      "epoch": 14.235052754982416,
      "grad_norm": 0.9482572674751282,
      "learning_rate": 0.00030778376431207094,
      "loss": 0.077,
      "step": 48570
    },
    {
      "epoch": 14.237983587338805,
      "grad_norm": 1.5788549184799194,
      "learning_rate": 0.00030769996802448255,
      "loss": 0.0649,
      "step": 48580
    },
    {
      "epoch": 14.240914419695194,
      "grad_norm": 0.22871063649654388,
      "learning_rate": 0.0003076161717368942,
      "loss": 0.0506,
      "step": 48590
    },
    {
      "epoch": 14.243845252051583,
      "grad_norm": 0.4837663173675537,
      "learning_rate": 0.0003075323754493058,
      "loss": 0.0686,
      "step": 48600
    },
    {
      "epoch": 14.246776084407973,
      "grad_norm": 0.13562336564064026,
      "learning_rate": 0.00030744857916171743,
      "loss": 0.0729,
      "step": 48610
    },
    {
      "epoch": 14.24970691676436,
      "grad_norm": 1.0389357805252075,
      "learning_rate": 0.0003073647828741291,
      "loss": 0.0526,
      "step": 48620
    },
    {
      "epoch": 14.25263774912075,
      "grad_norm": 0.5273837447166443,
      "learning_rate": 0.0003072809865865407,
      "loss": 0.0553,
      "step": 48630
    },
    {
      "epoch": 14.255568581477139,
      "grad_norm": 0.040451809763908386,
      "learning_rate": 0.00030719719029895237,
      "loss": 0.049,
      "step": 48640
    },
    {
      "epoch": 14.258499413833528,
      "grad_norm": 0.9825632572174072,
      "learning_rate": 0.000307113394011364,
      "loss": 0.0769,
      "step": 48650
    },
    {
      "epoch": 14.261430246189917,
      "grad_norm": 0.5116894841194153,
      "learning_rate": 0.0003070295977237756,
      "loss": 0.0499,
      "step": 48660
    },
    {
      "epoch": 14.264361078546306,
      "grad_norm": 1.6780602931976318,
      "learning_rate": 0.00030694580143618725,
      "loss": 0.0661,
      "step": 48670
    },
    {
      "epoch": 14.267291910902696,
      "grad_norm": 0.6126606464385986,
      "learning_rate": 0.00030686200514859886,
      "loss": 0.0676,
      "step": 48680
    },
    {
      "epoch": 14.270222743259085,
      "grad_norm": 0.8865259885787964,
      "learning_rate": 0.00030677820886101053,
      "loss": 0.0705,
      "step": 48690
    },
    {
      "epoch": 14.273153575615474,
      "grad_norm": 0.7562714219093323,
      "learning_rate": 0.0003066944125734222,
      "loss": 0.0742,
      "step": 48700
    },
    {
      "epoch": 14.276084407971863,
      "grad_norm": 0.17008166015148163,
      "learning_rate": 0.00030661061628583375,
      "loss": 0.0651,
      "step": 48710
    },
    {
      "epoch": 14.279015240328253,
      "grad_norm": 1.030137300491333,
      "learning_rate": 0.0003065268199982454,
      "loss": 0.07,
      "step": 48720
    },
    {
      "epoch": 14.281946072684642,
      "grad_norm": 0.7265994548797607,
      "learning_rate": 0.0003064430237106571,
      "loss": 0.0594,
      "step": 48730
    },
    {
      "epoch": 14.284876905041031,
      "grad_norm": 1.2609840631484985,
      "learning_rate": 0.0003063592274230687,
      "loss": 0.0697,
      "step": 48740
    },
    {
      "epoch": 14.28780773739742,
      "grad_norm": 1.3201591968536377,
      "learning_rate": 0.00030627543113548035,
      "loss": 0.0445,
      "step": 48750
    },
    {
      "epoch": 14.29073856975381,
      "grad_norm": 1.6256372928619385,
      "learning_rate": 0.00030619163484789196,
      "loss": 0.0378,
      "step": 48760
    },
    {
      "epoch": 14.293669402110199,
      "grad_norm": 0.9964553117752075,
      "learning_rate": 0.00030610783856030357,
      "loss": 0.0826,
      "step": 48770
    },
    {
      "epoch": 14.296600234466588,
      "grad_norm": 1.388344407081604,
      "learning_rate": 0.00030602404227271523,
      "loss": 0.0792,
      "step": 48780
    },
    {
      "epoch": 14.299531066822977,
      "grad_norm": 0.9558440446853638,
      "learning_rate": 0.00030594024598512684,
      "loss": 0.0746,
      "step": 48790
    },
    {
      "epoch": 14.302461899179367,
      "grad_norm": 1.1219111680984497,
      "learning_rate": 0.0003058564496975385,
      "loss": 0.0775,
      "step": 48800
    },
    {
      "epoch": 14.305392731535756,
      "grad_norm": 0.791120707988739,
      "learning_rate": 0.0003057726534099501,
      "loss": 0.0512,
      "step": 48810
    },
    {
      "epoch": 14.308323563892145,
      "grad_norm": 0.1321428120136261,
      "learning_rate": 0.0003056888571223617,
      "loss": 0.053,
      "step": 48820
    },
    {
      "epoch": 14.311254396248534,
      "grad_norm": 1.4117379188537598,
      "learning_rate": 0.0003056050608347734,
      "loss": 0.0787,
      "step": 48830
    },
    {
      "epoch": 14.314185228604924,
      "grad_norm": 1.6802822351455688,
      "learning_rate": 0.000305521264547185,
      "loss": 0.0772,
      "step": 48840
    },
    {
      "epoch": 14.317116060961313,
      "grad_norm": 0.7177008390426636,
      "learning_rate": 0.00030543746825959666,
      "loss": 0.0776,
      "step": 48850
    },
    {
      "epoch": 14.320046893317702,
      "grad_norm": 0.7347695231437683,
      "learning_rate": 0.00030535367197200827,
      "loss": 0.0602,
      "step": 48860
    },
    {
      "epoch": 14.322977725674091,
      "grad_norm": 1.261735200881958,
      "learning_rate": 0.00030526987568441994,
      "loss": 0.0419,
      "step": 48870
    },
    {
      "epoch": 14.32590855803048,
      "grad_norm": 1.707120656967163,
      "learning_rate": 0.00030518607939683155,
      "loss": 0.0758,
      "step": 48880
    },
    {
      "epoch": 14.32883939038687,
      "grad_norm": 0.6473353505134583,
      "learning_rate": 0.00030510228310924316,
      "loss": 0.0555,
      "step": 48890
    },
    {
      "epoch": 14.331770222743259,
      "grad_norm": 1.269286036491394,
      "learning_rate": 0.0003050184868216548,
      "loss": 0.0734,
      "step": 48900
    },
    {
      "epoch": 14.334701055099648,
      "grad_norm": 0.5474580526351929,
      "learning_rate": 0.00030493469053406643,
      "loss": 0.0846,
      "step": 48910
    },
    {
      "epoch": 14.337631887456038,
      "grad_norm": 0.9991636872291565,
      "learning_rate": 0.0003048508942464781,
      "loss": 0.0678,
      "step": 48920
    },
    {
      "epoch": 14.340562719812427,
      "grad_norm": 0.17738942801952362,
      "learning_rate": 0.00030476709795888976,
      "loss": 0.0495,
      "step": 48930
    },
    {
      "epoch": 14.343493552168816,
      "grad_norm": 1.1012672185897827,
      "learning_rate": 0.0003046833016713013,
      "loss": 0.0512,
      "step": 48940
    },
    {
      "epoch": 14.346424384525205,
      "grad_norm": 2.1190645694732666,
      "learning_rate": 0.000304599505383713,
      "loss": 0.0731,
      "step": 48950
    },
    {
      "epoch": 14.349355216881595,
      "grad_norm": 0.7238661050796509,
      "learning_rate": 0.00030451570909612464,
      "loss": 0.0769,
      "step": 48960
    },
    {
      "epoch": 14.352286049237984,
      "grad_norm": 0.9198707938194275,
      "learning_rate": 0.00030443191280853625,
      "loss": 0.0461,
      "step": 48970
    },
    {
      "epoch": 14.355216881594373,
      "grad_norm": 0.6284890174865723,
      "learning_rate": 0.0003043481165209479,
      "loss": 0.0767,
      "step": 48980
    },
    {
      "epoch": 14.358147713950762,
      "grad_norm": 0.11253801733255386,
      "learning_rate": 0.0003042643202333595,
      "loss": 0.0676,
      "step": 48990
    },
    {
      "epoch": 14.361078546307152,
      "grad_norm": 1.068886399269104,
      "learning_rate": 0.00030418052394577113,
      "loss": 0.0394,
      "step": 49000
    },
    {
      "epoch": 14.36400937866354,
      "grad_norm": 0.6088024973869324,
      "learning_rate": 0.0003040967276581828,
      "loss": 0.0927,
      "step": 49010
    },
    {
      "epoch": 14.36694021101993,
      "grad_norm": 1.4150586128234863,
      "learning_rate": 0.0003040129313705944,
      "loss": 0.0713,
      "step": 49020
    },
    {
      "epoch": 14.36987104337632,
      "grad_norm": 0.7207571268081665,
      "learning_rate": 0.00030392913508300607,
      "loss": 0.0713,
      "step": 49030
    },
    {
      "epoch": 14.372801875732709,
      "grad_norm": 0.28447139263153076,
      "learning_rate": 0.0003038453387954177,
      "loss": 0.0581,
      "step": 49040
    },
    {
      "epoch": 14.375732708089098,
      "grad_norm": 0.8468025922775269,
      "learning_rate": 0.0003037615425078293,
      "loss": 0.0788,
      "step": 49050
    },
    {
      "epoch": 14.378663540445487,
      "grad_norm": 1.6005959510803223,
      "learning_rate": 0.00030367774622024095,
      "loss": 0.0755,
      "step": 49060
    },
    {
      "epoch": 14.381594372801876,
      "grad_norm": 0.5933248996734619,
      "learning_rate": 0.00030359394993265256,
      "loss": 0.0525,
      "step": 49070
    },
    {
      "epoch": 14.384525205158265,
      "grad_norm": 1.3684765100479126,
      "learning_rate": 0.00030351015364506423,
      "loss": 0.0486,
      "step": 49080
    },
    {
      "epoch": 14.387456037514655,
      "grad_norm": 1.547398567199707,
      "learning_rate": 0.00030342635735747584,
      "loss": 0.0457,
      "step": 49090
    },
    {
      "epoch": 14.390386869871044,
      "grad_norm": 0.6446128487586975,
      "learning_rate": 0.0003033425610698875,
      "loss": 0.0589,
      "step": 49100
    },
    {
      "epoch": 14.393317702227433,
      "grad_norm": 1.3174657821655273,
      "learning_rate": 0.0003032587647822991,
      "loss": 0.0645,
      "step": 49110
    },
    {
      "epoch": 14.396248534583822,
      "grad_norm": 1.2053184509277344,
      "learning_rate": 0.0003031749684947107,
      "loss": 0.0883,
      "step": 49120
    },
    {
      "epoch": 14.399179366940212,
      "grad_norm": 1.0621448755264282,
      "learning_rate": 0.0003030911722071224,
      "loss": 0.0819,
      "step": 49130
    },
    {
      "epoch": 14.402110199296601,
      "grad_norm": 0.6916871070861816,
      "learning_rate": 0.00030300737591953405,
      "loss": 0.0909,
      "step": 49140
    },
    {
      "epoch": 14.40504103165299,
      "grad_norm": 1.982013463973999,
      "learning_rate": 0.00030292357963194566,
      "loss": 0.0658,
      "step": 49150
    },
    {
      "epoch": 14.40797186400938,
      "grad_norm": 1.9454529285430908,
      "learning_rate": 0.00030283978334435727,
      "loss": 0.075,
      "step": 49160
    },
    {
      "epoch": 14.410902696365769,
      "grad_norm": 1.0143561363220215,
      "learning_rate": 0.00030275598705676893,
      "loss": 0.0556,
      "step": 49170
    },
    {
      "epoch": 14.413833528722158,
      "grad_norm": 0.9621984958648682,
      "learning_rate": 0.00030267219076918054,
      "loss": 0.0806,
      "step": 49180
    },
    {
      "epoch": 14.416764361078545,
      "grad_norm": 0.9795873165130615,
      "learning_rate": 0.0003025883944815922,
      "loss": 0.0854,
      "step": 49190
    },
    {
      "epoch": 14.419695193434935,
      "grad_norm": 0.934939980506897,
      "learning_rate": 0.0003025045981940038,
      "loss": 0.069,
      "step": 49200
    },
    {
      "epoch": 14.422626025791324,
      "grad_norm": 0.23697996139526367,
      "learning_rate": 0.0003024208019064155,
      "loss": 0.0733,
      "step": 49210
    },
    {
      "epoch": 14.425556858147713,
      "grad_norm": 0.9400820136070251,
      "learning_rate": 0.0003023370056188271,
      "loss": 0.081,
      "step": 49220
    },
    {
      "epoch": 14.428487690504102,
      "grad_norm": 1.029904842376709,
      "learning_rate": 0.0003022532093312387,
      "loss": 0.0704,
      "step": 49230
    },
    {
      "epoch": 14.431418522860492,
      "grad_norm": 0.7221469879150391,
      "learning_rate": 0.00030216941304365036,
      "loss": 0.0599,
      "step": 49240
    },
    {
      "epoch": 14.434349355216881,
      "grad_norm": 0.5214663147926331,
      "learning_rate": 0.00030208561675606197,
      "loss": 0.0522,
      "step": 49250
    },
    {
      "epoch": 14.43728018757327,
      "grad_norm": 0.775244951248169,
      "learning_rate": 0.00030200182046847364,
      "loss": 0.0569,
      "step": 49260
    },
    {
      "epoch": 14.44021101992966,
      "grad_norm": 1.1349637508392334,
      "learning_rate": 0.00030191802418088525,
      "loss": 0.0502,
      "step": 49270
    },
    {
      "epoch": 14.443141852286049,
      "grad_norm": 0.5823032259941101,
      "learning_rate": 0.00030183422789329685,
      "loss": 0.0528,
      "step": 49280
    },
    {
      "epoch": 14.446072684642438,
      "grad_norm": 1.122230887413025,
      "learning_rate": 0.0003017504316057085,
      "loss": 0.0634,
      "step": 49290
    },
    {
      "epoch": 14.449003516998827,
      "grad_norm": 2.5856211185455322,
      "learning_rate": 0.00030166663531812013,
      "loss": 0.0969,
      "step": 49300
    },
    {
      "epoch": 14.451934349355216,
      "grad_norm": 0.7777029275894165,
      "learning_rate": 0.0003015828390305318,
      "loss": 0.0686,
      "step": 49310
    },
    {
      "epoch": 14.454865181711606,
      "grad_norm": 0.4878436326980591,
      "learning_rate": 0.00030149904274294346,
      "loss": 0.0521,
      "step": 49320
    },
    {
      "epoch": 14.457796014067995,
      "grad_norm": 1.2670243978500366,
      "learning_rate": 0.000301415246455355,
      "loss": 0.081,
      "step": 49330
    },
    {
      "epoch": 14.460726846424384,
      "grad_norm": 0.8846834301948547,
      "learning_rate": 0.0003013314501677667,
      "loss": 0.0791,
      "step": 49340
    },
    {
      "epoch": 14.463657678780773,
      "grad_norm": 1.5891962051391602,
      "learning_rate": 0.00030124765388017834,
      "loss": 0.0862,
      "step": 49350
    },
    {
      "epoch": 14.466588511137163,
      "grad_norm": 0.8054207563400269,
      "learning_rate": 0.00030116385759258995,
      "loss": 0.053,
      "step": 49360
    },
    {
      "epoch": 14.469519343493552,
      "grad_norm": 0.2433011680841446,
      "learning_rate": 0.0003010800613050016,
      "loss": 0.0809,
      "step": 49370
    },
    {
      "epoch": 14.472450175849941,
      "grad_norm": 0.6388727426528931,
      "learning_rate": 0.0003009962650174132,
      "loss": 0.0458,
      "step": 49380
    },
    {
      "epoch": 14.47538100820633,
      "grad_norm": 1.1584384441375732,
      "learning_rate": 0.00030091246872982483,
      "loss": 0.063,
      "step": 49390
    },
    {
      "epoch": 14.47831184056272,
      "grad_norm": 1.1188678741455078,
      "learning_rate": 0.0003008286724422365,
      "loss": 0.0687,
      "step": 49400
    },
    {
      "epoch": 14.481242672919109,
      "grad_norm": 0.19068320095539093,
      "learning_rate": 0.0003007448761546481,
      "loss": 0.0707,
      "step": 49410
    },
    {
      "epoch": 14.484173505275498,
      "grad_norm": 0.7302474975585938,
      "learning_rate": 0.00030066107986705977,
      "loss": 0.0642,
      "step": 49420
    },
    {
      "epoch": 14.487104337631887,
      "grad_norm": 0.5654675364494324,
      "learning_rate": 0.0003005772835794714,
      "loss": 0.064,
      "step": 49430
    },
    {
      "epoch": 14.490035169988277,
      "grad_norm": 1.432259202003479,
      "learning_rate": 0.000300493487291883,
      "loss": 0.0673,
      "step": 49440
    },
    {
      "epoch": 14.492966002344666,
      "grad_norm": 2.456465005874634,
      "learning_rate": 0.00030040969100429465,
      "loss": 0.0673,
      "step": 49450
    },
    {
      "epoch": 14.495896834701055,
      "grad_norm": 1.3256076574325562,
      "learning_rate": 0.00030032589471670626,
      "loss": 0.0695,
      "step": 49460
    },
    {
      "epoch": 14.498827667057444,
      "grad_norm": 1.3457129001617432,
      "learning_rate": 0.00030024209842911793,
      "loss": 0.0615,
      "step": 49470
    },
    {
      "epoch": 14.501758499413834,
      "grad_norm": 1.3117595911026,
      "learning_rate": 0.00030015830214152954,
      "loss": 0.0869,
      "step": 49480
    },
    {
      "epoch": 14.504689331770223,
      "grad_norm": 0.808408796787262,
      "learning_rate": 0.0003000745058539412,
      "loss": 0.1029,
      "step": 49490
    },
    {
      "epoch": 14.507620164126612,
      "grad_norm": 0.7358691096305847,
      "learning_rate": 0.0002999907095663528,
      "loss": 0.0528,
      "step": 49500
    },
    {
      "epoch": 14.510550996483001,
      "grad_norm": 1.1878306865692139,
      "learning_rate": 0.0002999069132787644,
      "loss": 0.0639,
      "step": 49510
    },
    {
      "epoch": 14.51348182883939,
      "grad_norm": 1.3411788940429688,
      "learning_rate": 0.0002998231169911761,
      "loss": 0.0684,
      "step": 49520
    },
    {
      "epoch": 14.51641266119578,
      "grad_norm": 2.125319242477417,
      "learning_rate": 0.0002997393207035877,
      "loss": 0.0813,
      "step": 49530
    },
    {
      "epoch": 14.519343493552169,
      "grad_norm": 0.7295176386833191,
      "learning_rate": 0.00029965552441599936,
      "loss": 0.0826,
      "step": 49540
    },
    {
      "epoch": 14.522274325908558,
      "grad_norm": 0.6328847408294678,
      "learning_rate": 0.000299571728128411,
      "loss": 0.062,
      "step": 49550
    },
    {
      "epoch": 14.525205158264948,
      "grad_norm": 0.5771353840827942,
      "learning_rate": 0.0002994879318408226,
      "loss": 0.073,
      "step": 49560
    },
    {
      "epoch": 14.528135990621337,
      "grad_norm": 1.0808181762695312,
      "learning_rate": 0.00029940413555323424,
      "loss": 0.0487,
      "step": 49570
    },
    {
      "epoch": 14.531066822977726,
      "grad_norm": 1.030508279800415,
      "learning_rate": 0.0002993203392656459,
      "loss": 0.0742,
      "step": 49580
    },
    {
      "epoch": 14.533997655334115,
      "grad_norm": 0.24644510447978973,
      "learning_rate": 0.0002992365429780575,
      "loss": 0.0655,
      "step": 49590
    },
    {
      "epoch": 14.536928487690504,
      "grad_norm": 1.388780117034912,
      "learning_rate": 0.0002991527466904692,
      "loss": 0.0641,
      "step": 49600
    },
    {
      "epoch": 14.539859320046894,
      "grad_norm": 1.1027911901474,
      "learning_rate": 0.0002990689504028808,
      "loss": 0.0587,
      "step": 49610
    },
    {
      "epoch": 14.542790152403283,
      "grad_norm": 1.1013227701187134,
      "learning_rate": 0.0002989851541152924,
      "loss": 0.0833,
      "step": 49620
    },
    {
      "epoch": 14.545720984759672,
      "grad_norm": 1.0622694492340088,
      "learning_rate": 0.00029890135782770406,
      "loss": 0.0562,
      "step": 49630
    },
    {
      "epoch": 14.548651817116061,
      "grad_norm": 0.1317131072282791,
      "learning_rate": 0.00029881756154011567,
      "loss": 0.0801,
      "step": 49640
    },
    {
      "epoch": 14.55158264947245,
      "grad_norm": 0.6653870940208435,
      "learning_rate": 0.00029873376525252734,
      "loss": 0.063,
      "step": 49650
    },
    {
      "epoch": 14.55451348182884,
      "grad_norm": 1.0840424299240112,
      "learning_rate": 0.00029864996896493895,
      "loss": 0.0792,
      "step": 49660
    },
    {
      "epoch": 14.55744431418523,
      "grad_norm": 2.081987142562866,
      "learning_rate": 0.00029856617267735055,
      "loss": 0.0614,
      "step": 49670
    },
    {
      "epoch": 14.560375146541618,
      "grad_norm": 0.5395758748054504,
      "learning_rate": 0.0002984823763897622,
      "loss": 0.0693,
      "step": 49680
    },
    {
      "epoch": 14.563305978898008,
      "grad_norm": 2.4073829650878906,
      "learning_rate": 0.00029839858010217383,
      "loss": 0.0749,
      "step": 49690
    },
    {
      "epoch": 14.566236811254397,
      "grad_norm": 0.3979542851448059,
      "learning_rate": 0.0002983147838145855,
      "loss": 0.0594,
      "step": 49700
    },
    {
      "epoch": 14.569167643610786,
      "grad_norm": 1.8029615879058838,
      "learning_rate": 0.0002982309875269971,
      "loss": 0.0868,
      "step": 49710
    },
    {
      "epoch": 14.572098475967175,
      "grad_norm": 1.0848177671432495,
      "learning_rate": 0.00029814719123940877,
      "loss": 0.0448,
      "step": 49720
    },
    {
      "epoch": 14.575029308323565,
      "grad_norm": 0.8339686989784241,
      "learning_rate": 0.0002980633949518204,
      "loss": 0.0608,
      "step": 49730
    },
    {
      "epoch": 14.577960140679954,
      "grad_norm": 1.4975858926773071,
      "learning_rate": 0.000297979598664232,
      "loss": 0.0581,
      "step": 49740
    },
    {
      "epoch": 14.580890973036343,
      "grad_norm": 0.33367258310317993,
      "learning_rate": 0.00029789580237664365,
      "loss": 0.0616,
      "step": 49750
    },
    {
      "epoch": 14.58382180539273,
      "grad_norm": 0.5855733752250671,
      "learning_rate": 0.0002978120060890553,
      "loss": 0.076,
      "step": 49760
    },
    {
      "epoch": 14.586752637749122,
      "grad_norm": 0.9456401467323303,
      "learning_rate": 0.0002977282098014669,
      "loss": 0.0851,
      "step": 49770
    },
    {
      "epoch": 14.58968347010551,
      "grad_norm": 1.3118176460266113,
      "learning_rate": 0.00029764441351387853,
      "loss": 0.0553,
      "step": 49780
    },
    {
      "epoch": 14.592614302461898,
      "grad_norm": 1.4821277856826782,
      "learning_rate": 0.0002975606172262902,
      "loss": 0.073,
      "step": 49790
    },
    {
      "epoch": 14.595545134818288,
      "grad_norm": 0.9418249130249023,
      "learning_rate": 0.0002974768209387018,
      "loss": 0.041,
      "step": 49800
    },
    {
      "epoch": 14.598475967174677,
      "grad_norm": 1.0349669456481934,
      "learning_rate": 0.00029739302465111347,
      "loss": 0.0431,
      "step": 49810
    },
    {
      "epoch": 14.601406799531066,
      "grad_norm": 6.117302417755127,
      "learning_rate": 0.0002973092283635251,
      "loss": 0.0551,
      "step": 49820
    },
    {
      "epoch": 14.604337631887455,
      "grad_norm": 0.7211228013038635,
      "learning_rate": 0.00029722543207593674,
      "loss": 0.0616,
      "step": 49830
    },
    {
      "epoch": 14.607268464243845,
      "grad_norm": 1.7620114088058472,
      "learning_rate": 0.00029714163578834835,
      "loss": 0.0703,
      "step": 49840
    },
    {
      "epoch": 14.610199296600234,
      "grad_norm": 0.42408859729766846,
      "learning_rate": 0.00029705783950075996,
      "loss": 0.0858,
      "step": 49850
    },
    {
      "epoch": 14.613130128956623,
      "grad_norm": 1.1114169359207153,
      "learning_rate": 0.0002969740432131716,
      "loss": 0.0653,
      "step": 49860
    },
    {
      "epoch": 14.616060961313012,
      "grad_norm": 1.2023316621780396,
      "learning_rate": 0.00029689024692558324,
      "loss": 0.0667,
      "step": 49870
    },
    {
      "epoch": 14.618991793669402,
      "grad_norm": 1.4934678077697754,
      "learning_rate": 0.0002968064506379949,
      "loss": 0.0802,
      "step": 49880
    },
    {
      "epoch": 14.62192262602579,
      "grad_norm": 1.0635429620742798,
      "learning_rate": 0.0002967226543504065,
      "loss": 0.066,
      "step": 49890
    },
    {
      "epoch": 14.62485345838218,
      "grad_norm": 1.6580984592437744,
      "learning_rate": 0.0002966388580628181,
      "loss": 0.0673,
      "step": 49900
    },
    {
      "epoch": 14.62778429073857,
      "grad_norm": 0.9257819652557373,
      "learning_rate": 0.0002965550617752298,
      "loss": 0.0759,
      "step": 49910
    },
    {
      "epoch": 14.630715123094959,
      "grad_norm": 1.1143561601638794,
      "learning_rate": 0.0002964712654876414,
      "loss": 0.0669,
      "step": 49920
    },
    {
      "epoch": 14.633645955451348,
      "grad_norm": 0.688025712966919,
      "learning_rate": 0.00029638746920005306,
      "loss": 0.0601,
      "step": 49930
    },
    {
      "epoch": 14.636576787807737,
      "grad_norm": 2.1717135906219482,
      "learning_rate": 0.0002963036729124647,
      "loss": 0.068,
      "step": 49940
    },
    {
      "epoch": 14.639507620164126,
      "grad_norm": 1.1559271812438965,
      "learning_rate": 0.0002962198766248763,
      "loss": 0.1077,
      "step": 49950
    },
    {
      "epoch": 14.642438452520516,
      "grad_norm": 1.8779356479644775,
      "learning_rate": 0.00029613608033728794,
      "loss": 0.1186,
      "step": 49960
    },
    {
      "epoch": 14.645369284876905,
      "grad_norm": 0.36148348450660706,
      "learning_rate": 0.00029605228404969955,
      "loss": 0.0599,
      "step": 49970
    },
    {
      "epoch": 14.648300117233294,
      "grad_norm": 0.7136821150779724,
      "learning_rate": 0.0002959684877621112,
      "loss": 0.073,
      "step": 49980
    },
    {
      "epoch": 14.651230949589683,
      "grad_norm": 0.4454635679721832,
      "learning_rate": 0.0002958846914745229,
      "loss": 0.0492,
      "step": 49990
    },
    {
      "epoch": 14.654161781946073,
      "grad_norm": 1.6024161577224731,
      "learning_rate": 0.0002958008951869345,
      "loss": 0.0917,
      "step": 50000
    },
    {
      "epoch": 14.657092614302462,
      "grad_norm": 0.5758274793624878,
      "learning_rate": 0.0002957170988993461,
      "loss": 0.0713,
      "step": 50010
    },
    {
      "epoch": 14.660023446658851,
      "grad_norm": 0.5932855606079102,
      "learning_rate": 0.00029563330261175776,
      "loss": 0.0634,
      "step": 50020
    },
    {
      "epoch": 14.66295427901524,
      "grad_norm": 1.3708692789077759,
      "learning_rate": 0.00029554950632416937,
      "loss": 0.0795,
      "step": 50030
    },
    {
      "epoch": 14.66588511137163,
      "grad_norm": 0.6418472528457642,
      "learning_rate": 0.00029546571003658104,
      "loss": 0.0875,
      "step": 50040
    },
    {
      "epoch": 14.668815943728019,
      "grad_norm": 0.8735381364822388,
      "learning_rate": 0.00029538191374899265,
      "loss": 0.0473,
      "step": 50050
    },
    {
      "epoch": 14.671746776084408,
      "grad_norm": 2.312882661819458,
      "learning_rate": 0.00029529811746140425,
      "loss": 0.0684,
      "step": 50060
    },
    {
      "epoch": 14.674677608440797,
      "grad_norm": 0.663130521774292,
      "learning_rate": 0.0002952143211738159,
      "loss": 0.0649,
      "step": 50070
    },
    {
      "epoch": 14.677608440797187,
      "grad_norm": 0.8789533972740173,
      "learning_rate": 0.00029513052488622753,
      "loss": 0.0601,
      "step": 50080
    },
    {
      "epoch": 14.680539273153576,
      "grad_norm": 1.8325824737548828,
      "learning_rate": 0.0002950467285986392,
      "loss": 0.0587,
      "step": 50090
    },
    {
      "epoch": 14.683470105509965,
      "grad_norm": 0.933824360370636,
      "learning_rate": 0.0002949629323110508,
      "loss": 0.0671,
      "step": 50100
    },
    {
      "epoch": 14.686400937866354,
      "grad_norm": 0.24311019480228424,
      "learning_rate": 0.00029487913602346247,
      "loss": 0.058,
      "step": 50110
    },
    {
      "epoch": 14.689331770222744,
      "grad_norm": 1.83169424533844,
      "learning_rate": 0.0002947953397358741,
      "loss": 0.0645,
      "step": 50120
    },
    {
      "epoch": 14.692262602579133,
      "grad_norm": 0.36856934428215027,
      "learning_rate": 0.0002947115434482857,
      "loss": 0.0495,
      "step": 50130
    },
    {
      "epoch": 14.695193434935522,
      "grad_norm": 0.7290777564048767,
      "learning_rate": 0.00029462774716069735,
      "loss": 0.1099,
      "step": 50140
    },
    {
      "epoch": 14.698124267291911,
      "grad_norm": 0.15310531854629517,
      "learning_rate": 0.00029454395087310896,
      "loss": 0.0847,
      "step": 50150
    },
    {
      "epoch": 14.7010550996483,
      "grad_norm": 1.4626572132110596,
      "learning_rate": 0.0002944601545855206,
      "loss": 0.0607,
      "step": 50160
    },
    {
      "epoch": 14.70398593200469,
      "grad_norm": 0.5820820927619934,
      "learning_rate": 0.0002943763582979323,
      "loss": 0.0661,
      "step": 50170
    },
    {
      "epoch": 14.706916764361079,
      "grad_norm": 0.8817124962806702,
      "learning_rate": 0.00029429256201034384,
      "loss": 0.0476,
      "step": 50180
    },
    {
      "epoch": 14.709847596717468,
      "grad_norm": 0.9415869116783142,
      "learning_rate": 0.0002942087657227555,
      "loss": 0.0696,
      "step": 50190
    },
    {
      "epoch": 14.712778429073857,
      "grad_norm": 0.7997698187828064,
      "learning_rate": 0.00029412496943516717,
      "loss": 0.0377,
      "step": 50200
    },
    {
      "epoch": 14.715709261430247,
      "grad_norm": 2.1124234199523926,
      "learning_rate": 0.0002940411731475788,
      "loss": 0.0821,
      "step": 50210
    },
    {
      "epoch": 14.718640093786636,
      "grad_norm": 1.9273781776428223,
      "learning_rate": 0.00029395737685999044,
      "loss": 0.0533,
      "step": 50220
    },
    {
      "epoch": 14.721570926143025,
      "grad_norm": 1.170237421989441,
      "learning_rate": 0.00029387358057240205,
      "loss": 0.0496,
      "step": 50230
    },
    {
      "epoch": 14.724501758499414,
      "grad_norm": 4.611919403076172,
      "learning_rate": 0.00029378978428481366,
      "loss": 0.0634,
      "step": 50240
    },
    {
      "epoch": 14.727432590855804,
      "grad_norm": 0.39811593294143677,
      "learning_rate": 0.0002937059879972253,
      "loss": 0.0648,
      "step": 50250
    },
    {
      "epoch": 14.730363423212193,
      "grad_norm": 0.8903324007987976,
      "learning_rate": 0.00029362219170963694,
      "loss": 0.0596,
      "step": 50260
    },
    {
      "epoch": 14.733294255568582,
      "grad_norm": 0.8728121519088745,
      "learning_rate": 0.0002935383954220486,
      "loss": 0.0479,
      "step": 50270
    },
    {
      "epoch": 14.736225087924971,
      "grad_norm": 1.3061939477920532,
      "learning_rate": 0.0002934545991344602,
      "loss": 0.0807,
      "step": 50280
    },
    {
      "epoch": 14.73915592028136,
      "grad_norm": 0.6778063178062439,
      "learning_rate": 0.0002933708028468718,
      "loss": 0.0551,
      "step": 50290
    },
    {
      "epoch": 14.74208675263775,
      "grad_norm": 0.5112071633338928,
      "learning_rate": 0.0002932870065592835,
      "loss": 0.0797,
      "step": 50300
    },
    {
      "epoch": 14.745017584994137,
      "grad_norm": 2.0215656757354736,
      "learning_rate": 0.0002932032102716951,
      "loss": 0.0886,
      "step": 50310
    },
    {
      "epoch": 14.747948417350528,
      "grad_norm": 0.888224184513092,
      "learning_rate": 0.00029311941398410676,
      "loss": 0.0723,
      "step": 50320
    },
    {
      "epoch": 14.750879249706916,
      "grad_norm": 1.2489882707595825,
      "learning_rate": 0.00029303561769651837,
      "loss": 0.0735,
      "step": 50330
    },
    {
      "epoch": 14.753810082063307,
      "grad_norm": 1.5321608781814575,
      "learning_rate": 0.00029295182140893003,
      "loss": 0.054,
      "step": 50340
    },
    {
      "epoch": 14.756740914419694,
      "grad_norm": 0.37434935569763184,
      "learning_rate": 0.00029286802512134164,
      "loss": 0.0452,
      "step": 50350
    },
    {
      "epoch": 14.759671746776084,
      "grad_norm": 0.5125227570533752,
      "learning_rate": 0.00029278422883375325,
      "loss": 0.0544,
      "step": 50360
    },
    {
      "epoch": 14.762602579132473,
      "grad_norm": 1.4087773561477661,
      "learning_rate": 0.0002927004325461649,
      "loss": 0.0582,
      "step": 50370
    },
    {
      "epoch": 14.765533411488862,
      "grad_norm": 0.987290620803833,
      "learning_rate": 0.0002926166362585766,
      "loss": 0.049,
      "step": 50380
    },
    {
      "epoch": 14.768464243845251,
      "grad_norm": 0.49555864930152893,
      "learning_rate": 0.0002925328399709882,
      "loss": 0.0677,
      "step": 50390
    },
    {
      "epoch": 14.77139507620164,
      "grad_norm": 0.6374099850654602,
      "learning_rate": 0.0002924490436833998,
      "loss": 0.0529,
      "step": 50400
    },
    {
      "epoch": 14.77432590855803,
      "grad_norm": 1.7421672344207764,
      "learning_rate": 0.0002923652473958114,
      "loss": 0.0787,
      "step": 50410
    },
    {
      "epoch": 14.777256740914419,
      "grad_norm": 0.2834952175617218,
      "learning_rate": 0.00029228145110822307,
      "loss": 0.069,
      "step": 50420
    },
    {
      "epoch": 14.780187573270808,
      "grad_norm": 0.3405732214450836,
      "learning_rate": 0.00029219765482063474,
      "loss": 0.0714,
      "step": 50430
    },
    {
      "epoch": 14.783118405627198,
      "grad_norm": 1.3530672788619995,
      "learning_rate": 0.00029211385853304635,
      "loss": 0.0657,
      "step": 50440
    },
    {
      "epoch": 14.786049237983587,
      "grad_norm": 0.5058804750442505,
      "learning_rate": 0.000292030062245458,
      "loss": 0.0633,
      "step": 50450
    },
    {
      "epoch": 14.788980070339976,
      "grad_norm": 1.5016529560089111,
      "learning_rate": 0.0002919462659578696,
      "loss": 0.0861,
      "step": 50460
    },
    {
      "epoch": 14.791910902696365,
      "grad_norm": 0.6677150130271912,
      "learning_rate": 0.00029186246967028123,
      "loss": 0.0625,
      "step": 50470
    },
    {
      "epoch": 14.794841735052755,
      "grad_norm": 0.6239057183265686,
      "learning_rate": 0.0002917786733826929,
      "loss": 0.0533,
      "step": 50480
    },
    {
      "epoch": 14.797772567409144,
      "grad_norm": 1.4938552379608154,
      "learning_rate": 0.0002916948770951045,
      "loss": 0.0766,
      "step": 50490
    },
    {
      "epoch": 14.800703399765533,
      "grad_norm": 1.4172745943069458,
      "learning_rate": 0.00029161108080751617,
      "loss": 0.06,
      "step": 50500
    },
    {
      "epoch": 14.803634232121922,
      "grad_norm": 0.3467101752758026,
      "learning_rate": 0.0002915272845199278,
      "loss": 0.0672,
      "step": 50510
    },
    {
      "epoch": 14.806565064478312,
      "grad_norm": 1.1387691497802734,
      "learning_rate": 0.0002914434882323394,
      "loss": 0.059,
      "step": 50520
    },
    {
      "epoch": 14.8094958968347,
      "grad_norm": 0.3254851698875427,
      "learning_rate": 0.00029135969194475105,
      "loss": 0.0656,
      "step": 50530
    },
    {
      "epoch": 14.81242672919109,
      "grad_norm": 2.2111456394195557,
      "learning_rate": 0.00029127589565716266,
      "loss": 0.0773,
      "step": 50540
    },
    {
      "epoch": 14.81535756154748,
      "grad_norm": 1.2085756063461304,
      "learning_rate": 0.0002911920993695743,
      "loss": 0.0563,
      "step": 50550
    },
    {
      "epoch": 14.818288393903869,
      "grad_norm": 1.3397101163864136,
      "learning_rate": 0.000291108303081986,
      "loss": 0.0686,
      "step": 50560
    },
    {
      "epoch": 14.821219226260258,
      "grad_norm": 0.5942093729972839,
      "learning_rate": 0.00029102450679439754,
      "loss": 0.0677,
      "step": 50570
    },
    {
      "epoch": 14.824150058616647,
      "grad_norm": 1.1803570985794067,
      "learning_rate": 0.0002909407105068092,
      "loss": 0.0608,
      "step": 50580
    },
    {
      "epoch": 14.827080890973036,
      "grad_norm": 0.9409845471382141,
      "learning_rate": 0.0002908569142192208,
      "loss": 0.0603,
      "step": 50590
    },
    {
      "epoch": 14.830011723329426,
      "grad_norm": 1.81209397315979,
      "learning_rate": 0.0002907731179316325,
      "loss": 0.0709,
      "step": 50600
    },
    {
      "epoch": 14.832942555685815,
      "grad_norm": 1.667360544204712,
      "learning_rate": 0.00029068932164404414,
      "loss": 0.0446,
      "step": 50610
    },
    {
      "epoch": 14.835873388042204,
      "grad_norm": 1.3650974035263062,
      "learning_rate": 0.00029060552535645575,
      "loss": 0.0698,
      "step": 50620
    },
    {
      "epoch": 14.838804220398593,
      "grad_norm": 1.01335871219635,
      "learning_rate": 0.00029052172906886736,
      "loss": 0.0694,
      "step": 50630
    },
    {
      "epoch": 14.841735052754983,
      "grad_norm": 1.3689942359924316,
      "learning_rate": 0.000290437932781279,
      "loss": 0.0787,
      "step": 50640
    },
    {
      "epoch": 14.844665885111372,
      "grad_norm": 1.465817928314209,
      "learning_rate": 0.00029035413649369064,
      "loss": 0.0658,
      "step": 50650
    },
    {
      "epoch": 14.847596717467761,
      "grad_norm": 0.26803821325302124,
      "learning_rate": 0.0002902703402061023,
      "loss": 0.0579,
      "step": 50660
    },
    {
      "epoch": 14.85052754982415,
      "grad_norm": 1.0451312065124512,
      "learning_rate": 0.0002901865439185139,
      "loss": 0.0478,
      "step": 50670
    },
    {
      "epoch": 14.85345838218054,
      "grad_norm": 0.7854064106941223,
      "learning_rate": 0.0002901027476309255,
      "loss": 0.0625,
      "step": 50680
    },
    {
      "epoch": 14.856389214536929,
      "grad_norm": 0.9309899806976318,
      "learning_rate": 0.0002900189513433372,
      "loss": 0.0599,
      "step": 50690
    },
    {
      "epoch": 14.859320046893318,
      "grad_norm": 2.278264045715332,
      "learning_rate": 0.0002899351550557488,
      "loss": 0.0731,
      "step": 50700
    },
    {
      "epoch": 14.862250879249707,
      "grad_norm": 0.47821521759033203,
      "learning_rate": 0.00028985135876816046,
      "loss": 0.0585,
      "step": 50710
    },
    {
      "epoch": 14.865181711606096,
      "grad_norm": 0.1317870318889618,
      "learning_rate": 0.00028976756248057207,
      "loss": 0.0785,
      "step": 50720
    },
    {
      "epoch": 14.868112543962486,
      "grad_norm": 1.0140900611877441,
      "learning_rate": 0.00028968376619298373,
      "loss": 0.0472,
      "step": 50730
    },
    {
      "epoch": 14.871043376318875,
      "grad_norm": 0.3157273530960083,
      "learning_rate": 0.00028959996990539534,
      "loss": 0.0798,
      "step": 50740
    },
    {
      "epoch": 14.873974208675264,
      "grad_norm": 0.25696349143981934,
      "learning_rate": 0.00028951617361780695,
      "loss": 0.076,
      "step": 50750
    },
    {
      "epoch": 14.876905041031653,
      "grad_norm": 1.8536120653152466,
      "learning_rate": 0.0002894323773302186,
      "loss": 0.0556,
      "step": 50760
    },
    {
      "epoch": 14.879835873388043,
      "grad_norm": 1.0690068006515503,
      "learning_rate": 0.0002893485810426302,
      "loss": 0.087,
      "step": 50770
    },
    {
      "epoch": 14.882766705744432,
      "grad_norm": 0.31180059909820557,
      "learning_rate": 0.0002892647847550419,
      "loss": 0.0661,
      "step": 50780
    },
    {
      "epoch": 14.885697538100821,
      "grad_norm": 0.8909292221069336,
      "learning_rate": 0.00028918098846745355,
      "loss": 0.0739,
      "step": 50790
    },
    {
      "epoch": 14.88862837045721,
      "grad_norm": 2.8909802436828613,
      "learning_rate": 0.0002890971921798651,
      "loss": 0.0531,
      "step": 50800
    },
    {
      "epoch": 14.8915592028136,
      "grad_norm": 1.7449041604995728,
      "learning_rate": 0.00028901339589227677,
      "loss": 0.0681,
      "step": 50810
    },
    {
      "epoch": 14.894490035169989,
      "grad_norm": 0.8574650883674622,
      "learning_rate": 0.00028892959960468844,
      "loss": 0.056,
      "step": 50820
    },
    {
      "epoch": 14.897420867526378,
      "grad_norm": 2.0430524349212646,
      "learning_rate": 0.00028884580331710004,
      "loss": 0.0619,
      "step": 50830
    },
    {
      "epoch": 14.900351699882767,
      "grad_norm": 0.6434128284454346,
      "learning_rate": 0.0002887620070295117,
      "loss": 0.0651,
      "step": 50840
    },
    {
      "epoch": 14.903282532239157,
      "grad_norm": 0.8087924718856812,
      "learning_rate": 0.0002886782107419233,
      "loss": 0.0794,
      "step": 50850
    },
    {
      "epoch": 14.906213364595546,
      "grad_norm": 1.4714641571044922,
      "learning_rate": 0.00028859441445433493,
      "loss": 0.0683,
      "step": 50860
    },
    {
      "epoch": 14.909144196951935,
      "grad_norm": 1.0230801105499268,
      "learning_rate": 0.0002885106181667466,
      "loss": 0.0653,
      "step": 50870
    },
    {
      "epoch": 14.912075029308323,
      "grad_norm": 1.862481951713562,
      "learning_rate": 0.0002884268218791582,
      "loss": 0.072,
      "step": 50880
    },
    {
      "epoch": 14.915005861664714,
      "grad_norm": 0.9880383014678955,
      "learning_rate": 0.00028834302559156987,
      "loss": 0.0711,
      "step": 50890
    },
    {
      "epoch": 14.917936694021101,
      "grad_norm": 1.320739507675171,
      "learning_rate": 0.0002882592293039815,
      "loss": 0.0721,
      "step": 50900
    },
    {
      "epoch": 14.920867526377492,
      "grad_norm": 1.2821930646896362,
      "learning_rate": 0.0002881754330163931,
      "loss": 0.0543,
      "step": 50910
    },
    {
      "epoch": 14.92379835873388,
      "grad_norm": 0.9578102827072144,
      "learning_rate": 0.00028809163672880475,
      "loss": 0.0863,
      "step": 50920
    },
    {
      "epoch": 14.926729191090269,
      "grad_norm": 0.5935840606689453,
      "learning_rate": 0.00028800784044121636,
      "loss": 0.0598,
      "step": 50930
    },
    {
      "epoch": 14.929660023446658,
      "grad_norm": 1.2285254001617432,
      "learning_rate": 0.000287924044153628,
      "loss": 0.0677,
      "step": 50940
    },
    {
      "epoch": 14.932590855803047,
      "grad_norm": 0.7008522748947144,
      "learning_rate": 0.00028784024786603963,
      "loss": 0.0846,
      "step": 50950
    },
    {
      "epoch": 14.935521688159437,
      "grad_norm": 0.35879766941070557,
      "learning_rate": 0.0002877564515784513,
      "loss": 0.0669,
      "step": 50960
    },
    {
      "epoch": 14.938452520515826,
      "grad_norm": 1.3037782907485962,
      "learning_rate": 0.0002876726552908629,
      "loss": 0.0389,
      "step": 50970
    },
    {
      "epoch": 14.941383352872215,
      "grad_norm": 0.9851685166358948,
      "learning_rate": 0.0002875888590032745,
      "loss": 0.0467,
      "step": 50980
    },
    {
      "epoch": 14.944314185228604,
      "grad_norm": 2.9457077980041504,
      "learning_rate": 0.0002875050627156862,
      "loss": 0.0749,
      "step": 50990
    },
    {
      "epoch": 14.947245017584994,
      "grad_norm": 0.5144038200378418,
      "learning_rate": 0.00028742126642809784,
      "loss": 0.0618,
      "step": 51000
    },
    {
      "epoch": 14.950175849941383,
      "grad_norm": 1.0896830558776855,
      "learning_rate": 0.00028733747014050945,
      "loss": 0.0581,
      "step": 51010
    },
    {
      "epoch": 14.953106682297772,
      "grad_norm": 0.8577635288238525,
      "learning_rate": 0.00028725367385292106,
      "loss": 0.0471,
      "step": 51020
    },
    {
      "epoch": 14.956037514654161,
      "grad_norm": 0.7127417325973511,
      "learning_rate": 0.00028716987756533267,
      "loss": 0.0603,
      "step": 51030
    },
    {
      "epoch": 14.95896834701055,
      "grad_norm": 3.7472753524780273,
      "learning_rate": 0.00028708608127774434,
      "loss": 0.055,
      "step": 51040
    },
    {
      "epoch": 14.96189917936694,
      "grad_norm": 0.3080337643623352,
      "learning_rate": 0.000287002284990156,
      "loss": 0.0711,
      "step": 51050
    },
    {
      "epoch": 14.964830011723329,
      "grad_norm": 1.5918874740600586,
      "learning_rate": 0.0002869184887025676,
      "loss": 0.0685,
      "step": 51060
    },
    {
      "epoch": 14.967760844079718,
      "grad_norm": 0.8516974449157715,
      "learning_rate": 0.0002868346924149793,
      "loss": 0.0706,
      "step": 51070
    },
    {
      "epoch": 14.970691676436108,
      "grad_norm": 0.8470181226730347,
      "learning_rate": 0.0002867508961273909,
      "loss": 0.0681,
      "step": 51080
    },
    {
      "epoch": 14.973622508792497,
      "grad_norm": 1.7923011779785156,
      "learning_rate": 0.0002866670998398025,
      "loss": 0.094,
      "step": 51090
    },
    {
      "epoch": 14.976553341148886,
      "grad_norm": 0.5291940569877625,
      "learning_rate": 0.00028658330355221416,
      "loss": 0.0784,
      "step": 51100
    },
    {
      "epoch": 14.979484173505275,
      "grad_norm": 0.16520005464553833,
      "learning_rate": 0.00028649950726462577,
      "loss": 0.0734,
      "step": 51110
    },
    {
      "epoch": 14.982415005861665,
      "grad_norm": 0.5447828769683838,
      "learning_rate": 0.00028641571097703743,
      "loss": 0.0718,
      "step": 51120
    },
    {
      "epoch": 14.985345838218054,
      "grad_norm": 0.9120280742645264,
      "learning_rate": 0.00028633191468944904,
      "loss": 0.0772,
      "step": 51130
    },
    {
      "epoch": 14.988276670574443,
      "grad_norm": 1.7300087213516235,
      "learning_rate": 0.00028624811840186065,
      "loss": 0.054,
      "step": 51140
    },
    {
      "epoch": 14.991207502930832,
      "grad_norm": 1.2931666374206543,
      "learning_rate": 0.0002861643221142723,
      "loss": 0.098,
      "step": 51150
    },
    {
      "epoch": 14.994138335287222,
      "grad_norm": 0.9697428345680237,
      "learning_rate": 0.0002860805258266839,
      "loss": 0.0688,
      "step": 51160
    },
    {
      "epoch": 14.99706916764361,
      "grad_norm": 0.34264805912971497,
      "learning_rate": 0.0002859967295390956,
      "loss": 0.0795,
      "step": 51170
    },
    {
      "epoch": 15.0,
      "grad_norm": 3.482424736022949,
      "learning_rate": 0.0002859129332515072,
      "loss": 0.0629,
      "step": 51180
    },
    {
      "epoch": 15.0,
      "eval_accuracy": 0.7523903725684141,
      "eval_f1_macro": 0.78794272533042,
      "eval_f1_micro": 0.8266708307307934,
      "eval_f1_weighted": 0.8175966228456841,
      "eval_loss": 0.07709819078445435,
      "eval_roc_auc": 0.8818709061286308,
      "eval_runtime": 142.2083,
      "eval_samples_per_second": 21.328,
      "eval_steps_per_second": 2.672,
      "step": 51180
    },
    {
      "epoch": 15.00293083235639,
      "grad_norm": 0.5482380390167236,
      "learning_rate": 0.0002858291369639188,
      "loss": 0.0621,
      "step": 51190
    },
    {
      "epoch": 15.005861664712778,
      "grad_norm": 0.9864110350608826,
      "learning_rate": 0.00028574534067633047,
      "loss": 0.0494,
      "step": 51200
    },
    {
      "epoch": 15.008792497069168,
      "grad_norm": 0.5081949234008789,
      "learning_rate": 0.0002856615443887421,
      "loss": 0.0599,
      "step": 51210
    },
    {
      "epoch": 15.011723329425557,
      "grad_norm": 2.3637466430664062,
      "learning_rate": 0.00028557774810115374,
      "loss": 0.0799,
      "step": 51220
    },
    {
      "epoch": 15.014654161781946,
      "grad_norm": 2.535085439682007,
      "learning_rate": 0.0002854939518135654,
      "loss": 0.0952,
      "step": 51230
    },
    {
      "epoch": 15.017584994138335,
      "grad_norm": 1.5218476057052612,
      "learning_rate": 0.000285410155525977,
      "loss": 0.0638,
      "step": 51240
    },
    {
      "epoch": 15.020515826494725,
      "grad_norm": 2.691131830215454,
      "learning_rate": 0.00028532635923838863,
      "loss": 0.0467,
      "step": 51250
    },
    {
      "epoch": 15.023446658851114,
      "grad_norm": 1.3501026630401611,
      "learning_rate": 0.0002852425629508003,
      "loss": 0.0492,
      "step": 51260
    },
    {
      "epoch": 15.026377491207503,
      "grad_norm": 0.5210558176040649,
      "learning_rate": 0.0002851587666632119,
      "loss": 0.0875,
      "step": 51270
    },
    {
      "epoch": 15.029308323563892,
      "grad_norm": 0.10707774013280869,
      "learning_rate": 0.00028507497037562357,
      "loss": 0.0483,
      "step": 51280
    },
    {
      "epoch": 15.032239155920282,
      "grad_norm": 1.234490156173706,
      "learning_rate": 0.0002849911740880352,
      "loss": 0.0636,
      "step": 51290
    },
    {
      "epoch": 15.035169988276671,
      "grad_norm": 0.573274552822113,
      "learning_rate": 0.0002849073778004468,
      "loss": 0.0468,
      "step": 51300
    },
    {
      "epoch": 15.03810082063306,
      "grad_norm": 0.3877272605895996,
      "learning_rate": 0.00028482358151285845,
      "loss": 0.0406,
      "step": 51310
    },
    {
      "epoch": 15.04103165298945,
      "grad_norm": 0.24835297465324402,
      "learning_rate": 0.00028473978522527006,
      "loss": 0.0407,
      "step": 51320
    },
    {
      "epoch": 15.043962485345839,
      "grad_norm": 1.2225432395935059,
      "learning_rate": 0.0002846559889376817,
      "loss": 0.0795,
      "step": 51330
    },
    {
      "epoch": 15.046893317702228,
      "grad_norm": 1.7391475439071655,
      "learning_rate": 0.00028457219265009333,
      "loss": 0.0599,
      "step": 51340
    },
    {
      "epoch": 15.049824150058617,
      "grad_norm": 0.8430820107460022,
      "learning_rate": 0.000284488396362505,
      "loss": 0.06,
      "step": 51350
    },
    {
      "epoch": 15.052754982415006,
      "grad_norm": 1.070231556892395,
      "learning_rate": 0.0002844046000749166,
      "loss": 0.0543,
      "step": 51360
    },
    {
      "epoch": 15.055685814771396,
      "grad_norm": 0.20482410490512848,
      "learning_rate": 0.0002843208037873282,
      "loss": 0.0567,
      "step": 51370
    },
    {
      "epoch": 15.058616647127785,
      "grad_norm": 0.29152223467826843,
      "learning_rate": 0.0002842370074997399,
      "loss": 0.0675,
      "step": 51380
    },
    {
      "epoch": 15.061547479484174,
      "grad_norm": 0.8544949293136597,
      "learning_rate": 0.0002841532112121515,
      "loss": 0.0644,
      "step": 51390
    },
    {
      "epoch": 15.064478311840563,
      "grad_norm": 0.3208880126476288,
      "learning_rate": 0.00028406941492456315,
      "loss": 0.0795,
      "step": 51400
    },
    {
      "epoch": 15.067409144196953,
      "grad_norm": 1.818705677986145,
      "learning_rate": 0.0002839856186369748,
      "loss": 0.0755,
      "step": 51410
    },
    {
      "epoch": 15.070339976553342,
      "grad_norm": 0.651347279548645,
      "learning_rate": 0.00028390182234938637,
      "loss": 0.0497,
      "step": 51420
    },
    {
      "epoch": 15.073270808909731,
      "grad_norm": 0.6944334506988525,
      "learning_rate": 0.00028381802606179804,
      "loss": 0.0583,
      "step": 51430
    },
    {
      "epoch": 15.07620164126612,
      "grad_norm": 2.641972541809082,
      "learning_rate": 0.0002837342297742097,
      "loss": 0.0754,
      "step": 51440
    },
    {
      "epoch": 15.07913247362251,
      "grad_norm": 0.6852163076400757,
      "learning_rate": 0.0002836504334866213,
      "loss": 0.0613,
      "step": 51450
    },
    {
      "epoch": 15.082063305978899,
      "grad_norm": 0.17251476645469666,
      "learning_rate": 0.000283566637199033,
      "loss": 0.0519,
      "step": 51460
    },
    {
      "epoch": 15.084994138335286,
      "grad_norm": 1.0534744262695312,
      "learning_rate": 0.00028348284091144453,
      "loss": 0.0632,
      "step": 51470
    },
    {
      "epoch": 15.087924970691676,
      "grad_norm": 1.149113416671753,
      "learning_rate": 0.0002833990446238562,
      "loss": 0.0646,
      "step": 51480
    },
    {
      "epoch": 15.090855803048065,
      "grad_norm": 1.054916501045227,
      "learning_rate": 0.00028331524833626786,
      "loss": 0.0631,
      "step": 51490
    },
    {
      "epoch": 15.093786635404454,
      "grad_norm": 0.2718624472618103,
      "learning_rate": 0.00028323145204867947,
      "loss": 0.0477,
      "step": 51500
    },
    {
      "epoch": 15.096717467760843,
      "grad_norm": 0.44405123591423035,
      "learning_rate": 0.00028314765576109113,
      "loss": 0.0634,
      "step": 51510
    },
    {
      "epoch": 15.099648300117233,
      "grad_norm": 2.5067005157470703,
      "learning_rate": 0.00028306385947350274,
      "loss": 0.0549,
      "step": 51520
    },
    {
      "epoch": 15.102579132473622,
      "grad_norm": 1.0024224519729614,
      "learning_rate": 0.00028298006318591435,
      "loss": 0.0591,
      "step": 51530
    },
    {
      "epoch": 15.105509964830011,
      "grad_norm": 0.09470817446708679,
      "learning_rate": 0.000282896266898326,
      "loss": 0.034,
      "step": 51540
    },
    {
      "epoch": 15.1084407971864,
      "grad_norm": 0.7020331025123596,
      "learning_rate": 0.0002828124706107376,
      "loss": 0.0481,
      "step": 51550
    },
    {
      "epoch": 15.11137162954279,
      "grad_norm": 1.14100980758667,
      "learning_rate": 0.0002827286743231493,
      "loss": 0.0855,
      "step": 51560
    },
    {
      "epoch": 15.114302461899179,
      "grad_norm": 1.0734868049621582,
      "learning_rate": 0.0002826448780355609,
      "loss": 0.0788,
      "step": 51570
    },
    {
      "epoch": 15.117233294255568,
      "grad_norm": 1.4432427883148193,
      "learning_rate": 0.00028256108174797256,
      "loss": 0.0707,
      "step": 51580
    },
    {
      "epoch": 15.120164126611957,
      "grad_norm": 1.6310780048370361,
      "learning_rate": 0.00028247728546038417,
      "loss": 0.0651,
      "step": 51590
    },
    {
      "epoch": 15.123094958968347,
      "grad_norm": 0.5277265906333923,
      "learning_rate": 0.0002823934891727958,
      "loss": 0.0509,
      "step": 51600
    },
    {
      "epoch": 15.126025791324736,
      "grad_norm": 0.3508606255054474,
      "learning_rate": 0.00028230969288520744,
      "loss": 0.0401,
      "step": 51610
    },
    {
      "epoch": 15.128956623681125,
      "grad_norm": 2.3777682781219482,
      "learning_rate": 0.0002822258965976191,
      "loss": 0.0767,
      "step": 51620
    },
    {
      "epoch": 15.131887456037514,
      "grad_norm": 1.660830020904541,
      "learning_rate": 0.0002821421003100307,
      "loss": 0.0767,
      "step": 51630
    },
    {
      "epoch": 15.134818288393904,
      "grad_norm": 2.426873207092285,
      "learning_rate": 0.00028205830402244233,
      "loss": 0.0798,
      "step": 51640
    },
    {
      "epoch": 15.137749120750293,
      "grad_norm": 1.034237027168274,
      "learning_rate": 0.00028197450773485394,
      "loss": 0.0477,
      "step": 51650
    },
    {
      "epoch": 15.140679953106682,
      "grad_norm": 0.8243153691291809,
      "learning_rate": 0.0002818907114472656,
      "loss": 0.0863,
      "step": 51660
    },
    {
      "epoch": 15.143610785463071,
      "grad_norm": 0.8677259087562561,
      "learning_rate": 0.00028180691515967727,
      "loss": 0.0759,
      "step": 51670
    },
    {
      "epoch": 15.14654161781946,
      "grad_norm": 0.5750635266304016,
      "learning_rate": 0.0002817231188720889,
      "loss": 0.0631,
      "step": 51680
    },
    {
      "epoch": 15.14947245017585,
      "grad_norm": 2.6728227138519287,
      "learning_rate": 0.00028163932258450054,
      "loss": 0.0542,
      "step": 51690
    },
    {
      "epoch": 15.152403282532239,
      "grad_norm": 1.8322607278823853,
      "learning_rate": 0.00028155552629691215,
      "loss": 0.0637,
      "step": 51700
    },
    {
      "epoch": 15.155334114888628,
      "grad_norm": 0.8036349415779114,
      "learning_rate": 0.00028147173000932376,
      "loss": 0.0774,
      "step": 51710
    },
    {
      "epoch": 15.158264947245017,
      "grad_norm": 0.6794881224632263,
      "learning_rate": 0.0002813879337217354,
      "loss": 0.0597,
      "step": 51720
    },
    {
      "epoch": 15.161195779601407,
      "grad_norm": 0.7259800434112549,
      "learning_rate": 0.00028130413743414703,
      "loss": 0.0676,
      "step": 51730
    },
    {
      "epoch": 15.164126611957796,
      "grad_norm": 2.465894937515259,
      "learning_rate": 0.0002812203411465587,
      "loss": 0.0651,
      "step": 51740
    },
    {
      "epoch": 15.167057444314185,
      "grad_norm": 0.9146049618721008,
      "learning_rate": 0.0002811365448589703,
      "loss": 0.076,
      "step": 51750
    },
    {
      "epoch": 15.169988276670574,
      "grad_norm": 1.8014508485794067,
      "learning_rate": 0.0002810527485713819,
      "loss": 0.0657,
      "step": 51760
    },
    {
      "epoch": 15.172919109026964,
      "grad_norm": 1.4325908422470093,
      "learning_rate": 0.0002809689522837936,
      "loss": 0.0604,
      "step": 51770
    },
    {
      "epoch": 15.175849941383353,
      "grad_norm": 0.5646610260009766,
      "learning_rate": 0.0002808851559962052,
      "loss": 0.0595,
      "step": 51780
    },
    {
      "epoch": 15.178780773739742,
      "grad_norm": 0.39887991547584534,
      "learning_rate": 0.00028080135970861685,
      "loss": 0.0637,
      "step": 51790
    },
    {
      "epoch": 15.181711606096131,
      "grad_norm": 0.9747325778007507,
      "learning_rate": 0.00028071756342102846,
      "loss": 0.0677,
      "step": 51800
    },
    {
      "epoch": 15.18464243845252,
      "grad_norm": 1.3093708753585815,
      "learning_rate": 0.00028063376713344007,
      "loss": 0.0471,
      "step": 51810
    },
    {
      "epoch": 15.18757327080891,
      "grad_norm": 0.9577109217643738,
      "learning_rate": 0.00028054997084585174,
      "loss": 0.0731,
      "step": 51820
    },
    {
      "epoch": 15.1905041031653,
      "grad_norm": 0.5645009875297546,
      "learning_rate": 0.00028046617455826335,
      "loss": 0.0545,
      "step": 51830
    },
    {
      "epoch": 15.193434935521688,
      "grad_norm": 0.4879787862300873,
      "learning_rate": 0.000280382378270675,
      "loss": 0.0459,
      "step": 51840
    },
    {
      "epoch": 15.196365767878078,
      "grad_norm": 0.3224005699157715,
      "learning_rate": 0.0002802985819830867,
      "loss": 0.0574,
      "step": 51850
    },
    {
      "epoch": 15.199296600234467,
      "grad_norm": 0.6619260907173157,
      "learning_rate": 0.0002802147856954983,
      "loss": 0.0396,
      "step": 51860
    },
    {
      "epoch": 15.202227432590856,
      "grad_norm": 4.02855110168457,
      "learning_rate": 0.0002801309894079099,
      "loss": 0.0686,
      "step": 51870
    },
    {
      "epoch": 15.205158264947245,
      "grad_norm": 1.5001567602157593,
      "learning_rate": 0.00028004719312032156,
      "loss": 0.0758,
      "step": 51880
    },
    {
      "epoch": 15.208089097303635,
      "grad_norm": 0.6728359460830688,
      "learning_rate": 0.00027996339683273317,
      "loss": 0.0509,
      "step": 51890
    },
    {
      "epoch": 15.211019929660024,
      "grad_norm": 0.44644877314567566,
      "learning_rate": 0.00027987960054514483,
      "loss": 0.0597,
      "step": 51900
    },
    {
      "epoch": 15.213950762016413,
      "grad_norm": 0.6405308246612549,
      "learning_rate": 0.00027979580425755644,
      "loss": 0.0743,
      "step": 51910
    },
    {
      "epoch": 15.216881594372802,
      "grad_norm": 0.4307156801223755,
      "learning_rate": 0.00027971200796996805,
      "loss": 0.1,
      "step": 51920
    },
    {
      "epoch": 15.219812426729192,
      "grad_norm": 1.1171443462371826,
      "learning_rate": 0.0002796282116823797,
      "loss": 0.0636,
      "step": 51930
    },
    {
      "epoch": 15.222743259085581,
      "grad_norm": 1.5095537900924683,
      "learning_rate": 0.0002795444153947913,
      "loss": 0.0641,
      "step": 51940
    },
    {
      "epoch": 15.22567409144197,
      "grad_norm": 0.9765751361846924,
      "learning_rate": 0.000279460619107203,
      "loss": 0.0476,
      "step": 51950
    },
    {
      "epoch": 15.22860492379836,
      "grad_norm": 1.2646390199661255,
      "learning_rate": 0.0002793768228196146,
      "loss": 0.0529,
      "step": 51960
    },
    {
      "epoch": 15.231535756154749,
      "grad_norm": 0.6156870722770691,
      "learning_rate": 0.00027929302653202626,
      "loss": 0.0494,
      "step": 51970
    },
    {
      "epoch": 15.234466588511138,
      "grad_norm": 0.7258378863334656,
      "learning_rate": 0.00027920923024443787,
      "loss": 0.0562,
      "step": 51980
    },
    {
      "epoch": 15.237397420867527,
      "grad_norm": 1.5805790424346924,
      "learning_rate": 0.0002791254339568495,
      "loss": 0.0553,
      "step": 51990
    },
    {
      "epoch": 15.240328253223916,
      "grad_norm": 2.139054536819458,
      "learning_rate": 0.00027904163766926114,
      "loss": 0.0557,
      "step": 52000
    },
    {
      "epoch": 15.243259085580306,
      "grad_norm": 0.2565165162086487,
      "learning_rate": 0.00027895784138167275,
      "loss": 0.0428,
      "step": 52010
    },
    {
      "epoch": 15.246189917936695,
      "grad_norm": 0.979997456073761,
      "learning_rate": 0.0002788740450940844,
      "loss": 0.0776,
      "step": 52020
    },
    {
      "epoch": 15.249120750293084,
      "grad_norm": 0.5414625406265259,
      "learning_rate": 0.0002787902488064961,
      "loss": 0.0819,
      "step": 52030
    },
    {
      "epoch": 15.252051582649472,
      "grad_norm": 1.3529062271118164,
      "learning_rate": 0.00027870645251890764,
      "loss": 0.0681,
      "step": 52040
    },
    {
      "epoch": 15.25498241500586,
      "grad_norm": 0.9618628621101379,
      "learning_rate": 0.0002786226562313193,
      "loss": 0.0721,
      "step": 52050
    },
    {
      "epoch": 15.25791324736225,
      "grad_norm": 0.4562082588672638,
      "learning_rate": 0.00027853885994373097,
      "loss": 0.0621,
      "step": 52060
    },
    {
      "epoch": 15.26084407971864,
      "grad_norm": 0.5770368576049805,
      "learning_rate": 0.0002784550636561426,
      "loss": 0.0447,
      "step": 52070
    },
    {
      "epoch": 15.263774912075029,
      "grad_norm": 1.3133490085601807,
      "learning_rate": 0.00027837126736855424,
      "loss": 0.0654,
      "step": 52080
    },
    {
      "epoch": 15.266705744431418,
      "grad_norm": 0.9630529284477234,
      "learning_rate": 0.0002782874710809658,
      "loss": 0.0521,
      "step": 52090
    },
    {
      "epoch": 15.269636576787807,
      "grad_norm": 1.1611310243606567,
      "learning_rate": 0.00027820367479337746,
      "loss": 0.0776,
      "step": 52100
    },
    {
      "epoch": 15.272567409144196,
      "grad_norm": 1.184402346611023,
      "learning_rate": 0.0002781198785057891,
      "loss": 0.0646,
      "step": 52110
    },
    {
      "epoch": 15.275498241500586,
      "grad_norm": 0.9078012108802795,
      "learning_rate": 0.00027803608221820073,
      "loss": 0.076,
      "step": 52120
    },
    {
      "epoch": 15.278429073856975,
      "grad_norm": 0.665630042552948,
      "learning_rate": 0.0002779522859306124,
      "loss": 0.0643,
      "step": 52130
    },
    {
      "epoch": 15.281359906213364,
      "grad_norm": 0.6558281779289246,
      "learning_rate": 0.000277868489643024,
      "loss": 0.0677,
      "step": 52140
    },
    {
      "epoch": 15.284290738569753,
      "grad_norm": 1.2197777032852173,
      "learning_rate": 0.0002777846933554356,
      "loss": 0.069,
      "step": 52150
    },
    {
      "epoch": 15.287221570926143,
      "grad_norm": 1.654836893081665,
      "learning_rate": 0.0002777008970678473,
      "loss": 0.0621,
      "step": 52160
    },
    {
      "epoch": 15.290152403282532,
      "grad_norm": 0.530844509601593,
      "learning_rate": 0.0002776171007802589,
      "loss": 0.0696,
      "step": 52170
    },
    {
      "epoch": 15.293083235638921,
      "grad_norm": 1.3408559560775757,
      "learning_rate": 0.00027753330449267055,
      "loss": 0.0617,
      "step": 52180
    },
    {
      "epoch": 15.29601406799531,
      "grad_norm": 0.422124445438385,
      "learning_rate": 0.00027744950820508216,
      "loss": 0.0432,
      "step": 52190
    },
    {
      "epoch": 15.2989449003517,
      "grad_norm": 0.36469683051109314,
      "learning_rate": 0.00027736571191749377,
      "loss": 0.063,
      "step": 52200
    },
    {
      "epoch": 15.301875732708089,
      "grad_norm": 3.8477284908294678,
      "learning_rate": 0.00027728191562990544,
      "loss": 0.0579,
      "step": 52210
    },
    {
      "epoch": 15.304806565064478,
      "grad_norm": 0.266284316778183,
      "learning_rate": 0.00027719811934231705,
      "loss": 0.0647,
      "step": 52220
    },
    {
      "epoch": 15.307737397420867,
      "grad_norm": 1.2203789949417114,
      "learning_rate": 0.0002771143230547287,
      "loss": 0.0886,
      "step": 52230
    },
    {
      "epoch": 15.310668229777256,
      "grad_norm": 0.7913441061973572,
      "learning_rate": 0.0002770305267671403,
      "loss": 0.0618,
      "step": 52240
    },
    {
      "epoch": 15.313599062133646,
      "grad_norm": 1.523637056350708,
      "learning_rate": 0.000276946730479552,
      "loss": 0.0595,
      "step": 52250
    },
    {
      "epoch": 15.316529894490035,
      "grad_norm": 0.4390224814414978,
      "learning_rate": 0.0002768629341919636,
      "loss": 0.0763,
      "step": 52260
    },
    {
      "epoch": 15.319460726846424,
      "grad_norm": 1.1338177919387817,
      "learning_rate": 0.0002767791379043752,
      "loss": 0.0667,
      "step": 52270
    },
    {
      "epoch": 15.322391559202813,
      "grad_norm": 0.4673272371292114,
      "learning_rate": 0.00027669534161678687,
      "loss": 0.0533,
      "step": 52280
    },
    {
      "epoch": 15.325322391559203,
      "grad_norm": 0.5163565278053284,
      "learning_rate": 0.00027661154532919853,
      "loss": 0.0581,
      "step": 52290
    },
    {
      "epoch": 15.328253223915592,
      "grad_norm": 0.6380059719085693,
      "learning_rate": 0.00027652774904161014,
      "loss": 0.0651,
      "step": 52300
    },
    {
      "epoch": 15.331184056271981,
      "grad_norm": 0.8622810244560242,
      "learning_rate": 0.0002764439527540218,
      "loss": 0.0779,
      "step": 52310
    },
    {
      "epoch": 15.33411488862837,
      "grad_norm": 0.5018242001533508,
      "learning_rate": 0.0002763601564664334,
      "loss": 0.0595,
      "step": 52320
    },
    {
      "epoch": 15.33704572098476,
      "grad_norm": 0.5556350350379944,
      "learning_rate": 0.000276276360178845,
      "loss": 0.0685,
      "step": 52330
    },
    {
      "epoch": 15.339976553341149,
      "grad_norm": 0.5351439118385315,
      "learning_rate": 0.0002761925638912567,
      "loss": 0.0623,
      "step": 52340
    },
    {
      "epoch": 15.342907385697538,
      "grad_norm": 0.542329728603363,
      "learning_rate": 0.0002761087676036683,
      "loss": 0.066,
      "step": 52350
    },
    {
      "epoch": 15.345838218053927,
      "grad_norm": 0.4875442087650299,
      "learning_rate": 0.00027602497131607996,
      "loss": 0.0599,
      "step": 52360
    },
    {
      "epoch": 15.348769050410317,
      "grad_norm": 1.1041533946990967,
      "learning_rate": 0.00027594117502849157,
      "loss": 0.0517,
      "step": 52370
    },
    {
      "epoch": 15.351699882766706,
      "grad_norm": 1.1951862573623657,
      "learning_rate": 0.0002758573787409032,
      "loss": 0.0762,
      "step": 52380
    },
    {
      "epoch": 15.354630715123095,
      "grad_norm": 0.2284024953842163,
      "learning_rate": 0.00027577358245331484,
      "loss": 0.0463,
      "step": 52390
    },
    {
      "epoch": 15.357561547479484,
      "grad_norm": 0.9365644454956055,
      "learning_rate": 0.00027568978616572645,
      "loss": 0.073,
      "step": 52400
    },
    {
      "epoch": 15.360492379835874,
      "grad_norm": 0.47871842980384827,
      "learning_rate": 0.0002756059898781381,
      "loss": 0.0599,
      "step": 52410
    },
    {
      "epoch": 15.363423212192263,
      "grad_norm": 1.090417742729187,
      "learning_rate": 0.00027552219359054973,
      "loss": 0.0683,
      "step": 52420
    },
    {
      "epoch": 15.366354044548652,
      "grad_norm": 0.9809826612472534,
      "learning_rate": 0.00027543839730296134,
      "loss": 0.0931,
      "step": 52430
    },
    {
      "epoch": 15.369284876905041,
      "grad_norm": 1.9735462665557861,
      "learning_rate": 0.000275354601015373,
      "loss": 0.076,
      "step": 52440
    },
    {
      "epoch": 15.37221570926143,
      "grad_norm": 0.984872043132782,
      "learning_rate": 0.0002752708047277846,
      "loss": 0.033,
      "step": 52450
    },
    {
      "epoch": 15.37514654161782,
      "grad_norm": 1.3104760646820068,
      "learning_rate": 0.0002751870084401963,
      "loss": 0.0673,
      "step": 52460
    },
    {
      "epoch": 15.37807737397421,
      "grad_norm": 1.3927431106567383,
      "learning_rate": 0.00027510321215260794,
      "loss": 0.063,
      "step": 52470
    },
    {
      "epoch": 15.381008206330598,
      "grad_norm": 2.784142255783081,
      "learning_rate": 0.00027501941586501955,
      "loss": 0.0669,
      "step": 52480
    },
    {
      "epoch": 15.383939038686988,
      "grad_norm": 0.27315205335617065,
      "learning_rate": 0.00027493561957743116,
      "loss": 0.0542,
      "step": 52490
    },
    {
      "epoch": 15.386869871043377,
      "grad_norm": 1.2130225896835327,
      "learning_rate": 0.0002748518232898428,
      "loss": 0.0537,
      "step": 52500
    },
    {
      "epoch": 15.389800703399766,
      "grad_norm": 0.8512142300605774,
      "learning_rate": 0.00027476802700225443,
      "loss": 0.0609,
      "step": 52510
    },
    {
      "epoch": 15.392731535756155,
      "grad_norm": 0.4417295753955841,
      "learning_rate": 0.0002746842307146661,
      "loss": 0.0627,
      "step": 52520
    },
    {
      "epoch": 15.395662368112545,
      "grad_norm": 0.7838770151138306,
      "learning_rate": 0.0002746004344270777,
      "loss": 0.0582,
      "step": 52530
    },
    {
      "epoch": 15.398593200468934,
      "grad_norm": 0.7524587512016296,
      "learning_rate": 0.0002745166381394893,
      "loss": 0.0557,
      "step": 52540
    },
    {
      "epoch": 15.401524032825323,
      "grad_norm": 1.017321228981018,
      "learning_rate": 0.000274432841851901,
      "loss": 0.0738,
      "step": 52550
    },
    {
      "epoch": 15.404454865181712,
      "grad_norm": 0.6948654651641846,
      "learning_rate": 0.0002743490455643126,
      "loss": 0.055,
      "step": 52560
    },
    {
      "epoch": 15.407385697538102,
      "grad_norm": 0.5618778467178345,
      "learning_rate": 0.00027426524927672425,
      "loss": 0.0507,
      "step": 52570
    },
    {
      "epoch": 15.41031652989449,
      "grad_norm": 1.2881884574890137,
      "learning_rate": 0.00027418145298913586,
      "loss": 0.0521,
      "step": 52580
    },
    {
      "epoch": 15.41324736225088,
      "grad_norm": 0.9405159950256348,
      "learning_rate": 0.0002740976567015475,
      "loss": 0.088,
      "step": 52590
    },
    {
      "epoch": 15.41617819460727,
      "grad_norm": 0.6268483996391296,
      "learning_rate": 0.00027401386041395914,
      "loss": 0.0585,
      "step": 52600
    },
    {
      "epoch": 15.419109026963657,
      "grad_norm": 1.055194616317749,
      "learning_rate": 0.00027393006412637075,
      "loss": 0.0489,
      "step": 52610
    },
    {
      "epoch": 15.422039859320046,
      "grad_norm": 1.0204029083251953,
      "learning_rate": 0.0002738462678387824,
      "loss": 0.057,
      "step": 52620
    },
    {
      "epoch": 15.424970691676435,
      "grad_norm": 0.942723274230957,
      "learning_rate": 0.000273762471551194,
      "loss": 0.0486,
      "step": 52630
    },
    {
      "epoch": 15.427901524032825,
      "grad_norm": 0.48461633920669556,
      "learning_rate": 0.0002736786752636057,
      "loss": 0.0586,
      "step": 52640
    },
    {
      "epoch": 15.430832356389214,
      "grad_norm": 0.9235762357711792,
      "learning_rate": 0.00027359487897601735,
      "loss": 0.0561,
      "step": 52650
    },
    {
      "epoch": 15.433763188745603,
      "grad_norm": 1.1387187242507935,
      "learning_rate": 0.0002735110826884289,
      "loss": 0.0576,
      "step": 52660
    },
    {
      "epoch": 15.436694021101992,
      "grad_norm": 0.2388589233160019,
      "learning_rate": 0.00027342728640084057,
      "loss": 0.041,
      "step": 52670
    },
    {
      "epoch": 15.439624853458382,
      "grad_norm": 1.7324801683425903,
      "learning_rate": 0.0002733434901132522,
      "loss": 0.0638,
      "step": 52680
    },
    {
      "epoch": 15.44255568581477,
      "grad_norm": 2.700355052947998,
      "learning_rate": 0.00027325969382566384,
      "loss": 0.046,
      "step": 52690
    },
    {
      "epoch": 15.44548651817116,
      "grad_norm": 0.6235369443893433,
      "learning_rate": 0.0002731758975380755,
      "loss": 0.0539,
      "step": 52700
    },
    {
      "epoch": 15.44841735052755,
      "grad_norm": 0.3102584183216095,
      "learning_rate": 0.00027309210125048706,
      "loss": 0.0494,
      "step": 52710
    },
    {
      "epoch": 15.451348182883939,
      "grad_norm": 0.1775251030921936,
      "learning_rate": 0.0002730083049628987,
      "loss": 0.0643,
      "step": 52720
    },
    {
      "epoch": 15.454279015240328,
      "grad_norm": 2.2745437622070312,
      "learning_rate": 0.0002729245086753104,
      "loss": 0.0852,
      "step": 52730
    },
    {
      "epoch": 15.457209847596717,
      "grad_norm": 1.938204050064087,
      "learning_rate": 0.000272840712387722,
      "loss": 0.0681,
      "step": 52740
    },
    {
      "epoch": 15.460140679953106,
      "grad_norm": 0.5603933930397034,
      "learning_rate": 0.00027275691610013366,
      "loss": 0.0524,
      "step": 52750
    },
    {
      "epoch": 15.463071512309496,
      "grad_norm": 0.7281618714332581,
      "learning_rate": 0.00027267311981254527,
      "loss": 0.0584,
      "step": 52760
    },
    {
      "epoch": 15.466002344665885,
      "grad_norm": 1.859697699546814,
      "learning_rate": 0.0002725893235249569,
      "loss": 0.0377,
      "step": 52770
    },
    {
      "epoch": 15.468933177022274,
      "grad_norm": 2.0389790534973145,
      "learning_rate": 0.00027250552723736854,
      "loss": 0.0692,
      "step": 52780
    },
    {
      "epoch": 15.471864009378663,
      "grad_norm": 1.8955023288726807,
      "learning_rate": 0.00027242173094978015,
      "loss": 0.0887,
      "step": 52790
    },
    {
      "epoch": 15.474794841735052,
      "grad_norm": 0.4864809513092041,
      "learning_rate": 0.0002723379346621918,
      "loss": 0.0824,
      "step": 52800
    },
    {
      "epoch": 15.477725674091442,
      "grad_norm": 0.7650314569473267,
      "learning_rate": 0.00027225413837460343,
      "loss": 0.0529,
      "step": 52810
    },
    {
      "epoch": 15.480656506447831,
      "grad_norm": 0.3038008213043213,
      "learning_rate": 0.00027217034208701504,
      "loss": 0.0554,
      "step": 52820
    },
    {
      "epoch": 15.48358733880422,
      "grad_norm": 0.9039777517318726,
      "learning_rate": 0.0002720865457994267,
      "loss": 0.0641,
      "step": 52830
    },
    {
      "epoch": 15.48651817116061,
      "grad_norm": 1.5871005058288574,
      "learning_rate": 0.0002720027495118383,
      "loss": 0.09,
      "step": 52840
    },
    {
      "epoch": 15.489449003516999,
      "grad_norm": 0.2783443331718445,
      "learning_rate": 0.00027191895322425,
      "loss": 0.0499,
      "step": 52850
    },
    {
      "epoch": 15.492379835873388,
      "grad_norm": 0.2089681774377823,
      "learning_rate": 0.0002718351569366616,
      "loss": 0.057,
      "step": 52860
    },
    {
      "epoch": 15.495310668229777,
      "grad_norm": 0.11365144699811935,
      "learning_rate": 0.00027175136064907325,
      "loss": 0.041,
      "step": 52870
    },
    {
      "epoch": 15.498241500586166,
      "grad_norm": 0.9933415055274963,
      "learning_rate": 0.00027166756436148486,
      "loss": 0.0749,
      "step": 52880
    },
    {
      "epoch": 15.501172332942556,
      "grad_norm": 1.3031065464019775,
      "learning_rate": 0.00027158376807389647,
      "loss": 0.0492,
      "step": 52890
    },
    {
      "epoch": 15.504103165298945,
      "grad_norm": 4.448393821716309,
      "learning_rate": 0.00027149997178630813,
      "loss": 0.0706,
      "step": 52900
    },
    {
      "epoch": 15.507033997655334,
      "grad_norm": 0.8350573182106018,
      "learning_rate": 0.0002714161754987198,
      "loss": 0.0783,
      "step": 52910
    },
    {
      "epoch": 15.509964830011723,
      "grad_norm": 0.88735032081604,
      "learning_rate": 0.0002713323792111314,
      "loss": 0.0596,
      "step": 52920
    },
    {
      "epoch": 15.512895662368113,
      "grad_norm": 0.49427783489227295,
      "learning_rate": 0.00027124858292354307,
      "loss": 0.059,
      "step": 52930
    },
    {
      "epoch": 15.515826494724502,
      "grad_norm": 1.404870867729187,
      "learning_rate": 0.0002711647866359547,
      "loss": 0.0516,
      "step": 52940
    },
    {
      "epoch": 15.518757327080891,
      "grad_norm": 2.35546875,
      "learning_rate": 0.0002710809903483663,
      "loss": 0.0724,
      "step": 52950
    },
    {
      "epoch": 15.52168815943728,
      "grad_norm": 1.2263010740280151,
      "learning_rate": 0.00027099719406077795,
      "loss": 0.0858,
      "step": 52960
    },
    {
      "epoch": 15.52461899179367,
      "grad_norm": 0.8374985456466675,
      "learning_rate": 0.00027091339777318956,
      "loss": 0.0557,
      "step": 52970
    },
    {
      "epoch": 15.527549824150059,
      "grad_norm": 1.3939355611801147,
      "learning_rate": 0.0002708296014856012,
      "loss": 0.0564,
      "step": 52980
    },
    {
      "epoch": 15.530480656506448,
      "grad_norm": 1.331040620803833,
      "learning_rate": 0.00027074580519801284,
      "loss": 0.0937,
      "step": 52990
    },
    {
      "epoch": 15.533411488862837,
      "grad_norm": 0.5788941383361816,
      "learning_rate": 0.00027066200891042445,
      "loss": 0.0553,
      "step": 53000
    },
    {
      "epoch": 15.536342321219227,
      "grad_norm": 1.2192041873931885,
      "learning_rate": 0.0002705782126228361,
      "loss": 0.0586,
      "step": 53010
    },
    {
      "epoch": 15.539273153575616,
      "grad_norm": 0.4530477225780487,
      "learning_rate": 0.0002704944163352477,
      "loss": 0.0605,
      "step": 53020
    },
    {
      "epoch": 15.542203985932005,
      "grad_norm": 0.9292704463005066,
      "learning_rate": 0.0002704106200476594,
      "loss": 0.0623,
      "step": 53030
    },
    {
      "epoch": 15.545134818288394,
      "grad_norm": 3.959423780441284,
      "learning_rate": 0.000270326823760071,
      "loss": 0.0906,
      "step": 53040
    },
    {
      "epoch": 15.548065650644784,
      "grad_norm": 0.8834572434425354,
      "learning_rate": 0.0002702430274724826,
      "loss": 0.0672,
      "step": 53050
    },
    {
      "epoch": 15.550996483001173,
      "grad_norm": 1.0442990064620972,
      "learning_rate": 0.00027015923118489427,
      "loss": 0.0688,
      "step": 53060
    },
    {
      "epoch": 15.553927315357562,
      "grad_norm": 1.6788681745529175,
      "learning_rate": 0.0002700754348973059,
      "loss": 0.0505,
      "step": 53070
    },
    {
      "epoch": 15.556858147713951,
      "grad_norm": 0.6909381747245789,
      "learning_rate": 0.00026999163860971754,
      "loss": 0.0707,
      "step": 53080
    },
    {
      "epoch": 15.55978898007034,
      "grad_norm": 0.3666427731513977,
      "learning_rate": 0.0002699078423221292,
      "loss": 0.0486,
      "step": 53090
    },
    {
      "epoch": 15.56271981242673,
      "grad_norm": 1.710396647453308,
      "learning_rate": 0.0002698240460345408,
      "loss": 0.0692,
      "step": 53100
    },
    {
      "epoch": 15.565650644783119,
      "grad_norm": 0.8164163827896118,
      "learning_rate": 0.0002697402497469524,
      "loss": 0.0819,
      "step": 53110
    },
    {
      "epoch": 15.568581477139508,
      "grad_norm": 0.4110499918460846,
      "learning_rate": 0.0002696564534593641,
      "loss": 0.0689,
      "step": 53120
    },
    {
      "epoch": 15.571512309495898,
      "grad_norm": 1.112365961074829,
      "learning_rate": 0.0002695726571717757,
      "loss": 0.0328,
      "step": 53130
    },
    {
      "epoch": 15.574443141852287,
      "grad_norm": 1.6167680025100708,
      "learning_rate": 0.00026948886088418736,
      "loss": 0.0515,
      "step": 53140
    },
    {
      "epoch": 15.577373974208676,
      "grad_norm": 0.6204827427864075,
      "learning_rate": 0.00026940506459659897,
      "loss": 0.0525,
      "step": 53150
    },
    {
      "epoch": 15.580304806565064,
      "grad_norm": 0.9052794575691223,
      "learning_rate": 0.0002693212683090106,
      "loss": 0.0905,
      "step": 53160
    },
    {
      "epoch": 15.583235638921455,
      "grad_norm": 1.0354913473129272,
      "learning_rate": 0.00026923747202142224,
      "loss": 0.0816,
      "step": 53170
    },
    {
      "epoch": 15.586166471277842,
      "grad_norm": 1.6214570999145508,
      "learning_rate": 0.00026915367573383385,
      "loss": 0.0647,
      "step": 53180
    },
    {
      "epoch": 15.589097303634231,
      "grad_norm": 0.5493882894515991,
      "learning_rate": 0.0002690698794462455,
      "loss": 0.0573,
      "step": 53190
    },
    {
      "epoch": 15.59202813599062,
      "grad_norm": 0.08292244374752045,
      "learning_rate": 0.00026898608315865713,
      "loss": 0.0395,
      "step": 53200
    },
    {
      "epoch": 15.59495896834701,
      "grad_norm": 0.09395284205675125,
      "learning_rate": 0.0002689022868710688,
      "loss": 0.0668,
      "step": 53210
    },
    {
      "epoch": 15.597889800703399,
      "grad_norm": 1.210007905960083,
      "learning_rate": 0.0002688184905834804,
      "loss": 0.0824,
      "step": 53220
    },
    {
      "epoch": 15.600820633059788,
      "grad_norm": 1.3400585651397705,
      "learning_rate": 0.000268734694295892,
      "loss": 0.0522,
      "step": 53230
    },
    {
      "epoch": 15.603751465416178,
      "grad_norm": 1.3523718118667603,
      "learning_rate": 0.0002686508980083037,
      "loss": 0.0727,
      "step": 53240
    },
    {
      "epoch": 15.606682297772567,
      "grad_norm": 2.0278728008270264,
      "learning_rate": 0.0002685671017207153,
      "loss": 0.0576,
      "step": 53250
    },
    {
      "epoch": 15.609613130128956,
      "grad_norm": 2.0565755367279053,
      "learning_rate": 0.00026848330543312695,
      "loss": 0.0551,
      "step": 53260
    },
    {
      "epoch": 15.612543962485345,
      "grad_norm": 0.21866728365421295,
      "learning_rate": 0.0002683995091455386,
      "loss": 0.0434,
      "step": 53270
    },
    {
      "epoch": 15.615474794841735,
      "grad_norm": 0.7777007222175598,
      "learning_rate": 0.00026831571285795017,
      "loss": 0.07,
      "step": 53280
    },
    {
      "epoch": 15.618405627198124,
      "grad_norm": 1.489575743675232,
      "learning_rate": 0.00026823191657036183,
      "loss": 0.0698,
      "step": 53290
    },
    {
      "epoch": 15.621336459554513,
      "grad_norm": 0.7954210042953491,
      "learning_rate": 0.00026814812028277344,
      "loss": 0.0741,
      "step": 53300
    },
    {
      "epoch": 15.624267291910902,
      "grad_norm": 0.2780417501926422,
      "learning_rate": 0.0002680643239951851,
      "loss": 0.0467,
      "step": 53310
    },
    {
      "epoch": 15.627198124267291,
      "grad_norm": 0.9507538676261902,
      "learning_rate": 0.00026798052770759677,
      "loss": 0.0653,
      "step": 53320
    },
    {
      "epoch": 15.63012895662368,
      "grad_norm": 0.15570305287837982,
      "learning_rate": 0.0002678967314200083,
      "loss": 0.0696,
      "step": 53330
    },
    {
      "epoch": 15.63305978898007,
      "grad_norm": 1.056312084197998,
      "learning_rate": 0.00026781293513242,
      "loss": 0.0564,
      "step": 53340
    },
    {
      "epoch": 15.63599062133646,
      "grad_norm": 0.5457583665847778,
      "learning_rate": 0.00026772913884483165,
      "loss": 0.0654,
      "step": 53350
    },
    {
      "epoch": 15.638921453692848,
      "grad_norm": 0.9260385632514954,
      "learning_rate": 0.00026764534255724326,
      "loss": 0.0694,
      "step": 53360
    },
    {
      "epoch": 15.641852286049238,
      "grad_norm": 1.1418310403823853,
      "learning_rate": 0.0002675615462696549,
      "loss": 0.0528,
      "step": 53370
    },
    {
      "epoch": 15.644783118405627,
      "grad_norm": 0.15610024333000183,
      "learning_rate": 0.00026747774998206654,
      "loss": 0.0605,
      "step": 53380
    },
    {
      "epoch": 15.647713950762016,
      "grad_norm": 1.6179747581481934,
      "learning_rate": 0.00026739395369447815,
      "loss": 0.0583,
      "step": 53390
    },
    {
      "epoch": 15.650644783118405,
      "grad_norm": 1.2487918138504028,
      "learning_rate": 0.0002673101574068898,
      "loss": 0.0742,
      "step": 53400
    },
    {
      "epoch": 15.653575615474795,
      "grad_norm": 1.4655303955078125,
      "learning_rate": 0.0002672263611193014,
      "loss": 0.0753,
      "step": 53410
    },
    {
      "epoch": 15.656506447831184,
      "grad_norm": 2.3668301105499268,
      "learning_rate": 0.0002671425648317131,
      "loss": 0.0769,
      "step": 53420
    },
    {
      "epoch": 15.659437280187573,
      "grad_norm": 0.5694822669029236,
      "learning_rate": 0.0002670587685441247,
      "loss": 0.0518,
      "step": 53430
    },
    {
      "epoch": 15.662368112543962,
      "grad_norm": 0.7267892956733704,
      "learning_rate": 0.0002669749722565363,
      "loss": 0.0497,
      "step": 53440
    },
    {
      "epoch": 15.665298944900352,
      "grad_norm": 0.5177380442619324,
      "learning_rate": 0.00026689117596894797,
      "loss": 0.0445,
      "step": 53450
    },
    {
      "epoch": 15.668229777256741,
      "grad_norm": 0.4833203852176666,
      "learning_rate": 0.0002668073796813596,
      "loss": 0.0631,
      "step": 53460
    },
    {
      "epoch": 15.67116060961313,
      "grad_norm": 2.197394371032715,
      "learning_rate": 0.00026672358339377124,
      "loss": 0.0455,
      "step": 53470
    },
    {
      "epoch": 15.67409144196952,
      "grad_norm": 0.9228119254112244,
      "learning_rate": 0.00026663978710618285,
      "loss": 0.0764,
      "step": 53480
    },
    {
      "epoch": 15.677022274325909,
      "grad_norm": 0.29014018177986145,
      "learning_rate": 0.0002665559908185945,
      "loss": 0.0554,
      "step": 53490
    },
    {
      "epoch": 15.679953106682298,
      "grad_norm": 1.3872685432434082,
      "learning_rate": 0.0002664721945310061,
      "loss": 0.0656,
      "step": 53500
    },
    {
      "epoch": 15.682883939038687,
      "grad_norm": 1.1161805391311646,
      "learning_rate": 0.00026638839824341773,
      "loss": 0.0567,
      "step": 53510
    },
    {
      "epoch": 15.685814771395076,
      "grad_norm": 1.4485454559326172,
      "learning_rate": 0.0002663046019558294,
      "loss": 0.067,
      "step": 53520
    },
    {
      "epoch": 15.688745603751466,
      "grad_norm": 0.856059193611145,
      "learning_rate": 0.00026622080566824106,
      "loss": 0.0499,
      "step": 53530
    },
    {
      "epoch": 15.691676436107855,
      "grad_norm": 0.25732502341270447,
      "learning_rate": 0.00026613700938065267,
      "loss": 0.0589,
      "step": 53540
    },
    {
      "epoch": 15.694607268464244,
      "grad_norm": 1.1035549640655518,
      "learning_rate": 0.00026605321309306433,
      "loss": 0.0459,
      "step": 53550
    },
    {
      "epoch": 15.697538100820633,
      "grad_norm": 0.7619567513465881,
      "learning_rate": 0.00026596941680547594,
      "loss": 0.0692,
      "step": 53560
    },
    {
      "epoch": 15.700468933177023,
      "grad_norm": 0.09197916090488434,
      "learning_rate": 0.00026588562051788755,
      "loss": 0.0634,
      "step": 53570
    },
    {
      "epoch": 15.703399765533412,
      "grad_norm": 0.8438470959663391,
      "learning_rate": 0.0002658018242302992,
      "loss": 0.0514,
      "step": 53580
    },
    {
      "epoch": 15.706330597889801,
      "grad_norm": 0.9075633883476257,
      "learning_rate": 0.00026571802794271083,
      "loss": 0.0825,
      "step": 53590
    },
    {
      "epoch": 15.70926143024619,
      "grad_norm": 0.829750120639801,
      "learning_rate": 0.0002656342316551225,
      "loss": 0.0695,
      "step": 53600
    },
    {
      "epoch": 15.71219226260258,
      "grad_norm": 1.7987233400344849,
      "learning_rate": 0.0002655504353675341,
      "loss": 0.0721,
      "step": 53610
    },
    {
      "epoch": 15.715123094958969,
      "grad_norm": 1.0939971208572388,
      "learning_rate": 0.0002654666390799457,
      "loss": 0.0769,
      "step": 53620
    },
    {
      "epoch": 15.718053927315358,
      "grad_norm": 0.660555362701416,
      "learning_rate": 0.0002653828427923574,
      "loss": 0.0443,
      "step": 53630
    },
    {
      "epoch": 15.720984759671747,
      "grad_norm": 0.9079648852348328,
      "learning_rate": 0.000265299046504769,
      "loss": 0.0334,
      "step": 53640
    },
    {
      "epoch": 15.723915592028137,
      "grad_norm": 1.2595996856689453,
      "learning_rate": 0.00026521525021718065,
      "loss": 0.0555,
      "step": 53650
    },
    {
      "epoch": 15.726846424384526,
      "grad_norm": 1.6850839853286743,
      "learning_rate": 0.00026513145392959226,
      "loss": 0.0742,
      "step": 53660
    },
    {
      "epoch": 15.729777256740915,
      "grad_norm": 0.9024558663368225,
      "learning_rate": 0.00026504765764200387,
      "loss": 0.0646,
      "step": 53670
    },
    {
      "epoch": 15.732708089097304,
      "grad_norm": 1.3556638956069946,
      "learning_rate": 0.00026496386135441553,
      "loss": 0.0401,
      "step": 53680
    },
    {
      "epoch": 15.735638921453694,
      "grad_norm": 1.1236114501953125,
      "learning_rate": 0.00026488006506682714,
      "loss": 0.056,
      "step": 53690
    },
    {
      "epoch": 15.738569753810083,
      "grad_norm": 1.1593010425567627,
      "learning_rate": 0.0002647962687792388,
      "loss": 0.0642,
      "step": 53700
    },
    {
      "epoch": 15.741500586166472,
      "grad_norm": 0.9096616506576538,
      "learning_rate": 0.00026471247249165047,
      "loss": 0.0621,
      "step": 53710
    },
    {
      "epoch": 15.744431418522861,
      "grad_norm": 0.3857705891132355,
      "learning_rate": 0.0002646286762040621,
      "loss": 0.0515,
      "step": 53720
    },
    {
      "epoch": 15.747362250879249,
      "grad_norm": 0.27892452478408813,
      "learning_rate": 0.0002645448799164737,
      "loss": 0.0596,
      "step": 53730
    },
    {
      "epoch": 15.75029308323564,
      "grad_norm": 0.9560543894767761,
      "learning_rate": 0.0002644610836288853,
      "loss": 0.071,
      "step": 53740
    },
    {
      "epoch": 15.753223915592027,
      "grad_norm": 0.5293022990226746,
      "learning_rate": 0.00026437728734129696,
      "loss": 0.0793,
      "step": 53750
    },
    {
      "epoch": 15.756154747948417,
      "grad_norm": 1.1505082845687866,
      "learning_rate": 0.0002642934910537086,
      "loss": 0.0718,
      "step": 53760
    },
    {
      "epoch": 15.759085580304806,
      "grad_norm": 0.8181579113006592,
      "learning_rate": 0.00026420969476612024,
      "loss": 0.0539,
      "step": 53770
    },
    {
      "epoch": 15.762016412661195,
      "grad_norm": 0.504197895526886,
      "learning_rate": 0.00026412589847853185,
      "loss": 0.0579,
      "step": 53780
    },
    {
      "epoch": 15.764947245017584,
      "grad_norm": 1.266297459602356,
      "learning_rate": 0.0002640421021909435,
      "loss": 0.0638,
      "step": 53790
    },
    {
      "epoch": 15.767878077373974,
      "grad_norm": 1.0653749704360962,
      "learning_rate": 0.0002639583059033551,
      "loss": 0.0618,
      "step": 53800
    },
    {
      "epoch": 15.770808909730363,
      "grad_norm": 0.7299705743789673,
      "learning_rate": 0.0002638745096157668,
      "loss": 0.0603,
      "step": 53810
    },
    {
      "epoch": 15.773739742086752,
      "grad_norm": 2.7752525806427,
      "learning_rate": 0.0002637907133281784,
      "loss": 0.0844,
      "step": 53820
    },
    {
      "epoch": 15.776670574443141,
      "grad_norm": 0.6909577250480652,
      "learning_rate": 0.00026370691704059006,
      "loss": 0.0652,
      "step": 53830
    },
    {
      "epoch": 15.77960140679953,
      "grad_norm": 0.44599947333335876,
      "learning_rate": 0.00026362312075300167,
      "loss": 0.05,
      "step": 53840
    },
    {
      "epoch": 15.78253223915592,
      "grad_norm": 0.5796990394592285,
      "learning_rate": 0.0002635393244654133,
      "loss": 0.0467,
      "step": 53850
    },
    {
      "epoch": 15.785463071512309,
      "grad_norm": 0.5755321979522705,
      "learning_rate": 0.00026345552817782494,
      "loss": 0.0658,
      "step": 53860
    },
    {
      "epoch": 15.788393903868698,
      "grad_norm": 1.187341332435608,
      "learning_rate": 0.00026337173189023655,
      "loss": 0.0661,
      "step": 53870
    },
    {
      "epoch": 15.791324736225087,
      "grad_norm": 1.1024147272109985,
      "learning_rate": 0.0002632879356026482,
      "loss": 0.0568,
      "step": 53880
    },
    {
      "epoch": 15.794255568581477,
      "grad_norm": 0.30734068155288696,
      "learning_rate": 0.0002632041393150598,
      "loss": 0.0446,
      "step": 53890
    },
    {
      "epoch": 15.797186400937866,
      "grad_norm": 1.4927027225494385,
      "learning_rate": 0.00026312034302747143,
      "loss": 0.0627,
      "step": 53900
    },
    {
      "epoch": 15.800117233294255,
      "grad_norm": 0.576816201210022,
      "learning_rate": 0.0002630365467398831,
      "loss": 0.0383,
      "step": 53910
    },
    {
      "epoch": 15.803048065650644,
      "grad_norm": 1.3675826787948608,
      "learning_rate": 0.0002629527504522947,
      "loss": 0.0663,
      "step": 53920
    },
    {
      "epoch": 15.805978898007034,
      "grad_norm": 0.40923553705215454,
      "learning_rate": 0.00026286895416470637,
      "loss": 0.0578,
      "step": 53930
    },
    {
      "epoch": 15.808909730363423,
      "grad_norm": 0.8943547010421753,
      "learning_rate": 0.00026278515787711803,
      "loss": 0.0692,
      "step": 53940
    },
    {
      "epoch": 15.811840562719812,
      "grad_norm": 1.2631808519363403,
      "learning_rate": 0.0002627013615895296,
      "loss": 0.0578,
      "step": 53950
    },
    {
      "epoch": 15.814771395076201,
      "grad_norm": 0.7336814403533936,
      "learning_rate": 0.00026261756530194125,
      "loss": 0.0525,
      "step": 53960
    },
    {
      "epoch": 15.81770222743259,
      "grad_norm": 0.762194037437439,
      "learning_rate": 0.0002625337690143529,
      "loss": 0.0603,
      "step": 53970
    },
    {
      "epoch": 15.82063305978898,
      "grad_norm": 1.2396305799484253,
      "learning_rate": 0.00026244997272676453,
      "loss": 0.055,
      "step": 53980
    },
    {
      "epoch": 15.82356389214537,
      "grad_norm": 0.3121725916862488,
      "learning_rate": 0.0002623661764391762,
      "loss": 0.0442,
      "step": 53990
    },
    {
      "epoch": 15.826494724501758,
      "grad_norm": 2.9779720306396484,
      "learning_rate": 0.0002622823801515878,
      "loss": 0.0539,
      "step": 54000
    },
    {
      "epoch": 15.829425556858148,
      "grad_norm": 0.6896841526031494,
      "learning_rate": 0.0002621985838639994,
      "loss": 0.0549,
      "step": 54010
    },
    {
      "epoch": 15.832356389214537,
      "grad_norm": 1.8501276969909668,
      "learning_rate": 0.0002621147875764111,
      "loss": 0.0569,
      "step": 54020
    },
    {
      "epoch": 15.835287221570926,
      "grad_norm": 0.582195520401001,
      "learning_rate": 0.0002620309912888227,
      "loss": 0.0571,
      "step": 54030
    },
    {
      "epoch": 15.838218053927315,
      "grad_norm": 1.6418333053588867,
      "learning_rate": 0.00026194719500123435,
      "loss": 0.0662,
      "step": 54040
    },
    {
      "epoch": 15.841148886283705,
      "grad_norm": 1.0598149299621582,
      "learning_rate": 0.00026186339871364596,
      "loss": 0.055,
      "step": 54050
    },
    {
      "epoch": 15.844079718640094,
      "grad_norm": 0.08277883380651474,
      "learning_rate": 0.00026177960242605757,
      "loss": 0.045,
      "step": 54060
    },
    {
      "epoch": 15.847010550996483,
      "grad_norm": 0.9637494087219238,
      "learning_rate": 0.00026169580613846923,
      "loss": 0.0742,
      "step": 54070
    },
    {
      "epoch": 15.849941383352872,
      "grad_norm": 0.27349817752838135,
      "learning_rate": 0.00026161200985088084,
      "loss": 0.0643,
      "step": 54080
    },
    {
      "epoch": 15.852872215709262,
      "grad_norm": 0.6032925248146057,
      "learning_rate": 0.0002615282135632925,
      "loss": 0.0745,
      "step": 54090
    },
    {
      "epoch": 15.85580304806565,
      "grad_norm": 1.1957206726074219,
      "learning_rate": 0.0002614444172757041,
      "loss": 0.0807,
      "step": 54100
    },
    {
      "epoch": 15.85873388042204,
      "grad_norm": 0.7859172224998474,
      "learning_rate": 0.0002613606209881158,
      "loss": 0.047,
      "step": 54110
    },
    {
      "epoch": 15.86166471277843,
      "grad_norm": 0.8946223258972168,
      "learning_rate": 0.0002612768247005274,
      "loss": 0.0526,
      "step": 54120
    },
    {
      "epoch": 15.864595545134819,
      "grad_norm": 1.111892819404602,
      "learning_rate": 0.000261193028412939,
      "loss": 0.074,
      "step": 54130
    },
    {
      "epoch": 15.867526377491208,
      "grad_norm": 1.191871166229248,
      "learning_rate": 0.00026110923212535066,
      "loss": 0.0496,
      "step": 54140
    },
    {
      "epoch": 15.870457209847597,
      "grad_norm": 0.6162316799163818,
      "learning_rate": 0.0002610254358377623,
      "loss": 0.0474,
      "step": 54150
    },
    {
      "epoch": 15.873388042203986,
      "grad_norm": 1.9503182172775269,
      "learning_rate": 0.00026094163955017394,
      "loss": 0.0678,
      "step": 54160
    },
    {
      "epoch": 15.876318874560376,
      "grad_norm": 0.9065399169921875,
      "learning_rate": 0.0002608578432625856,
      "loss": 0.058,
      "step": 54170
    },
    {
      "epoch": 15.879249706916765,
      "grad_norm": 1.4406217336654663,
      "learning_rate": 0.00026077404697499716,
      "loss": 0.0747,
      "step": 54180
    },
    {
      "epoch": 15.882180539273154,
      "grad_norm": 0.2416156828403473,
      "learning_rate": 0.0002606902506874088,
      "loss": 0.0586,
      "step": 54190
    },
    {
      "epoch": 15.885111371629543,
      "grad_norm": 1.1393767595291138,
      "learning_rate": 0.0002606064543998205,
      "loss": 0.0727,
      "step": 54200
    },
    {
      "epoch": 15.888042203985933,
      "grad_norm": 0.8859521150588989,
      "learning_rate": 0.0002605226581122321,
      "loss": 0.0857,
      "step": 54210
    },
    {
      "epoch": 15.890973036342322,
      "grad_norm": 0.8242599964141846,
      "learning_rate": 0.00026043886182464376,
      "loss": 0.0558,
      "step": 54220
    },
    {
      "epoch": 15.893903868698711,
      "grad_norm": 0.73427814245224,
      "learning_rate": 0.00026035506553705537,
      "loss": 0.0529,
      "step": 54230
    },
    {
      "epoch": 15.8968347010551,
      "grad_norm": 0.3241698741912842,
      "learning_rate": 0.000260271269249467,
      "loss": 0.0655,
      "step": 54240
    },
    {
      "epoch": 15.89976553341149,
      "grad_norm": 0.35265567898750305,
      "learning_rate": 0.00026018747296187864,
      "loss": 0.0571,
      "step": 54250
    },
    {
      "epoch": 15.902696365767879,
      "grad_norm": 0.6681904196739197,
      "learning_rate": 0.00026010367667429025,
      "loss": 0.0532,
      "step": 54260
    },
    {
      "epoch": 15.905627198124268,
      "grad_norm": 2.128648281097412,
      "learning_rate": 0.0002600198803867019,
      "loss": 0.0787,
      "step": 54270
    },
    {
      "epoch": 15.908558030480657,
      "grad_norm": 0.9155353903770447,
      "learning_rate": 0.0002599360840991135,
      "loss": 0.0427,
      "step": 54280
    },
    {
      "epoch": 15.911488862837047,
      "grad_norm": 1.0631256103515625,
      "learning_rate": 0.00025985228781152513,
      "loss": 0.0778,
      "step": 54290
    },
    {
      "epoch": 15.914419695193434,
      "grad_norm": 2.049739122390747,
      "learning_rate": 0.0002597684915239368,
      "loss": 0.08,
      "step": 54300
    },
    {
      "epoch": 15.917350527549825,
      "grad_norm": 1.6230921745300293,
      "learning_rate": 0.0002596846952363484,
      "loss": 0.04,
      "step": 54310
    },
    {
      "epoch": 15.920281359906213,
      "grad_norm": 1.5915896892547607,
      "learning_rate": 0.00025960089894876007,
      "loss": 0.0768,
      "step": 54320
    },
    {
      "epoch": 15.923212192262602,
      "grad_norm": 1.0581132173538208,
      "learning_rate": 0.00025951710266117173,
      "loss": 0.0695,
      "step": 54330
    },
    {
      "epoch": 15.926143024618991,
      "grad_norm": 0.400856077671051,
      "learning_rate": 0.00025943330637358334,
      "loss": 0.0349,
      "step": 54340
    },
    {
      "epoch": 15.92907385697538,
      "grad_norm": 1.029802680015564,
      "learning_rate": 0.00025934951008599495,
      "loss": 0.0652,
      "step": 54350
    },
    {
      "epoch": 15.93200468933177,
      "grad_norm": 0.13484564423561096,
      "learning_rate": 0.00025926571379840656,
      "loss": 0.0768,
      "step": 54360
    },
    {
      "epoch": 15.934935521688159,
      "grad_norm": 1.1624693870544434,
      "learning_rate": 0.00025918191751081823,
      "loss": 0.0838,
      "step": 54370
    },
    {
      "epoch": 15.937866354044548,
      "grad_norm": 0.6887927055358887,
      "learning_rate": 0.0002590981212232299,
      "loss": 0.0754,
      "step": 54380
    },
    {
      "epoch": 15.940797186400937,
      "grad_norm": 0.36294132471084595,
      "learning_rate": 0.0002590143249356415,
      "loss": 0.0455,
      "step": 54390
    },
    {
      "epoch": 15.943728018757326,
      "grad_norm": 0.7643159627914429,
      "learning_rate": 0.0002589305286480531,
      "loss": 0.0589,
      "step": 54400
    },
    {
      "epoch": 15.946658851113716,
      "grad_norm": 0.984062135219574,
      "learning_rate": 0.0002588467323604648,
      "loss": 0.0523,
      "step": 54410
    },
    {
      "epoch": 15.949589683470105,
      "grad_norm": 0.6494501233100891,
      "learning_rate": 0.0002587629360728764,
      "loss": 0.0785,
      "step": 54420
    },
    {
      "epoch": 15.952520515826494,
      "grad_norm": 1.0893824100494385,
      "learning_rate": 0.00025867913978528805,
      "loss": 0.063,
      "step": 54430
    },
    {
      "epoch": 15.955451348182883,
      "grad_norm": 1.1280887126922607,
      "learning_rate": 0.00025859534349769966,
      "loss": 0.054,
      "step": 54440
    },
    {
      "epoch": 15.958382180539273,
      "grad_norm": 3.8018908500671387,
      "learning_rate": 0.0002585115472101113,
      "loss": 0.0424,
      "step": 54450
    },
    {
      "epoch": 15.961313012895662,
      "grad_norm": 1.1212289333343506,
      "learning_rate": 0.00025842775092252293,
      "loss": 0.0828,
      "step": 54460
    },
    {
      "epoch": 15.964243845252051,
      "grad_norm": 0.7268459796905518,
      "learning_rate": 0.00025834395463493454,
      "loss": 0.0596,
      "step": 54470
    },
    {
      "epoch": 15.96717467760844,
      "grad_norm": 1.4326262474060059,
      "learning_rate": 0.0002582601583473462,
      "loss": 0.0417,
      "step": 54480
    },
    {
      "epoch": 15.97010550996483,
      "grad_norm": 1.0876622200012207,
      "learning_rate": 0.0002581763620597578,
      "loss": 0.075,
      "step": 54490
    },
    {
      "epoch": 15.973036342321219,
      "grad_norm": 1.6447733640670776,
      "learning_rate": 0.0002580925657721695,
      "loss": 0.0581,
      "step": 54500
    },
    {
      "epoch": 15.975967174677608,
      "grad_norm": 1.5426385402679443,
      "learning_rate": 0.0002580087694845811,
      "loss": 0.0583,
      "step": 54510
    },
    {
      "epoch": 15.978898007033997,
      "grad_norm": 1.2239198684692383,
      "learning_rate": 0.0002579249731969927,
      "loss": 0.0514,
      "step": 54520
    },
    {
      "epoch": 15.981828839390387,
      "grad_norm": 0.5878308415412903,
      "learning_rate": 0.00025784117690940436,
      "loss": 0.0764,
      "step": 54530
    },
    {
      "epoch": 15.984759671746776,
      "grad_norm": 2.1532020568847656,
      "learning_rate": 0.00025775738062181597,
      "loss": 0.0849,
      "step": 54540
    },
    {
      "epoch": 15.987690504103165,
      "grad_norm": 1.3589868545532227,
      "learning_rate": 0.00025767358433422764,
      "loss": 0.0654,
      "step": 54550
    },
    {
      "epoch": 15.990621336459554,
      "grad_norm": 1.8543239831924438,
      "learning_rate": 0.0002575897880466393,
      "loss": 0.0751,
      "step": 54560
    },
    {
      "epoch": 15.993552168815944,
      "grad_norm": 1.3464208841323853,
      "learning_rate": 0.00025750599175905086,
      "loss": 0.0964,
      "step": 54570
    },
    {
      "epoch": 15.996483001172333,
      "grad_norm": 1.0127724409103394,
      "learning_rate": 0.0002574221954714625,
      "loss": 0.0587,
      "step": 54580
    },
    {
      "epoch": 15.999413833528722,
      "grad_norm": 1.2010036706924438,
      "learning_rate": 0.0002573383991838742,
      "loss": 0.0677,
      "step": 54590
    },
    {
      "epoch": 16.0,
      "eval_accuracy": 0.7504121332014507,
      "eval_f1_macro": 0.801042717353423,
      "eval_f1_micro": 0.8257986738999398,
      "eval_f1_weighted": 0.8233037830445964,
      "eval_loss": 0.08073530346155167,
      "eval_roc_auc": 0.8936551678699878,
      "eval_runtime": 141.8439,
      "eval_samples_per_second": 21.383,
      "eval_steps_per_second": 2.679,
      "step": 54592
    },
    {
      "epoch": 16.00234466588511,
      "grad_norm": 0.985771119594574,
      "learning_rate": 0.0002572546028962858,
      "loss": 0.0387,
      "step": 54600
    },
    {
      "epoch": 16.0052754982415,
      "grad_norm": 0.5886006355285645,
      "learning_rate": 0.00025717080660869746,
      "loss": 0.0364,
      "step": 54610
    },
    {
      "epoch": 16.00820633059789,
      "grad_norm": 0.4228660762310028,
      "learning_rate": 0.00025708701032110907,
      "loss": 0.0395,
      "step": 54620
    },
    {
      "epoch": 16.011137162954277,
      "grad_norm": 0.7492268085479736,
      "learning_rate": 0.0002570032140335207,
      "loss": 0.0656,
      "step": 54630
    },
    {
      "epoch": 16.01406799531067,
      "grad_norm": 0.9958779215812683,
      "learning_rate": 0.00025691941774593234,
      "loss": 0.0568,
      "step": 54640
    },
    {
      "epoch": 16.016998827667056,
      "grad_norm": 0.19181478023529053,
      "learning_rate": 0.00025683562145834395,
      "loss": 0.0534,
      "step": 54650
    },
    {
      "epoch": 16.019929660023447,
      "grad_norm": 0.4507853090763092,
      "learning_rate": 0.0002567518251707556,
      "loss": 0.0366,
      "step": 54660
    },
    {
      "epoch": 16.022860492379834,
      "grad_norm": 1.0621620416641235,
      "learning_rate": 0.0002566680288831672,
      "loss": 0.0662,
      "step": 54670
    },
    {
      "epoch": 16.025791324736225,
      "grad_norm": 0.5410140156745911,
      "learning_rate": 0.00025658423259557883,
      "loss": 0.0416,
      "step": 54680
    },
    {
      "epoch": 16.028722157092613,
      "grad_norm": 1.0429075956344604,
      "learning_rate": 0.0002565004363079905,
      "loss": 0.0728,
      "step": 54690
    },
    {
      "epoch": 16.031652989449004,
      "grad_norm": 2.868846893310547,
      "learning_rate": 0.0002564166400204021,
      "loss": 0.055,
      "step": 54700
    },
    {
      "epoch": 16.03458382180539,
      "grad_norm": 0.2551752030849457,
      "learning_rate": 0.00025633284373281377,
      "loss": 0.0514,
      "step": 54710
    },
    {
      "epoch": 16.037514654161782,
      "grad_norm": 1.0121822357177734,
      "learning_rate": 0.0002562490474452254,
      "loss": 0.0736,
      "step": 54720
    },
    {
      "epoch": 16.04044548651817,
      "grad_norm": 1.4677141904830933,
      "learning_rate": 0.00025616525115763704,
      "loss": 0.0595,
      "step": 54730
    },
    {
      "epoch": 16.04337631887456,
      "grad_norm": 1.440852165222168,
      "learning_rate": 0.00025608145487004865,
      "loss": 0.0673,
      "step": 54740
    },
    {
      "epoch": 16.04630715123095,
      "grad_norm": 1.1880284547805786,
      "learning_rate": 0.00025599765858246026,
      "loss": 0.0511,
      "step": 54750
    },
    {
      "epoch": 16.04923798358734,
      "grad_norm": 1.16874361038208,
      "learning_rate": 0.00025591386229487193,
      "loss": 0.0561,
      "step": 54760
    },
    {
      "epoch": 16.052168815943727,
      "grad_norm": 0.7817583084106445,
      "learning_rate": 0.0002558300660072836,
      "loss": 0.0653,
      "step": 54770
    },
    {
      "epoch": 16.055099648300118,
      "grad_norm": 1.2018617391586304,
      "learning_rate": 0.0002557462697196952,
      "loss": 0.0649,
      "step": 54780
    },
    {
      "epoch": 16.058030480656505,
      "grad_norm": 1.9491194486618042,
      "learning_rate": 0.00025566247343210686,
      "loss": 0.0608,
      "step": 54790
    },
    {
      "epoch": 16.060961313012896,
      "grad_norm": 1.254259705543518,
      "learning_rate": 0.0002555786771445184,
      "loss": 0.0559,
      "step": 54800
    },
    {
      "epoch": 16.063892145369284,
      "grad_norm": 2.036316156387329,
      "learning_rate": 0.0002554948808569301,
      "loss": 0.0886,
      "step": 54810
    },
    {
      "epoch": 16.066822977725675,
      "grad_norm": 1.791148066520691,
      "learning_rate": 0.00025541108456934175,
      "loss": 0.0676,
      "step": 54820
    },
    {
      "epoch": 16.069753810082062,
      "grad_norm": 1.3569523096084595,
      "learning_rate": 0.00025532728828175336,
      "loss": 0.0594,
      "step": 54830
    },
    {
      "epoch": 16.072684642438453,
      "grad_norm": 0.8602100610733032,
      "learning_rate": 0.000255243491994165,
      "loss": 0.0466,
      "step": 54840
    },
    {
      "epoch": 16.07561547479484,
      "grad_norm": 0.711354672908783,
      "learning_rate": 0.00025515969570657663,
      "loss": 0.0623,
      "step": 54850
    },
    {
      "epoch": 16.078546307151232,
      "grad_norm": 0.7317463159561157,
      "learning_rate": 0.00025507589941898824,
      "loss": 0.0596,
      "step": 54860
    },
    {
      "epoch": 16.08147713950762,
      "grad_norm": 2.185403347015381,
      "learning_rate": 0.0002549921031313999,
      "loss": 0.0661,
      "step": 54870
    },
    {
      "epoch": 16.08440797186401,
      "grad_norm": 1.4154285192489624,
      "learning_rate": 0.0002549083068438115,
      "loss": 0.0548,
      "step": 54880
    },
    {
      "epoch": 16.087338804220398,
      "grad_norm": 0.9695101976394653,
      "learning_rate": 0.0002548245105562232,
      "loss": 0.0593,
      "step": 54890
    },
    {
      "epoch": 16.09026963657679,
      "grad_norm": 0.07155787944793701,
      "learning_rate": 0.0002547407142686348,
      "loss": 0.0588,
      "step": 54900
    },
    {
      "epoch": 16.093200468933176,
      "grad_norm": 0.8436912298202515,
      "learning_rate": 0.0002546569179810464,
      "loss": 0.0712,
      "step": 54910
    },
    {
      "epoch": 16.096131301289567,
      "grad_norm": 0.9212819337844849,
      "learning_rate": 0.00025457312169345806,
      "loss": 0.0567,
      "step": 54920
    },
    {
      "epoch": 16.099062133645955,
      "grad_norm": 0.8167111277580261,
      "learning_rate": 0.00025448932540586967,
      "loss": 0.0544,
      "step": 54930
    },
    {
      "epoch": 16.101992966002346,
      "grad_norm": 0.9774285554885864,
      "learning_rate": 0.00025440552911828134,
      "loss": 0.0269,
      "step": 54940
    },
    {
      "epoch": 16.104923798358733,
      "grad_norm": 0.8055256009101868,
      "learning_rate": 0.00025432173283069295,
      "loss": 0.0718,
      "step": 54950
    },
    {
      "epoch": 16.107854630715124,
      "grad_norm": 1.6450884342193604,
      "learning_rate": 0.0002542379365431046,
      "loss": 0.0627,
      "step": 54960
    },
    {
      "epoch": 16.11078546307151,
      "grad_norm": 0.8443422913551331,
      "learning_rate": 0.0002541541402555162,
      "loss": 0.0477,
      "step": 54970
    },
    {
      "epoch": 16.113716295427903,
      "grad_norm": 0.37235042452812195,
      "learning_rate": 0.00025407034396792783,
      "loss": 0.0486,
      "step": 54980
    },
    {
      "epoch": 16.11664712778429,
      "grad_norm": 1.448478102684021,
      "learning_rate": 0.0002539865476803395,
      "loss": 0.0491,
      "step": 54990
    },
    {
      "epoch": 16.11957796014068,
      "grad_norm": 0.7049942016601562,
      "learning_rate": 0.00025390275139275116,
      "loss": 0.0667,
      "step": 55000
    },
    {
      "epoch": 16.12250879249707,
      "grad_norm": 0.09127557277679443,
      "learning_rate": 0.00025381895510516277,
      "loss": 0.028,
      "step": 55010
    },
    {
      "epoch": 16.12543962485346,
      "grad_norm": 0.27973923087120056,
      "learning_rate": 0.0002537351588175744,
      "loss": 0.0495,
      "step": 55020
    },
    {
      "epoch": 16.128370457209847,
      "grad_norm": 1.138236165046692,
      "learning_rate": 0.00025365136252998604,
      "loss": 0.0666,
      "step": 55030
    },
    {
      "epoch": 16.131301289566238,
      "grad_norm": 0.8255360126495361,
      "learning_rate": 0.00025356756624239765,
      "loss": 0.0713,
      "step": 55040
    },
    {
      "epoch": 16.134232121922626,
      "grad_norm": 0.6981263160705566,
      "learning_rate": 0.0002534837699548093,
      "loss": 0.0437,
      "step": 55050
    },
    {
      "epoch": 16.137162954279017,
      "grad_norm": 1.3230186700820923,
      "learning_rate": 0.0002533999736672209,
      "loss": 0.0371,
      "step": 55060
    },
    {
      "epoch": 16.140093786635404,
      "grad_norm": 1.1637217998504639,
      "learning_rate": 0.0002533161773796326,
      "loss": 0.0654,
      "step": 55070
    },
    {
      "epoch": 16.143024618991795,
      "grad_norm": 1.055420994758606,
      "learning_rate": 0.0002532323810920442,
      "loss": 0.0453,
      "step": 55080
    },
    {
      "epoch": 16.145955451348183,
      "grad_norm": 0.46721068024635315,
      "learning_rate": 0.0002531485848044558,
      "loss": 0.0911,
      "step": 55090
    },
    {
      "epoch": 16.148886283704574,
      "grad_norm": 0.19377794861793518,
      "learning_rate": 0.00025306478851686747,
      "loss": 0.0493,
      "step": 55100
    },
    {
      "epoch": 16.15181711606096,
      "grad_norm": 2.136807441711426,
      "learning_rate": 0.0002529809922292791,
      "loss": 0.0332,
      "step": 55110
    },
    {
      "epoch": 16.154747948417352,
      "grad_norm": 0.795281708240509,
      "learning_rate": 0.00025289719594169074,
      "loss": 0.0526,
      "step": 55120
    },
    {
      "epoch": 16.15767878077374,
      "grad_norm": 1.5180957317352295,
      "learning_rate": 0.00025281339965410235,
      "loss": 0.033,
      "step": 55130
    },
    {
      "epoch": 16.16060961313013,
      "grad_norm": 0.9513623118400574,
      "learning_rate": 0.00025272960336651396,
      "loss": 0.0469,
      "step": 55140
    },
    {
      "epoch": 16.163540445486518,
      "grad_norm": 0.5019536018371582,
      "learning_rate": 0.00025264580707892563,
      "loss": 0.0487,
      "step": 55150
    },
    {
      "epoch": 16.16647127784291,
      "grad_norm": 1.119347095489502,
      "learning_rate": 0.00025256201079133724,
      "loss": 0.0361,
      "step": 55160
    },
    {
      "epoch": 16.169402110199297,
      "grad_norm": 1.4619256258010864,
      "learning_rate": 0.0002524782145037489,
      "loss": 0.0695,
      "step": 55170
    },
    {
      "epoch": 16.172332942555684,
      "grad_norm": 0.2638256847858429,
      "learning_rate": 0.00025239441821616056,
      "loss": 0.0456,
      "step": 55180
    },
    {
      "epoch": 16.175263774912075,
      "grad_norm": 0.8613182902336121,
      "learning_rate": 0.0002523106219285721,
      "loss": 0.0658,
      "step": 55190
    },
    {
      "epoch": 16.178194607268463,
      "grad_norm": 1.0936490297317505,
      "learning_rate": 0.0002522268256409838,
      "loss": 0.0873,
      "step": 55200
    },
    {
      "epoch": 16.181125439624854,
      "grad_norm": 0.6820869445800781,
      "learning_rate": 0.00025214302935339545,
      "loss": 0.0528,
      "step": 55210
    },
    {
      "epoch": 16.18405627198124,
      "grad_norm": 1.435562252998352,
      "learning_rate": 0.00025205923306580706,
      "loss": 0.0506,
      "step": 55220
    },
    {
      "epoch": 16.186987104337632,
      "grad_norm": 0.32212746143341064,
      "learning_rate": 0.0002519754367782187,
      "loss": 0.0699,
      "step": 55230
    },
    {
      "epoch": 16.18991793669402,
      "grad_norm": 0.7321061491966248,
      "learning_rate": 0.00025189164049063033,
      "loss": 0.0299,
      "step": 55240
    },
    {
      "epoch": 16.19284876905041,
      "grad_norm": 1.7150354385375977,
      "learning_rate": 0.00025180784420304194,
      "loss": 0.0415,
      "step": 55250
    },
    {
      "epoch": 16.195779601406798,
      "grad_norm": 1.4911903142929077,
      "learning_rate": 0.0002517240479154536,
      "loss": 0.0591,
      "step": 55260
    },
    {
      "epoch": 16.19871043376319,
      "grad_norm": 0.8602710366249084,
      "learning_rate": 0.0002516402516278652,
      "loss": 0.0613,
      "step": 55270
    },
    {
      "epoch": 16.201641266119577,
      "grad_norm": 1.5427238941192627,
      "learning_rate": 0.0002515564553402769,
      "loss": 0.0561,
      "step": 55280
    },
    {
      "epoch": 16.204572098475968,
      "grad_norm": 1.6735082864761353,
      "learning_rate": 0.0002514726590526885,
      "loss": 0.0459,
      "step": 55290
    },
    {
      "epoch": 16.207502930832355,
      "grad_norm": 0.9167515635490417,
      "learning_rate": 0.0002513888627651001,
      "loss": 0.0621,
      "step": 55300
    },
    {
      "epoch": 16.210433763188746,
      "grad_norm": 1.5310349464416504,
      "learning_rate": 0.00025130506647751176,
      "loss": 0.0563,
      "step": 55310
    },
    {
      "epoch": 16.213364595545134,
      "grad_norm": 0.7993823289871216,
      "learning_rate": 0.00025122127018992337,
      "loss": 0.0712,
      "step": 55320
    },
    {
      "epoch": 16.216295427901525,
      "grad_norm": 1.0989446640014648,
      "learning_rate": 0.00025113747390233504,
      "loss": 0.0566,
      "step": 55330
    },
    {
      "epoch": 16.219226260257912,
      "grad_norm": 1.6135867834091187,
      "learning_rate": 0.00025105367761474665,
      "loss": 0.0645,
      "step": 55340
    },
    {
      "epoch": 16.222157092614303,
      "grad_norm": 0.05832724645733833,
      "learning_rate": 0.0002509698813271583,
      "loss": 0.0523,
      "step": 55350
    },
    {
      "epoch": 16.22508792497069,
      "grad_norm": 0.38742977380752563,
      "learning_rate": 0.0002508860850395699,
      "loss": 0.0505,
      "step": 55360
    },
    {
      "epoch": 16.22801875732708,
      "grad_norm": 0.5878650546073914,
      "learning_rate": 0.00025080228875198153,
      "loss": 0.0507,
      "step": 55370
    },
    {
      "epoch": 16.23094958968347,
      "grad_norm": 1.3812135457992554,
      "learning_rate": 0.0002507184924643932,
      "loss": 0.052,
      "step": 55380
    },
    {
      "epoch": 16.23388042203986,
      "grad_norm": 1.4723598957061768,
      "learning_rate": 0.0002506346961768048,
      "loss": 0.0589,
      "step": 55390
    },
    {
      "epoch": 16.236811254396248,
      "grad_norm": 1.65610933303833,
      "learning_rate": 0.00025055089988921647,
      "loss": 0.0708,
      "step": 55400
    },
    {
      "epoch": 16.23974208675264,
      "grad_norm": 3.2650177478790283,
      "learning_rate": 0.00025046710360162813,
      "loss": 0.0399,
      "step": 55410
    },
    {
      "epoch": 16.242672919109026,
      "grad_norm": 2.0490853786468506,
      "learning_rate": 0.0002503833073140397,
      "loss": 0.0762,
      "step": 55420
    },
    {
      "epoch": 16.245603751465417,
      "grad_norm": 0.5303860306739807,
      "learning_rate": 0.00025029951102645135,
      "loss": 0.0472,
      "step": 55430
    },
    {
      "epoch": 16.248534583821804,
      "grad_norm": 0.8128944039344788,
      "learning_rate": 0.000250215714738863,
      "loss": 0.0574,
      "step": 55440
    },
    {
      "epoch": 16.251465416178196,
      "grad_norm": 1.5159815549850464,
      "learning_rate": 0.0002501319184512746,
      "loss": 0.073,
      "step": 55450
    },
    {
      "epoch": 16.254396248534583,
      "grad_norm": 0.9006151556968689,
      "learning_rate": 0.0002500481221636863,
      "loss": 0.044,
      "step": 55460
    },
    {
      "epoch": 16.257327080890974,
      "grad_norm": 1.019561767578125,
      "learning_rate": 0.0002499643258760979,
      "loss": 0.0665,
      "step": 55470
    },
    {
      "epoch": 16.26025791324736,
      "grad_norm": 0.80116868019104,
      "learning_rate": 0.0002498805295885095,
      "loss": 0.0508,
      "step": 55480
    },
    {
      "epoch": 16.263188745603752,
      "grad_norm": 1.329756736755371,
      "learning_rate": 0.00024979673330092117,
      "loss": 0.0569,
      "step": 55490
    },
    {
      "epoch": 16.26611957796014,
      "grad_norm": 0.8287638425827026,
      "learning_rate": 0.0002497129370133328,
      "loss": 0.0515,
      "step": 55500
    },
    {
      "epoch": 16.26905041031653,
      "grad_norm": 0.7256782054901123,
      "learning_rate": 0.00024962914072574444,
      "loss": 0.042,
      "step": 55510
    },
    {
      "epoch": 16.27198124267292,
      "grad_norm": 1.0310516357421875,
      "learning_rate": 0.00024954534443815605,
      "loss": 0.067,
      "step": 55520
    },
    {
      "epoch": 16.27491207502931,
      "grad_norm": 2.5723917484283447,
      "learning_rate": 0.00024946154815056766,
      "loss": 0.1067,
      "step": 55530
    },
    {
      "epoch": 16.277842907385697,
      "grad_norm": 0.6064807176589966,
      "learning_rate": 0.00024937775186297933,
      "loss": 0.0385,
      "step": 55540
    },
    {
      "epoch": 16.280773739742088,
      "grad_norm": 0.8334108591079712,
      "learning_rate": 0.00024929395557539094,
      "loss": 0.0654,
      "step": 55550
    },
    {
      "epoch": 16.283704572098475,
      "grad_norm": 1.0196374654769897,
      "learning_rate": 0.0002492101592878026,
      "loss": 0.0459,
      "step": 55560
    },
    {
      "epoch": 16.286635404454866,
      "grad_norm": 0.43724384903907776,
      "learning_rate": 0.0002491263630002142,
      "loss": 0.0623,
      "step": 55570
    },
    {
      "epoch": 16.289566236811254,
      "grad_norm": 0.42724308371543884,
      "learning_rate": 0.0002490425667126259,
      "loss": 0.0578,
      "step": 55580
    },
    {
      "epoch": 16.292497069167645,
      "grad_norm": 1.254094123840332,
      "learning_rate": 0.0002489587704250375,
      "loss": 0.0447,
      "step": 55590
    },
    {
      "epoch": 16.295427901524032,
      "grad_norm": 1.083910584449768,
      "learning_rate": 0.0002488749741374491,
      "loss": 0.0606,
      "step": 55600
    },
    {
      "epoch": 16.298358733880423,
      "grad_norm": 0.6908829212188721,
      "learning_rate": 0.00024879117784986076,
      "loss": 0.0604,
      "step": 55610
    },
    {
      "epoch": 16.30128956623681,
      "grad_norm": 1.4158467054367065,
      "learning_rate": 0.0002487073815622724,
      "loss": 0.0636,
      "step": 55620
    },
    {
      "epoch": 16.304220398593202,
      "grad_norm": 0.6564189195632935,
      "learning_rate": 0.00024862358527468403,
      "loss": 0.0351,
      "step": 55630
    },
    {
      "epoch": 16.30715123094959,
      "grad_norm": 1.518774151802063,
      "learning_rate": 0.00024853978898709564,
      "loss": 0.0591,
      "step": 55640
    },
    {
      "epoch": 16.31008206330598,
      "grad_norm": 2.928333282470703,
      "learning_rate": 0.0002484559926995073,
      "loss": 0.0521,
      "step": 55650
    },
    {
      "epoch": 16.313012895662368,
      "grad_norm": 0.29469069838523865,
      "learning_rate": 0.0002483721964119189,
      "loss": 0.0451,
      "step": 55660
    },
    {
      "epoch": 16.31594372801876,
      "grad_norm": 0.7576450109481812,
      "learning_rate": 0.0002482884001243306,
      "loss": 0.0409,
      "step": 55670
    },
    {
      "epoch": 16.318874560375146,
      "grad_norm": 0.9672936201095581,
      "learning_rate": 0.0002482046038367422,
      "loss": 0.0425,
      "step": 55680
    },
    {
      "epoch": 16.321805392731537,
      "grad_norm": 0.981955885887146,
      "learning_rate": 0.00024812080754915385,
      "loss": 0.0705,
      "step": 55690
    },
    {
      "epoch": 16.324736225087925,
      "grad_norm": 1.8309683799743652,
      "learning_rate": 0.00024803701126156546,
      "loss": 0.0429,
      "step": 55700
    },
    {
      "epoch": 16.327667057444316,
      "grad_norm": 0.5838609933853149,
      "learning_rate": 0.00024795321497397707,
      "loss": 0.0459,
      "step": 55710
    },
    {
      "epoch": 16.330597889800703,
      "grad_norm": 2.906125068664551,
      "learning_rate": 0.00024786941868638874,
      "loss": 0.072,
      "step": 55720
    },
    {
      "epoch": 16.33352872215709,
      "grad_norm": 1.0484248399734497,
      "learning_rate": 0.00024778562239880035,
      "loss": 0.0603,
      "step": 55730
    },
    {
      "epoch": 16.336459554513482,
      "grad_norm": 0.5681920051574707,
      "learning_rate": 0.000247701826111212,
      "loss": 0.043,
      "step": 55740
    },
    {
      "epoch": 16.33939038686987,
      "grad_norm": 1.4769902229309082,
      "learning_rate": 0.0002476180298236236,
      "loss": 0.072,
      "step": 55750
    },
    {
      "epoch": 16.34232121922626,
      "grad_norm": 0.906792163848877,
      "learning_rate": 0.00024753423353603523,
      "loss": 0.0767,
      "step": 55760
    },
    {
      "epoch": 16.345252051582648,
      "grad_norm": 1.1921274662017822,
      "learning_rate": 0.0002474504372484469,
      "loss": 0.0873,
      "step": 55770
    },
    {
      "epoch": 16.34818288393904,
      "grad_norm": 0.6619272828102112,
      "learning_rate": 0.0002473666409608585,
      "loss": 0.0616,
      "step": 55780
    },
    {
      "epoch": 16.351113716295426,
      "grad_norm": 0.30668580532073975,
      "learning_rate": 0.00024728284467327017,
      "loss": 0.0551,
      "step": 55790
    },
    {
      "epoch": 16.354044548651817,
      "grad_norm": 0.7540615200996399,
      "learning_rate": 0.00024719904838568183,
      "loss": 0.0529,
      "step": 55800
    },
    {
      "epoch": 16.356975381008205,
      "grad_norm": 0.609673798084259,
      "learning_rate": 0.0002471152520980934,
      "loss": 0.0748,
      "step": 55810
    },
    {
      "epoch": 16.359906213364596,
      "grad_norm": 0.749097466468811,
      "learning_rate": 0.00024703145581050505,
      "loss": 0.0603,
      "step": 55820
    },
    {
      "epoch": 16.362837045720983,
      "grad_norm": 1.5496950149536133,
      "learning_rate": 0.0002469476595229167,
      "loss": 0.0565,
      "step": 55830
    },
    {
      "epoch": 16.365767878077374,
      "grad_norm": 0.7246092557907104,
      "learning_rate": 0.0002468638632353283,
      "loss": 0.0658,
      "step": 55840
    },
    {
      "epoch": 16.368698710433762,
      "grad_norm": 0.8043659925460815,
      "learning_rate": 0.00024678006694774,
      "loss": 0.0511,
      "step": 55850
    },
    {
      "epoch": 16.371629542790153,
      "grad_norm": 1.1364355087280273,
      "learning_rate": 0.0002466962706601516,
      "loss": 0.0562,
      "step": 55860
    },
    {
      "epoch": 16.37456037514654,
      "grad_norm": 0.2887089252471924,
      "learning_rate": 0.0002466124743725632,
      "loss": 0.0406,
      "step": 55870
    },
    {
      "epoch": 16.37749120750293,
      "grad_norm": 0.7717248201370239,
      "learning_rate": 0.00024652867808497487,
      "loss": 0.0659,
      "step": 55880
    },
    {
      "epoch": 16.38042203985932,
      "grad_norm": 0.6568138599395752,
      "learning_rate": 0.0002464448817973865,
      "loss": 0.0535,
      "step": 55890
    },
    {
      "epoch": 16.38335287221571,
      "grad_norm": 0.6741732358932495,
      "learning_rate": 0.00024636108550979814,
      "loss": 0.069,
      "step": 55900
    },
    {
      "epoch": 16.386283704572097,
      "grad_norm": 0.606025218963623,
      "learning_rate": 0.00024627728922220975,
      "loss": 0.0573,
      "step": 55910
    },
    {
      "epoch": 16.38921453692849,
      "grad_norm": 1.2486294507980347,
      "learning_rate": 0.00024619349293462136,
      "loss": 0.0624,
      "step": 55920
    },
    {
      "epoch": 16.392145369284876,
      "grad_norm": 0.3651238977909088,
      "learning_rate": 0.00024610969664703303,
      "loss": 0.0446,
      "step": 55930
    },
    {
      "epoch": 16.395076201641267,
      "grad_norm": 1.0432040691375732,
      "learning_rate": 0.00024602590035944464,
      "loss": 0.045,
      "step": 55940
    },
    {
      "epoch": 16.398007033997654,
      "grad_norm": 1.2199872732162476,
      "learning_rate": 0.0002459421040718563,
      "loss": 0.061,
      "step": 55950
    },
    {
      "epoch": 16.400937866354045,
      "grad_norm": 0.4730311930179596,
      "learning_rate": 0.0002458583077842679,
      "loss": 0.0366,
      "step": 55960
    },
    {
      "epoch": 16.403868698710433,
      "grad_norm": 0.9317472577095032,
      "learning_rate": 0.0002457745114966796,
      "loss": 0.0473,
      "step": 55970
    },
    {
      "epoch": 16.406799531066824,
      "grad_norm": 1.4407861232757568,
      "learning_rate": 0.0002456907152090912,
      "loss": 0.0692,
      "step": 55980
    },
    {
      "epoch": 16.40973036342321,
      "grad_norm": 1.136792540550232,
      "learning_rate": 0.0002456069189215028,
      "loss": 0.0764,
      "step": 55990
    },
    {
      "epoch": 16.412661195779602,
      "grad_norm": 1.9533056020736694,
      "learning_rate": 0.00024552312263391446,
      "loss": 0.0582,
      "step": 56000
    },
    {
      "epoch": 16.41559202813599,
      "grad_norm": 1.640653371810913,
      "learning_rate": 0.00024543932634632607,
      "loss": 0.0586,
      "step": 56010
    },
    {
      "epoch": 16.41852286049238,
      "grad_norm": 2.589945077896118,
      "learning_rate": 0.00024535553005873773,
      "loss": 0.0639,
      "step": 56020
    },
    {
      "epoch": 16.421453692848768,
      "grad_norm": 1.3715730905532837,
      "learning_rate": 0.0002452717337711494,
      "loss": 0.069,
      "step": 56030
    },
    {
      "epoch": 16.42438452520516,
      "grad_norm": 0.6733949780464172,
      "learning_rate": 0.00024518793748356095,
      "loss": 0.052,
      "step": 56040
    },
    {
      "epoch": 16.427315357561547,
      "grad_norm": 0.06185314431786537,
      "learning_rate": 0.0002451041411959726,
      "loss": 0.0439,
      "step": 56050
    },
    {
      "epoch": 16.430246189917938,
      "grad_norm": 1.4020178318023682,
      "learning_rate": 0.0002450203449083843,
      "loss": 0.0597,
      "step": 56060
    },
    {
      "epoch": 16.433177022274325,
      "grad_norm": 0.9459658265113831,
      "learning_rate": 0.0002449365486207959,
      "loss": 0.0661,
      "step": 56070
    },
    {
      "epoch": 16.436107854630716,
      "grad_norm": 1.1704943180084229,
      "learning_rate": 0.00024485275233320755,
      "loss": 0.0504,
      "step": 56080
    },
    {
      "epoch": 16.439038686987104,
      "grad_norm": 1.1210176944732666,
      "learning_rate": 0.00024476895604561916,
      "loss": 0.0561,
      "step": 56090
    },
    {
      "epoch": 16.441969519343495,
      "grad_norm": 1.3089805841445923,
      "learning_rate": 0.00024468515975803077,
      "loss": 0.055,
      "step": 56100
    },
    {
      "epoch": 16.444900351699882,
      "grad_norm": 1.1714668273925781,
      "learning_rate": 0.00024460136347044244,
      "loss": 0.0783,
      "step": 56110
    },
    {
      "epoch": 16.447831184056273,
      "grad_norm": 1.0040749311447144,
      "learning_rate": 0.00024451756718285405,
      "loss": 0.0607,
      "step": 56120
    },
    {
      "epoch": 16.45076201641266,
      "grad_norm": 0.8140006065368652,
      "learning_rate": 0.0002444337708952657,
      "loss": 0.0498,
      "step": 56130
    },
    {
      "epoch": 16.45369284876905,
      "grad_norm": 0.5254087448120117,
      "learning_rate": 0.0002443499746076773,
      "loss": 0.0874,
      "step": 56140
    },
    {
      "epoch": 16.45662368112544,
      "grad_norm": 1.7929670810699463,
      "learning_rate": 0.00024426617832008893,
      "loss": 0.071,
      "step": 56150
    },
    {
      "epoch": 16.45955451348183,
      "grad_norm": 2.557509422302246,
      "learning_rate": 0.0002441823820325006,
      "loss": 0.0817,
      "step": 56160
    },
    {
      "epoch": 16.462485345838218,
      "grad_norm": 0.5613402724266052,
      "learning_rate": 0.00024409858574491223,
      "loss": 0.0554,
      "step": 56170
    },
    {
      "epoch": 16.46541617819461,
      "grad_norm": 1.0874786376953125,
      "learning_rate": 0.00024401478945732387,
      "loss": 0.0514,
      "step": 56180
    },
    {
      "epoch": 16.468347010550996,
      "grad_norm": 1.4874666929244995,
      "learning_rate": 0.0002439309931697355,
      "loss": 0.0736,
      "step": 56190
    },
    {
      "epoch": 16.471277842907387,
      "grad_norm": 1.3982914686203003,
      "learning_rate": 0.0002438471968821471,
      "loss": 0.0453,
      "step": 56200
    },
    {
      "epoch": 16.474208675263775,
      "grad_norm": 0.8706484436988831,
      "learning_rate": 0.00024376340059455875,
      "loss": 0.0577,
      "step": 56210
    },
    {
      "epoch": 16.477139507620166,
      "grad_norm": 1.2090898752212524,
      "learning_rate": 0.00024367960430697039,
      "loss": 0.0822,
      "step": 56220
    },
    {
      "epoch": 16.480070339976553,
      "grad_norm": 1.1435123682022095,
      "learning_rate": 0.00024359580801938202,
      "loss": 0.0458,
      "step": 56230
    },
    {
      "epoch": 16.483001172332944,
      "grad_norm": 0.2530366778373718,
      "learning_rate": 0.00024351201173179366,
      "loss": 0.0458,
      "step": 56240
    },
    {
      "epoch": 16.48593200468933,
      "grad_norm": 0.29478123784065247,
      "learning_rate": 0.0002434282154442053,
      "loss": 0.0518,
      "step": 56250
    },
    {
      "epoch": 16.488862837045723,
      "grad_norm": 0.28654828667640686,
      "learning_rate": 0.0002433444191566169,
      "loss": 0.0406,
      "step": 56260
    },
    {
      "epoch": 16.49179366940211,
      "grad_norm": 1.2893277406692505,
      "learning_rate": 0.00024326062286902854,
      "loss": 0.0546,
      "step": 56270
    },
    {
      "epoch": 16.4947245017585,
      "grad_norm": 0.3510030508041382,
      "learning_rate": 0.00024317682658144018,
      "loss": 0.0499,
      "step": 56280
    },
    {
      "epoch": 16.49765533411489,
      "grad_norm": 0.9615991711616516,
      "learning_rate": 0.00024309303029385182,
      "loss": 0.0479,
      "step": 56290
    },
    {
      "epoch": 16.50058616647128,
      "grad_norm": 0.6470092535018921,
      "learning_rate": 0.00024300923400626345,
      "loss": 0.0452,
      "step": 56300
    },
    {
      "epoch": 16.503516998827667,
      "grad_norm": 1.104853868484497,
      "learning_rate": 0.00024292543771867512,
      "loss": 0.0622,
      "step": 56310
    },
    {
      "epoch": 16.506447831184055,
      "grad_norm": 2.1080691814422607,
      "learning_rate": 0.0002428416414310867,
      "loss": 0.0545,
      "step": 56320
    },
    {
      "epoch": 16.509378663540446,
      "grad_norm": 2.818932056427002,
      "learning_rate": 0.00024275784514349834,
      "loss": 0.0769,
      "step": 56330
    },
    {
      "epoch": 16.512309495896833,
      "grad_norm": 0.8605223298072815,
      "learning_rate": 0.00024267404885590997,
      "loss": 0.0447,
      "step": 56340
    },
    {
      "epoch": 16.515240328253224,
      "grad_norm": 1.7217854261398315,
      "learning_rate": 0.00024259025256832164,
      "loss": 0.0508,
      "step": 56350
    },
    {
      "epoch": 16.51817116060961,
      "grad_norm": 0.46625828742980957,
      "learning_rate": 0.00024250645628073327,
      "loss": 0.0405,
      "step": 56360
    },
    {
      "epoch": 16.521101992966003,
      "grad_norm": 0.6564679741859436,
      "learning_rate": 0.00024242265999314486,
      "loss": 0.0406,
      "step": 56370
    },
    {
      "epoch": 16.52403282532239,
      "grad_norm": 2.870185136795044,
      "learning_rate": 0.00024233886370555652,
      "loss": 0.0504,
      "step": 56380
    },
    {
      "epoch": 16.52696365767878,
      "grad_norm": 1.013230323791504,
      "learning_rate": 0.00024225506741796816,
      "loss": 0.0595,
      "step": 56390
    },
    {
      "epoch": 16.52989449003517,
      "grad_norm": 0.6141547560691833,
      "learning_rate": 0.0002421712711303798,
      "loss": 0.0354,
      "step": 56400
    },
    {
      "epoch": 16.53282532239156,
      "grad_norm": 0.6225295066833496,
      "learning_rate": 0.00024208747484279143,
      "loss": 0.0634,
      "step": 56410
    },
    {
      "epoch": 16.535756154747947,
      "grad_norm": 2.3174076080322266,
      "learning_rate": 0.00024200367855520307,
      "loss": 0.0516,
      "step": 56420
    },
    {
      "epoch": 16.538686987104338,
      "grad_norm": 1.5274252891540527,
      "learning_rate": 0.00024191988226761468,
      "loss": 0.0605,
      "step": 56430
    },
    {
      "epoch": 16.541617819460726,
      "grad_norm": 0.3652442693710327,
      "learning_rate": 0.00024183608598002631,
      "loss": 0.0596,
      "step": 56440
    },
    {
      "epoch": 16.544548651817117,
      "grad_norm": 1.4117103815078735,
      "learning_rate": 0.00024175228969243795,
      "loss": 0.0385,
      "step": 56450
    },
    {
      "epoch": 16.547479484173504,
      "grad_norm": 0.464631050825119,
      "learning_rate": 0.0002416684934048496,
      "loss": 0.0572,
      "step": 56460
    },
    {
      "epoch": 16.550410316529895,
      "grad_norm": 2.513495445251465,
      "learning_rate": 0.00024158469711726122,
      "loss": 0.0604,
      "step": 56470
    },
    {
      "epoch": 16.553341148886282,
      "grad_norm": 1.607685923576355,
      "learning_rate": 0.00024150090082967286,
      "loss": 0.085,
      "step": 56480
    },
    {
      "epoch": 16.556271981242674,
      "grad_norm": 0.8275643587112427,
      "learning_rate": 0.00024141710454208447,
      "loss": 0.0518,
      "step": 56490
    },
    {
      "epoch": 16.55920281359906,
      "grad_norm": 2.444629430770874,
      "learning_rate": 0.0002413333082544961,
      "loss": 0.076,
      "step": 56500
    },
    {
      "epoch": 16.562133645955452,
      "grad_norm": 0.5537977814674377,
      "learning_rate": 0.00024124951196690775,
      "loss": 0.0342,
      "step": 56510
    },
    {
      "epoch": 16.56506447831184,
      "grad_norm": 1.0130702257156372,
      "learning_rate": 0.00024116571567931938,
      "loss": 0.0695,
      "step": 56520
    },
    {
      "epoch": 16.56799531066823,
      "grad_norm": 1.5554277896881104,
      "learning_rate": 0.00024108191939173105,
      "loss": 0.0577,
      "step": 56530
    },
    {
      "epoch": 16.570926143024618,
      "grad_norm": 1.062196135520935,
      "learning_rate": 0.00024099812310414263,
      "loss": 0.0449,
      "step": 56540
    },
    {
      "epoch": 16.57385697538101,
      "grad_norm": 1.0571842193603516,
      "learning_rate": 0.00024091432681655427,
      "loss": 0.0664,
      "step": 56550
    },
    {
      "epoch": 16.576787807737396,
      "grad_norm": 0.3576001524925232,
      "learning_rate": 0.0002408305305289659,
      "loss": 0.03,
      "step": 56560
    },
    {
      "epoch": 16.579718640093787,
      "grad_norm": 0.8017150163650513,
      "learning_rate": 0.00024074673424137757,
      "loss": 0.0603,
      "step": 56570
    },
    {
      "epoch": 16.582649472450175,
      "grad_norm": 1.5143295526504517,
      "learning_rate": 0.0002406629379537892,
      "loss": 0.0522,
      "step": 56580
    },
    {
      "epoch": 16.585580304806566,
      "grad_norm": 1.166577935218811,
      "learning_rate": 0.00024057914166620084,
      "loss": 0.0581,
      "step": 56590
    },
    {
      "epoch": 16.588511137162953,
      "grad_norm": 1.3036915063858032,
      "learning_rate": 0.00024049534537861245,
      "loss": 0.0778,
      "step": 56600
    },
    {
      "epoch": 16.591441969519344,
      "grad_norm": 1.5246167182922363,
      "learning_rate": 0.00024041154909102409,
      "loss": 0.0657,
      "step": 56610
    },
    {
      "epoch": 16.594372801875732,
      "grad_norm": 1.8709499835968018,
      "learning_rate": 0.00024032775280343572,
      "loss": 0.0645,
      "step": 56620
    },
    {
      "epoch": 16.597303634232123,
      "grad_norm": 0.22853916883468628,
      "learning_rate": 0.00024024395651584736,
      "loss": 0.0666,
      "step": 56630
    },
    {
      "epoch": 16.60023446658851,
      "grad_norm": 0.9175599813461304,
      "learning_rate": 0.000240160160228259,
      "loss": 0.057,
      "step": 56640
    },
    {
      "epoch": 16.6031652989449,
      "grad_norm": 0.23889586329460144,
      "learning_rate": 0.00024007636394067063,
      "loss": 0.0497,
      "step": 56650
    },
    {
      "epoch": 16.60609613130129,
      "grad_norm": 1.3178118467330933,
      "learning_rate": 0.00023999256765308224,
      "loss": 0.0757,
      "step": 56660
    },
    {
      "epoch": 16.60902696365768,
      "grad_norm": 0.8813726305961609,
      "learning_rate": 0.00023990877136549388,
      "loss": 0.0376,
      "step": 56670
    },
    {
      "epoch": 16.611957796014067,
      "grad_norm": 0.9624388813972473,
      "learning_rate": 0.00023982497507790552,
      "loss": 0.0671,
      "step": 56680
    },
    {
      "epoch": 16.61488862837046,
      "grad_norm": 0.18038728833198547,
      "learning_rate": 0.00023974117879031715,
      "loss": 0.0523,
      "step": 56690
    },
    {
      "epoch": 16.617819460726846,
      "grad_norm": 0.8279384970664978,
      "learning_rate": 0.0002396573825027288,
      "loss": 0.0618,
      "step": 56700
    },
    {
      "epoch": 16.620750293083237,
      "grad_norm": 3.071211099624634,
      "learning_rate": 0.0002395735862151404,
      "loss": 0.0374,
      "step": 56710
    },
    {
      "epoch": 16.623681125439624,
      "grad_norm": 0.7047431468963623,
      "learning_rate": 0.00023948978992755204,
      "loss": 0.062,
      "step": 56720
    },
    {
      "epoch": 16.626611957796015,
      "grad_norm": 1.2573137283325195,
      "learning_rate": 0.00023940599363996367,
      "loss": 0.0447,
      "step": 56730
    },
    {
      "epoch": 16.629542790152403,
      "grad_norm": 0.6967083811759949,
      "learning_rate": 0.0002393221973523753,
      "loss": 0.0986,
      "step": 56740
    },
    {
      "epoch": 16.632473622508794,
      "grad_norm": 1.5606552362442017,
      "learning_rate": 0.00023923840106478697,
      "loss": 0.0581,
      "step": 56750
    },
    {
      "epoch": 16.63540445486518,
      "grad_norm": 1.4170955419540405,
      "learning_rate": 0.0002391546047771986,
      "loss": 0.0658,
      "step": 56760
    },
    {
      "epoch": 16.638335287221572,
      "grad_norm": 1.4045026302337646,
      "learning_rate": 0.0002390708084896102,
      "loss": 0.051,
      "step": 56770
    },
    {
      "epoch": 16.64126611957796,
      "grad_norm": 0.6408432126045227,
      "learning_rate": 0.00023898701220202183,
      "loss": 0.0341,
      "step": 56780
    },
    {
      "epoch": 16.64419695193435,
      "grad_norm": 2.7294085025787354,
      "learning_rate": 0.0002389032159144335,
      "loss": 0.0437,
      "step": 56790
    },
    {
      "epoch": 16.64712778429074,
      "grad_norm": 1.5049117803573608,
      "learning_rate": 0.00023881941962684513,
      "loss": 0.0659,
      "step": 56800
    },
    {
      "epoch": 16.65005861664713,
      "grad_norm": 0.3908262252807617,
      "learning_rate": 0.00023873562333925677,
      "loss": 0.0703,
      "step": 56810
    },
    {
      "epoch": 16.652989449003517,
      "grad_norm": 0.801066517829895,
      "learning_rate": 0.00023865182705166838,
      "loss": 0.0466,
      "step": 56820
    },
    {
      "epoch": 16.655920281359908,
      "grad_norm": 1.193995475769043,
      "learning_rate": 0.00023856803076408001,
      "loss": 0.088,
      "step": 56830
    },
    {
      "epoch": 16.658851113716295,
      "grad_norm": 0.7361841797828674,
      "learning_rate": 0.00023848423447649165,
      "loss": 0.0504,
      "step": 56840
    },
    {
      "epoch": 16.661781946072686,
      "grad_norm": 1.0587917566299438,
      "learning_rate": 0.0002384004381889033,
      "loss": 0.073,
      "step": 56850
    },
    {
      "epoch": 16.664712778429074,
      "grad_norm": 0.23138946294784546,
      "learning_rate": 0.00023831664190131492,
      "loss": 0.0205,
      "step": 56860
    },
    {
      "epoch": 16.66764361078546,
      "grad_norm": 2.3194682598114014,
      "learning_rate": 0.00023823284561372656,
      "loss": 0.0737,
      "step": 56870
    },
    {
      "epoch": 16.670574443141852,
      "grad_norm": 0.1585819274187088,
      "learning_rate": 0.00023814904932613817,
      "loss": 0.0717,
      "step": 56880
    },
    {
      "epoch": 16.67350527549824,
      "grad_norm": 0.7929664850234985,
      "learning_rate": 0.0002380652530385498,
      "loss": 0.044,
      "step": 56890
    },
    {
      "epoch": 16.67643610785463,
      "grad_norm": 0.34695184230804443,
      "learning_rate": 0.00023798145675096145,
      "loss": 0.0577,
      "step": 56900
    },
    {
      "epoch": 16.67936694021102,
      "grad_norm": 1.0887402296066284,
      "learning_rate": 0.00023789766046337308,
      "loss": 0.0445,
      "step": 56910
    },
    {
      "epoch": 16.68229777256741,
      "grad_norm": 0.5242360830307007,
      "learning_rate": 0.00023781386417578472,
      "loss": 0.0502,
      "step": 56920
    },
    {
      "epoch": 16.685228604923797,
      "grad_norm": 0.6227028965950012,
      "learning_rate": 0.00023773006788819638,
      "loss": 0.0508,
      "step": 56930
    },
    {
      "epoch": 16.688159437280188,
      "grad_norm": 2.601592540740967,
      "learning_rate": 0.00023764627160060797,
      "loss": 0.0817,
      "step": 56940
    },
    {
      "epoch": 16.691090269636575,
      "grad_norm": 0.2853024899959564,
      "learning_rate": 0.0002375624753130196,
      "loss": 0.0457,
      "step": 56950
    },
    {
      "epoch": 16.694021101992966,
      "grad_norm": 0.9897869825363159,
      "learning_rate": 0.00023747867902543124,
      "loss": 0.0388,
      "step": 56960
    },
    {
      "epoch": 16.696951934349354,
      "grad_norm": 0.5706741809844971,
      "learning_rate": 0.0002373948827378429,
      "loss": 0.0434,
      "step": 56970
    },
    {
      "epoch": 16.699882766705745,
      "grad_norm": 0.4507645070552826,
      "learning_rate": 0.00023731108645025454,
      "loss": 0.0495,
      "step": 56980
    },
    {
      "epoch": 16.702813599062132,
      "grad_norm": 1.0652239322662354,
      "learning_rate": 0.00023722729016266612,
      "loss": 0.0523,
      "step": 56990
    },
    {
      "epoch": 16.705744431418523,
      "grad_norm": 0.29646608233451843,
      "learning_rate": 0.00023714349387507776,
      "loss": 0.045,
      "step": 57000
    },
    {
      "epoch": 16.70867526377491,
      "grad_norm": 0.7727565765380859,
      "learning_rate": 0.00023705969758748942,
      "loss": 0.0453,
      "step": 57010
    },
    {
      "epoch": 16.7116060961313,
      "grad_norm": 1.1615720987319946,
      "learning_rate": 0.00023697590129990106,
      "loss": 0.0758,
      "step": 57020
    },
    {
      "epoch": 16.71453692848769,
      "grad_norm": 0.6113003492355347,
      "learning_rate": 0.0002368921050123127,
      "loss": 0.0553,
      "step": 57030
    },
    {
      "epoch": 16.71746776084408,
      "grad_norm": 1.317585825920105,
      "learning_rate": 0.00023680830872472433,
      "loss": 0.0521,
      "step": 57040
    },
    {
      "epoch": 16.720398593200468,
      "grad_norm": 1.1353212594985962,
      "learning_rate": 0.00023672451243713594,
      "loss": 0.0492,
      "step": 57050
    },
    {
      "epoch": 16.72332942555686,
      "grad_norm": 0.46784114837646484,
      "learning_rate": 0.00023664071614954758,
      "loss": 0.0522,
      "step": 57060
    },
    {
      "epoch": 16.726260257913246,
      "grad_norm": 1.084524393081665,
      "learning_rate": 0.00023655691986195922,
      "loss": 0.0501,
      "step": 57070
    },
    {
      "epoch": 16.729191090269637,
      "grad_norm": 1.9807647466659546,
      "learning_rate": 0.00023647312357437085,
      "loss": 0.0671,
      "step": 57080
    },
    {
      "epoch": 16.732121922626025,
      "grad_norm": 0.6001014709472656,
      "learning_rate": 0.0002363893272867825,
      "loss": 0.0525,
      "step": 57090
    },
    {
      "epoch": 16.735052754982416,
      "grad_norm": 0.9076671004295349,
      "learning_rate": 0.00023630553099919413,
      "loss": 0.0635,
      "step": 57100
    },
    {
      "epoch": 16.737983587338803,
      "grad_norm": 0.899068295955658,
      "learning_rate": 0.00023622173471160574,
      "loss": 0.0573,
      "step": 57110
    },
    {
      "epoch": 16.740914419695194,
      "grad_norm": 0.7812381386756897,
      "learning_rate": 0.00023613793842401737,
      "loss": 0.0561,
      "step": 57120
    },
    {
      "epoch": 16.74384525205158,
      "grad_norm": 1.1011978387832642,
      "learning_rate": 0.000236054142136429,
      "loss": 0.0557,
      "step": 57130
    },
    {
      "epoch": 16.746776084407973,
      "grad_norm": 0.3071471154689789,
      "learning_rate": 0.00023597034584884065,
      "loss": 0.0676,
      "step": 57140
    },
    {
      "epoch": 16.74970691676436,
      "grad_norm": 0.689467191696167,
      "learning_rate": 0.0002358865495612523,
      "loss": 0.0866,
      "step": 57150
    },
    {
      "epoch": 16.75263774912075,
      "grad_norm": 0.3615763783454895,
      "learning_rate": 0.0002358027532736639,
      "loss": 0.0759,
      "step": 57160
    },
    {
      "epoch": 16.75556858147714,
      "grad_norm": 1.0446869134902954,
      "learning_rate": 0.00023571895698607553,
      "loss": 0.0553,
      "step": 57170
    },
    {
      "epoch": 16.75849941383353,
      "grad_norm": 1.675184726715088,
      "learning_rate": 0.00023563516069848717,
      "loss": 0.0604,
      "step": 57180
    },
    {
      "epoch": 16.761430246189917,
      "grad_norm": 0.43245336413383484,
      "learning_rate": 0.00023555136441089883,
      "loss": 0.0868,
      "step": 57190
    },
    {
      "epoch": 16.764361078546308,
      "grad_norm": 0.1269567608833313,
      "learning_rate": 0.00023546756812331047,
      "loss": 0.0571,
      "step": 57200
    },
    {
      "epoch": 16.767291910902696,
      "grad_norm": 0.9241619110107422,
      "learning_rate": 0.0002353837718357221,
      "loss": 0.0773,
      "step": 57210
    },
    {
      "epoch": 16.770222743259087,
      "grad_norm": 0.9648683667182922,
      "learning_rate": 0.00023529997554813371,
      "loss": 0.0543,
      "step": 57220
    },
    {
      "epoch": 16.773153575615474,
      "grad_norm": 0.9618004560470581,
      "learning_rate": 0.00023521617926054535,
      "loss": 0.0564,
      "step": 57230
    },
    {
      "epoch": 16.776084407971865,
      "grad_norm": 0.3795812726020813,
      "learning_rate": 0.000235132382972957,
      "loss": 0.047,
      "step": 57240
    },
    {
      "epoch": 16.779015240328253,
      "grad_norm": 0.38233259320259094,
      "learning_rate": 0.00023504858668536862,
      "loss": 0.0621,
      "step": 57250
    },
    {
      "epoch": 16.781946072684644,
      "grad_norm": 0.6063804030418396,
      "learning_rate": 0.00023496479039778026,
      "loss": 0.0347,
      "step": 57260
    },
    {
      "epoch": 16.78487690504103,
      "grad_norm": 0.8438224196434021,
      "learning_rate": 0.0002348809941101919,
      "loss": 0.0402,
      "step": 57270
    },
    {
      "epoch": 16.787807737397422,
      "grad_norm": 1.223942756652832,
      "learning_rate": 0.0002347971978226035,
      "loss": 0.0605,
      "step": 57280
    },
    {
      "epoch": 16.79073856975381,
      "grad_norm": 1.9510318040847778,
      "learning_rate": 0.00023471340153501514,
      "loss": 0.0776,
      "step": 57290
    },
    {
      "epoch": 16.7936694021102,
      "grad_norm": 0.49511173367500305,
      "learning_rate": 0.00023462960524742678,
      "loss": 0.0447,
      "step": 57300
    },
    {
      "epoch": 16.796600234466588,
      "grad_norm": 0.4025669991970062,
      "learning_rate": 0.00023454580895983842,
      "loss": 0.051,
      "step": 57310
    },
    {
      "epoch": 16.79953106682298,
      "grad_norm": 1.6941556930541992,
      "learning_rate": 0.00023446201267225006,
      "loss": 0.0506,
      "step": 57320
    },
    {
      "epoch": 16.802461899179367,
      "grad_norm": 1.0475035905838013,
      "learning_rate": 0.00023437821638466167,
      "loss": 0.0457,
      "step": 57330
    },
    {
      "epoch": 16.805392731535758,
      "grad_norm": 0.9587920904159546,
      "learning_rate": 0.0002342944200970733,
      "loss": 0.0663,
      "step": 57340
    },
    {
      "epoch": 16.808323563892145,
      "grad_norm": 1.3229695558547974,
      "learning_rate": 0.00023421062380948494,
      "loss": 0.0506,
      "step": 57350
    },
    {
      "epoch": 16.811254396248536,
      "grad_norm": 1.8498915433883667,
      "learning_rate": 0.00023412682752189658,
      "loss": 0.0792,
      "step": 57360
    },
    {
      "epoch": 16.814185228604924,
      "grad_norm": 0.3812229335308075,
      "learning_rate": 0.00023404303123430824,
      "loss": 0.0633,
      "step": 57370
    },
    {
      "epoch": 16.817116060961315,
      "grad_norm": 0.6960421800613403,
      "learning_rate": 0.00023395923494671988,
      "loss": 0.0663,
      "step": 57380
    },
    {
      "epoch": 16.820046893317702,
      "grad_norm": 1.2881672382354736,
      "learning_rate": 0.00023387543865913146,
      "loss": 0.0584,
      "step": 57390
    },
    {
      "epoch": 16.822977725674093,
      "grad_norm": 0.9986827373504639,
      "learning_rate": 0.0002337916423715431,
      "loss": 0.0563,
      "step": 57400
    },
    {
      "epoch": 16.82590855803048,
      "grad_norm": 1.8639262914657593,
      "learning_rate": 0.00023370784608395476,
      "loss": 0.0652,
      "step": 57410
    },
    {
      "epoch": 16.828839390386868,
      "grad_norm": 1.4220709800720215,
      "learning_rate": 0.0002336240497963664,
      "loss": 0.0585,
      "step": 57420
    },
    {
      "epoch": 16.83177022274326,
      "grad_norm": 0.9565818309783936,
      "learning_rate": 0.00023354025350877803,
      "loss": 0.0459,
      "step": 57430
    },
    {
      "epoch": 16.83470105509965,
      "grad_norm": 1.6072359085083008,
      "learning_rate": 0.00023345645722118964,
      "loss": 0.0496,
      "step": 57440
    },
    {
      "epoch": 16.837631887456038,
      "grad_norm": 2.0727760791778564,
      "learning_rate": 0.00023337266093360128,
      "loss": 0.0701,
      "step": 57450
    },
    {
      "epoch": 16.840562719812425,
      "grad_norm": 2.8109190464019775,
      "learning_rate": 0.00023328886464601292,
      "loss": 0.0632,
      "step": 57460
    },
    {
      "epoch": 16.843493552168816,
      "grad_norm": 0.7880880832672119,
      "learning_rate": 0.00023320506835842455,
      "loss": 0.0463,
      "step": 57470
    },
    {
      "epoch": 16.846424384525204,
      "grad_norm": 0.4421011805534363,
      "learning_rate": 0.0002331212720708362,
      "loss": 0.0593,
      "step": 57480
    },
    {
      "epoch": 16.849355216881595,
      "grad_norm": 0.20993518829345703,
      "learning_rate": 0.00023303747578324783,
      "loss": 0.0562,
      "step": 57490
    },
    {
      "epoch": 16.852286049237982,
      "grad_norm": 2.4924230575561523,
      "learning_rate": 0.00023295367949565944,
      "loss": 0.0654,
      "step": 57500
    },
    {
      "epoch": 16.855216881594373,
      "grad_norm": 1.998553991317749,
      "learning_rate": 0.00023286988320807107,
      "loss": 0.0516,
      "step": 57510
    },
    {
      "epoch": 16.85814771395076,
      "grad_norm": 0.545624315738678,
      "learning_rate": 0.0002327860869204827,
      "loss": 0.0633,
      "step": 57520
    },
    {
      "epoch": 16.86107854630715,
      "grad_norm": 0.9157349467277527,
      "learning_rate": 0.00023270229063289435,
      "loss": 0.0833,
      "step": 57530
    },
    {
      "epoch": 16.86400937866354,
      "grad_norm": 0.42519626021385193,
      "learning_rate": 0.00023261849434530598,
      "loss": 0.0449,
      "step": 57540
    },
    {
      "epoch": 16.86694021101993,
      "grad_norm": 0.6015186309814453,
      "learning_rate": 0.00023253469805771762,
      "loss": 0.0539,
      "step": 57550
    },
    {
      "epoch": 16.869871043376317,
      "grad_norm": 0.431610107421875,
      "learning_rate": 0.00023245090177012923,
      "loss": 0.0598,
      "step": 57560
    },
    {
      "epoch": 16.87280187573271,
      "grad_norm": 0.21144810318946838,
      "learning_rate": 0.00023236710548254087,
      "loss": 0.046,
      "step": 57570
    },
    {
      "epoch": 16.875732708089096,
      "grad_norm": 0.6314613819122314,
      "learning_rate": 0.0002322833091949525,
      "loss": 0.0424,
      "step": 57580
    },
    {
      "epoch": 16.878663540445487,
      "grad_norm": 2.7842957973480225,
      "learning_rate": 0.00023219951290736417,
      "loss": 0.0686,
      "step": 57590
    },
    {
      "epoch": 16.881594372801874,
      "grad_norm": 0.8561092019081116,
      "learning_rate": 0.0002321157166197758,
      "loss": 0.0524,
      "step": 57600
    },
    {
      "epoch": 16.884525205158265,
      "grad_norm": 0.5040108561515808,
      "learning_rate": 0.0002320319203321874,
      "loss": 0.0745,
      "step": 57610
    },
    {
      "epoch": 16.887456037514653,
      "grad_norm": 0.3726811110973358,
      "learning_rate": 0.00023194812404459902,
      "loss": 0.0707,
      "step": 57620
    },
    {
      "epoch": 16.890386869871044,
      "grad_norm": 0.6002641916275024,
      "learning_rate": 0.0002318643277570107,
      "loss": 0.0271,
      "step": 57630
    },
    {
      "epoch": 16.89331770222743,
      "grad_norm": 0.6672850251197815,
      "learning_rate": 0.00023178053146942232,
      "loss": 0.0425,
      "step": 57640
    },
    {
      "epoch": 16.896248534583822,
      "grad_norm": 0.09818768501281738,
      "learning_rate": 0.00023169673518183396,
      "loss": 0.0604,
      "step": 57650
    },
    {
      "epoch": 16.89917936694021,
      "grad_norm": 1.4942344427108765,
      "learning_rate": 0.0002316129388942456,
      "loss": 0.0548,
      "step": 57660
    },
    {
      "epoch": 16.9021101992966,
      "grad_norm": 0.650521457195282,
      "learning_rate": 0.0002315291426066572,
      "loss": 0.0588,
      "step": 57670
    },
    {
      "epoch": 16.90504103165299,
      "grad_norm": 2.031358003616333,
      "learning_rate": 0.00023144534631906884,
      "loss": 0.0424,
      "step": 57680
    },
    {
      "epoch": 16.90797186400938,
      "grad_norm": 1.5899027585983276,
      "learning_rate": 0.00023136155003148048,
      "loss": 0.056,
      "step": 57690
    },
    {
      "epoch": 16.910902696365767,
      "grad_norm": 0.9415717720985413,
      "learning_rate": 0.00023127775374389212,
      "loss": 0.0539,
      "step": 57700
    },
    {
      "epoch": 16.913833528722158,
      "grad_norm": 1.650925874710083,
      "learning_rate": 0.00023119395745630376,
      "loss": 0.055,
      "step": 57710
    },
    {
      "epoch": 16.916764361078545,
      "grad_norm": 0.6476997137069702,
      "learning_rate": 0.0002311101611687154,
      "loss": 0.0662,
      "step": 57720
    },
    {
      "epoch": 16.919695193434936,
      "grad_norm": 1.5784668922424316,
      "learning_rate": 0.000231026364881127,
      "loss": 0.0643,
      "step": 57730
    },
    {
      "epoch": 16.922626025791324,
      "grad_norm": 2.368197202682495,
      "learning_rate": 0.00023094256859353864,
      "loss": 0.093,
      "step": 57740
    },
    {
      "epoch": 16.925556858147715,
      "grad_norm": 0.9934499263763428,
      "learning_rate": 0.00023085877230595028,
      "loss": 0.067,
      "step": 57750
    },
    {
      "epoch": 16.928487690504102,
      "grad_norm": 1.3867608308792114,
      "learning_rate": 0.0002307749760183619,
      "loss": 0.0751,
      "step": 57760
    },
    {
      "epoch": 16.931418522860493,
      "grad_norm": 0.47821006178855896,
      "learning_rate": 0.00023069117973077355,
      "loss": 0.0538,
      "step": 57770
    },
    {
      "epoch": 16.93434935521688,
      "grad_norm": 0.9458280205726624,
      "learning_rate": 0.00023060738344318516,
      "loss": 0.0575,
      "step": 57780
    },
    {
      "epoch": 16.937280187573272,
      "grad_norm": 0.7585957050323486,
      "learning_rate": 0.0002305235871555968,
      "loss": 0.0426,
      "step": 57790
    },
    {
      "epoch": 16.94021101992966,
      "grad_norm": 1.0462290048599243,
      "learning_rate": 0.00023043979086800843,
      "loss": 0.0513,
      "step": 57800
    },
    {
      "epoch": 16.94314185228605,
      "grad_norm": 0.9160336256027222,
      "learning_rate": 0.0002303559945804201,
      "loss": 0.0555,
      "step": 57810
    },
    {
      "epoch": 16.946072684642438,
      "grad_norm": 0.4027286767959595,
      "learning_rate": 0.00023027219829283173,
      "loss": 0.0493,
      "step": 57820
    },
    {
      "epoch": 16.94900351699883,
      "grad_norm": 1.429522156715393,
      "learning_rate": 0.00023018840200524337,
      "loss": 0.079,
      "step": 57830
    },
    {
      "epoch": 16.951934349355216,
      "grad_norm": 1.4248934984207153,
      "learning_rate": 0.00023010460571765495,
      "loss": 0.0538,
      "step": 57840
    },
    {
      "epoch": 16.954865181711607,
      "grad_norm": 0.9089158773422241,
      "learning_rate": 0.00023002080943006662,
      "loss": 0.0714,
      "step": 57850
    },
    {
      "epoch": 16.957796014067995,
      "grad_norm": 1.3263790607452393,
      "learning_rate": 0.00022993701314247825,
      "loss": 0.0707,
      "step": 57860
    },
    {
      "epoch": 16.960726846424386,
      "grad_norm": 3.9256484508514404,
      "learning_rate": 0.0002298532168548899,
      "loss": 0.0632,
      "step": 57870
    },
    {
      "epoch": 16.963657678780773,
      "grad_norm": 1.5706558227539062,
      "learning_rate": 0.00022976942056730153,
      "loss": 0.0674,
      "step": 57880
    },
    {
      "epoch": 16.966588511137164,
      "grad_norm": 0.6132291555404663,
      "learning_rate": 0.00022968562427971316,
      "loss": 0.0486,
      "step": 57890
    },
    {
      "epoch": 16.969519343493552,
      "grad_norm": 0.6324123740196228,
      "learning_rate": 0.00022960182799212477,
      "loss": 0.0515,
      "step": 57900
    },
    {
      "epoch": 16.972450175849943,
      "grad_norm": 1.079862117767334,
      "learning_rate": 0.0002295180317045364,
      "loss": 0.0694,
      "step": 57910
    },
    {
      "epoch": 16.97538100820633,
      "grad_norm": 1.223181128501892,
      "learning_rate": 0.00022943423541694805,
      "loss": 0.0553,
      "step": 57920
    },
    {
      "epoch": 16.97831184056272,
      "grad_norm": 1.4596314430236816,
      "learning_rate": 0.00022935043912935968,
      "loss": 0.0517,
      "step": 57930
    },
    {
      "epoch": 16.98124267291911,
      "grad_norm": 0.9007350206375122,
      "learning_rate": 0.00022926664284177132,
      "loss": 0.0397,
      "step": 57940
    },
    {
      "epoch": 16.9841735052755,
      "grad_norm": 0.5342485904693604,
      "learning_rate": 0.00022918284655418293,
      "loss": 0.0458,
      "step": 57950
    },
    {
      "epoch": 16.987104337631887,
      "grad_norm": 1.103360891342163,
      "learning_rate": 0.00022909905026659457,
      "loss": 0.0575,
      "step": 57960
    },
    {
      "epoch": 16.99003516998828,
      "grad_norm": 0.5145969986915588,
      "learning_rate": 0.0002290152539790062,
      "loss": 0.0438,
      "step": 57970
    },
    {
      "epoch": 16.992966002344666,
      "grad_norm": 0.7840801477432251,
      "learning_rate": 0.00022893145769141784,
      "loss": 0.0897,
      "step": 57980
    },
    {
      "epoch": 16.995896834701057,
      "grad_norm": 1.3436845541000366,
      "learning_rate": 0.00022884766140382948,
      "loss": 0.0428,
      "step": 57990
    },
    {
      "epoch": 16.998827667057444,
      "grad_norm": 0.6472829580307007,
      "learning_rate": 0.00022876386511624114,
      "loss": 0.0654,
      "step": 58000
    },
    {
      "epoch": 17.0,
      "eval_accuracy": 0.7662380481371579,
      "eval_f1_macro": 0.8070376610639357,
      "eval_f1_micro": 0.8393907404614689,
      "eval_f1_weighted": 0.8383166878090447,
      "eval_loss": 0.07396203279495239,
      "eval_roc_auc": 0.9005418951099627,
      "eval_runtime": 142.9203,
      "eval_samples_per_second": 21.222,
      "eval_steps_per_second": 2.659,
      "step": 58004
    },
    {
      "epoch": 17.00175849941383,
      "grad_norm": 1.2072224617004395,
      "learning_rate": 0.00022868006882865272,
      "loss": 0.0687,
      "step": 58010
    },
    {
      "epoch": 17.004689331770223,
      "grad_norm": 1.273929476737976,
      "learning_rate": 0.00022859627254106436,
      "loss": 0.0578,
      "step": 58020
    },
    {
      "epoch": 17.00762016412661,
      "grad_norm": 1.2243448495864868,
      "learning_rate": 0.00022851247625347602,
      "loss": 0.0567,
      "step": 58030
    },
    {
      "epoch": 17.010550996483,
      "grad_norm": 0.23783686757087708,
      "learning_rate": 0.00022842867996588766,
      "loss": 0.0376,
      "step": 58040
    },
    {
      "epoch": 17.01348182883939,
      "grad_norm": 1.3310389518737793,
      "learning_rate": 0.0002283448836782993,
      "loss": 0.0601,
      "step": 58050
    },
    {
      "epoch": 17.01641266119578,
      "grad_norm": 1.3493363857269287,
      "learning_rate": 0.00022826108739071088,
      "loss": 0.0455,
      "step": 58060
    },
    {
      "epoch": 17.019343493552167,
      "grad_norm": 1.3308242559432983,
      "learning_rate": 0.00022817729110312254,
      "loss": 0.0617,
      "step": 58070
    },
    {
      "epoch": 17.02227432590856,
      "grad_norm": 1.1178430318832397,
      "learning_rate": 0.00022809349481553418,
      "loss": 0.0655,
      "step": 58080
    },
    {
      "epoch": 17.025205158264946,
      "grad_norm": 2.203378677368164,
      "learning_rate": 0.00022800969852794582,
      "loss": 0.053,
      "step": 58090
    },
    {
      "epoch": 17.028135990621337,
      "grad_norm": 0.6386584639549255,
      "learning_rate": 0.00022792590224035746,
      "loss": 0.0539,
      "step": 58100
    },
    {
      "epoch": 17.031066822977724,
      "grad_norm": 3.8399410247802734,
      "learning_rate": 0.0002278421059527691,
      "loss": 0.0537,
      "step": 58110
    },
    {
      "epoch": 17.033997655334115,
      "grad_norm": 1.0881726741790771,
      "learning_rate": 0.0002277583096651807,
      "loss": 0.0526,
      "step": 58120
    },
    {
      "epoch": 17.036928487690503,
      "grad_norm": 0.27619409561157227,
      "learning_rate": 0.00022767451337759234,
      "loss": 0.0332,
      "step": 58130
    },
    {
      "epoch": 17.039859320046894,
      "grad_norm": 1.2181475162506104,
      "learning_rate": 0.00022759071709000398,
      "loss": 0.063,
      "step": 58140
    },
    {
      "epoch": 17.04279015240328,
      "grad_norm": 1.05109703540802,
      "learning_rate": 0.0002275069208024156,
      "loss": 0.0494,
      "step": 58150
    },
    {
      "epoch": 17.045720984759672,
      "grad_norm": 0.8264890313148499,
      "learning_rate": 0.00022742312451482725,
      "loss": 0.0477,
      "step": 58160
    },
    {
      "epoch": 17.04865181711606,
      "grad_norm": 1.2021324634552002,
      "learning_rate": 0.00022733932822723889,
      "loss": 0.0506,
      "step": 58170
    },
    {
      "epoch": 17.05158264947245,
      "grad_norm": 0.9924341440200806,
      "learning_rate": 0.0002272555319396505,
      "loss": 0.0515,
      "step": 58180
    },
    {
      "epoch": 17.054513481828838,
      "grad_norm": 1.0616673231124878,
      "learning_rate": 0.00022717173565206213,
      "loss": 0.0523,
      "step": 58190
    },
    {
      "epoch": 17.05744431418523,
      "grad_norm": 1.0518640279769897,
      "learning_rate": 0.00022708793936447377,
      "loss": 0.041,
      "step": 58200
    },
    {
      "epoch": 17.060375146541617,
      "grad_norm": 1.820676565170288,
      "learning_rate": 0.00022700414307688543,
      "loss": 0.0515,
      "step": 58210
    },
    {
      "epoch": 17.063305978898008,
      "grad_norm": 2.1378097534179688,
      "learning_rate": 0.00022692034678929707,
      "loss": 0.0412,
      "step": 58220
    },
    {
      "epoch": 17.066236811254395,
      "grad_norm": 0.7397027015686035,
      "learning_rate": 0.00022683655050170865,
      "loss": 0.059,
      "step": 58230
    },
    {
      "epoch": 17.069167643610786,
      "grad_norm": 0.49121975898742676,
      "learning_rate": 0.0002267527542141203,
      "loss": 0.0459,
      "step": 58240
    },
    {
      "epoch": 17.072098475967174,
      "grad_norm": 0.9418732523918152,
      "learning_rate": 0.00022666895792653195,
      "loss": 0.0456,
      "step": 58250
    },
    {
      "epoch": 17.075029308323565,
      "grad_norm": 0.5806396007537842,
      "learning_rate": 0.0002265851616389436,
      "loss": 0.0277,
      "step": 58260
    },
    {
      "epoch": 17.077960140679952,
      "grad_norm": 0.7851726412773132,
      "learning_rate": 0.00022650136535135523,
      "loss": 0.0365,
      "step": 58270
    },
    {
      "epoch": 17.080890973036343,
      "grad_norm": 1.4861351251602173,
      "learning_rate": 0.00022641756906376686,
      "loss": 0.0649,
      "step": 58280
    },
    {
      "epoch": 17.08382180539273,
      "grad_norm": 0.7073215246200562,
      "learning_rate": 0.00022633377277617847,
      "loss": 0.0263,
      "step": 58290
    },
    {
      "epoch": 17.08675263774912,
      "grad_norm": 0.11917757242918015,
      "learning_rate": 0.0002262499764885901,
      "loss": 0.0494,
      "step": 58300
    },
    {
      "epoch": 17.08968347010551,
      "grad_norm": 0.6008628606796265,
      "learning_rate": 0.00022616618020100175,
      "loss": 0.0242,
      "step": 58310
    },
    {
      "epoch": 17.0926143024619,
      "grad_norm": 1.5044645071029663,
      "learning_rate": 0.00022608238391341338,
      "loss": 0.0543,
      "step": 58320
    },
    {
      "epoch": 17.095545134818288,
      "grad_norm": 0.8109355568885803,
      "learning_rate": 0.00022599858762582502,
      "loss": 0.0461,
      "step": 58330
    },
    {
      "epoch": 17.09847596717468,
      "grad_norm": 1.3100030422210693,
      "learning_rate": 0.00022591479133823666,
      "loss": 0.0672,
      "step": 58340
    },
    {
      "epoch": 17.101406799531066,
      "grad_norm": 1.4230594635009766,
      "learning_rate": 0.00022583099505064827,
      "loss": 0.0627,
      "step": 58350
    },
    {
      "epoch": 17.104337631887457,
      "grad_norm": 1.0659668445587158,
      "learning_rate": 0.0002257471987630599,
      "loss": 0.0448,
      "step": 58360
    },
    {
      "epoch": 17.107268464243845,
      "grad_norm": 0.7992005944252014,
      "learning_rate": 0.00022566340247547154,
      "loss": 0.0716,
      "step": 58370
    },
    {
      "epoch": 17.110199296600236,
      "grad_norm": 0.2080429047346115,
      "learning_rate": 0.00022557960618788318,
      "loss": 0.0639,
      "step": 58380
    },
    {
      "epoch": 17.113130128956623,
      "grad_norm": 0.7892271280288696,
      "learning_rate": 0.00022549580990029481,
      "loss": 0.0361,
      "step": 58390
    },
    {
      "epoch": 17.116060961313014,
      "grad_norm": 0.8626320958137512,
      "learning_rate": 0.00022541201361270642,
      "loss": 0.0343,
      "step": 58400
    },
    {
      "epoch": 17.1189917936694,
      "grad_norm": 0.820520281791687,
      "learning_rate": 0.00022532821732511806,
      "loss": 0.0402,
      "step": 58410
    },
    {
      "epoch": 17.121922626025793,
      "grad_norm": 0.7187249660491943,
      "learning_rate": 0.0002252444210375297,
      "loss": 0.0592,
      "step": 58420
    },
    {
      "epoch": 17.12485345838218,
      "grad_norm": 0.33508679270744324,
      "learning_rate": 0.00022516062474994136,
      "loss": 0.0386,
      "step": 58430
    },
    {
      "epoch": 17.12778429073857,
      "grad_norm": 0.43135201930999756,
      "learning_rate": 0.000225076828462353,
      "loss": 0.0421,
      "step": 58440
    },
    {
      "epoch": 17.13071512309496,
      "grad_norm": 1.1926190853118896,
      "learning_rate": 0.00022499303217476463,
      "loss": 0.07,
      "step": 58450
    },
    {
      "epoch": 17.13364595545135,
      "grad_norm": 0.8512811660766602,
      "learning_rate": 0.00022490923588717622,
      "loss": 0.033,
      "step": 58460
    },
    {
      "epoch": 17.136576787807737,
      "grad_norm": 0.7985081076622009,
      "learning_rate": 0.00022482543959958788,
      "loss": 0.0404,
      "step": 58470
    },
    {
      "epoch": 17.139507620164128,
      "grad_norm": 1.956530213356018,
      "learning_rate": 0.00022474164331199952,
      "loss": 0.0416,
      "step": 58480
    },
    {
      "epoch": 17.142438452520516,
      "grad_norm": 0.1911439448595047,
      "learning_rate": 0.00022465784702441116,
      "loss": 0.0462,
      "step": 58490
    },
    {
      "epoch": 17.145369284876907,
      "grad_norm": 0.11768706142902374,
      "learning_rate": 0.0002245740507368228,
      "loss": 0.0419,
      "step": 58500
    },
    {
      "epoch": 17.148300117233294,
      "grad_norm": 0.17919686436653137,
      "learning_rate": 0.0002244902544492344,
      "loss": 0.0438,
      "step": 58510
    },
    {
      "epoch": 17.151230949589685,
      "grad_norm": 0.30216941237449646,
      "learning_rate": 0.00022440645816164604,
      "loss": 0.0473,
      "step": 58520
    },
    {
      "epoch": 17.154161781946073,
      "grad_norm": 0.3817001283168793,
      "learning_rate": 0.00022432266187405768,
      "loss": 0.0456,
      "step": 58530
    },
    {
      "epoch": 17.157092614302464,
      "grad_norm": 3.637291669845581,
      "learning_rate": 0.0002242388655864693,
      "loss": 0.0435,
      "step": 58540
    },
    {
      "epoch": 17.16002344665885,
      "grad_norm": 0.8371201753616333,
      "learning_rate": 0.00022415506929888095,
      "loss": 0.055,
      "step": 58550
    },
    {
      "epoch": 17.162954279015242,
      "grad_norm": 0.5650162696838379,
      "learning_rate": 0.00022407127301129259,
      "loss": 0.0536,
      "step": 58560
    },
    {
      "epoch": 17.16588511137163,
      "grad_norm": 0.9775275588035583,
      "learning_rate": 0.0002239874767237042,
      "loss": 0.0387,
      "step": 58570
    },
    {
      "epoch": 17.168815943728017,
      "grad_norm": 1.0134989023208618,
      "learning_rate": 0.00022390368043611583,
      "loss": 0.0327,
      "step": 58580
    },
    {
      "epoch": 17.171746776084408,
      "grad_norm": 0.8128973841667175,
      "learning_rate": 0.00022381988414852747,
      "loss": 0.0522,
      "step": 58590
    },
    {
      "epoch": 17.174677608440795,
      "grad_norm": 0.5966589450836182,
      "learning_rate": 0.0002237360878609391,
      "loss": 0.0445,
      "step": 58600
    },
    {
      "epoch": 17.177608440797187,
      "grad_norm": 1.4771430492401123,
      "learning_rate": 0.00022365229157335074,
      "loss": 0.0476,
      "step": 58610
    },
    {
      "epoch": 17.180539273153574,
      "grad_norm": 1.374750018119812,
      "learning_rate": 0.0002235684952857624,
      "loss": 0.0513,
      "step": 58620
    },
    {
      "epoch": 17.183470105509965,
      "grad_norm": 1.103417158126831,
      "learning_rate": 0.000223484698998174,
      "loss": 0.0486,
      "step": 58630
    },
    {
      "epoch": 17.186400937866352,
      "grad_norm": 0.20593753457069397,
      "learning_rate": 0.00022340090271058563,
      "loss": 0.0381,
      "step": 58640
    },
    {
      "epoch": 17.189331770222744,
      "grad_norm": 0.34538373351097107,
      "learning_rate": 0.0002233171064229973,
      "loss": 0.0581,
      "step": 58650
    },
    {
      "epoch": 17.19226260257913,
      "grad_norm": 0.7849096655845642,
      "learning_rate": 0.00022323331013540893,
      "loss": 0.0316,
      "step": 58660
    },
    {
      "epoch": 17.195193434935522,
      "grad_norm": 2.000490665435791,
      "learning_rate": 0.00022314951384782056,
      "loss": 0.0501,
      "step": 58670
    },
    {
      "epoch": 17.19812426729191,
      "grad_norm": 2.5871922969818115,
      "learning_rate": 0.00022306571756023215,
      "loss": 0.097,
      "step": 58680
    },
    {
      "epoch": 17.2010550996483,
      "grad_norm": 1.770223617553711,
      "learning_rate": 0.0002229819212726438,
      "loss": 0.0524,
      "step": 58690
    },
    {
      "epoch": 17.203985932004688,
      "grad_norm": 0.8977581262588501,
      "learning_rate": 0.00022289812498505545,
      "loss": 0.062,
      "step": 58700
    },
    {
      "epoch": 17.20691676436108,
      "grad_norm": 1.0315546989440918,
      "learning_rate": 0.00022281432869746708,
      "loss": 0.0724,
      "step": 58710
    },
    {
      "epoch": 17.209847596717466,
      "grad_norm": 1.9420783519744873,
      "learning_rate": 0.00022273053240987872,
      "loss": 0.0853,
      "step": 58720
    },
    {
      "epoch": 17.212778429073857,
      "grad_norm": 0.5801407694816589,
      "learning_rate": 0.00022264673612229036,
      "loss": 0.0974,
      "step": 58730
    },
    {
      "epoch": 17.215709261430245,
      "grad_norm": 2.1677300930023193,
      "learning_rate": 0.00022256293983470197,
      "loss": 0.0631,
      "step": 58740
    },
    {
      "epoch": 17.218640093786636,
      "grad_norm": 1.5889345407485962,
      "learning_rate": 0.0002224791435471136,
      "loss": 0.037,
      "step": 58750
    },
    {
      "epoch": 17.221570926143023,
      "grad_norm": 0.54046231508255,
      "learning_rate": 0.00022239534725952524,
      "loss": 0.0342,
      "step": 58760
    },
    {
      "epoch": 17.224501758499414,
      "grad_norm": 0.39681243896484375,
      "learning_rate": 0.00022231155097193688,
      "loss": 0.0397,
      "step": 58770
    },
    {
      "epoch": 17.227432590855802,
      "grad_norm": 0.9003074169158936,
      "learning_rate": 0.00022222775468434851,
      "loss": 0.0406,
      "step": 58780
    },
    {
      "epoch": 17.230363423212193,
      "grad_norm": 1.5095173120498657,
      "learning_rate": 0.00022214395839676015,
      "loss": 0.0414,
      "step": 58790
    },
    {
      "epoch": 17.23329425556858,
      "grad_norm": 0.4161272644996643,
      "learning_rate": 0.00022206016210917176,
      "loss": 0.0483,
      "step": 58800
    },
    {
      "epoch": 17.23622508792497,
      "grad_norm": 3.405471086502075,
      "learning_rate": 0.0002219763658215834,
      "loss": 0.0474,
      "step": 58810
    },
    {
      "epoch": 17.23915592028136,
      "grad_norm": 1.2824060916900635,
      "learning_rate": 0.00022189256953399503,
      "loss": 0.0488,
      "step": 58820
    },
    {
      "epoch": 17.24208675263775,
      "grad_norm": 0.30062681436538696,
      "learning_rate": 0.00022180877324640667,
      "loss": 0.0371,
      "step": 58830
    },
    {
      "epoch": 17.245017584994137,
      "grad_norm": 1.9572837352752686,
      "learning_rate": 0.00022172497695881833,
      "loss": 0.0456,
      "step": 58840
    },
    {
      "epoch": 17.24794841735053,
      "grad_norm": 0.5270318984985352,
      "learning_rate": 0.00022164118067122992,
      "loss": 0.0345,
      "step": 58850
    },
    {
      "epoch": 17.250879249706916,
      "grad_norm": 0.37360620498657227,
      "learning_rate": 0.00022155738438364155,
      "loss": 0.0326,
      "step": 58860
    },
    {
      "epoch": 17.253810082063307,
      "grad_norm": 0.7259995937347412,
      "learning_rate": 0.00022147358809605322,
      "loss": 0.0434,
      "step": 58870
    },
    {
      "epoch": 17.256740914419694,
      "grad_norm": 1.0928454399108887,
      "learning_rate": 0.00022138979180846486,
      "loss": 0.0458,
      "step": 58880
    },
    {
      "epoch": 17.259671746776085,
      "grad_norm": 0.5144338011741638,
      "learning_rate": 0.0002213059955208765,
      "loss": 0.0835,
      "step": 58890
    },
    {
      "epoch": 17.262602579132473,
      "grad_norm": 0.8967286348342896,
      "learning_rate": 0.00022122219923328813,
      "loss": 0.0464,
      "step": 58900
    },
    {
      "epoch": 17.265533411488864,
      "grad_norm": 1.6848475933074951,
      "learning_rate": 0.00022113840294569974,
      "loss": 0.0605,
      "step": 58910
    },
    {
      "epoch": 17.26846424384525,
      "grad_norm": 0.818390965461731,
      "learning_rate": 0.00022105460665811138,
      "loss": 0.0647,
      "step": 58920
    },
    {
      "epoch": 17.271395076201642,
      "grad_norm": 0.1866733580827713,
      "learning_rate": 0.000220970810370523,
      "loss": 0.0529,
      "step": 58930
    },
    {
      "epoch": 17.27432590855803,
      "grad_norm": 0.36560794711112976,
      "learning_rate": 0.00022088701408293465,
      "loss": 0.0651,
      "step": 58940
    },
    {
      "epoch": 17.27725674091442,
      "grad_norm": 1.125608205795288,
      "learning_rate": 0.00022080321779534629,
      "loss": 0.058,
      "step": 58950
    },
    {
      "epoch": 17.28018757327081,
      "grad_norm": 1.5229837894439697,
      "learning_rate": 0.00022071942150775792,
      "loss": 0.0585,
      "step": 58960
    },
    {
      "epoch": 17.2831184056272,
      "grad_norm": 0.7423944473266602,
      "learning_rate": 0.00022063562522016953,
      "loss": 0.0542,
      "step": 58970
    },
    {
      "epoch": 17.286049237983587,
      "grad_norm": 1.876478910446167,
      "learning_rate": 0.00022055182893258117,
      "loss": 0.0564,
      "step": 58980
    },
    {
      "epoch": 17.288980070339978,
      "grad_norm": 0.33294954895973206,
      "learning_rate": 0.0002204680326449928,
      "loss": 0.0594,
      "step": 58990
    },
    {
      "epoch": 17.291910902696365,
      "grad_norm": 1.3372911214828491,
      "learning_rate": 0.00022038423635740444,
      "loss": 0.0714,
      "step": 59000
    },
    {
      "epoch": 17.294841735052756,
      "grad_norm": 1.9778259992599487,
      "learning_rate": 0.00022030044006981608,
      "loss": 0.0415,
      "step": 59010
    },
    {
      "epoch": 17.297772567409144,
      "grad_norm": 0.9946535229682922,
      "learning_rate": 0.0002202166437822277,
      "loss": 0.0236,
      "step": 59020
    },
    {
      "epoch": 17.300703399765535,
      "grad_norm": 1.335259199142456,
      "learning_rate": 0.00022013284749463933,
      "loss": 0.043,
      "step": 59030
    },
    {
      "epoch": 17.303634232121922,
      "grad_norm": 1.2271332740783691,
      "learning_rate": 0.00022004905120705096,
      "loss": 0.0483,
      "step": 59040
    },
    {
      "epoch": 17.306565064478313,
      "grad_norm": 0.9489218592643738,
      "learning_rate": 0.0002199652549194626,
      "loss": 0.0544,
      "step": 59050
    },
    {
      "epoch": 17.3094958968347,
      "grad_norm": 0.7085642218589783,
      "learning_rate": 0.00021988145863187426,
      "loss": 0.0529,
      "step": 59060
    },
    {
      "epoch": 17.312426729191092,
      "grad_norm": 1.0330902338027954,
      "learning_rate": 0.0002197976623442859,
      "loss": 0.0588,
      "step": 59070
    },
    {
      "epoch": 17.31535756154748,
      "grad_norm": 0.574134349822998,
      "learning_rate": 0.00021971386605669748,
      "loss": 0.0508,
      "step": 59080
    },
    {
      "epoch": 17.31828839390387,
      "grad_norm": 1.1000627279281616,
      "learning_rate": 0.00021963006976910915,
      "loss": 0.0723,
      "step": 59090
    },
    {
      "epoch": 17.321219226260258,
      "grad_norm": 0.3702948987483978,
      "learning_rate": 0.00021954627348152078,
      "loss": 0.0495,
      "step": 59100
    },
    {
      "epoch": 17.32415005861665,
      "grad_norm": 0.07546132802963257,
      "learning_rate": 0.00021946247719393242,
      "loss": 0.0688,
      "step": 59110
    },
    {
      "epoch": 17.327080890973036,
      "grad_norm": 2.028167247772217,
      "learning_rate": 0.00021937868090634406,
      "loss": 0.0535,
      "step": 59120
    },
    {
      "epoch": 17.330011723329427,
      "grad_norm": 1.1343202590942383,
      "learning_rate": 0.00021929488461875567,
      "loss": 0.0752,
      "step": 59130
    },
    {
      "epoch": 17.332942555685815,
      "grad_norm": 0.3719528913497925,
      "learning_rate": 0.0002192110883311673,
      "loss": 0.0411,
      "step": 59140
    },
    {
      "epoch": 17.335873388042202,
      "grad_norm": 1.7121824026107788,
      "learning_rate": 0.00021912729204357894,
      "loss": 0.0553,
      "step": 59150
    },
    {
      "epoch": 17.338804220398593,
      "grad_norm": 1.1485633850097656,
      "learning_rate": 0.00021904349575599058,
      "loss": 0.0486,
      "step": 59160
    },
    {
      "epoch": 17.34173505275498,
      "grad_norm": 1.2844960689544678,
      "learning_rate": 0.00021895969946840221,
      "loss": 0.048,
      "step": 59170
    },
    {
      "epoch": 17.34466588511137,
      "grad_norm": 1.12116277217865,
      "learning_rate": 0.00021887590318081385,
      "loss": 0.0622,
      "step": 59180
    },
    {
      "epoch": 17.34759671746776,
      "grad_norm": 1.448482632637024,
      "learning_rate": 0.00021879210689322546,
      "loss": 0.0692,
      "step": 59190
    },
    {
      "epoch": 17.35052754982415,
      "grad_norm": 0.740959644317627,
      "learning_rate": 0.0002187083106056371,
      "loss": 0.04,
      "step": 59200
    },
    {
      "epoch": 17.353458382180538,
      "grad_norm": 0.8420868515968323,
      "learning_rate": 0.00021862451431804873,
      "loss": 0.0504,
      "step": 59210
    },
    {
      "epoch": 17.35638921453693,
      "grad_norm": 1.2498221397399902,
      "learning_rate": 0.00021854071803046037,
      "loss": 0.0352,
      "step": 59220
    },
    {
      "epoch": 17.359320046893316,
      "grad_norm": 1.4147627353668213,
      "learning_rate": 0.000218456921742872,
      "loss": 0.0585,
      "step": 59230
    },
    {
      "epoch": 17.362250879249707,
      "grad_norm": 0.624199390411377,
      "learning_rate": 0.00021837312545528367,
      "loss": 0.0477,
      "step": 59240
    },
    {
      "epoch": 17.365181711606095,
      "grad_norm": 0.566225528717041,
      "learning_rate": 0.00021828932916769525,
      "loss": 0.0621,
      "step": 59250
    },
    {
      "epoch": 17.368112543962486,
      "grad_norm": 0.5138099193572998,
      "learning_rate": 0.0002182055328801069,
      "loss": 0.049,
      "step": 59260
    },
    {
      "epoch": 17.371043376318873,
      "grad_norm": 0.4038636088371277,
      "learning_rate": 0.00021812173659251853,
      "loss": 0.0458,
      "step": 59270
    },
    {
      "epoch": 17.373974208675264,
      "grad_norm": 1.4954266548156738,
      "learning_rate": 0.0002180379403049302,
      "loss": 0.0503,
      "step": 59280
    },
    {
      "epoch": 17.37690504103165,
      "grad_norm": 0.9062033295631409,
      "learning_rate": 0.00021795414401734183,
      "loss": 0.0493,
      "step": 59290
    },
    {
      "epoch": 17.379835873388043,
      "grad_norm": 1.0447182655334473,
      "learning_rate": 0.0002178703477297534,
      "loss": 0.0427,
      "step": 59300
    },
    {
      "epoch": 17.38276670574443,
      "grad_norm": 1.0867230892181396,
      "learning_rate": 0.00021778655144216508,
      "loss": 0.0373,
      "step": 59310
    },
    {
      "epoch": 17.38569753810082,
      "grad_norm": 0.23367197811603546,
      "learning_rate": 0.0002177027551545767,
      "loss": 0.0652,
      "step": 59320
    },
    {
      "epoch": 17.38862837045721,
      "grad_norm": 2.3294193744659424,
      "learning_rate": 0.00021761895886698835,
      "loss": 0.0632,
      "step": 59330
    },
    {
      "epoch": 17.3915592028136,
      "grad_norm": 1.62235689163208,
      "learning_rate": 0.00021753516257939999,
      "loss": 0.0704,
      "step": 59340
    },
    {
      "epoch": 17.394490035169987,
      "grad_norm": 1.7306889295578003,
      "learning_rate": 0.00021745136629181162,
      "loss": 0.0408,
      "step": 59350
    },
    {
      "epoch": 17.397420867526378,
      "grad_norm": 1.749848484992981,
      "learning_rate": 0.00021736757000422323,
      "loss": 0.0578,
      "step": 59360
    },
    {
      "epoch": 17.400351699882766,
      "grad_norm": 0.8225094676017761,
      "learning_rate": 0.00021728377371663487,
      "loss": 0.0744,
      "step": 59370
    },
    {
      "epoch": 17.403282532239157,
      "grad_norm": 0.6672694683074951,
      "learning_rate": 0.0002171999774290465,
      "loss": 0.0457,
      "step": 59380
    },
    {
      "epoch": 17.406213364595544,
      "grad_norm": 1.5524523258209229,
      "learning_rate": 0.00021711618114145814,
      "loss": 0.0648,
      "step": 59390
    },
    {
      "epoch": 17.409144196951935,
      "grad_norm": 0.6862384080886841,
      "learning_rate": 0.00021703238485386978,
      "loss": 0.0573,
      "step": 59400
    },
    {
      "epoch": 17.412075029308323,
      "grad_norm": 0.9766805171966553,
      "learning_rate": 0.00021694858856628142,
      "loss": 0.0782,
      "step": 59410
    },
    {
      "epoch": 17.415005861664714,
      "grad_norm": 0.7036551237106323,
      "learning_rate": 0.00021686479227869303,
      "loss": 0.0503,
      "step": 59420
    },
    {
      "epoch": 17.4179366940211,
      "grad_norm": 0.6000773310661316,
      "learning_rate": 0.00021678099599110466,
      "loss": 0.0543,
      "step": 59430
    },
    {
      "epoch": 17.420867526377492,
      "grad_norm": 0.573943018913269,
      "learning_rate": 0.0002166971997035163,
      "loss": 0.0483,
      "step": 59440
    },
    {
      "epoch": 17.42379835873388,
      "grad_norm": 0.49846363067626953,
      "learning_rate": 0.00021661340341592794,
      "loss": 0.0531,
      "step": 59450
    },
    {
      "epoch": 17.42672919109027,
      "grad_norm": 1.0756499767303467,
      "learning_rate": 0.0002165296071283396,
      "loss": 0.0607,
      "step": 59460
    },
    {
      "epoch": 17.429660023446658,
      "grad_norm": 1.3895422220230103,
      "learning_rate": 0.00021644581084075118,
      "loss": 0.0691,
      "step": 59470
    },
    {
      "epoch": 17.43259085580305,
      "grad_norm": 0.8185135722160339,
      "learning_rate": 0.00021636201455316282,
      "loss": 0.045,
      "step": 59480
    },
    {
      "epoch": 17.435521688159437,
      "grad_norm": 0.6783867478370667,
      "learning_rate": 0.00021627821826557446,
      "loss": 0.0324,
      "step": 59490
    },
    {
      "epoch": 17.438452520515828,
      "grad_norm": 1.0857497453689575,
      "learning_rate": 0.00021619442197798612,
      "loss": 0.0587,
      "step": 59500
    },
    {
      "epoch": 17.441383352872215,
      "grad_norm": 0.1876550018787384,
      "learning_rate": 0.00021611062569039776,
      "loss": 0.0436,
      "step": 59510
    },
    {
      "epoch": 17.444314185228606,
      "grad_norm": 1.1975672245025635,
      "learning_rate": 0.0002160268294028094,
      "loss": 0.0518,
      "step": 59520
    },
    {
      "epoch": 17.447245017584994,
      "grad_norm": 0.6270818114280701,
      "learning_rate": 0.000215943033115221,
      "loss": 0.0495,
      "step": 59530
    },
    {
      "epoch": 17.450175849941385,
      "grad_norm": 0.8123618364334106,
      "learning_rate": 0.00021585923682763264,
      "loss": 0.0644,
      "step": 59540
    },
    {
      "epoch": 17.453106682297772,
      "grad_norm": 1.6692417860031128,
      "learning_rate": 0.00021577544054004428,
      "loss": 0.0528,
      "step": 59550
    },
    {
      "epoch": 17.456037514654163,
      "grad_norm": 1.0501055717468262,
      "learning_rate": 0.00021569164425245591,
      "loss": 0.06,
      "step": 59560
    },
    {
      "epoch": 17.45896834701055,
      "grad_norm": 0.5758420825004578,
      "learning_rate": 0.00021560784796486755,
      "loss": 0.0534,
      "step": 59570
    },
    {
      "epoch": 17.46189917936694,
      "grad_norm": 0.9254379868507385,
      "learning_rate": 0.0002155240516772792,
      "loss": 0.0533,
      "step": 59580
    },
    {
      "epoch": 17.46483001172333,
      "grad_norm": 1.8486922979354858,
      "learning_rate": 0.0002154402553896908,
      "loss": 0.0579,
      "step": 59590
    },
    {
      "epoch": 17.46776084407972,
      "grad_norm": 0.9549976587295532,
      "learning_rate": 0.00021535645910210243,
      "loss": 0.0753,
      "step": 59600
    },
    {
      "epoch": 17.470691676436108,
      "grad_norm": 1.5603355169296265,
      "learning_rate": 0.00021527266281451407,
      "loss": 0.0612,
      "step": 59610
    },
    {
      "epoch": 17.4736225087925,
      "grad_norm": 0.7758236527442932,
      "learning_rate": 0.0002151888665269257,
      "loss": 0.041,
      "step": 59620
    },
    {
      "epoch": 17.476553341148886,
      "grad_norm": 0.7614761590957642,
      "learning_rate": 0.00021510507023933734,
      "loss": 0.0377,
      "step": 59630
    },
    {
      "epoch": 17.479484173505277,
      "grad_norm": 0.4178480803966522,
      "learning_rate": 0.00021502127395174895,
      "loss": 0.0441,
      "step": 59640
    },
    {
      "epoch": 17.482415005861665,
      "grad_norm": 1.007217526435852,
      "learning_rate": 0.0002149374776641606,
      "loss": 0.0446,
      "step": 59650
    },
    {
      "epoch": 17.485345838218056,
      "grad_norm": 0.526931881904602,
      "learning_rate": 0.00021485368137657223,
      "loss": 0.0642,
      "step": 59660
    },
    {
      "epoch": 17.488276670574443,
      "grad_norm": 2.0482571125030518,
      "learning_rate": 0.00021476988508898386,
      "loss": 0.0472,
      "step": 59670
    },
    {
      "epoch": 17.491207502930834,
      "grad_norm": 0.6843859553337097,
      "learning_rate": 0.00021468608880139553,
      "loss": 0.0597,
      "step": 59680
    },
    {
      "epoch": 17.49413833528722,
      "grad_norm": 2.3286280632019043,
      "learning_rate": 0.00021460229251380717,
      "loss": 0.0673,
      "step": 59690
    },
    {
      "epoch": 17.497069167643613,
      "grad_norm": 3.0031416416168213,
      "learning_rate": 0.00021451849622621875,
      "loss": 0.0551,
      "step": 59700
    },
    {
      "epoch": 17.5,
      "grad_norm": 0.7674077153205872,
      "learning_rate": 0.0002144346999386304,
      "loss": 0.0526,
      "step": 59710
    },
    {
      "epoch": 17.50293083235639,
      "grad_norm": 2.603602409362793,
      "learning_rate": 0.00021435090365104205,
      "loss": 0.0382,
      "step": 59720
    },
    {
      "epoch": 17.50586166471278,
      "grad_norm": 0.8174610733985901,
      "learning_rate": 0.00021426710736345369,
      "loss": 0.0565,
      "step": 59730
    },
    {
      "epoch": 17.508792497069166,
      "grad_norm": 0.8568909168243408,
      "learning_rate": 0.00021418331107586532,
      "loss": 0.0575,
      "step": 59740
    },
    {
      "epoch": 17.511723329425557,
      "grad_norm": 0.7965527176856995,
      "learning_rate": 0.00021409951478827693,
      "loss": 0.0378,
      "step": 59750
    },
    {
      "epoch": 17.514654161781944,
      "grad_norm": 1.8130797147750854,
      "learning_rate": 0.00021401571850068857,
      "loss": 0.0737,
      "step": 59760
    },
    {
      "epoch": 17.517584994138335,
      "grad_norm": 2.0018064975738525,
      "learning_rate": 0.0002139319222131002,
      "loss": 0.0844,
      "step": 59770
    },
    {
      "epoch": 17.520515826494723,
      "grad_norm": 0.3162807524204254,
      "learning_rate": 0.00021384812592551184,
      "loss": 0.0466,
      "step": 59780
    },
    {
      "epoch": 17.523446658851114,
      "grad_norm": 0.22331678867340088,
      "learning_rate": 0.00021376432963792348,
      "loss": 0.0337,
      "step": 59790
    },
    {
      "epoch": 17.5263774912075,
      "grad_norm": 0.8744692802429199,
      "learning_rate": 0.00021368053335033512,
      "loss": 0.0559,
      "step": 59800
    },
    {
      "epoch": 17.529308323563892,
      "grad_norm": 1.384211778640747,
      "learning_rate": 0.00021359673706274673,
      "loss": 0.0783,
      "step": 59810
    },
    {
      "epoch": 17.53223915592028,
      "grad_norm": 1.0445691347122192,
      "learning_rate": 0.00021351294077515836,
      "loss": 0.0483,
      "step": 59820
    },
    {
      "epoch": 17.53516998827667,
      "grad_norm": 0.19921748340129852,
      "learning_rate": 0.00021342914448757,
      "loss": 0.0469,
      "step": 59830
    },
    {
      "epoch": 17.53810082063306,
      "grad_norm": 1.5273845195770264,
      "learning_rate": 0.00021334534819998164,
      "loss": 0.0665,
      "step": 59840
    },
    {
      "epoch": 17.54103165298945,
      "grad_norm": 1.1532139778137207,
      "learning_rate": 0.00021326155191239327,
      "loss": 0.0544,
      "step": 59850
    },
    {
      "epoch": 17.543962485345837,
      "grad_norm": 1.2870107889175415,
      "learning_rate": 0.00021317775562480494,
      "loss": 0.0386,
      "step": 59860
    },
    {
      "epoch": 17.546893317702228,
      "grad_norm": 0.2517838180065155,
      "learning_rate": 0.00021309395933721652,
      "loss": 0.0296,
      "step": 59870
    },
    {
      "epoch": 17.549824150058615,
      "grad_norm": 1.386230230331421,
      "learning_rate": 0.00021301016304962816,
      "loss": 0.0603,
      "step": 59880
    },
    {
      "epoch": 17.552754982415006,
      "grad_norm": 1.1901508569717407,
      "learning_rate": 0.0002129263667620398,
      "loss": 0.0634,
      "step": 59890
    },
    {
      "epoch": 17.555685814771394,
      "grad_norm": 0.2644878029823303,
      "learning_rate": 0.00021284257047445146,
      "loss": 0.0473,
      "step": 59900
    },
    {
      "epoch": 17.558616647127785,
      "grad_norm": 1.4019144773483276,
      "learning_rate": 0.0002127587741868631,
      "loss": 0.0693,
      "step": 59910
    },
    {
      "epoch": 17.561547479484172,
      "grad_norm": 0.5772209763526917,
      "learning_rate": 0.00021267497789927468,
      "loss": 0.0469,
      "step": 59920
    },
    {
      "epoch": 17.564478311840563,
      "grad_norm": 0.6870613098144531,
      "learning_rate": 0.00021259118161168634,
      "loss": 0.0482,
      "step": 59930
    },
    {
      "epoch": 17.56740914419695,
      "grad_norm": 2.198920488357544,
      "learning_rate": 0.00021250738532409798,
      "loss": 0.0768,
      "step": 59940
    },
    {
      "epoch": 17.570339976553342,
      "grad_norm": 0.46380263566970825,
      "learning_rate": 0.00021242358903650961,
      "loss": 0.0506,
      "step": 59950
    },
    {
      "epoch": 17.57327080890973,
      "grad_norm": 0.503737211227417,
      "learning_rate": 0.00021233979274892125,
      "loss": 0.0552,
      "step": 59960
    },
    {
      "epoch": 17.57620164126612,
      "grad_norm": 0.25080832839012146,
      "learning_rate": 0.0002122559964613329,
      "loss": 0.0453,
      "step": 59970
    },
    {
      "epoch": 17.579132473622508,
      "grad_norm": 0.6648755669593811,
      "learning_rate": 0.0002121722001737445,
      "loss": 0.0443,
      "step": 59980
    },
    {
      "epoch": 17.5820633059789,
      "grad_norm": 1.8289742469787598,
      "learning_rate": 0.00021208840388615613,
      "loss": 0.061,
      "step": 59990
    },
    {
      "epoch": 17.584994138335286,
      "grad_norm": 0.7446855902671814,
      "learning_rate": 0.00021200460759856777,
      "loss": 0.042,
      "step": 60000
    },
    {
      "epoch": 17.587924970691677,
      "grad_norm": 0.9781436920166016,
      "learning_rate": 0.0002119208113109794,
      "loss": 0.0427,
      "step": 60010
    },
    {
      "epoch": 17.590855803048065,
      "grad_norm": 0.5846877098083496,
      "learning_rate": 0.00021183701502339104,
      "loss": 0.0482,
      "step": 60020
    },
    {
      "epoch": 17.593786635404456,
      "grad_norm": 1.1065077781677246,
      "learning_rate": 0.00021175321873580268,
      "loss": 0.044,
      "step": 60030
    },
    {
      "epoch": 17.596717467760843,
      "grad_norm": 0.48680219054222107,
      "learning_rate": 0.0002116694224482143,
      "loss": 0.0751,
      "step": 60040
    },
    {
      "epoch": 17.599648300117234,
      "grad_norm": 0.9502524733543396,
      "learning_rate": 0.00021158562616062593,
      "loss": 0.0616,
      "step": 60050
    },
    {
      "epoch": 17.602579132473622,
      "grad_norm": 0.33980730175971985,
      "learning_rate": 0.00021150182987303756,
      "loss": 0.0426,
      "step": 60060
    },
    {
      "epoch": 17.605509964830013,
      "grad_norm": 0.528041660785675,
      "learning_rate": 0.0002114180335854492,
      "loss": 0.0805,
      "step": 60070
    },
    {
      "epoch": 17.6084407971864,
      "grad_norm": 1.2728919982910156,
      "learning_rate": 0.00021133423729786087,
      "loss": 0.0602,
      "step": 60080
    },
    {
      "epoch": 17.61137162954279,
      "grad_norm": 0.5867295265197754,
      "learning_rate": 0.00021125044101027245,
      "loss": 0.0421,
      "step": 60090
    },
    {
      "epoch": 17.61430246189918,
      "grad_norm": 0.23126353323459625,
      "learning_rate": 0.00021116664472268408,
      "loss": 0.0389,
      "step": 60100
    },
    {
      "epoch": 17.61723329425557,
      "grad_norm": 1.0078195333480835,
      "learning_rate": 0.00021108284843509572,
      "loss": 0.0457,
      "step": 60110
    },
    {
      "epoch": 17.620164126611957,
      "grad_norm": 0.29640981554985046,
      "learning_rate": 0.00021099905214750739,
      "loss": 0.0332,
      "step": 60120
    },
    {
      "epoch": 17.62309495896835,
      "grad_norm": 1.1899049282073975,
      "learning_rate": 0.00021091525585991902,
      "loss": 0.0792,
      "step": 60130
    },
    {
      "epoch": 17.626025791324736,
      "grad_norm": 1.565504789352417,
      "learning_rate": 0.00021083145957233066,
      "loss": 0.0782,
      "step": 60140
    },
    {
      "epoch": 17.628956623681127,
      "grad_norm": 1.5125164985656738,
      "learning_rate": 0.00021074766328474227,
      "loss": 0.0503,
      "step": 60150
    },
    {
      "epoch": 17.631887456037514,
      "grad_norm": 1.4682586193084717,
      "learning_rate": 0.0002106638669971539,
      "loss": 0.0723,
      "step": 60160
    },
    {
      "epoch": 17.634818288393905,
      "grad_norm": 1.8721755743026733,
      "learning_rate": 0.00021058007070956554,
      "loss": 0.0389,
      "step": 60170
    },
    {
      "epoch": 17.637749120750293,
      "grad_norm": 0.6789193153381348,
      "learning_rate": 0.00021049627442197718,
      "loss": 0.0393,
      "step": 60180
    },
    {
      "epoch": 17.640679953106684,
      "grad_norm": 0.8951444029808044,
      "learning_rate": 0.00021041247813438882,
      "loss": 0.0471,
      "step": 60190
    },
    {
      "epoch": 17.64361078546307,
      "grad_norm": 1.6102869510650635,
      "learning_rate": 0.00021032868184680045,
      "loss": 0.0486,
      "step": 60200
    },
    {
      "epoch": 17.646541617819462,
      "grad_norm": 0.526042103767395,
      "learning_rate": 0.00021024488555921206,
      "loss": 0.0695,
      "step": 60210
    },
    {
      "epoch": 17.64947245017585,
      "grad_norm": 0.7463669776916504,
      "learning_rate": 0.0002101610892716237,
      "loss": 0.0723,
      "step": 60220
    },
    {
      "epoch": 17.65240328253224,
      "grad_norm": 0.6689584255218506,
      "learning_rate": 0.00021007729298403534,
      "loss": 0.0616,
      "step": 60230
    },
    {
      "epoch": 17.65533411488863,
      "grad_norm": 0.5595630407333374,
      "learning_rate": 0.00020999349669644697,
      "loss": 0.0613,
      "step": 60240
    },
    {
      "epoch": 17.65826494724502,
      "grad_norm": 0.8393499255180359,
      "learning_rate": 0.0002099097004088586,
      "loss": 0.077,
      "step": 60250
    },
    {
      "epoch": 17.661195779601407,
      "grad_norm": 0.2436235547065735,
      "learning_rate": 0.00020982590412127022,
      "loss": 0.0442,
      "step": 60260
    },
    {
      "epoch": 17.664126611957798,
      "grad_norm": 0.5108157396316528,
      "learning_rate": 0.00020974210783368186,
      "loss": 0.0551,
      "step": 60270
    },
    {
      "epoch": 17.667057444314185,
      "grad_norm": 1.3590761423110962,
      "learning_rate": 0.0002096583115460935,
      "loss": 0.0443,
      "step": 60280
    },
    {
      "epoch": 17.669988276670573,
      "grad_norm": 0.34591931104660034,
      "learning_rate": 0.00020957451525850513,
      "loss": 0.0372,
      "step": 60290
    },
    {
      "epoch": 17.672919109026964,
      "grad_norm": 0.7371742129325867,
      "learning_rate": 0.0002094907189709168,
      "loss": 0.0425,
      "step": 60300
    },
    {
      "epoch": 17.67584994138335,
      "grad_norm": 1.6628522872924805,
      "learning_rate": 0.00020940692268332843,
      "loss": 0.0447,
      "step": 60310
    },
    {
      "epoch": 17.678780773739742,
      "grad_norm": 0.16032478213310242,
      "learning_rate": 0.00020932312639574,
      "loss": 0.0517,
      "step": 60320
    },
    {
      "epoch": 17.68171160609613,
      "grad_norm": 1.1542251110076904,
      "learning_rate": 0.00020923933010815165,
      "loss": 0.0688,
      "step": 60330
    },
    {
      "epoch": 17.68464243845252,
      "grad_norm": 2.283761739730835,
      "learning_rate": 0.00020915553382056331,
      "loss": 0.0746,
      "step": 60340
    },
    {
      "epoch": 17.687573270808908,
      "grad_norm": 1.161214828491211,
      "learning_rate": 0.00020907173753297495,
      "loss": 0.0634,
      "step": 60350
    },
    {
      "epoch": 17.6905041031653,
      "grad_norm": 0.42153677344322205,
      "learning_rate": 0.0002089879412453866,
      "loss": 0.069,
      "step": 60360
    },
    {
      "epoch": 17.693434935521687,
      "grad_norm": 0.49165207147598267,
      "learning_rate": 0.0002089041449577982,
      "loss": 0.0589,
      "step": 60370
    },
    {
      "epoch": 17.696365767878078,
      "grad_norm": 0.5789960026741028,
      "learning_rate": 0.00020882034867020983,
      "loss": 0.0706,
      "step": 60380
    },
    {
      "epoch": 17.699296600234465,
      "grad_norm": 1.2344220876693726,
      "learning_rate": 0.00020873655238262147,
      "loss": 0.0561,
      "step": 60390
    },
    {
      "epoch": 17.702227432590856,
      "grad_norm": 2.1200625896453857,
      "learning_rate": 0.0002086527560950331,
      "loss": 0.0676,
      "step": 60400
    },
    {
      "epoch": 17.705158264947244,
      "grad_norm": 0.2788316011428833,
      "learning_rate": 0.00020856895980744474,
      "loss": 0.028,
      "step": 60410
    },
    {
      "epoch": 17.708089097303635,
      "grad_norm": 0.747273862361908,
      "learning_rate": 0.00020848516351985638,
      "loss": 0.0369,
      "step": 60420
    },
    {
      "epoch": 17.711019929660022,
      "grad_norm": 1.069566249847412,
      "learning_rate": 0.000208401367232268,
      "loss": 0.0471,
      "step": 60430
    },
    {
      "epoch": 17.713950762016413,
      "grad_norm": 0.3381005823612213,
      "learning_rate": 0.00020831757094467963,
      "loss": 0.0778,
      "step": 60440
    },
    {
      "epoch": 17.7168815943728,
      "grad_norm": 0.49224549531936646,
      "learning_rate": 0.00020823377465709126,
      "loss": 0.0519,
      "step": 60450
    },
    {
      "epoch": 17.71981242672919,
      "grad_norm": 3.855289936065674,
      "learning_rate": 0.0002081499783695029,
      "loss": 0.0625,
      "step": 60460
    },
    {
      "epoch": 17.72274325908558,
      "grad_norm": 1.1889835596084595,
      "learning_rate": 0.00020806618208191454,
      "loss": 0.0432,
      "step": 60470
    },
    {
      "epoch": 17.72567409144197,
      "grad_norm": 0.8882027268409729,
      "learning_rate": 0.00020798238579432617,
      "loss": 0.0454,
      "step": 60480
    },
    {
      "epoch": 17.728604923798358,
      "grad_norm": 1.4718210697174072,
      "learning_rate": 0.00020789858950673778,
      "loss": 0.0659,
      "step": 60490
    },
    {
      "epoch": 17.73153575615475,
      "grad_norm": 1.1339911222457886,
      "learning_rate": 0.00020781479321914942,
      "loss": 0.0498,
      "step": 60500
    },
    {
      "epoch": 17.734466588511136,
      "grad_norm": 0.31991511583328247,
      "learning_rate": 0.00020773099693156106,
      "loss": 0.0541,
      "step": 60510
    },
    {
      "epoch": 17.737397420867527,
      "grad_norm": 0.4405389428138733,
      "learning_rate": 0.00020764720064397272,
      "loss": 0.0519,
      "step": 60520
    },
    {
      "epoch": 17.740328253223915,
      "grad_norm": 0.33783790469169617,
      "learning_rate": 0.00020756340435638436,
      "loss": 0.0556,
      "step": 60530
    },
    {
      "epoch": 17.743259085580306,
      "grad_norm": 1.2058594226837158,
      "learning_rate": 0.00020747960806879594,
      "loss": 0.0524,
      "step": 60540
    },
    {
      "epoch": 17.746189917936693,
      "grad_norm": 1.7402100563049316,
      "learning_rate": 0.00020739581178120758,
      "loss": 0.0701,
      "step": 60550
    },
    {
      "epoch": 17.749120750293084,
      "grad_norm": 0.982140302658081,
      "learning_rate": 0.00020731201549361924,
      "loss": 0.0295,
      "step": 60560
    },
    {
      "epoch": 17.75205158264947,
      "grad_norm": 0.7341508865356445,
      "learning_rate": 0.00020722821920603088,
      "loss": 0.0363,
      "step": 60570
    },
    {
      "epoch": 17.754982415005863,
      "grad_norm": 1.4491153955459595,
      "learning_rate": 0.00020714442291844252,
      "loss": 0.0549,
      "step": 60580
    },
    {
      "epoch": 17.75791324736225,
      "grad_norm": 0.7665767073631287,
      "learning_rate": 0.00020706062663085415,
      "loss": 0.0383,
      "step": 60590
    },
    {
      "epoch": 17.76084407971864,
      "grad_norm": 0.1484367400407791,
      "learning_rate": 0.00020697683034326576,
      "loss": 0.056,
      "step": 60600
    },
    {
      "epoch": 17.76377491207503,
      "grad_norm": 0.46695807576179504,
      "learning_rate": 0.0002068930340556774,
      "loss": 0.0482,
      "step": 60610
    },
    {
      "epoch": 17.76670574443142,
      "grad_norm": 0.5553112030029297,
      "learning_rate": 0.00020680923776808904,
      "loss": 0.0729,
      "step": 60620
    },
    {
      "epoch": 17.769636576787807,
      "grad_norm": 0.4801768362522125,
      "learning_rate": 0.00020672544148050067,
      "loss": 0.0425,
      "step": 60630
    },
    {
      "epoch": 17.772567409144198,
      "grad_norm": 1.0891788005828857,
      "learning_rate": 0.0002066416451929123,
      "loss": 0.0466,
      "step": 60640
    },
    {
      "epoch": 17.775498241500586,
      "grad_norm": 0.7777740359306335,
      "learning_rate": 0.00020655784890532395,
      "loss": 0.044,
      "step": 60650
    },
    {
      "epoch": 17.778429073856977,
      "grad_norm": 1.3648422956466675,
      "learning_rate": 0.00020647405261773556,
      "loss": 0.0537,
      "step": 60660
    },
    {
      "epoch": 17.781359906213364,
      "grad_norm": 1.0624477863311768,
      "learning_rate": 0.0002063902563301472,
      "loss": 0.0718,
      "step": 60670
    },
    {
      "epoch": 17.784290738569755,
      "grad_norm": 1.6715443134307861,
      "learning_rate": 0.00020630646004255883,
      "loss": 0.0494,
      "step": 60680
    },
    {
      "epoch": 17.787221570926143,
      "grad_norm": 1.065270185470581,
      "learning_rate": 0.00020622266375497047,
      "loss": 0.0629,
      "step": 60690
    },
    {
      "epoch": 17.790152403282534,
      "grad_norm": 0.9940627217292786,
      "learning_rate": 0.00020613886746738213,
      "loss": 0.0348,
      "step": 60700
    },
    {
      "epoch": 17.79308323563892,
      "grad_norm": 0.8751676678657532,
      "learning_rate": 0.0002060550711797937,
      "loss": 0.0476,
      "step": 60710
    },
    {
      "epoch": 17.796014067995312,
      "grad_norm": 0.23305365443229675,
      "learning_rate": 0.00020597127489220535,
      "loss": 0.0581,
      "step": 60720
    },
    {
      "epoch": 17.7989449003517,
      "grad_norm": 1.4458417892456055,
      "learning_rate": 0.000205887478604617,
      "loss": 0.0411,
      "step": 60730
    },
    {
      "epoch": 17.80187573270809,
      "grad_norm": 0.9581611752510071,
      "learning_rate": 0.00020580368231702865,
      "loss": 0.0529,
      "step": 60740
    },
    {
      "epoch": 17.804806565064478,
      "grad_norm": 1.3280467987060547,
      "learning_rate": 0.0002057198860294403,
      "loss": 0.0522,
      "step": 60750
    },
    {
      "epoch": 17.80773739742087,
      "grad_norm": 1.5675736665725708,
      "learning_rate": 0.00020563608974185192,
      "loss": 0.0676,
      "step": 60760
    },
    {
      "epoch": 17.810668229777256,
      "grad_norm": 1.4026628732681274,
      "learning_rate": 0.0002055522934542635,
      "loss": 0.032,
      "step": 60770
    },
    {
      "epoch": 17.813599062133648,
      "grad_norm": 0.6036444902420044,
      "learning_rate": 0.00020546849716667517,
      "loss": 0.0631,
      "step": 60780
    },
    {
      "epoch": 17.816529894490035,
      "grad_norm": 0.30631935596466064,
      "learning_rate": 0.0002053847008790868,
      "loss": 0.0524,
      "step": 60790
    },
    {
      "epoch": 17.819460726846426,
      "grad_norm": 0.9226969480514526,
      "learning_rate": 0.00020530090459149844,
      "loss": 0.0582,
      "step": 60800
    },
    {
      "epoch": 17.822391559202813,
      "grad_norm": 1.077572226524353,
      "learning_rate": 0.00020521710830391008,
      "loss": 0.0649,
      "step": 60810
    },
    {
      "epoch": 17.825322391559205,
      "grad_norm": 1.0914140939712524,
      "learning_rate": 0.0002051333120163217,
      "loss": 0.0527,
      "step": 60820
    },
    {
      "epoch": 17.828253223915592,
      "grad_norm": 0.5870574712753296,
      "learning_rate": 0.00020504951572873333,
      "loss": 0.0505,
      "step": 60830
    },
    {
      "epoch": 17.83118405627198,
      "grad_norm": 0.8364757895469666,
      "learning_rate": 0.00020496571944114496,
      "loss": 0.0673,
      "step": 60840
    },
    {
      "epoch": 17.83411488862837,
      "grad_norm": 0.14256808161735535,
      "learning_rate": 0.0002048819231535566,
      "loss": 0.0495,
      "step": 60850
    },
    {
      "epoch": 17.83704572098476,
      "grad_norm": 0.7082804441452026,
      "learning_rate": 0.00020479812686596824,
      "loss": 0.0506,
      "step": 60860
    },
    {
      "epoch": 17.83997655334115,
      "grad_norm": 0.6993632912635803,
      "learning_rate": 0.00020471433057837987,
      "loss": 0.0643,
      "step": 60870
    },
    {
      "epoch": 17.842907385697536,
      "grad_norm": 0.8196465373039246,
      "learning_rate": 0.00020463053429079148,
      "loss": 0.0622,
      "step": 60880
    },
    {
      "epoch": 17.845838218053927,
      "grad_norm": 0.994672417640686,
      "learning_rate": 0.00020454673800320312,
      "loss": 0.0475,
      "step": 60890
    },
    {
      "epoch": 17.848769050410315,
      "grad_norm": 2.427987813949585,
      "learning_rate": 0.00020446294171561476,
      "loss": 0.0829,
      "step": 60900
    },
    {
      "epoch": 17.851699882766706,
      "grad_norm": 0.6488357782363892,
      "learning_rate": 0.0002043791454280264,
      "loss": 0.0379,
      "step": 60910
    },
    {
      "epoch": 17.854630715123093,
      "grad_norm": 1.4906442165374756,
      "learning_rate": 0.00020429534914043806,
      "loss": 0.0754,
      "step": 60920
    },
    {
      "epoch": 17.857561547479484,
      "grad_norm": 0.8335126042366028,
      "learning_rate": 0.0002042115528528497,
      "loss": 0.0542,
      "step": 60930
    },
    {
      "epoch": 17.860492379835872,
      "grad_norm": 1.4346656799316406,
      "learning_rate": 0.00020412775656526128,
      "loss": 0.0487,
      "step": 60940
    },
    {
      "epoch": 17.863423212192263,
      "grad_norm": 0.43300706148147583,
      "learning_rate": 0.00020404396027767291,
      "loss": 0.0522,
      "step": 60950
    },
    {
      "epoch": 17.86635404454865,
      "grad_norm": 0.8035789728164673,
      "learning_rate": 0.00020396016399008458,
      "loss": 0.0691,
      "step": 60960
    },
    {
      "epoch": 17.86928487690504,
      "grad_norm": 1.1525342464447021,
      "learning_rate": 0.00020387636770249622,
      "loss": 0.0654,
      "step": 60970
    },
    {
      "epoch": 17.87221570926143,
      "grad_norm": 0.8367418646812439,
      "learning_rate": 0.00020379257141490785,
      "loss": 0.051,
      "step": 60980
    },
    {
      "epoch": 17.87514654161782,
      "grad_norm": 1.4110296964645386,
      "learning_rate": 0.00020370877512731944,
      "loss": 0.0431,
      "step": 60990
    },
    {
      "epoch": 17.878077373974207,
      "grad_norm": 0.7561894655227661,
      "learning_rate": 0.0002036249788397311,
      "loss": 0.0559,
      "step": 61000
    },
    {
      "epoch": 17.8810082063306,
      "grad_norm": 0.7365705370903015,
      "learning_rate": 0.00020354118255214274,
      "loss": 0.0541,
      "step": 61010
    },
    {
      "epoch": 17.883939038686986,
      "grad_norm": 0.580847442150116,
      "learning_rate": 0.00020345738626455437,
      "loss": 0.0508,
      "step": 61020
    },
    {
      "epoch": 17.886869871043377,
      "grad_norm": 1.1515295505523682,
      "learning_rate": 0.000203373589976966,
      "loss": 0.039,
      "step": 61030
    },
    {
      "epoch": 17.889800703399764,
      "grad_norm": 1.4056159257888794,
      "learning_rate": 0.00020328979368937765,
      "loss": 0.0362,
      "step": 61040
    },
    {
      "epoch": 17.892731535756155,
      "grad_norm": 1.367558240890503,
      "learning_rate": 0.00020320599740178926,
      "loss": 0.0569,
      "step": 61050
    },
    {
      "epoch": 17.895662368112543,
      "grad_norm": 0.4464152753353119,
      "learning_rate": 0.0002031222011142009,
      "loss": 0.0488,
      "step": 61060
    },
    {
      "epoch": 17.898593200468934,
      "grad_norm": 1.3979895114898682,
      "learning_rate": 0.00020303840482661253,
      "loss": 0.0579,
      "step": 61070
    },
    {
      "epoch": 17.90152403282532,
      "grad_norm": 1.3694446086883545,
      "learning_rate": 0.00020295460853902417,
      "loss": 0.0508,
      "step": 61080
    },
    {
      "epoch": 17.904454865181712,
      "grad_norm": 0.20092031359672546,
      "learning_rate": 0.0002028708122514358,
      "loss": 0.0529,
      "step": 61090
    },
    {
      "epoch": 17.9073856975381,
      "grad_norm": 1.355653166770935,
      "learning_rate": 0.00020278701596384744,
      "loss": 0.0408,
      "step": 61100
    },
    {
      "epoch": 17.91031652989449,
      "grad_norm": 0.7137035727500916,
      "learning_rate": 0.00020270321967625905,
      "loss": 0.0374,
      "step": 61110
    },
    {
      "epoch": 17.91324736225088,
      "grad_norm": 0.25800350308418274,
      "learning_rate": 0.0002026194233886707,
      "loss": 0.0454,
      "step": 61120
    },
    {
      "epoch": 17.91617819460727,
      "grad_norm": 1.420007348060608,
      "learning_rate": 0.00020253562710108232,
      "loss": 0.0501,
      "step": 61130
    },
    {
      "epoch": 17.919109026963657,
      "grad_norm": 0.8717094659805298,
      "learning_rate": 0.000202451830813494,
      "loss": 0.049,
      "step": 61140
    },
    {
      "epoch": 17.922039859320048,
      "grad_norm": 1.1131608486175537,
      "learning_rate": 0.00020236803452590562,
      "loss": 0.0364,
      "step": 61150
    },
    {
      "epoch": 17.924970691676435,
      "grad_norm": 2.1513726711273193,
      "learning_rate": 0.0002022842382383172,
      "loss": 0.0581,
      "step": 61160
    },
    {
      "epoch": 17.927901524032826,
      "grad_norm": 0.6892092227935791,
      "learning_rate": 0.00020220044195072884,
      "loss": 0.0483,
      "step": 61170
    },
    {
      "epoch": 17.930832356389214,
      "grad_norm": 1.9894953966140747,
      "learning_rate": 0.0002021166456631405,
      "loss": 0.0521,
      "step": 61180
    },
    {
      "epoch": 17.933763188745605,
      "grad_norm": 0.6120898127555847,
      "learning_rate": 0.00020203284937555214,
      "loss": 0.0306,
      "step": 61190
    },
    {
      "epoch": 17.936694021101992,
      "grad_norm": 2.282115936279297,
      "learning_rate": 0.00020194905308796378,
      "loss": 0.045,
      "step": 61200
    },
    {
      "epoch": 17.939624853458383,
      "grad_norm": 0.7848424911499023,
      "learning_rate": 0.00020186525680037542,
      "loss": 0.0463,
      "step": 61210
    },
    {
      "epoch": 17.94255568581477,
      "grad_norm": 0.5171106457710266,
      "learning_rate": 0.00020178146051278703,
      "loss": 0.0456,
      "step": 61220
    },
    {
      "epoch": 17.945486518171162,
      "grad_norm": 1.228361964225769,
      "learning_rate": 0.00020169766422519866,
      "loss": 0.0752,
      "step": 61230
    },
    {
      "epoch": 17.94841735052755,
      "grad_norm": 0.13130252063274384,
      "learning_rate": 0.0002016138679376103,
      "loss": 0.0377,
      "step": 61240
    },
    {
      "epoch": 17.95134818288394,
      "grad_norm": 0.7648493051528931,
      "learning_rate": 0.00020153007165002194,
      "loss": 0.0793,
      "step": 61250
    },
    {
      "epoch": 17.954279015240328,
      "grad_norm": 0.8689357042312622,
      "learning_rate": 0.00020144627536243357,
      "loss": 0.0476,
      "step": 61260
    },
    {
      "epoch": 17.95720984759672,
      "grad_norm": 0.9509373903274536,
      "learning_rate": 0.0002013624790748452,
      "loss": 0.0428,
      "step": 61270
    },
    {
      "epoch": 17.960140679953106,
      "grad_norm": 0.6852840781211853,
      "learning_rate": 0.00020127868278725682,
      "loss": 0.0513,
      "step": 61280
    },
    {
      "epoch": 17.963071512309497,
      "grad_norm": 0.521416962146759,
      "learning_rate": 0.00020119488649966846,
      "loss": 0.0336,
      "step": 61290
    },
    {
      "epoch": 17.966002344665885,
      "grad_norm": 0.8087714314460754,
      "learning_rate": 0.0002011110902120801,
      "loss": 0.0678,
      "step": 61300
    },
    {
      "epoch": 17.968933177022276,
      "grad_norm": 2.008599281311035,
      "learning_rate": 0.00020102729392449173,
      "loss": 0.0721,
      "step": 61310
    },
    {
      "epoch": 17.971864009378663,
      "grad_norm": 0.680380642414093,
      "learning_rate": 0.00020094349763690337,
      "loss": 0.0761,
      "step": 61320
    },
    {
      "epoch": 17.974794841735054,
      "grad_norm": 1.1521176099777222,
      "learning_rate": 0.00020085970134931498,
      "loss": 0.0456,
      "step": 61330
    },
    {
      "epoch": 17.97772567409144,
      "grad_norm": 1.8094428777694702,
      "learning_rate": 0.00020077590506172661,
      "loss": 0.0423,
      "step": 61340
    },
    {
      "epoch": 17.980656506447833,
      "grad_norm": 1.7394471168518066,
      "learning_rate": 0.00020069210877413825,
      "loss": 0.0543,
      "step": 61350
    },
    {
      "epoch": 17.98358733880422,
      "grad_norm": 0.22870813310146332,
      "learning_rate": 0.00020060831248654992,
      "loss": 0.0569,
      "step": 61360
    },
    {
      "epoch": 17.98651817116061,
      "grad_norm": 1.100701928138733,
      "learning_rate": 0.00020052451619896155,
      "loss": 0.0556,
      "step": 61370
    },
    {
      "epoch": 17.989449003517,
      "grad_norm": 0.9927546381950378,
      "learning_rate": 0.0002004407199113732,
      "loss": 0.0379,
      "step": 61380
    },
    {
      "epoch": 17.99237983587339,
      "grad_norm": 0.8282303810119629,
      "learning_rate": 0.00020035692362378477,
      "loss": 0.0709,
      "step": 61390
    },
    {
      "epoch": 17.995310668229777,
      "grad_norm": 0.8343839645385742,
      "learning_rate": 0.00020027312733619644,
      "loss": 0.0601,
      "step": 61400
    },
    {
      "epoch": 17.99824150058617,
      "grad_norm": 1.1852800846099854,
      "learning_rate": 0.00020018933104860807,
      "loss": 0.0556,
      "step": 61410
    },
    {
      "epoch": 18.0,
      "eval_accuracy": 0.7800857237059018,
      "eval_f1_macro": 0.8307201262050361,
      "eval_f1_micro": 0.8494688922610015,
      "eval_f1_weighted": 0.8472215815226406,
      "eval_loss": 0.070367731153965,
      "eval_roc_auc": 0.9036057501018321,
      "eval_runtime": 142.0094,
      "eval_samples_per_second": 21.358,
      "eval_steps_per_second": 2.676,
      "step": 61416
    },
    {
      "epoch": 18.001172332942556,
      "grad_norm": 2.047377824783325,
      "learning_rate": 0.0002001055347610197,
      "loss": 0.0342,
      "step": 61420
    },
    {
      "epoch": 18.004103165298943,
      "grad_norm": 0.9291992783546448,
      "learning_rate": 0.00020002173847343135,
      "loss": 0.0747,
      "step": 61430
    },
    {
      "epoch": 18.007033997655334,
      "grad_norm": 0.11767890304327011,
      "learning_rate": 0.00019993794218584296,
      "loss": 0.0342,
      "step": 61440
    },
    {
      "epoch": 18.00996483001172,
      "grad_norm": 0.8840166330337524,
      "learning_rate": 0.0001998541458982546,
      "loss": 0.0372,
      "step": 61450
    },
    {
      "epoch": 18.012895662368113,
      "grad_norm": 1.0048056840896606,
      "learning_rate": 0.00019977034961066623,
      "loss": 0.0584,
      "step": 61460
    },
    {
      "epoch": 18.0158264947245,
      "grad_norm": 0.34577879309654236,
      "learning_rate": 0.00019968655332307787,
      "loss": 0.0504,
      "step": 61470
    },
    {
      "epoch": 18.01875732708089,
      "grad_norm": 0.5491781234741211,
      "learning_rate": 0.0001996027570354895,
      "loss": 0.0376,
      "step": 61480
    },
    {
      "epoch": 18.02168815943728,
      "grad_norm": 0.1920730471611023,
      "learning_rate": 0.00019951896074790114,
      "loss": 0.0402,
      "step": 61490
    },
    {
      "epoch": 18.02461899179367,
      "grad_norm": 0.07703649997711182,
      "learning_rate": 0.00019943516446031275,
      "loss": 0.0528,
      "step": 61500
    },
    {
      "epoch": 18.027549824150057,
      "grad_norm": 1.1307713985443115,
      "learning_rate": 0.00019935136817272439,
      "loss": 0.0402,
      "step": 61510
    },
    {
      "epoch": 18.030480656506448,
      "grad_norm": 0.6720020174980164,
      "learning_rate": 0.00019926757188513602,
      "loss": 0.0518,
      "step": 61520
    },
    {
      "epoch": 18.033411488862836,
      "grad_norm": 0.6382586359977722,
      "learning_rate": 0.00019918377559754766,
      "loss": 0.0395,
      "step": 61530
    },
    {
      "epoch": 18.036342321219227,
      "grad_norm": 0.30430132150650024,
      "learning_rate": 0.0001990999793099593,
      "loss": 0.0299,
      "step": 61540
    },
    {
      "epoch": 18.039273153575614,
      "grad_norm": 0.7680038809776306,
      "learning_rate": 0.00019901618302237096,
      "loss": 0.0375,
      "step": 61550
    },
    {
      "epoch": 18.042203985932005,
      "grad_norm": 0.7704225182533264,
      "learning_rate": 0.00019893238673478254,
      "loss": 0.0352,
      "step": 61560
    },
    {
      "epoch": 18.045134818288393,
      "grad_norm": 0.5154466032981873,
      "learning_rate": 0.00019884859044719418,
      "loss": 0.0251,
      "step": 61570
    },
    {
      "epoch": 18.048065650644784,
      "grad_norm": 0.3483596742153168,
      "learning_rate": 0.00019876479415960584,
      "loss": 0.0731,
      "step": 61580
    },
    {
      "epoch": 18.05099648300117,
      "grad_norm": 0.8197758197784424,
      "learning_rate": 0.00019868099787201748,
      "loss": 0.0406,
      "step": 61590
    },
    {
      "epoch": 18.053927315357562,
      "grad_norm": 1.660610318183899,
      "learning_rate": 0.00019859720158442912,
      "loss": 0.0564,
      "step": 61600
    },
    {
      "epoch": 18.05685814771395,
      "grad_norm": 2.6001529693603516,
      "learning_rate": 0.0001985134052968407,
      "loss": 0.0314,
      "step": 61610
    },
    {
      "epoch": 18.05978898007034,
      "grad_norm": 2.0800325870513916,
      "learning_rate": 0.00019842960900925236,
      "loss": 0.0509,
      "step": 61620
    },
    {
      "epoch": 18.062719812426728,
      "grad_norm": 0.8463642597198486,
      "learning_rate": 0.000198345812721664,
      "loss": 0.0316,
      "step": 61630
    },
    {
      "epoch": 18.06565064478312,
      "grad_norm": 0.7620782852172852,
      "learning_rate": 0.00019826201643407564,
      "loss": 0.0673,
      "step": 61640
    },
    {
      "epoch": 18.068581477139507,
      "grad_norm": 0.9000734090805054,
      "learning_rate": 0.00019817822014648727,
      "loss": 0.0473,
      "step": 61650
    },
    {
      "epoch": 18.071512309495898,
      "grad_norm": 1.151721715927124,
      "learning_rate": 0.0001980944238588989,
      "loss": 0.0582,
      "step": 61660
    },
    {
      "epoch": 18.074443141852285,
      "grad_norm": 1.5542926788330078,
      "learning_rate": 0.00019801062757131052,
      "loss": 0.0417,
      "step": 61670
    },
    {
      "epoch": 18.077373974208676,
      "grad_norm": 0.7242867946624756,
      "learning_rate": 0.00019792683128372216,
      "loss": 0.0488,
      "step": 61680
    },
    {
      "epoch": 18.080304806565064,
      "grad_norm": 2.423089027404785,
      "learning_rate": 0.0001978430349961338,
      "loss": 0.0577,
      "step": 61690
    },
    {
      "epoch": 18.083235638921455,
      "grad_norm": 0.7390894293785095,
      "learning_rate": 0.00019775923870854543,
      "loss": 0.0483,
      "step": 61700
    },
    {
      "epoch": 18.086166471277842,
      "grad_norm": 1.6002521514892578,
      "learning_rate": 0.00019767544242095707,
      "loss": 0.0535,
      "step": 61710
    },
    {
      "epoch": 18.089097303634233,
      "grad_norm": 0.08398938179016113,
      "learning_rate": 0.0001975916461333687,
      "loss": 0.0467,
      "step": 61720
    },
    {
      "epoch": 18.09202813599062,
      "grad_norm": 3.655621290206909,
      "learning_rate": 0.00019750784984578031,
      "loss": 0.0575,
      "step": 61730
    },
    {
      "epoch": 18.09495896834701,
      "grad_norm": 0.7113277912139893,
      "learning_rate": 0.00019742405355819195,
      "loss": 0.0626,
      "step": 61740
    },
    {
      "epoch": 18.0978898007034,
      "grad_norm": 0.834690272808075,
      "learning_rate": 0.0001973402572706036,
      "loss": 0.0387,
      "step": 61750
    },
    {
      "epoch": 18.10082063305979,
      "grad_norm": 0.30373889207839966,
      "learning_rate": 0.00019725646098301523,
      "loss": 0.0393,
      "step": 61760
    },
    {
      "epoch": 18.103751465416178,
      "grad_norm": 1.9957083463668823,
      "learning_rate": 0.0001971726646954269,
      "loss": 0.0578,
      "step": 61770
    },
    {
      "epoch": 18.10668229777257,
      "grad_norm": 0.524996817111969,
      "learning_rate": 0.00019708886840783847,
      "loss": 0.0387,
      "step": 61780
    },
    {
      "epoch": 18.109613130128956,
      "grad_norm": 0.3704318404197693,
      "learning_rate": 0.0001970050721202501,
      "loss": 0.0484,
      "step": 61790
    },
    {
      "epoch": 18.112543962485347,
      "grad_norm": 0.3420209586620331,
      "learning_rate": 0.00019692127583266177,
      "loss": 0.0324,
      "step": 61800
    },
    {
      "epoch": 18.115474794841735,
      "grad_norm": 0.45248377323150635,
      "learning_rate": 0.0001968374795450734,
      "loss": 0.0498,
      "step": 61810
    },
    {
      "epoch": 18.118405627198126,
      "grad_norm": 0.4782114326953888,
      "learning_rate": 0.00019675368325748505,
      "loss": 0.0347,
      "step": 61820
    },
    {
      "epoch": 18.121336459554513,
      "grad_norm": 0.6819761991500854,
      "learning_rate": 0.00019666988696989668,
      "loss": 0.0342,
      "step": 61830
    },
    {
      "epoch": 18.124267291910904,
      "grad_norm": 1.1189345121383667,
      "learning_rate": 0.0001965860906823083,
      "loss": 0.0379,
      "step": 61840
    },
    {
      "epoch": 18.12719812426729,
      "grad_norm": 0.8556411862373352,
      "learning_rate": 0.00019650229439471993,
      "loss": 0.0353,
      "step": 61850
    },
    {
      "epoch": 18.130128956623683,
      "grad_norm": 1.3123613595962524,
      "learning_rate": 0.00019641849810713157,
      "loss": 0.0619,
      "step": 61860
    },
    {
      "epoch": 18.13305978898007,
      "grad_norm": 0.5752726793289185,
      "learning_rate": 0.0001963347018195432,
      "loss": 0.0448,
      "step": 61870
    },
    {
      "epoch": 18.13599062133646,
      "grad_norm": 0.7504987120628357,
      "learning_rate": 0.00019625090553195484,
      "loss": 0.0424,
      "step": 61880
    },
    {
      "epoch": 18.13892145369285,
      "grad_norm": 0.2843773663043976,
      "learning_rate": 0.00019616710924436648,
      "loss": 0.0434,
      "step": 61890
    },
    {
      "epoch": 18.14185228604924,
      "grad_norm": 0.9885108470916748,
      "learning_rate": 0.00019608331295677809,
      "loss": 0.0382,
      "step": 61900
    },
    {
      "epoch": 18.144783118405627,
      "grad_norm": 1.5917391777038574,
      "learning_rate": 0.00019599951666918972,
      "loss": 0.0471,
      "step": 61910
    },
    {
      "epoch": 18.147713950762018,
      "grad_norm": 0.4389682412147522,
      "learning_rate": 0.00019591572038160136,
      "loss": 0.0405,
      "step": 61920
    },
    {
      "epoch": 18.150644783118405,
      "grad_norm": 1.008341670036316,
      "learning_rate": 0.000195831924094013,
      "loss": 0.0371,
      "step": 61930
    },
    {
      "epoch": 18.153575615474796,
      "grad_norm": 1.582821011543274,
      "learning_rate": 0.00019574812780642463,
      "loss": 0.0358,
      "step": 61940
    },
    {
      "epoch": 18.156506447831184,
      "grad_norm": 0.9251640439033508,
      "learning_rate": 0.00019566433151883624,
      "loss": 0.0633,
      "step": 61950
    },
    {
      "epoch": 18.159437280187575,
      "grad_norm": 0.5714484453201294,
      "learning_rate": 0.00019558053523124788,
      "loss": 0.0339,
      "step": 61960
    },
    {
      "epoch": 18.162368112543962,
      "grad_norm": 0.6231436133384705,
      "learning_rate": 0.00019549673894365952,
      "loss": 0.0511,
      "step": 61970
    },
    {
      "epoch": 18.165298944900353,
      "grad_norm": 0.09911590069532394,
      "learning_rate": 0.00019541294265607115,
      "loss": 0.0435,
      "step": 61980
    },
    {
      "epoch": 18.16822977725674,
      "grad_norm": 3.1626200675964355,
      "learning_rate": 0.00019532914636848282,
      "loss": 0.0584,
      "step": 61990
    },
    {
      "epoch": 18.17116060961313,
      "grad_norm": 0.6312474608421326,
      "learning_rate": 0.00019524535008089445,
      "loss": 0.0241,
      "step": 62000
    },
    {
      "epoch": 18.17409144196952,
      "grad_norm": 0.20243078470230103,
      "learning_rate": 0.00019516155379330604,
      "loss": 0.0425,
      "step": 62010
    },
    {
      "epoch": 18.177022274325907,
      "grad_norm": 1.1578339338302612,
      "learning_rate": 0.0001950777575057177,
      "loss": 0.0541,
      "step": 62020
    },
    {
      "epoch": 18.179953106682298,
      "grad_norm": 1.590157151222229,
      "learning_rate": 0.00019499396121812934,
      "loss": 0.0479,
      "step": 62030
    },
    {
      "epoch": 18.182883939038685,
      "grad_norm": 1.1921908855438232,
      "learning_rate": 0.00019491016493054097,
      "loss": 0.0589,
      "step": 62040
    },
    {
      "epoch": 18.185814771395076,
      "grad_norm": 0.9579015970230103,
      "learning_rate": 0.0001948263686429526,
      "loss": 0.0742,
      "step": 62050
    },
    {
      "epoch": 18.188745603751464,
      "grad_norm": 0.8618579506874084,
      "learning_rate": 0.00019474257235536422,
      "loss": 0.0525,
      "step": 62060
    },
    {
      "epoch": 18.191676436107855,
      "grad_norm": 0.16508404910564423,
      "learning_rate": 0.00019465877606777586,
      "loss": 0.0382,
      "step": 62070
    },
    {
      "epoch": 18.194607268464242,
      "grad_norm": 1.3643113374710083,
      "learning_rate": 0.0001945749797801875,
      "loss": 0.0233,
      "step": 62080
    },
    {
      "epoch": 18.197538100820633,
      "grad_norm": 0.3382143974304199,
      "learning_rate": 0.00019449118349259913,
      "loss": 0.0575,
      "step": 62090
    },
    {
      "epoch": 18.20046893317702,
      "grad_norm": 1.2637405395507812,
      "learning_rate": 0.00019440738720501077,
      "loss": 0.0498,
      "step": 62100
    },
    {
      "epoch": 18.203399765533412,
      "grad_norm": 1.8554058074951172,
      "learning_rate": 0.0001943235909174224,
      "loss": 0.0463,
      "step": 62110
    },
    {
      "epoch": 18.2063305978898,
      "grad_norm": 0.4960104525089264,
      "learning_rate": 0.00019423979462983401,
      "loss": 0.0361,
      "step": 62120
    },
    {
      "epoch": 18.20926143024619,
      "grad_norm": 1.3179181814193726,
      "learning_rate": 0.00019415599834224565,
      "loss": 0.0498,
      "step": 62130
    },
    {
      "epoch": 18.212192262602578,
      "grad_norm": 1.7197847366333008,
      "learning_rate": 0.0001940722020546573,
      "loss": 0.0472,
      "step": 62140
    },
    {
      "epoch": 18.21512309495897,
      "grad_norm": 2.0700485706329346,
      "learning_rate": 0.00019398840576706893,
      "loss": 0.0589,
      "step": 62150
    },
    {
      "epoch": 18.218053927315356,
      "grad_norm": 0.7811291217803955,
      "learning_rate": 0.00019390460947948056,
      "loss": 0.0458,
      "step": 62160
    },
    {
      "epoch": 18.220984759671747,
      "grad_norm": 1.1210037469863892,
      "learning_rate": 0.00019382081319189223,
      "loss": 0.0603,
      "step": 62170
    },
    {
      "epoch": 18.223915592028135,
      "grad_norm": 0.5565643906593323,
      "learning_rate": 0.0001937370169043038,
      "loss": 0.0592,
      "step": 62180
    },
    {
      "epoch": 18.226846424384526,
      "grad_norm": 1.2032297849655151,
      "learning_rate": 0.00019365322061671545,
      "loss": 0.0378,
      "step": 62190
    },
    {
      "epoch": 18.229777256740913,
      "grad_norm": 0.7592519521713257,
      "learning_rate": 0.0001935694243291271,
      "loss": 0.0352,
      "step": 62200
    },
    {
      "epoch": 18.232708089097304,
      "grad_norm": 0.7721235156059265,
      "learning_rate": 0.00019348562804153875,
      "loss": 0.0416,
      "step": 62210
    },
    {
      "epoch": 18.235638921453692,
      "grad_norm": 0.46176791191101074,
      "learning_rate": 0.00019340183175395038,
      "loss": 0.0489,
      "step": 62220
    },
    {
      "epoch": 18.238569753810083,
      "grad_norm": 0.7645359635353088,
      "learning_rate": 0.00019331803546636197,
      "loss": 0.0426,
      "step": 62230
    },
    {
      "epoch": 18.24150058616647,
      "grad_norm": 2.3774712085723877,
      "learning_rate": 0.00019323423917877363,
      "loss": 0.0481,
      "step": 62240
    },
    {
      "epoch": 18.24443141852286,
      "grad_norm": 0.8693655133247375,
      "learning_rate": 0.00019315044289118527,
      "loss": 0.0592,
      "step": 62250
    },
    {
      "epoch": 18.24736225087925,
      "grad_norm": 0.6513160467147827,
      "learning_rate": 0.0001930666466035969,
      "loss": 0.0389,
      "step": 62260
    },
    {
      "epoch": 18.25029308323564,
      "grad_norm": 0.5887414216995239,
      "learning_rate": 0.00019298285031600854,
      "loss": 0.0758,
      "step": 62270
    },
    {
      "epoch": 18.253223915592027,
      "grad_norm": 0.6391506195068359,
      "learning_rate": 0.00019289905402842018,
      "loss": 0.0432,
      "step": 62280
    },
    {
      "epoch": 18.25615474794842,
      "grad_norm": 1.7859281301498413,
      "learning_rate": 0.00019281525774083179,
      "loss": 0.0676,
      "step": 62290
    },
    {
      "epoch": 18.259085580304806,
      "grad_norm": 0.17266902327537537,
      "learning_rate": 0.00019273146145324342,
      "loss": 0.0567,
      "step": 62300
    },
    {
      "epoch": 18.262016412661197,
      "grad_norm": 0.6462023258209229,
      "learning_rate": 0.00019264766516565506,
      "loss": 0.0693,
      "step": 62310
    },
    {
      "epoch": 18.264947245017584,
      "grad_norm": 0.7386743426322937,
      "learning_rate": 0.0001925638688780667,
      "loss": 0.0668,
      "step": 62320
    },
    {
      "epoch": 18.267878077373975,
      "grad_norm": 0.31378626823425293,
      "learning_rate": 0.00019248007259047833,
      "loss": 0.0508,
      "step": 62330
    },
    {
      "epoch": 18.270808909730363,
      "grad_norm": 1.7513206005096436,
      "learning_rate": 0.00019239627630288997,
      "loss": 0.061,
      "step": 62340
    },
    {
      "epoch": 18.273739742086754,
      "grad_norm": 0.16183222830295563,
      "learning_rate": 0.00019231248001530158,
      "loss": 0.0509,
      "step": 62350
    },
    {
      "epoch": 18.27667057444314,
      "grad_norm": 2.091470241546631,
      "learning_rate": 0.00019222868372771322,
      "loss": 0.0523,
      "step": 62360
    },
    {
      "epoch": 18.279601406799532,
      "grad_norm": 0.4684848487377167,
      "learning_rate": 0.00019214488744012485,
      "loss": 0.0381,
      "step": 62370
    },
    {
      "epoch": 18.28253223915592,
      "grad_norm": 1.1701757907867432,
      "learning_rate": 0.0001920610911525365,
      "loss": 0.0375,
      "step": 62380
    },
    {
      "epoch": 18.28546307151231,
      "grad_norm": 2.232229471206665,
      "learning_rate": 0.00019197729486494815,
      "loss": 0.0552,
      "step": 62390
    },
    {
      "epoch": 18.2883939038687,
      "grad_norm": 1.4469807147979736,
      "learning_rate": 0.00019189349857735974,
      "loss": 0.0366,
      "step": 62400
    },
    {
      "epoch": 18.29132473622509,
      "grad_norm": 0.8814454674720764,
      "learning_rate": 0.00019180970228977137,
      "loss": 0.0385,
      "step": 62410
    },
    {
      "epoch": 18.294255568581477,
      "grad_norm": 0.21635177731513977,
      "learning_rate": 0.00019172590600218304,
      "loss": 0.0467,
      "step": 62420
    },
    {
      "epoch": 18.297186400937868,
      "grad_norm": 0.6179328560829163,
      "learning_rate": 0.00019164210971459467,
      "loss": 0.0557,
      "step": 62430
    },
    {
      "epoch": 18.300117233294255,
      "grad_norm": 2.2393643856048584,
      "learning_rate": 0.0001915583134270063,
      "loss": 0.037,
      "step": 62440
    },
    {
      "epoch": 18.303048065650646,
      "grad_norm": 0.7368000149726868,
      "learning_rate": 0.00019147451713941795,
      "loss": 0.0327,
      "step": 62450
    },
    {
      "epoch": 18.305978898007034,
      "grad_norm": 0.7895013093948364,
      "learning_rate": 0.00019139072085182956,
      "loss": 0.0448,
      "step": 62460
    },
    {
      "epoch": 18.308909730363425,
      "grad_norm": 1.0828914642333984,
      "learning_rate": 0.0001913069245642412,
      "loss": 0.0404,
      "step": 62470
    },
    {
      "epoch": 18.311840562719812,
      "grad_norm": 1.2328144311904907,
      "learning_rate": 0.00019122312827665283,
      "loss": 0.0551,
      "step": 62480
    },
    {
      "epoch": 18.314771395076203,
      "grad_norm": 0.6942320466041565,
      "learning_rate": 0.00019113933198906447,
      "loss": 0.0384,
      "step": 62490
    },
    {
      "epoch": 18.31770222743259,
      "grad_norm": 1.5554887056350708,
      "learning_rate": 0.0001910555357014761,
      "loss": 0.0641,
      "step": 62500
    },
    {
      "epoch": 18.32063305978898,
      "grad_norm": 0.9591432213783264,
      "learning_rate": 0.00019097173941388774,
      "loss": 0.0491,
      "step": 62510
    },
    {
      "epoch": 18.32356389214537,
      "grad_norm": 0.9205980896949768,
      "learning_rate": 0.00019088794312629935,
      "loss": 0.0469,
      "step": 62520
    },
    {
      "epoch": 18.32649472450176,
      "grad_norm": 0.7655157446861267,
      "learning_rate": 0.000190804146838711,
      "loss": 0.0375,
      "step": 62530
    },
    {
      "epoch": 18.329425556858148,
      "grad_norm": 1.775118112564087,
      "learning_rate": 0.00019072035055112263,
      "loss": 0.0447,
      "step": 62540
    },
    {
      "epoch": 18.33235638921454,
      "grad_norm": 0.49991634488105774,
      "learning_rate": 0.00019063655426353426,
      "loss": 0.0474,
      "step": 62550
    },
    {
      "epoch": 18.335287221570926,
      "grad_norm": 1.5849542617797852,
      "learning_rate": 0.0001905527579759459,
      "loss": 0.0428,
      "step": 62560
    },
    {
      "epoch": 18.338218053927314,
      "grad_norm": 3.129352331161499,
      "learning_rate": 0.0001904689616883575,
      "loss": 0.0553,
      "step": 62570
    },
    {
      "epoch": 18.341148886283705,
      "grad_norm": 1.4028369188308716,
      "learning_rate": 0.00019038516540076915,
      "loss": 0.0367,
      "step": 62580
    },
    {
      "epoch": 18.344079718640092,
      "grad_norm": 1.6206544637680054,
      "learning_rate": 0.00019030136911318078,
      "loss": 0.0484,
      "step": 62590
    },
    {
      "epoch": 18.347010550996483,
      "grad_norm": 2.247974395751953,
      "learning_rate": 0.00019021757282559242,
      "loss": 0.0699,
      "step": 62600
    },
    {
      "epoch": 18.34994138335287,
      "grad_norm": 1.4161795377731323,
      "learning_rate": 0.00019013377653800408,
      "loss": 0.0543,
      "step": 62610
    },
    {
      "epoch": 18.35287221570926,
      "grad_norm": 0.2984424829483032,
      "learning_rate": 0.00019004998025041572,
      "loss": 0.0706,
      "step": 62620
    },
    {
      "epoch": 18.35580304806565,
      "grad_norm": 0.6061822772026062,
      "learning_rate": 0.0001899661839628273,
      "loss": 0.0388,
      "step": 62630
    },
    {
      "epoch": 18.35873388042204,
      "grad_norm": 1.6751585006713867,
      "learning_rate": 0.00018988238767523897,
      "loss": 0.0402,
      "step": 62640
    },
    {
      "epoch": 18.361664712778428,
      "grad_norm": 3.5720415115356445,
      "learning_rate": 0.0001897985913876506,
      "loss": 0.0567,
      "step": 62650
    },
    {
      "epoch": 18.36459554513482,
      "grad_norm": 0.827846109867096,
      "learning_rate": 0.00018971479510006224,
      "loss": 0.0697,
      "step": 62660
    },
    {
      "epoch": 18.367526377491206,
      "grad_norm": 1.1275583505630493,
      "learning_rate": 0.00018963099881247388,
      "loss": 0.0414,
      "step": 62670
    },
    {
      "epoch": 18.370457209847597,
      "grad_norm": 0.5469502806663513,
      "learning_rate": 0.00018954720252488549,
      "loss": 0.0349,
      "step": 62680
    },
    {
      "epoch": 18.373388042203985,
      "grad_norm": 0.750542402267456,
      "learning_rate": 0.00018946340623729712,
      "loss": 0.0472,
      "step": 62690
    },
    {
      "epoch": 18.376318874560376,
      "grad_norm": 0.5982492566108704,
      "learning_rate": 0.00018937960994970876,
      "loss": 0.0431,
      "step": 62700
    },
    {
      "epoch": 18.379249706916763,
      "grad_norm": 0.5157386660575867,
      "learning_rate": 0.0001892958136621204,
      "loss": 0.0351,
      "step": 62710
    },
    {
      "epoch": 18.382180539273154,
      "grad_norm": 0.39957964420318604,
      "learning_rate": 0.00018921201737453203,
      "loss": 0.0542,
      "step": 62720
    },
    {
      "epoch": 18.38511137162954,
      "grad_norm": 1.2954281568527222,
      "learning_rate": 0.00018912822108694367,
      "loss": 0.037,
      "step": 62730
    },
    {
      "epoch": 18.388042203985933,
      "grad_norm": 1.250036597251892,
      "learning_rate": 0.00018904442479935528,
      "loss": 0.0327,
      "step": 62740
    },
    {
      "epoch": 18.39097303634232,
      "grad_norm": 1.337790608406067,
      "learning_rate": 0.00018896062851176692,
      "loss": 0.0386,
      "step": 62750
    },
    {
      "epoch": 18.39390386869871,
      "grad_norm": 1.6111263036727905,
      "learning_rate": 0.00018887683222417855,
      "loss": 0.0529,
      "step": 62760
    },
    {
      "epoch": 18.3968347010551,
      "grad_norm": 0.7106382250785828,
      "learning_rate": 0.0001887930359365902,
      "loss": 0.0499,
      "step": 62770
    },
    {
      "epoch": 18.39976553341149,
      "grad_norm": 1.3065521717071533,
      "learning_rate": 0.00018870923964900183,
      "loss": 0.048,
      "step": 62780
    },
    {
      "epoch": 18.402696365767877,
      "grad_norm": 1.6824713945388794,
      "learning_rate": 0.0001886254433614135,
      "loss": 0.0633,
      "step": 62790
    },
    {
      "epoch": 18.405627198124268,
      "grad_norm": 3.3494670391082764,
      "learning_rate": 0.00018854164707382507,
      "loss": 0.0435,
      "step": 62800
    },
    {
      "epoch": 18.408558030480656,
      "grad_norm": 0.5819660425186157,
      "learning_rate": 0.0001884578507862367,
      "loss": 0.0485,
      "step": 62810
    },
    {
      "epoch": 18.411488862837047,
      "grad_norm": 1.1990647315979004,
      "learning_rate": 0.00018837405449864835,
      "loss": 0.0286,
      "step": 62820
    },
    {
      "epoch": 18.414419695193434,
      "grad_norm": 0.5876824855804443,
      "learning_rate": 0.00018829025821106,
      "loss": 0.0718,
      "step": 62830
    },
    {
      "epoch": 18.417350527549825,
      "grad_norm": 0.1727055460214615,
      "learning_rate": 0.00018820646192347165,
      "loss": 0.0495,
      "step": 62840
    },
    {
      "epoch": 18.420281359906213,
      "grad_norm": 1.9566901922225952,
      "learning_rate": 0.00018812266563588323,
      "loss": 0.0447,
      "step": 62850
    },
    {
      "epoch": 18.423212192262604,
      "grad_norm": 0.691332221031189,
      "learning_rate": 0.0001880388693482949,
      "loss": 0.0725,
      "step": 62860
    },
    {
      "epoch": 18.42614302461899,
      "grad_norm": 1.2731443643569946,
      "learning_rate": 0.00018795507306070653,
      "loss": 0.0379,
      "step": 62870
    },
    {
      "epoch": 18.429073856975382,
      "grad_norm": 0.37826821208000183,
      "learning_rate": 0.00018787127677311817,
      "loss": 0.0413,
      "step": 62880
    },
    {
      "epoch": 18.43200468933177,
      "grad_norm": 0.720695436000824,
      "learning_rate": 0.0001877874804855298,
      "loss": 0.0753,
      "step": 62890
    },
    {
      "epoch": 18.43493552168816,
      "grad_norm": 0.6345866918563843,
      "learning_rate": 0.00018770368419794144,
      "loss": 0.0463,
      "step": 62900
    },
    {
      "epoch": 18.437866354044548,
      "grad_norm": 0.6441212892532349,
      "learning_rate": 0.00018761988791035305,
      "loss": 0.0575,
      "step": 62910
    },
    {
      "epoch": 18.44079718640094,
      "grad_norm": 0.6134320497512817,
      "learning_rate": 0.0001875360916227647,
      "loss": 0.051,
      "step": 62920
    },
    {
      "epoch": 18.443728018757326,
      "grad_norm": 1.0531195402145386,
      "learning_rate": 0.00018745229533517632,
      "loss": 0.0426,
      "step": 62930
    },
    {
      "epoch": 18.446658851113718,
      "grad_norm": 0.5815616846084595,
      "learning_rate": 0.00018736849904758796,
      "loss": 0.0486,
      "step": 62940
    },
    {
      "epoch": 18.449589683470105,
      "grad_norm": 1.2234103679656982,
      "learning_rate": 0.0001872847027599996,
      "loss": 0.0603,
      "step": 62950
    },
    {
      "epoch": 18.452520515826496,
      "grad_norm": 0.510791540145874,
      "learning_rate": 0.00018720090647241124,
      "loss": 0.0687,
      "step": 62960
    },
    {
      "epoch": 18.455451348182883,
      "grad_norm": 1.9801914691925049,
      "learning_rate": 0.00018711711018482285,
      "loss": 0.073,
      "step": 62970
    },
    {
      "epoch": 18.458382180539274,
      "grad_norm": 0.7337771058082581,
      "learning_rate": 0.00018703331389723448,
      "loss": 0.0529,
      "step": 62980
    },
    {
      "epoch": 18.461313012895662,
      "grad_norm": 2.133516788482666,
      "learning_rate": 0.00018694951760964612,
      "loss": 0.0449,
      "step": 62990
    },
    {
      "epoch": 18.464243845252053,
      "grad_norm": 0.3072279095649719,
      "learning_rate": 0.00018686572132205776,
      "loss": 0.0348,
      "step": 63000
    },
    {
      "epoch": 18.46717467760844,
      "grad_norm": 0.43197646737098694,
      "learning_rate": 0.00018678192503446942,
      "loss": 0.039,
      "step": 63010
    },
    {
      "epoch": 18.47010550996483,
      "grad_norm": 1.6436642408370972,
      "learning_rate": 0.000186698128746881,
      "loss": 0.0461,
      "step": 63020
    },
    {
      "epoch": 18.47303634232122,
      "grad_norm": 0.9954093098640442,
      "learning_rate": 0.00018661433245929264,
      "loss": 0.0563,
      "step": 63030
    },
    {
      "epoch": 18.47596717467761,
      "grad_norm": 0.8094033598899841,
      "learning_rate": 0.00018653053617170428,
      "loss": 0.0601,
      "step": 63040
    },
    {
      "epoch": 18.478898007033997,
      "grad_norm": 1.2643526792526245,
      "learning_rate": 0.00018644673988411594,
      "loss": 0.0742,
      "step": 63050
    },
    {
      "epoch": 18.48182883939039,
      "grad_norm": 1.1382020711898804,
      "learning_rate": 0.00018636294359652758,
      "loss": 0.0638,
      "step": 63060
    },
    {
      "epoch": 18.484759671746776,
      "grad_norm": 0.5004520416259766,
      "learning_rate": 0.0001862791473089392,
      "loss": 0.0498,
      "step": 63070
    },
    {
      "epoch": 18.487690504103167,
      "grad_norm": 0.9235092997550964,
      "learning_rate": 0.00018619535102135082,
      "loss": 0.0389,
      "step": 63080
    },
    {
      "epoch": 18.490621336459554,
      "grad_norm": 2.4049878120422363,
      "learning_rate": 0.00018611155473376246,
      "loss": 0.0437,
      "step": 63090
    },
    {
      "epoch": 18.493552168815945,
      "grad_norm": 1.4371179342269897,
      "learning_rate": 0.0001860277584461741,
      "loss": 0.0508,
      "step": 63100
    },
    {
      "epoch": 18.496483001172333,
      "grad_norm": 0.8456428050994873,
      "learning_rate": 0.00018594396215858573,
      "loss": 0.0511,
      "step": 63110
    },
    {
      "epoch": 18.49941383352872,
      "grad_norm": 0.5189993381500244,
      "learning_rate": 0.00018586016587099737,
      "loss": 0.0655,
      "step": 63120
    },
    {
      "epoch": 18.50234466588511,
      "grad_norm": 0.8506479263305664,
      "learning_rate": 0.00018577636958340898,
      "loss": 0.043,
      "step": 63130
    },
    {
      "epoch": 18.5052754982415,
      "grad_norm": 0.5295676589012146,
      "learning_rate": 0.00018569257329582062,
      "loss": 0.041,
      "step": 63140
    },
    {
      "epoch": 18.50820633059789,
      "grad_norm": 0.9592120051383972,
      "learning_rate": 0.00018560877700823225,
      "loss": 0.0443,
      "step": 63150
    },
    {
      "epoch": 18.511137162954277,
      "grad_norm": 0.9736570715904236,
      "learning_rate": 0.0001855249807206439,
      "loss": 0.0331,
      "step": 63160
    },
    {
      "epoch": 18.51406799531067,
      "grad_norm": 0.391456663608551,
      "learning_rate": 0.00018544118443305553,
      "loss": 0.0335,
      "step": 63170
    },
    {
      "epoch": 18.516998827667056,
      "grad_norm": 1.4973269701004028,
      "learning_rate": 0.00018535738814546716,
      "loss": 0.0645,
      "step": 63180
    },
    {
      "epoch": 18.519929660023447,
      "grad_norm": 0.850141704082489,
      "learning_rate": 0.00018527359185787877,
      "loss": 0.0479,
      "step": 63190
    },
    {
      "epoch": 18.522860492379834,
      "grad_norm": 3.2704718112945557,
      "learning_rate": 0.0001851897955702904,
      "loss": 0.0358,
      "step": 63200
    },
    {
      "epoch": 18.525791324736225,
      "grad_norm": 0.20208711922168732,
      "learning_rate": 0.00018510599928270205,
      "loss": 0.0461,
      "step": 63210
    },
    {
      "epoch": 18.528722157092613,
      "grad_norm": 0.04180670529603958,
      "learning_rate": 0.00018502220299511368,
      "loss": 0.0392,
      "step": 63220
    },
    {
      "epoch": 18.531652989449004,
      "grad_norm": 1.7965586185455322,
      "learning_rate": 0.00018493840670752535,
      "loss": 0.0486,
      "step": 63230
    },
    {
      "epoch": 18.53458382180539,
      "grad_norm": 0.12137683480978012,
      "learning_rate": 0.00018485461041993698,
      "loss": 0.0465,
      "step": 63240
    },
    {
      "epoch": 18.537514654161782,
      "grad_norm": 1.1599805355072021,
      "learning_rate": 0.00018477081413234857,
      "loss": 0.044,
      "step": 63250
    },
    {
      "epoch": 18.54044548651817,
      "grad_norm": 1.1938894987106323,
      "learning_rate": 0.0001846870178447602,
      "loss": 0.0346,
      "step": 63260
    },
    {
      "epoch": 18.54337631887456,
      "grad_norm": 0.27045169472694397,
      "learning_rate": 0.00018460322155717187,
      "loss": 0.0524,
      "step": 63270
    },
    {
      "epoch": 18.54630715123095,
      "grad_norm": 1.134792447090149,
      "learning_rate": 0.0001845194252695835,
      "loss": 0.0401,
      "step": 63280
    },
    {
      "epoch": 18.54923798358734,
      "grad_norm": 2.2591493129730225,
      "learning_rate": 0.00018443562898199514,
      "loss": 0.0553,
      "step": 63290
    },
    {
      "epoch": 18.552168815943727,
      "grad_norm": 0.4904284179210663,
      "learning_rate": 0.00018435183269440675,
      "loss": 0.0401,
      "step": 63300
    },
    {
      "epoch": 18.555099648300118,
      "grad_norm": 0.18319645524024963,
      "learning_rate": 0.0001842680364068184,
      "loss": 0.0306,
      "step": 63310
    },
    {
      "epoch": 18.558030480656505,
      "grad_norm": 0.5025209784507751,
      "learning_rate": 0.00018418424011923002,
      "loss": 0.0435,
      "step": 63320
    },
    {
      "epoch": 18.560961313012896,
      "grad_norm": 1.8306342363357544,
      "learning_rate": 0.00018410044383164166,
      "loss": 0.0599,
      "step": 63330
    },
    {
      "epoch": 18.563892145369284,
      "grad_norm": 0.639602541923523,
      "learning_rate": 0.0001840166475440533,
      "loss": 0.0675,
      "step": 63340
    },
    {
      "epoch": 18.566822977725675,
      "grad_norm": 0.09761466830968857,
      "learning_rate": 0.00018393285125646494,
      "loss": 0.0434,
      "step": 63350
    },
    {
      "epoch": 18.569753810082062,
      "grad_norm": 0.05576074868440628,
      "learning_rate": 0.00018384905496887654,
      "loss": 0.0388,
      "step": 63360
    },
    {
      "epoch": 18.572684642438453,
      "grad_norm": 0.1488763988018036,
      "learning_rate": 0.00018376525868128818,
      "loss": 0.0381,
      "step": 63370
    },
    {
      "epoch": 18.57561547479484,
      "grad_norm": 0.2912801504135132,
      "learning_rate": 0.00018368146239369982,
      "loss": 0.0338,
      "step": 63380
    },
    {
      "epoch": 18.578546307151232,
      "grad_norm": 0.9748186469078064,
      "learning_rate": 0.00018359766610611146,
      "loss": 0.039,
      "step": 63390
    },
    {
      "epoch": 18.58147713950762,
      "grad_norm": 0.13935241103172302,
      "learning_rate": 0.0001835138698185231,
      "loss": 0.0334,
      "step": 63400
    },
    {
      "epoch": 18.58440797186401,
      "grad_norm": 0.6780036091804504,
      "learning_rate": 0.00018343007353093476,
      "loss": 0.0556,
      "step": 63410
    },
    {
      "epoch": 18.587338804220398,
      "grad_norm": 0.9594418406486511,
      "learning_rate": 0.00018334627724334634,
      "loss": 0.0517,
      "step": 63420
    },
    {
      "epoch": 18.59026963657679,
      "grad_norm": 0.6512362957000732,
      "learning_rate": 0.00018326248095575798,
      "loss": 0.0437,
      "step": 63430
    },
    {
      "epoch": 18.593200468933176,
      "grad_norm": 1.2387921810150146,
      "learning_rate": 0.0001831786846681696,
      "loss": 0.0576,
      "step": 63440
    },
    {
      "epoch": 18.596131301289567,
      "grad_norm": 0.46983370184898376,
      "learning_rate": 0.00018309488838058128,
      "loss": 0.0556,
      "step": 63450
    },
    {
      "epoch": 18.599062133645955,
      "grad_norm": 1.6817222833633423,
      "learning_rate": 0.0001830110920929929,
      "loss": 0.0607,
      "step": 63460
    },
    {
      "epoch": 18.601992966002346,
      "grad_norm": 2.1239418983459473,
      "learning_rate": 0.0001829272958054045,
      "loss": 0.0462,
      "step": 63470
    },
    {
      "epoch": 18.604923798358733,
      "grad_norm": 1.022372841835022,
      "learning_rate": 0.00018284349951781613,
      "loss": 0.0569,
      "step": 63480
    },
    {
      "epoch": 18.607854630715124,
      "grad_norm": 1.1673954725265503,
      "learning_rate": 0.0001827597032302278,
      "loss": 0.043,
      "step": 63490
    },
    {
      "epoch": 18.61078546307151,
      "grad_norm": 1.2726516723632812,
      "learning_rate": 0.00018267590694263943,
      "loss": 0.0491,
      "step": 63500
    },
    {
      "epoch": 18.613716295427903,
      "grad_norm": 1.9590965509414673,
      "learning_rate": 0.00018259211065505107,
      "loss": 0.0602,
      "step": 63510
    },
    {
      "epoch": 18.61664712778429,
      "grad_norm": 1.581573247909546,
      "learning_rate": 0.0001825083143674627,
      "loss": 0.0694,
      "step": 63520
    },
    {
      "epoch": 18.61957796014068,
      "grad_norm": 3.533679723739624,
      "learning_rate": 0.00018242451807987432,
      "loss": 0.0651,
      "step": 63530
    },
    {
      "epoch": 18.62250879249707,
      "grad_norm": 0.3813805878162384,
      "learning_rate": 0.00018234072179228595,
      "loss": 0.0556,
      "step": 63540
    },
    {
      "epoch": 18.62543962485346,
      "grad_norm": 1.2506103515625,
      "learning_rate": 0.0001822569255046976,
      "loss": 0.0461,
      "step": 63550
    },
    {
      "epoch": 18.628370457209847,
      "grad_norm": 0.3885163366794586,
      "learning_rate": 0.00018217312921710923,
      "loss": 0.052,
      "step": 63560
    },
    {
      "epoch": 18.631301289566238,
      "grad_norm": 0.9843066930770874,
      "learning_rate": 0.00018208933292952086,
      "loss": 0.0464,
      "step": 63570
    },
    {
      "epoch": 18.634232121922626,
      "grad_norm": 1.7535452842712402,
      "learning_rate": 0.0001820055366419325,
      "loss": 0.0467,
      "step": 63580
    },
    {
      "epoch": 18.637162954279017,
      "grad_norm": 1.228699803352356,
      "learning_rate": 0.0001819217403543441,
      "loss": 0.0431,
      "step": 63590
    },
    {
      "epoch": 18.640093786635404,
      "grad_norm": 0.48393887281417847,
      "learning_rate": 0.00018183794406675575,
      "loss": 0.0691,
      "step": 63600
    },
    {
      "epoch": 18.643024618991795,
      "grad_norm": 0.5866817831993103,
      "learning_rate": 0.00018175414777916738,
      "loss": 0.0374,
      "step": 63610
    },
    {
      "epoch": 18.645955451348183,
      "grad_norm": 1.0879920721054077,
      "learning_rate": 0.00018167035149157902,
      "loss": 0.052,
      "step": 63620
    },
    {
      "epoch": 18.648886283704574,
      "grad_norm": 0.4850212633609772,
      "learning_rate": 0.00018158655520399068,
      "loss": 0.0625,
      "step": 63630
    },
    {
      "epoch": 18.65181711606096,
      "grad_norm": 0.7042484879493713,
      "learning_rate": 0.00018150275891640227,
      "loss": 0.0903,
      "step": 63640
    },
    {
      "epoch": 18.654747948417352,
      "grad_norm": 1.1914844512939453,
      "learning_rate": 0.0001814189626288139,
      "loss": 0.0484,
      "step": 63650
    },
    {
      "epoch": 18.65767878077374,
      "grad_norm": 2.3853280544281006,
      "learning_rate": 0.00018133516634122554,
      "loss": 0.0418,
      "step": 63660
    },
    {
      "epoch": 18.66060961313013,
      "grad_norm": 2.1504364013671875,
      "learning_rate": 0.0001812513700536372,
      "loss": 0.0626,
      "step": 63670
    },
    {
      "epoch": 18.663540445486518,
      "grad_norm": 0.9253413677215576,
      "learning_rate": 0.00018116757376604884,
      "loss": 0.0391,
      "step": 63680
    },
    {
      "epoch": 18.66647127784291,
      "grad_norm": 0.4481034278869629,
      "learning_rate": 0.00018108377747846048,
      "loss": 0.0426,
      "step": 63690
    },
    {
      "epoch": 18.669402110199297,
      "grad_norm": 1.105061411857605,
      "learning_rate": 0.0001809999811908721,
      "loss": 0.0615,
      "step": 63700
    },
    {
      "epoch": 18.672332942555684,
      "grad_norm": 0.2691434919834137,
      "learning_rate": 0.00018091618490328372,
      "loss": 0.0485,
      "step": 63710
    },
    {
      "epoch": 18.675263774912075,
      "grad_norm": 0.3696107566356659,
      "learning_rate": 0.00018083238861569536,
      "loss": 0.0609,
      "step": 63720
    },
    {
      "epoch": 18.678194607268463,
      "grad_norm": 1.503519892692566,
      "learning_rate": 0.000180748592328107,
      "loss": 0.0662,
      "step": 63730
    },
    {
      "epoch": 18.681125439624854,
      "grad_norm": 1.6580525636672974,
      "learning_rate": 0.00018066479604051864,
      "loss": 0.0461,
      "step": 63740
    },
    {
      "epoch": 18.68405627198124,
      "grad_norm": 0.4468934237957001,
      "learning_rate": 0.00018058099975293024,
      "loss": 0.0568,
      "step": 63750
    },
    {
      "epoch": 18.686987104337632,
      "grad_norm": 1.7214629650115967,
      "learning_rate": 0.00018049720346534188,
      "loss": 0.0434,
      "step": 63760
    },
    {
      "epoch": 18.68991793669402,
      "grad_norm": 1.869163990020752,
      "learning_rate": 0.00018041340717775352,
      "loss": 0.0489,
      "step": 63770
    },
    {
      "epoch": 18.69284876905041,
      "grad_norm": 0.8691806793212891,
      "learning_rate": 0.00018032961089016516,
      "loss": 0.0287,
      "step": 63780
    },
    {
      "epoch": 18.695779601406798,
      "grad_norm": 0.3917739987373352,
      "learning_rate": 0.0001802458146025768,
      "loss": 0.0415,
      "step": 63790
    },
    {
      "epoch": 18.69871043376319,
      "grad_norm": 1.302902340888977,
      "learning_rate": 0.00018016201831498843,
      "loss": 0.0534,
      "step": 63800
    },
    {
      "epoch": 18.701641266119577,
      "grad_norm": 1.0732145309448242,
      "learning_rate": 0.00018007822202740004,
      "loss": 0.0503,
      "step": 63810
    },
    {
      "epoch": 18.704572098475968,
      "grad_norm": 1.2417323589324951,
      "learning_rate": 0.00017999442573981168,
      "loss": 0.0719,
      "step": 63820
    },
    {
      "epoch": 18.707502930832355,
      "grad_norm": 0.7249910831451416,
      "learning_rate": 0.0001799106294522233,
      "loss": 0.068,
      "step": 63830
    },
    {
      "epoch": 18.710433763188746,
      "grad_norm": 0.8554570078849792,
      "learning_rate": 0.00017982683316463495,
      "loss": 0.0547,
      "step": 63840
    },
    {
      "epoch": 18.713364595545134,
      "grad_norm": 0.3385803997516632,
      "learning_rate": 0.0001797430368770466,
      "loss": 0.0385,
      "step": 63850
    },
    {
      "epoch": 18.716295427901525,
      "grad_norm": 1.4307162761688232,
      "learning_rate": 0.00017965924058945825,
      "loss": 0.0581,
      "step": 63860
    },
    {
      "epoch": 18.719226260257912,
      "grad_norm": 0.28906479477882385,
      "learning_rate": 0.00017957544430186983,
      "loss": 0.0662,
      "step": 63870
    },
    {
      "epoch": 18.722157092614303,
      "grad_norm": 0.506808340549469,
      "learning_rate": 0.00017949164801428147,
      "loss": 0.0537,
      "step": 63880
    },
    {
      "epoch": 18.72508792497069,
      "grad_norm": 0.514237642288208,
      "learning_rate": 0.00017940785172669313,
      "loss": 0.0422,
      "step": 63890
    },
    {
      "epoch": 18.72801875732708,
      "grad_norm": 0.534675121307373,
      "learning_rate": 0.00017932405543910477,
      "loss": 0.0377,
      "step": 63900
    },
    {
      "epoch": 18.73094958968347,
      "grad_norm": 0.7731007933616638,
      "learning_rate": 0.0001792402591515164,
      "loss": 0.0387,
      "step": 63910
    },
    {
      "epoch": 18.73388042203986,
      "grad_norm": 1.4660073518753052,
      "learning_rate": 0.00017915646286392802,
      "loss": 0.0436,
      "step": 63920
    },
    {
      "epoch": 18.736811254396248,
      "grad_norm": 1.9793795347213745,
      "learning_rate": 0.00017907266657633965,
      "loss": 0.0666,
      "step": 63930
    },
    {
      "epoch": 18.73974208675264,
      "grad_norm": 1.421858787536621,
      "learning_rate": 0.0001789888702887513,
      "loss": 0.0684,
      "step": 63940
    },
    {
      "epoch": 18.742672919109026,
      "grad_norm": 3.553248167037964,
      "learning_rate": 0.00017890507400116293,
      "loss": 0.0365,
      "step": 63950
    },
    {
      "epoch": 18.745603751465417,
      "grad_norm": 0.4144240617752075,
      "learning_rate": 0.00017882127771357456,
      "loss": 0.0435,
      "step": 63960
    },
    {
      "epoch": 18.748534583821804,
      "grad_norm": 0.9209104180335999,
      "learning_rate": 0.0001787374814259862,
      "loss": 0.04,
      "step": 63970
    },
    {
      "epoch": 18.751465416178196,
      "grad_norm": 0.5220848321914673,
      "learning_rate": 0.00017865368513839784,
      "loss": 0.0466,
      "step": 63980
    },
    {
      "epoch": 18.754396248534583,
      "grad_norm": 0.9499117732048035,
      "learning_rate": 0.00017856988885080945,
      "loss": 0.0495,
      "step": 63990
    },
    {
      "epoch": 18.757327080890974,
      "grad_norm": 3.455373525619507,
      "learning_rate": 0.00017848609256322108,
      "loss": 0.079,
      "step": 64000
    },
    {
      "epoch": 18.76025791324736,
      "grad_norm": 1.2937145233154297,
      "learning_rate": 0.00017840229627563272,
      "loss": 0.0522,
      "step": 64010
    },
    {
      "epoch": 18.763188745603752,
      "grad_norm": 0.0455954410135746,
      "learning_rate": 0.00017831849998804436,
      "loss": 0.0349,
      "step": 64020
    },
    {
      "epoch": 18.76611957796014,
      "grad_norm": 0.742673397064209,
      "learning_rate": 0.000178234703700456,
      "loss": 0.0438,
      "step": 64030
    },
    {
      "epoch": 18.76905041031653,
      "grad_norm": 0.7813184857368469,
      "learning_rate": 0.00017815090741286763,
      "loss": 0.0581,
      "step": 64040
    },
    {
      "epoch": 18.77198124267292,
      "grad_norm": 0.06681305170059204,
      "learning_rate": 0.00017806711112527924,
      "loss": 0.0651,
      "step": 64050
    },
    {
      "epoch": 18.77491207502931,
      "grad_norm": 0.8084352612495422,
      "learning_rate": 0.00017798331483769088,
      "loss": 0.0573,
      "step": 64060
    },
    {
      "epoch": 18.777842907385697,
      "grad_norm": 1.0604585409164429,
      "learning_rate": 0.00017789951855010254,
      "loss": 0.0645,
      "step": 64070
    },
    {
      "epoch": 18.780773739742088,
      "grad_norm": 1.100205898284912,
      "learning_rate": 0.00017781572226251415,
      "loss": 0.0431,
      "step": 64080
    },
    {
      "epoch": 18.783704572098475,
      "grad_norm": 0.4325663447380066,
      "learning_rate": 0.0001777319259749258,
      "loss": 0.0281,
      "step": 64090
    },
    {
      "epoch": 18.786635404454866,
      "grad_norm": 0.8473408818244934,
      "learning_rate": 0.00017764812968733742,
      "loss": 0.0508,
      "step": 64100
    },
    {
      "epoch": 18.789566236811254,
      "grad_norm": 0.8840370774269104,
      "learning_rate": 0.00017756433339974906,
      "loss": 0.0616,
      "step": 64110
    },
    {
      "epoch": 18.792497069167645,
      "grad_norm": 0.7939295768737793,
      "learning_rate": 0.0001774805371121607,
      "loss": 0.0509,
      "step": 64120
    },
    {
      "epoch": 18.795427901524032,
      "grad_norm": 1.1213353872299194,
      "learning_rate": 0.0001773967408245723,
      "loss": 0.0579,
      "step": 64130
    },
    {
      "epoch": 18.798358733880423,
      "grad_norm": 1.7147296667099,
      "learning_rate": 0.00017731294453698394,
      "loss": 0.0456,
      "step": 64140
    },
    {
      "epoch": 18.80128956623681,
      "grad_norm": 0.8992264270782471,
      "learning_rate": 0.00017722914824939558,
      "loss": 0.036,
      "step": 64150
    },
    {
      "epoch": 18.804220398593202,
      "grad_norm": 2.0379726886749268,
      "learning_rate": 0.00017714535196180722,
      "loss": 0.0504,
      "step": 64160
    },
    {
      "epoch": 18.80715123094959,
      "grad_norm": 0.20626257359981537,
      "learning_rate": 0.00017706155567421886,
      "loss": 0.0787,
      "step": 64170
    },
    {
      "epoch": 18.81008206330598,
      "grad_norm": 1.3181802034378052,
      "learning_rate": 0.0001769777593866305,
      "loss": 0.0557,
      "step": 64180
    },
    {
      "epoch": 18.813012895662368,
      "grad_norm": 0.5181068778038025,
      "learning_rate": 0.0001768939630990421,
      "loss": 0.0443,
      "step": 64190
    },
    {
      "epoch": 18.81594372801876,
      "grad_norm": 1.8260139226913452,
      "learning_rate": 0.00017681016681145377,
      "loss": 0.0491,
      "step": 64200
    },
    {
      "epoch": 18.818874560375146,
      "grad_norm": 1.4621102809906006,
      "learning_rate": 0.0001767263705238654,
      "loss": 0.0559,
      "step": 64210
    },
    {
      "epoch": 18.821805392731537,
      "grad_norm": 0.8366745114326477,
      "learning_rate": 0.000176642574236277,
      "loss": 0.0528,
      "step": 64220
    },
    {
      "epoch": 18.824736225087925,
      "grad_norm": 0.27732494473457336,
      "learning_rate": 0.00017655877794868865,
      "loss": 0.0438,
      "step": 64230
    },
    {
      "epoch": 18.827667057444316,
      "grad_norm": 0.7464929223060608,
      "learning_rate": 0.00017647498166110029,
      "loss": 0.0432,
      "step": 64240
    },
    {
      "epoch": 18.830597889800703,
      "grad_norm": 1.2260448932647705,
      "learning_rate": 0.00017639118537351192,
      "loss": 0.0454,
      "step": 64250
    },
    {
      "epoch": 18.83352872215709,
      "grad_norm": 0.561156690120697,
      "learning_rate": 0.00017630738908592356,
      "loss": 0.0492,
      "step": 64260
    },
    {
      "epoch": 18.836459554513482,
      "grad_norm": 2.398397207260132,
      "learning_rate": 0.0001762235927983352,
      "loss": 0.0597,
      "step": 64270
    },
    {
      "epoch": 18.83939038686987,
      "grad_norm": 0.25565358996391296,
      "learning_rate": 0.0001761397965107468,
      "loss": 0.0473,
      "step": 64280
    },
    {
      "epoch": 18.84232121922626,
      "grad_norm": 0.5349704027175903,
      "learning_rate": 0.00017605600022315847,
      "loss": 0.0256,
      "step": 64290
    },
    {
      "epoch": 18.845252051582648,
      "grad_norm": 1.7194372415542603,
      "learning_rate": 0.00017597220393557008,
      "loss": 0.0519,
      "step": 64300
    },
    {
      "epoch": 18.84818288393904,
      "grad_norm": 0.798126757144928,
      "learning_rate": 0.00017588840764798172,
      "loss": 0.0485,
      "step": 64310
    },
    {
      "epoch": 18.851113716295426,
      "grad_norm": 2.157379627227783,
      "learning_rate": 0.00017580461136039335,
      "loss": 0.061,
      "step": 64320
    },
    {
      "epoch": 18.854044548651817,
      "grad_norm": 0.4071821868419647,
      "learning_rate": 0.000175720815072805,
      "loss": 0.0417,
      "step": 64330
    },
    {
      "epoch": 18.856975381008205,
      "grad_norm": 0.6747916340827942,
      "learning_rate": 0.00017563701878521663,
      "loss": 0.0402,
      "step": 64340
    },
    {
      "epoch": 18.859906213364596,
      "grad_norm": 0.5538906455039978,
      "learning_rate": 0.00017555322249762826,
      "loss": 0.0335,
      "step": 64350
    },
    {
      "epoch": 18.862837045720983,
      "grad_norm": 0.7722347378730774,
      "learning_rate": 0.00017546942621003987,
      "loss": 0.0371,
      "step": 64360
    },
    {
      "epoch": 18.865767878077374,
      "grad_norm": 0.8752144575119019,
      "learning_rate": 0.0001753856299224515,
      "loss": 0.0362,
      "step": 64370
    },
    {
      "epoch": 18.868698710433762,
      "grad_norm": 2.614536762237549,
      "learning_rate": 0.00017530183363486317,
      "loss": 0.0443,
      "step": 64380
    },
    {
      "epoch": 18.871629542790153,
      "grad_norm": 2.060333013534546,
      "learning_rate": 0.00017521803734727478,
      "loss": 0.043,
      "step": 64390
    },
    {
      "epoch": 18.87456037514654,
      "grad_norm": 0.24857459962368011,
      "learning_rate": 0.00017513424105968642,
      "loss": 0.0367,
      "step": 64400
    },
    {
      "epoch": 18.87749120750293,
      "grad_norm": 1.0295205116271973,
      "learning_rate": 0.00017505044477209806,
      "loss": 0.0579,
      "step": 64410
    },
    {
      "epoch": 18.88042203985932,
      "grad_norm": 0.3795527517795563,
      "learning_rate": 0.0001749666484845097,
      "loss": 0.0359,
      "step": 64420
    },
    {
      "epoch": 18.88335287221571,
      "grad_norm": 0.993791401386261,
      "learning_rate": 0.00017488285219692133,
      "loss": 0.034,
      "step": 64430
    },
    {
      "epoch": 18.886283704572097,
      "grad_norm": 1.781303882598877,
      "learning_rate": 0.00017479905590933294,
      "loss": 0.0747,
      "step": 64440
    },
    {
      "epoch": 18.88921453692849,
      "grad_norm": 0.4362820088863373,
      "learning_rate": 0.00017471525962174458,
      "loss": 0.048,
      "step": 64450
    },
    {
      "epoch": 18.892145369284876,
      "grad_norm": 2.746912717819214,
      "learning_rate": 0.00017463146333415621,
      "loss": 0.0445,
      "step": 64460
    },
    {
      "epoch": 18.895076201641267,
      "grad_norm": 0.12816019356250763,
      "learning_rate": 0.00017454766704656785,
      "loss": 0.0417,
      "step": 64470
    },
    {
      "epoch": 18.898007033997654,
      "grad_norm": 0.6978217363357544,
      "learning_rate": 0.0001744638707589795,
      "loss": 0.0395,
      "step": 64480
    },
    {
      "epoch": 18.900937866354045,
      "grad_norm": 1.8698519468307495,
      "learning_rate": 0.00017438007447139112,
      "loss": 0.0509,
      "step": 64490
    },
    {
      "epoch": 18.903868698710433,
      "grad_norm": 1.56705904006958,
      "learning_rate": 0.00017429627818380273,
      "loss": 0.0494,
      "step": 64500
    },
    {
      "epoch": 18.906799531066824,
      "grad_norm": 1.0026047229766846,
      "learning_rate": 0.0001742124818962144,
      "loss": 0.0536,
      "step": 64510
    },
    {
      "epoch": 18.90973036342321,
      "grad_norm": 1.8655240535736084,
      "learning_rate": 0.00017412868560862604,
      "loss": 0.0519,
      "step": 64520
    },
    {
      "epoch": 18.912661195779602,
      "grad_norm": 0.7362657189369202,
      "learning_rate": 0.00017404488932103764,
      "loss": 0.0517,
      "step": 64530
    },
    {
      "epoch": 18.91559202813599,
      "grad_norm": 0.9004964828491211,
      "learning_rate": 0.00017396109303344928,
      "loss": 0.0638,
      "step": 64540
    },
    {
      "epoch": 18.91852286049238,
      "grad_norm": 0.9706523418426514,
      "learning_rate": 0.00017387729674586092,
      "loss": 0.0512,
      "step": 64550
    },
    {
      "epoch": 18.921453692848768,
      "grad_norm": 0.9924344420433044,
      "learning_rate": 0.00017379350045827256,
      "loss": 0.0569,
      "step": 64560
    },
    {
      "epoch": 18.92438452520516,
      "grad_norm": 0.3780801296234131,
      "learning_rate": 0.0001737097041706842,
      "loss": 0.0417,
      "step": 64570
    },
    {
      "epoch": 18.927315357561547,
      "grad_norm": 1.13489830493927,
      "learning_rate": 0.00017362590788309583,
      "loss": 0.043,
      "step": 64580
    },
    {
      "epoch": 18.930246189917938,
      "grad_norm": 1.1143348217010498,
      "learning_rate": 0.00017354211159550744,
      "loss": 0.0364,
      "step": 64590
    },
    {
      "epoch": 18.933177022274325,
      "grad_norm": 1.8663835525512695,
      "learning_rate": 0.0001734583153079191,
      "loss": 0.0398,
      "step": 64600
    },
    {
      "epoch": 18.936107854630716,
      "grad_norm": 0.3903455138206482,
      "learning_rate": 0.0001733745190203307,
      "loss": 0.0394,
      "step": 64610
    },
    {
      "epoch": 18.939038686987104,
      "grad_norm": 0.7230385541915894,
      "learning_rate": 0.00017329072273274235,
      "loss": 0.0404,
      "step": 64620
    },
    {
      "epoch": 18.941969519343495,
      "grad_norm": 2.286972999572754,
      "learning_rate": 0.00017320692644515399,
      "loss": 0.0604,
      "step": 64630
    },
    {
      "epoch": 18.944900351699882,
      "grad_norm": 0.20061656832695007,
      "learning_rate": 0.00017312313015756562,
      "loss": 0.0402,
      "step": 64640
    },
    {
      "epoch": 18.947831184056273,
      "grad_norm": 1.2829726934432983,
      "learning_rate": 0.00017303933386997726,
      "loss": 0.0795,
      "step": 64650
    },
    {
      "epoch": 18.95076201641266,
      "grad_norm": 1.9087226390838623,
      "learning_rate": 0.0001729555375823889,
      "loss": 0.0436,
      "step": 64660
    },
    {
      "epoch": 18.95369284876905,
      "grad_norm": 0.9757925868034363,
      "learning_rate": 0.0001728717412948005,
      "loss": 0.0511,
      "step": 64670
    },
    {
      "epoch": 18.95662368112544,
      "grad_norm": 0.03263363987207413,
      "learning_rate": 0.00017278794500721214,
      "loss": 0.0299,
      "step": 64680
    },
    {
      "epoch": 18.95955451348183,
      "grad_norm": 1.56315279006958,
      "learning_rate": 0.0001727041487196238,
      "loss": 0.0544,
      "step": 64690
    },
    {
      "epoch": 18.962485345838218,
      "grad_norm": 2.0113816261291504,
      "learning_rate": 0.00017262035243203542,
      "loss": 0.0562,
      "step": 64700
    },
    {
      "epoch": 18.96541617819461,
      "grad_norm": 1.7096232175827026,
      "learning_rate": 0.00017253655614444705,
      "loss": 0.0842,
      "step": 64710
    },
    {
      "epoch": 18.968347010550996,
      "grad_norm": 2.4957385063171387,
      "learning_rate": 0.0001724527598568587,
      "loss": 0.0415,
      "step": 64720
    },
    {
      "epoch": 18.971277842907387,
      "grad_norm": 0.028162676841020584,
      "learning_rate": 0.00017236896356927033,
      "loss": 0.0566,
      "step": 64730
    },
    {
      "epoch": 18.974208675263775,
      "grad_norm": 0.8339806199073792,
      "learning_rate": 0.00017228516728168196,
      "loss": 0.0509,
      "step": 64740
    },
    {
      "epoch": 18.977139507620166,
      "grad_norm": 2.690373659133911,
      "learning_rate": 0.00017220137099409357,
      "loss": 0.0454,
      "step": 64750
    },
    {
      "epoch": 18.980070339976553,
      "grad_norm": 0.7614949345588684,
      "learning_rate": 0.0001721175747065052,
      "loss": 0.0464,
      "step": 64760
    },
    {
      "epoch": 18.983001172332944,
      "grad_norm": 1.6863218545913696,
      "learning_rate": 0.00017203377841891685,
      "loss": 0.045,
      "step": 64770
    },
    {
      "epoch": 18.98593200468933,
      "grad_norm": 0.5300759673118591,
      "learning_rate": 0.00017194998213132848,
      "loss": 0.0332,
      "step": 64780
    },
    {
      "epoch": 18.988862837045723,
      "grad_norm": 1.0329214334487915,
      "learning_rate": 0.00017186618584374012,
      "loss": 0.0585,
      "step": 64790
    },
    {
      "epoch": 18.99179366940211,
      "grad_norm": 1.3048384189605713,
      "learning_rate": 0.00017178238955615176,
      "loss": 0.0576,
      "step": 64800
    },
    {
      "epoch": 18.9947245017585,
      "grad_norm": 1.1573033332824707,
      "learning_rate": 0.00017169859326856337,
      "loss": 0.0729,
      "step": 64810
    },
    {
      "epoch": 18.99765533411489,
      "grad_norm": 1.2031928300857544,
      "learning_rate": 0.00017161479698097503,
      "loss": 0.0304,
      "step": 64820
    },
    {
      "epoch": 19.0,
      "eval_accuracy": 0.7912957467853611,
      "eval_f1_macro": 0.8343066980901372,
      "eval_f1_micro": 0.8508638981509549,
      "eval_f1_weighted": 0.845634769493157,
      "eval_loss": 0.07352565228939056,
      "eval_roc_auc": 0.9047726112453559,
      "eval_runtime": 142.5774,
      "eval_samples_per_second": 21.273,
      "eval_steps_per_second": 2.665,
      "step": 64828
    },
    {
      "epoch": 19.000586166471276,
      "grad_norm": 1.0191491842269897,
      "learning_rate": 0.00017153100069338667,
      "loss": 0.0652,
      "step": 64830
    },
    {
      "epoch": 19.003516998827667,
      "grad_norm": 0.3074735999107361,
      "learning_rate": 0.00017144720440579828,
      "loss": 0.0474,
      "step": 64840
    },
    {
      "epoch": 19.006447831184055,
      "grad_norm": 1.2632019519805908,
      "learning_rate": 0.00017136340811820991,
      "loss": 0.0381,
      "step": 64850
    },
    {
      "epoch": 19.009378663540446,
      "grad_norm": 1.23468017578125,
      "learning_rate": 0.00017127961183062155,
      "loss": 0.0284,
      "step": 64860
    },
    {
      "epoch": 19.012309495896833,
      "grad_norm": 1.6223304271697998,
      "learning_rate": 0.0001711958155430332,
      "loss": 0.05,
      "step": 64870
    },
    {
      "epoch": 19.015240328253224,
      "grad_norm": 0.5476719737052917,
      "learning_rate": 0.00017111201925544482,
      "loss": 0.0337,
      "step": 64880
    },
    {
      "epoch": 19.01817116060961,
      "grad_norm": 1.0444282293319702,
      "learning_rate": 0.00017102822296785646,
      "loss": 0.021,
      "step": 64890
    },
    {
      "epoch": 19.021101992966003,
      "grad_norm": 1.3467910289764404,
      "learning_rate": 0.00017094442668026807,
      "loss": 0.0437,
      "step": 64900
    },
    {
      "epoch": 19.02403282532239,
      "grad_norm": 1.4544495344161987,
      "learning_rate": 0.00017086063039267973,
      "loss": 0.0327,
      "step": 64910
    },
    {
      "epoch": 19.02696365767878,
      "grad_norm": 1.860947847366333,
      "learning_rate": 0.00017077683410509134,
      "loss": 0.0401,
      "step": 64920
    },
    {
      "epoch": 19.02989449003517,
      "grad_norm": 0.6408898830413818,
      "learning_rate": 0.00017069303781750298,
      "loss": 0.0277,
      "step": 64930
    },
    {
      "epoch": 19.03282532239156,
      "grad_norm": 0.7993152141571045,
      "learning_rate": 0.00017060924152991462,
      "loss": 0.046,
      "step": 64940
    },
    {
      "epoch": 19.035756154747947,
      "grad_norm": 0.7143929600715637,
      "learning_rate": 0.00017052544524232626,
      "loss": 0.0326,
      "step": 64950
    },
    {
      "epoch": 19.038686987104338,
      "grad_norm": 0.09858333319425583,
      "learning_rate": 0.0001704416489547379,
      "loss": 0.0402,
      "step": 64960
    },
    {
      "epoch": 19.041617819460726,
      "grad_norm": 1.0724753141403198,
      "learning_rate": 0.00017035785266714953,
      "loss": 0.055,
      "step": 64970
    },
    {
      "epoch": 19.044548651817117,
      "grad_norm": 0.6419381499290466,
      "learning_rate": 0.00017027405637956114,
      "loss": 0.0449,
      "step": 64980
    },
    {
      "epoch": 19.047479484173504,
      "grad_norm": 0.8520150184631348,
      "learning_rate": 0.00017019026009197278,
      "loss": 0.0356,
      "step": 64990
    },
    {
      "epoch": 19.050410316529895,
      "grad_norm": 1.0847413539886475,
      "learning_rate": 0.0001701064638043844,
      "loss": 0.0544,
      "step": 65000
    },
    {
      "epoch": 19.053341148886282,
      "grad_norm": 0.8241640329360962,
      "learning_rate": 0.00017002266751679605,
      "loss": 0.0429,
      "step": 65010
    },
    {
      "epoch": 19.056271981242674,
      "grad_norm": 2.388326644897461,
      "learning_rate": 0.00016993887122920769,
      "loss": 0.0595,
      "step": 65020
    },
    {
      "epoch": 19.05920281359906,
      "grad_norm": 0.4808879494667053,
      "learning_rate": 0.00016985507494161932,
      "loss": 0.0329,
      "step": 65030
    },
    {
      "epoch": 19.062133645955452,
      "grad_norm": 1.4097779989242554,
      "learning_rate": 0.00016977127865403096,
      "loss": 0.0529,
      "step": 65040
    },
    {
      "epoch": 19.06506447831184,
      "grad_norm": 0.16539150476455688,
      "learning_rate": 0.0001696874823664426,
      "loss": 0.059,
      "step": 65050
    },
    {
      "epoch": 19.06799531066823,
      "grad_norm": 0.786678671836853,
      "learning_rate": 0.0001696036860788542,
      "loss": 0.0523,
      "step": 65060
    },
    {
      "epoch": 19.070926143024618,
      "grad_norm": 1.623477578163147,
      "learning_rate": 0.00016951988979126584,
      "loss": 0.062,
      "step": 65070
    },
    {
      "epoch": 19.07385697538101,
      "grad_norm": 0.06854760646820068,
      "learning_rate": 0.00016943609350367748,
      "loss": 0.0351,
      "step": 65080
    },
    {
      "epoch": 19.076787807737396,
      "grad_norm": 2.129523754119873,
      "learning_rate": 0.00016935229721608912,
      "loss": 0.0472,
      "step": 65090
    },
    {
      "epoch": 19.079718640093787,
      "grad_norm": 0.5106465220451355,
      "learning_rate": 0.00016926850092850075,
      "loss": 0.0593,
      "step": 65100
    },
    {
      "epoch": 19.082649472450175,
      "grad_norm": 1.8504506349563599,
      "learning_rate": 0.0001691847046409124,
      "loss": 0.0422,
      "step": 65110
    },
    {
      "epoch": 19.085580304806566,
      "grad_norm": 0.3139974772930145,
      "learning_rate": 0.000169100908353324,
      "loss": 0.0189,
      "step": 65120
    },
    {
      "epoch": 19.088511137162953,
      "grad_norm": 0.6102636456489563,
      "learning_rate": 0.00016901711206573566,
      "loss": 0.0462,
      "step": 65130
    },
    {
      "epoch": 19.091441969519344,
      "grad_norm": 2.2377052307128906,
      "learning_rate": 0.0001689333157781473,
      "loss": 0.0432,
      "step": 65140
    },
    {
      "epoch": 19.094372801875732,
      "grad_norm": 0.7795796394348145,
      "learning_rate": 0.0001688495194905589,
      "loss": 0.0721,
      "step": 65150
    },
    {
      "epoch": 19.097303634232123,
      "grad_norm": 1.0010185241699219,
      "learning_rate": 0.00016876572320297055,
      "loss": 0.051,
      "step": 65160
    },
    {
      "epoch": 19.10023446658851,
      "grad_norm": 1.4826894998550415,
      "learning_rate": 0.00016868192691538218,
      "loss": 0.0546,
      "step": 65170
    },
    {
      "epoch": 19.1031652989449,
      "grad_norm": 0.15178945660591125,
      "learning_rate": 0.00016859813062779382,
      "loss": 0.0341,
      "step": 65180
    },
    {
      "epoch": 19.10609613130129,
      "grad_norm": 1.1737529039382935,
      "learning_rate": 0.00016851433434020546,
      "loss": 0.0211,
      "step": 65190
    },
    {
      "epoch": 19.10902696365768,
      "grad_norm": 0.3045100271701813,
      "learning_rate": 0.0001684305380526171,
      "loss": 0.0245,
      "step": 65200
    },
    {
      "epoch": 19.111957796014067,
      "grad_norm": 2.1203482151031494,
      "learning_rate": 0.0001683467417650287,
      "loss": 0.0474,
      "step": 65210
    },
    {
      "epoch": 19.11488862837046,
      "grad_norm": 2.06319260597229,
      "learning_rate": 0.00016826294547744037,
      "loss": 0.0419,
      "step": 65220
    },
    {
      "epoch": 19.117819460726846,
      "grad_norm": 1.3717761039733887,
      "learning_rate": 0.00016817914918985198,
      "loss": 0.0317,
      "step": 65230
    },
    {
      "epoch": 19.120750293083237,
      "grad_norm": 0.962554395198822,
      "learning_rate": 0.00016809535290226361,
      "loss": 0.0297,
      "step": 65240
    },
    {
      "epoch": 19.123681125439624,
      "grad_norm": 1.0853418111801147,
      "learning_rate": 0.00016801155661467525,
      "loss": 0.0405,
      "step": 65250
    },
    {
      "epoch": 19.126611957796015,
      "grad_norm": 1.014135718345642,
      "learning_rate": 0.0001679277603270869,
      "loss": 0.0348,
      "step": 65260
    },
    {
      "epoch": 19.129542790152403,
      "grad_norm": 0.3558329641819,
      "learning_rate": 0.00016784396403949852,
      "loss": 0.0394,
      "step": 65270
    },
    {
      "epoch": 19.132473622508794,
      "grad_norm": 0.7025149464607239,
      "learning_rate": 0.00016776016775191016,
      "loss": 0.0621,
      "step": 65280
    },
    {
      "epoch": 19.13540445486518,
      "grad_norm": 0.23836535215377808,
      "learning_rate": 0.00016767637146432177,
      "loss": 0.0222,
      "step": 65290
    },
    {
      "epoch": 19.138335287221572,
      "grad_norm": 0.5392988324165344,
      "learning_rate": 0.0001675925751767334,
      "loss": 0.0312,
      "step": 65300
    },
    {
      "epoch": 19.14126611957796,
      "grad_norm": 1.4004484415054321,
      "learning_rate": 0.00016750877888914504,
      "loss": 0.0617,
      "step": 65310
    },
    {
      "epoch": 19.14419695193435,
      "grad_norm": 0.7290021777153015,
      "learning_rate": 0.00016742498260155668,
      "loss": 0.0672,
      "step": 65320
    },
    {
      "epoch": 19.14712778429074,
      "grad_norm": 1.5094010829925537,
      "learning_rate": 0.00016734118631396832,
      "loss": 0.0321,
      "step": 65330
    },
    {
      "epoch": 19.15005861664713,
      "grad_norm": 1.8050270080566406,
      "learning_rate": 0.00016725739002637996,
      "loss": 0.0256,
      "step": 65340
    },
    {
      "epoch": 19.152989449003517,
      "grad_norm": 0.9685491919517517,
      "learning_rate": 0.0001671735937387916,
      "loss": 0.0557,
      "step": 65350
    },
    {
      "epoch": 19.155920281359908,
      "grad_norm": 1.7904198169708252,
      "learning_rate": 0.00016708979745120323,
      "loss": 0.0598,
      "step": 65360
    },
    {
      "epoch": 19.158851113716295,
      "grad_norm": 0.3862229585647583,
      "learning_rate": 0.00016700600116361484,
      "loss": 0.0334,
      "step": 65370
    },
    {
      "epoch": 19.161781946072686,
      "grad_norm": 0.46968308091163635,
      "learning_rate": 0.00016692220487602648,
      "loss": 0.0585,
      "step": 65380
    },
    {
      "epoch": 19.164712778429074,
      "grad_norm": 1.7024590969085693,
      "learning_rate": 0.0001668384085884381,
      "loss": 0.0391,
      "step": 65390
    },
    {
      "epoch": 19.16764361078546,
      "grad_norm": 0.7378230690956116,
      "learning_rate": 0.00016675461230084975,
      "loss": 0.0452,
      "step": 65400
    },
    {
      "epoch": 19.170574443141852,
      "grad_norm": 0.5217903852462769,
      "learning_rate": 0.00016667081601326139,
      "loss": 0.071,
      "step": 65410
    },
    {
      "epoch": 19.17350527549824,
      "grad_norm": 2.3141393661499023,
      "learning_rate": 0.00016658701972567302,
      "loss": 0.0463,
      "step": 65420
    },
    {
      "epoch": 19.17643610785463,
      "grad_norm": 0.4833148121833801,
      "learning_rate": 0.00016650322343808463,
      "loss": 0.0383,
      "step": 65430
    },
    {
      "epoch": 19.17936694021102,
      "grad_norm": 1.4222583770751953,
      "learning_rate": 0.0001664194271504963,
      "loss": 0.0389,
      "step": 65440
    },
    {
      "epoch": 19.18229777256741,
      "grad_norm": 0.2328743189573288,
      "learning_rate": 0.00016633563086290793,
      "loss": 0.0304,
      "step": 65450
    },
    {
      "epoch": 19.185228604923797,
      "grad_norm": 0.8712077140808105,
      "learning_rate": 0.00016625183457531954,
      "loss": 0.046,
      "step": 65460
    },
    {
      "epoch": 19.188159437280188,
      "grad_norm": 0.373115599155426,
      "learning_rate": 0.00016616803828773118,
      "loss": 0.0154,
      "step": 65470
    },
    {
      "epoch": 19.191090269636575,
      "grad_norm": 1.3064905405044556,
      "learning_rate": 0.00016608424200014282,
      "loss": 0.0511,
      "step": 65480
    },
    {
      "epoch": 19.194021101992966,
      "grad_norm": 0.37452974915504456,
      "learning_rate": 0.00016600044571255445,
      "loss": 0.0528,
      "step": 65490
    },
    {
      "epoch": 19.196951934349354,
      "grad_norm": 0.48456379771232605,
      "learning_rate": 0.0001659166494249661,
      "loss": 0.0395,
      "step": 65500
    },
    {
      "epoch": 19.199882766705745,
      "grad_norm": 0.2834559679031372,
      "learning_rate": 0.00016583285313737773,
      "loss": 0.0674,
      "step": 65510
    },
    {
      "epoch": 19.202813599062132,
      "grad_norm": 2.2597591876983643,
      "learning_rate": 0.00016574905684978934,
      "loss": 0.0536,
      "step": 65520
    },
    {
      "epoch": 19.205744431418523,
      "grad_norm": 0.5904665589332581,
      "learning_rate": 0.00016566526056220097,
      "loss": 0.036,
      "step": 65530
    },
    {
      "epoch": 19.20867526377491,
      "grad_norm": 2.0822248458862305,
      "learning_rate": 0.0001655814642746126,
      "loss": 0.0589,
      "step": 65540
    },
    {
      "epoch": 19.2116060961313,
      "grad_norm": 0.4381951093673706,
      "learning_rate": 0.00016549766798702425,
      "loss": 0.0332,
      "step": 65550
    },
    {
      "epoch": 19.21453692848769,
      "grad_norm": 1.4815877676010132,
      "learning_rate": 0.00016541387169943588,
      "loss": 0.0599,
      "step": 65560
    },
    {
      "epoch": 19.21746776084408,
      "grad_norm": 2.0293283462524414,
      "learning_rate": 0.00016533007541184752,
      "loss": 0.0285,
      "step": 65570
    },
    {
      "epoch": 19.220398593200468,
      "grad_norm": 1.6431564092636108,
      "learning_rate": 0.00016524627912425916,
      "loss": 0.0381,
      "step": 65580
    },
    {
      "epoch": 19.22332942555686,
      "grad_norm": 0.39508581161499023,
      "learning_rate": 0.0001651624828366708,
      "loss": 0.0222,
      "step": 65590
    },
    {
      "epoch": 19.226260257913246,
      "grad_norm": 2.0077834129333496,
      "learning_rate": 0.0001650786865490824,
      "loss": 0.0577,
      "step": 65600
    },
    {
      "epoch": 19.229191090269637,
      "grad_norm": 1.769773006439209,
      "learning_rate": 0.00016499489026149404,
      "loss": 0.0621,
      "step": 65610
    },
    {
      "epoch": 19.232121922626025,
      "grad_norm": 0.43097957968711853,
      "learning_rate": 0.00016491109397390568,
      "loss": 0.056,
      "step": 65620
    },
    {
      "epoch": 19.235052754982416,
      "grad_norm": 0.055141329765319824,
      "learning_rate": 0.00016482729768631731,
      "loss": 0.0561,
      "step": 65630
    },
    {
      "epoch": 19.237983587338803,
      "grad_norm": 0.4365830421447754,
      "learning_rate": 0.00016474350139872895,
      "loss": 0.037,
      "step": 65640
    },
    {
      "epoch": 19.240914419695194,
      "grad_norm": 0.18299786746501923,
      "learning_rate": 0.0001646597051111406,
      "loss": 0.0295,
      "step": 65650
    },
    {
      "epoch": 19.24384525205158,
      "grad_norm": 1.5486644506454468,
      "learning_rate": 0.00016457590882355222,
      "loss": 0.0181,
      "step": 65660
    },
    {
      "epoch": 19.246776084407973,
      "grad_norm": 0.6200344562530518,
      "learning_rate": 0.00016449211253596386,
      "loss": 0.0425,
      "step": 65670
    },
    {
      "epoch": 19.24970691676436,
      "grad_norm": 0.05370369553565979,
      "learning_rate": 0.00016440831624837547,
      "loss": 0.0478,
      "step": 65680
    },
    {
      "epoch": 19.25263774912075,
      "grad_norm": 0.2742859721183777,
      "learning_rate": 0.0001643245199607871,
      "loss": 0.0398,
      "step": 65690
    },
    {
      "epoch": 19.25556858147714,
      "grad_norm": 0.5144375562667847,
      "learning_rate": 0.00016424072367319874,
      "loss": 0.0393,
      "step": 65700
    },
    {
      "epoch": 19.25849941383353,
      "grad_norm": 1.0046364068984985,
      "learning_rate": 0.00016415692738561038,
      "loss": 0.0469,
      "step": 65710
    },
    {
      "epoch": 19.261430246189917,
      "grad_norm": 1.756626009941101,
      "learning_rate": 0.00016407313109802202,
      "loss": 0.0278,
      "step": 65720
    },
    {
      "epoch": 19.264361078546308,
      "grad_norm": 1.7227181196212769,
      "learning_rate": 0.00016398933481043365,
      "loss": 0.0388,
      "step": 65730
    },
    {
      "epoch": 19.267291910902696,
      "grad_norm": 0.9077825546264648,
      "learning_rate": 0.00016390553852284526,
      "loss": 0.0404,
      "step": 65740
    },
    {
      "epoch": 19.270222743259087,
      "grad_norm": 0.36789625883102417,
      "learning_rate": 0.0001638217422352569,
      "loss": 0.0625,
      "step": 65750
    },
    {
      "epoch": 19.273153575615474,
      "grad_norm": 1.096208930015564,
      "learning_rate": 0.00016373794594766857,
      "loss": 0.0393,
      "step": 65760
    },
    {
      "epoch": 19.276084407971865,
      "grad_norm": 0.779470682144165,
      "learning_rate": 0.00016365414966008018,
      "loss": 0.054,
      "step": 65770
    },
    {
      "epoch": 19.279015240328253,
      "grad_norm": 0.8366193771362305,
      "learning_rate": 0.0001635703533724918,
      "loss": 0.0459,
      "step": 65780
    },
    {
      "epoch": 19.281946072684644,
      "grad_norm": 0.13017171621322632,
      "learning_rate": 0.00016348655708490345,
      "loss": 0.0429,
      "step": 65790
    },
    {
      "epoch": 19.28487690504103,
      "grad_norm": 0.5222428441047668,
      "learning_rate": 0.00016340276079731509,
      "loss": 0.0312,
      "step": 65800
    },
    {
      "epoch": 19.287807737397422,
      "grad_norm": 1.5069020986557007,
      "learning_rate": 0.00016331896450972672,
      "loss": 0.0454,
      "step": 65810
    },
    {
      "epoch": 19.29073856975381,
      "grad_norm": 0.80336594581604,
      "learning_rate": 0.00016323516822213836,
      "loss": 0.0203,
      "step": 65820
    },
    {
      "epoch": 19.2936694021102,
      "grad_norm": 0.6410239338874817,
      "learning_rate": 0.00016315137193454997,
      "loss": 0.0477,
      "step": 65830
    },
    {
      "epoch": 19.296600234466588,
      "grad_norm": 0.5961664319038391,
      "learning_rate": 0.0001630675756469616,
      "loss": 0.04,
      "step": 65840
    },
    {
      "epoch": 19.29953106682298,
      "grad_norm": 1.909249186515808,
      "learning_rate": 0.00016298377935937324,
      "loss": 0.07,
      "step": 65850
    },
    {
      "epoch": 19.302461899179367,
      "grad_norm": 0.7068327069282532,
      "learning_rate": 0.00016289998307178488,
      "loss": 0.0338,
      "step": 65860
    },
    {
      "epoch": 19.305392731535758,
      "grad_norm": 2.6161928176879883,
      "learning_rate": 0.00016281618678419652,
      "loss": 0.0337,
      "step": 65870
    },
    {
      "epoch": 19.308323563892145,
      "grad_norm": 1.9839047193527222,
      "learning_rate": 0.00016273239049660815,
      "loss": 0.0453,
      "step": 65880
    },
    {
      "epoch": 19.311254396248536,
      "grad_norm": 0.2865496277809143,
      "learning_rate": 0.0001626485942090198,
      "loss": 0.0451,
      "step": 65890
    },
    {
      "epoch": 19.314185228604924,
      "grad_norm": 1.3504518270492554,
      "learning_rate": 0.00016256479792143143,
      "loss": 0.0552,
      "step": 65900
    },
    {
      "epoch": 19.317116060961315,
      "grad_norm": 0.489075630903244,
      "learning_rate": 0.00016248100163384304,
      "loss": 0.0344,
      "step": 65910
    },
    {
      "epoch": 19.320046893317702,
      "grad_norm": 2.5069499015808105,
      "learning_rate": 0.00016239720534625467,
      "loss": 0.0382,
      "step": 65920
    },
    {
      "epoch": 19.322977725674093,
      "grad_norm": 1.8696808815002441,
      "learning_rate": 0.0001623134090586663,
      "loss": 0.0429,
      "step": 65930
    },
    {
      "epoch": 19.32590855803048,
      "grad_norm": 0.29063448309898376,
      "learning_rate": 0.00016222961277107795,
      "loss": 0.0593,
      "step": 65940
    },
    {
      "epoch": 19.32883939038687,
      "grad_norm": 0.8089517951011658,
      "learning_rate": 0.00016214581648348958,
      "loss": 0.0412,
      "step": 65950
    },
    {
      "epoch": 19.33177022274326,
      "grad_norm": 0.781358003616333,
      "learning_rate": 0.00016206202019590122,
      "loss": 0.0264,
      "step": 65960
    },
    {
      "epoch": 19.33470105509965,
      "grad_norm": 0.5958795547485352,
      "learning_rate": 0.00016197822390831286,
      "loss": 0.0416,
      "step": 65970
    },
    {
      "epoch": 19.337631887456038,
      "grad_norm": 0.622929573059082,
      "learning_rate": 0.0001618944276207245,
      "loss": 0.0339,
      "step": 65980
    },
    {
      "epoch": 19.340562719812425,
      "grad_norm": 0.6237767934799194,
      "learning_rate": 0.0001618106313331361,
      "loss": 0.036,
      "step": 65990
    },
    {
      "epoch": 19.343493552168816,
      "grad_norm": 0.6972803473472595,
      "learning_rate": 0.00016172683504554774,
      "loss": 0.0623,
      "step": 66000
    },
    {
      "epoch": 19.346424384525204,
      "grad_norm": 2.705915927886963,
      "learning_rate": 0.00016164303875795938,
      "loss": 0.0421,
      "step": 66010
    },
    {
      "epoch": 19.349355216881595,
      "grad_norm": 1.1300216913223267,
      "learning_rate": 0.00016155924247037101,
      "loss": 0.0443,
      "step": 66020
    },
    {
      "epoch": 19.352286049237982,
      "grad_norm": 1.8478643894195557,
      "learning_rate": 0.00016147544618278265,
      "loss": 0.0369,
      "step": 66030
    },
    {
      "epoch": 19.355216881594373,
      "grad_norm": 1.8582943677902222,
      "learning_rate": 0.0001613916498951943,
      "loss": 0.0375,
      "step": 66040
    },
    {
      "epoch": 19.35814771395076,
      "grad_norm": 1.4058667421340942,
      "learning_rate": 0.0001613078536076059,
      "loss": 0.0444,
      "step": 66050
    },
    {
      "epoch": 19.36107854630715,
      "grad_norm": 0.5070368647575378,
      "learning_rate": 0.00016122405732001753,
      "loss": 0.0385,
      "step": 66060
    },
    {
      "epoch": 19.36400937866354,
      "grad_norm": 0.6116582155227661,
      "learning_rate": 0.0001611402610324292,
      "loss": 0.0267,
      "step": 66070
    },
    {
      "epoch": 19.36694021101993,
      "grad_norm": 1.676649808883667,
      "learning_rate": 0.0001610564647448408,
      "loss": 0.0528,
      "step": 66080
    },
    {
      "epoch": 19.369871043376317,
      "grad_norm": 0.3999696969985962,
      "learning_rate": 0.00016097266845725244,
      "loss": 0.0344,
      "step": 66090
    },
    {
      "epoch": 19.37280187573271,
      "grad_norm": 0.31968599557876587,
      "learning_rate": 0.00016088887216966408,
      "loss": 0.0463,
      "step": 66100
    },
    {
      "epoch": 19.375732708089096,
      "grad_norm": 0.39680996537208557,
      "learning_rate": 0.00016080507588207572,
      "loss": 0.032,
      "step": 66110
    },
    {
      "epoch": 19.378663540445487,
      "grad_norm": 0.8035998940467834,
      "learning_rate": 0.00016072127959448735,
      "loss": 0.0362,
      "step": 66120
    },
    {
      "epoch": 19.381594372801874,
      "grad_norm": 2.2111990451812744,
      "learning_rate": 0.000160637483306899,
      "loss": 0.0339,
      "step": 66130
    },
    {
      "epoch": 19.384525205158265,
      "grad_norm": 0.33080992102622986,
      "learning_rate": 0.0001605536870193106,
      "loss": 0.0262,
      "step": 66140
    },
    {
      "epoch": 19.387456037514653,
      "grad_norm": 1.9235877990722656,
      "learning_rate": 0.00016046989073172224,
      "loss": 0.0551,
      "step": 66150
    },
    {
      "epoch": 19.390386869871044,
      "grad_norm": 3.084528684616089,
      "learning_rate": 0.00016038609444413387,
      "loss": 0.0535,
      "step": 66160
    },
    {
      "epoch": 19.39331770222743,
      "grad_norm": 0.5830543041229248,
      "learning_rate": 0.0001603022981565455,
      "loss": 0.0368,
      "step": 66170
    },
    {
      "epoch": 19.396248534583822,
      "grad_norm": 0.8968657851219177,
      "learning_rate": 0.00016021850186895715,
      "loss": 0.0489,
      "step": 66180
    },
    {
      "epoch": 19.39917936694021,
      "grad_norm": 0.7504646182060242,
      "learning_rate": 0.00016013470558136879,
      "loss": 0.0481,
      "step": 66190
    },
    {
      "epoch": 19.4021101992966,
      "grad_norm": 2.2693772315979004,
      "learning_rate": 0.00016005090929378042,
      "loss": 0.0444,
      "step": 66200
    },
    {
      "epoch": 19.40504103165299,
      "grad_norm": 0.4787943959236145,
      "learning_rate": 0.00015996711300619206,
      "loss": 0.0331,
      "step": 66210
    },
    {
      "epoch": 19.40797186400938,
      "grad_norm": 1.5431078672409058,
      "learning_rate": 0.00015988331671860367,
      "loss": 0.0447,
      "step": 66220
    },
    {
      "epoch": 19.410902696365767,
      "grad_norm": 1.6886787414550781,
      "learning_rate": 0.0001597995204310153,
      "loss": 0.0464,
      "step": 66230
    },
    {
      "epoch": 19.413833528722158,
      "grad_norm": 1.3175889253616333,
      "learning_rate": 0.00015971572414342694,
      "loss": 0.0639,
      "step": 66240
    },
    {
      "epoch": 19.416764361078545,
      "grad_norm": 1.1895339488983154,
      "learning_rate": 0.00015963192785583858,
      "loss": 0.0391,
      "step": 66250
    },
    {
      "epoch": 19.419695193434936,
      "grad_norm": 1.1336829662322998,
      "learning_rate": 0.00015954813156825022,
      "loss": 0.0453,
      "step": 66260
    },
    {
      "epoch": 19.422626025791324,
      "grad_norm": 0.996846616268158,
      "learning_rate": 0.00015946433528066185,
      "loss": 0.0594,
      "step": 66270
    },
    {
      "epoch": 19.425556858147715,
      "grad_norm": 0.24627017974853516,
      "learning_rate": 0.00015938053899307346,
      "loss": 0.064,
      "step": 66280
    },
    {
      "epoch": 19.428487690504102,
      "grad_norm": 0.7261713743209839,
      "learning_rate": 0.00015929674270548513,
      "loss": 0.0264,
      "step": 66290
    },
    {
      "epoch": 19.431418522860493,
      "grad_norm": 0.10030007362365723,
      "learning_rate": 0.00015921294641789674,
      "loss": 0.0306,
      "step": 66300
    },
    {
      "epoch": 19.43434935521688,
      "grad_norm": 1.162497639656067,
      "learning_rate": 0.00015912915013030837,
      "loss": 0.0326,
      "step": 66310
    },
    {
      "epoch": 19.437280187573272,
      "grad_norm": 0.7776024341583252,
      "learning_rate": 0.00015904535384272,
      "loss": 0.0498,
      "step": 66320
    },
    {
      "epoch": 19.44021101992966,
      "grad_norm": 1.2822818756103516,
      "learning_rate": 0.00015896155755513165,
      "loss": 0.0597,
      "step": 66330
    },
    {
      "epoch": 19.44314185228605,
      "grad_norm": 0.2321111261844635,
      "learning_rate": 0.00015887776126754328,
      "loss": 0.036,
      "step": 66340
    },
    {
      "epoch": 19.446072684642438,
      "grad_norm": 0.25710877776145935,
      "learning_rate": 0.00015879396497995492,
      "loss": 0.044,
      "step": 66350
    },
    {
      "epoch": 19.44900351699883,
      "grad_norm": 0.9871839880943298,
      "learning_rate": 0.00015871016869236653,
      "loss": 0.0479,
      "step": 66360
    },
    {
      "epoch": 19.451934349355216,
      "grad_norm": 0.19572916626930237,
      "learning_rate": 0.00015862637240477817,
      "loss": 0.04,
      "step": 66370
    },
    {
      "epoch": 19.454865181711607,
      "grad_norm": 0.8622390031814575,
      "learning_rate": 0.00015854257611718983,
      "loss": 0.0615,
      "step": 66380
    },
    {
      "epoch": 19.457796014067995,
      "grad_norm": 1.1568694114685059,
      "learning_rate": 0.00015845877982960144,
      "loss": 0.0291,
      "step": 66390
    },
    {
      "epoch": 19.460726846424386,
      "grad_norm": 2.4752297401428223,
      "learning_rate": 0.00015837498354201308,
      "loss": 0.0633,
      "step": 66400
    },
    {
      "epoch": 19.463657678780773,
      "grad_norm": 0.5510818958282471,
      "learning_rate": 0.00015829118725442471,
      "loss": 0.0335,
      "step": 66410
    },
    {
      "epoch": 19.466588511137164,
      "grad_norm": 0.19284632802009583,
      "learning_rate": 0.00015820739096683635,
      "loss": 0.0377,
      "step": 66420
    },
    {
      "epoch": 19.469519343493552,
      "grad_norm": 1.2128008604049683,
      "learning_rate": 0.000158123594679248,
      "loss": 0.0342,
      "step": 66430
    },
    {
      "epoch": 19.472450175849943,
      "grad_norm": 1.273678183555603,
      "learning_rate": 0.0001580397983916596,
      "loss": 0.0382,
      "step": 66440
    },
    {
      "epoch": 19.47538100820633,
      "grad_norm": 1.3799278736114502,
      "learning_rate": 0.00015795600210407123,
      "loss": 0.0571,
      "step": 66450
    },
    {
      "epoch": 19.47831184056272,
      "grad_norm": 0.31032130122184753,
      "learning_rate": 0.00015787220581648287,
      "loss": 0.0486,
      "step": 66460
    },
    {
      "epoch": 19.48124267291911,
      "grad_norm": 0.7725948691368103,
      "learning_rate": 0.0001577884095288945,
      "loss": 0.0454,
      "step": 66470
    },
    {
      "epoch": 19.4841735052755,
      "grad_norm": 2.9665956497192383,
      "learning_rate": 0.00015770461324130614,
      "loss": 0.0627,
      "step": 66480
    },
    {
      "epoch": 19.487104337631887,
      "grad_norm": 1.5410596132278442,
      "learning_rate": 0.00015762081695371778,
      "loss": 0.0351,
      "step": 66490
    },
    {
      "epoch": 19.49003516998828,
      "grad_norm": 1.3444849252700806,
      "learning_rate": 0.0001575370206661294,
      "loss": 0.0414,
      "step": 66500
    },
    {
      "epoch": 19.492966002344666,
      "grad_norm": 0.8991488814353943,
      "learning_rate": 0.00015745322437854105,
      "loss": 0.0507,
      "step": 66510
    },
    {
      "epoch": 19.495896834701057,
      "grad_norm": 1.2102746963500977,
      "learning_rate": 0.0001573694280909527,
      "loss": 0.0419,
      "step": 66520
    },
    {
      "epoch": 19.498827667057444,
      "grad_norm": 0.36421120166778564,
      "learning_rate": 0.0001572856318033643,
      "loss": 0.05,
      "step": 66530
    },
    {
      "epoch": 19.50175849941383,
      "grad_norm": 0.188930481672287,
      "learning_rate": 0.00015720183551577594,
      "loss": 0.0209,
      "step": 66540
    },
    {
      "epoch": 19.504689331770223,
      "grad_norm": 0.8137379288673401,
      "learning_rate": 0.00015711803922818757,
      "loss": 0.0281,
      "step": 66550
    },
    {
      "epoch": 19.50762016412661,
      "grad_norm": 1.0462228059768677,
      "learning_rate": 0.0001570342429405992,
      "loss": 0.0379,
      "step": 66560
    },
    {
      "epoch": 19.510550996483,
      "grad_norm": 0.7063676714897156,
      "learning_rate": 0.00015695044665301085,
      "loss": 0.0326,
      "step": 66570
    },
    {
      "epoch": 19.51348182883939,
      "grad_norm": 0.6536592245101929,
      "learning_rate": 0.00015686665036542249,
      "loss": 0.0584,
      "step": 66580
    },
    {
      "epoch": 19.51641266119578,
      "grad_norm": 1.06947922706604,
      "learning_rate": 0.0001567828540778341,
      "loss": 0.0459,
      "step": 66590
    },
    {
      "epoch": 19.519343493552167,
      "grad_norm": 1.3430646657943726,
      "learning_rate": 0.00015669905779024576,
      "loss": 0.0228,
      "step": 66600
    },
    {
      "epoch": 19.52227432590856,
      "grad_norm": 0.19603918492794037,
      "learning_rate": 0.00015661526150265737,
      "loss": 0.0506,
      "step": 66610
    },
    {
      "epoch": 19.525205158264946,
      "grad_norm": 0.04512479528784752,
      "learning_rate": 0.000156531465215069,
      "loss": 0.0453,
      "step": 66620
    },
    {
      "epoch": 19.528135990621337,
      "grad_norm": 0.5338731408119202,
      "learning_rate": 0.00015644766892748064,
      "loss": 0.0629,
      "step": 66630
    },
    {
      "epoch": 19.531066822977724,
      "grad_norm": 1.185410976409912,
      "learning_rate": 0.00015636387263989228,
      "loss": 0.0334,
      "step": 66640
    },
    {
      "epoch": 19.533997655334115,
      "grad_norm": 1.8851509094238281,
      "learning_rate": 0.00015628007635230392,
      "loss": 0.0406,
      "step": 66650
    },
    {
      "epoch": 19.536928487690503,
      "grad_norm": 2.0183629989624023,
      "learning_rate": 0.00015619628006471555,
      "loss": 0.0331,
      "step": 66660
    },
    {
      "epoch": 19.539859320046894,
      "grad_norm": 0.42457062005996704,
      "learning_rate": 0.00015611248377712716,
      "loss": 0.035,
      "step": 66670
    },
    {
      "epoch": 19.54279015240328,
      "grad_norm": 2.3423030376434326,
      "learning_rate": 0.0001560286874895388,
      "loss": 0.0296,
      "step": 66680
    },
    {
      "epoch": 19.545720984759672,
      "grad_norm": 0.41065555810928345,
      "learning_rate": 0.00015594489120195046,
      "loss": 0.0276,
      "step": 66690
    },
    {
      "epoch": 19.54865181711606,
      "grad_norm": 1.8669754266738892,
      "learning_rate": 0.00015586109491436207,
      "loss": 0.0584,
      "step": 66700
    },
    {
      "epoch": 19.55158264947245,
      "grad_norm": 0.6705443263053894,
      "learning_rate": 0.0001557772986267737,
      "loss": 0.0345,
      "step": 66710
    },
    {
      "epoch": 19.554513481828838,
      "grad_norm": 0.7517929673194885,
      "learning_rate": 0.00015569350233918535,
      "loss": 0.0382,
      "step": 66720
    },
    {
      "epoch": 19.55744431418523,
      "grad_norm": 0.808168888092041,
      "learning_rate": 0.00015560970605159698,
      "loss": 0.035,
      "step": 66730
    },
    {
      "epoch": 19.560375146541617,
      "grad_norm": 0.5013377070426941,
      "learning_rate": 0.00015552590976400862,
      "loss": 0.0428,
      "step": 66740
    },
    {
      "epoch": 19.563305978898008,
      "grad_norm": 1.5388914346694946,
      "learning_rate": 0.00015544211347642023,
      "loss": 0.0567,
      "step": 66750
    },
    {
      "epoch": 19.566236811254395,
      "grad_norm": 1.581073522567749,
      "learning_rate": 0.00015535831718883187,
      "loss": 0.024,
      "step": 66760
    },
    {
      "epoch": 19.569167643610786,
      "grad_norm": 1.8067996501922607,
      "learning_rate": 0.0001552745209012435,
      "loss": 0.046,
      "step": 66770
    },
    {
      "epoch": 19.572098475967174,
      "grad_norm": 3.604689121246338,
      "learning_rate": 0.00015519072461365514,
      "loss": 0.0554,
      "step": 66780
    },
    {
      "epoch": 19.575029308323565,
      "grad_norm": 0.521245002746582,
      "learning_rate": 0.00015510692832606678,
      "loss": 0.0471,
      "step": 66790
    },
    {
      "epoch": 19.577960140679952,
      "grad_norm": 1.0907853841781616,
      "learning_rate": 0.00015502313203847841,
      "loss": 0.042,
      "step": 66800
    },
    {
      "epoch": 19.580890973036343,
      "grad_norm": 0.04485354572534561,
      "learning_rate": 0.00015493933575089002,
      "loss": 0.0353,
      "step": 66810
    },
    {
      "epoch": 19.58382180539273,
      "grad_norm": 1.1339874267578125,
      "learning_rate": 0.0001548555394633017,
      "loss": 0.064,
      "step": 66820
    },
    {
      "epoch": 19.58675263774912,
      "grad_norm": 0.4698766767978668,
      "learning_rate": 0.00015477174317571332,
      "loss": 0.0506,
      "step": 66830
    },
    {
      "epoch": 19.58968347010551,
      "grad_norm": 1.405699372291565,
      "learning_rate": 0.00015468794688812493,
      "loss": 0.0738,
      "step": 66840
    },
    {
      "epoch": 19.5926143024619,
      "grad_norm": 1.879841923713684,
      "learning_rate": 0.00015460415060053657,
      "loss": 0.0521,
      "step": 66850
    },
    {
      "epoch": 19.595545134818288,
      "grad_norm": 1.982535719871521,
      "learning_rate": 0.0001545203543129482,
      "loss": 0.0341,
      "step": 66860
    },
    {
      "epoch": 19.59847596717468,
      "grad_norm": 1.5072047710418701,
      "learning_rate": 0.00015443655802535984,
      "loss": 0.0419,
      "step": 66870
    },
    {
      "epoch": 19.601406799531066,
      "grad_norm": 1.5212571620941162,
      "learning_rate": 0.00015435276173777148,
      "loss": 0.0515,
      "step": 66880
    },
    {
      "epoch": 19.604337631887457,
      "grad_norm": 1.5721328258514404,
      "learning_rate": 0.00015426896545018312,
      "loss": 0.0311,
      "step": 66890
    },
    {
      "epoch": 19.607268464243845,
      "grad_norm": 0.7322824597358704,
      "learning_rate": 0.00015418516916259473,
      "loss": 0.0454,
      "step": 66900
    },
    {
      "epoch": 19.610199296600236,
      "grad_norm": 0.36252203583717346,
      "learning_rate": 0.0001541013728750064,
      "loss": 0.0425,
      "step": 66910
    },
    {
      "epoch": 19.613130128956623,
      "grad_norm": 2.2145893573760986,
      "learning_rate": 0.000154017576587418,
      "loss": 0.0438,
      "step": 66920
    },
    {
      "epoch": 19.616060961313014,
      "grad_norm": 1.3441358804702759,
      "learning_rate": 0.00015393378029982964,
      "loss": 0.0425,
      "step": 66930
    },
    {
      "epoch": 19.6189917936694,
      "grad_norm": 1.49595308303833,
      "learning_rate": 0.00015384998401224127,
      "loss": 0.0687,
      "step": 66940
    },
    {
      "epoch": 19.621922626025793,
      "grad_norm": 1.4761667251586914,
      "learning_rate": 0.0001537661877246529,
      "loss": 0.0345,
      "step": 66950
    },
    {
      "epoch": 19.62485345838218,
      "grad_norm": 0.6107930541038513,
      "learning_rate": 0.00015368239143706455,
      "loss": 0.0431,
      "step": 66960
    },
    {
      "epoch": 19.62778429073857,
      "grad_norm": 0.6468716859817505,
      "learning_rate": 0.00015359859514947619,
      "loss": 0.0654,
      "step": 66970
    },
    {
      "epoch": 19.63071512309496,
      "grad_norm": 1.952398419380188,
      "learning_rate": 0.0001535147988618878,
      "loss": 0.0444,
      "step": 66980
    },
    {
      "epoch": 19.63364595545135,
      "grad_norm": 2.1283538341522217,
      "learning_rate": 0.00015343100257429943,
      "loss": 0.0558,
      "step": 66990
    },
    {
      "epoch": 19.636576787807737,
      "grad_norm": 1.0874487161636353,
      "learning_rate": 0.0001533472062867111,
      "loss": 0.0504,
      "step": 67000
    },
    {
      "epoch": 19.639507620164128,
      "grad_norm": 0.2775527238845825,
      "learning_rate": 0.0001532634099991227,
      "loss": 0.059,
      "step": 67010
    },
    {
      "epoch": 19.642438452520516,
      "grad_norm": 0.9939175844192505,
      "learning_rate": 0.00015317961371153434,
      "loss": 0.051,
      "step": 67020
    },
    {
      "epoch": 19.645369284876907,
      "grad_norm": 0.5974088907241821,
      "learning_rate": 0.00015309581742394598,
      "loss": 0.0452,
      "step": 67030
    },
    {
      "epoch": 19.648300117233294,
      "grad_norm": 1.750585675239563,
      "learning_rate": 0.00015301202113635762,
      "loss": 0.0656,
      "step": 67040
    },
    {
      "epoch": 19.651230949589685,
      "grad_norm": 0.6569688320159912,
      "learning_rate": 0.00015292822484876925,
      "loss": 0.0268,
      "step": 67050
    },
    {
      "epoch": 19.654161781946073,
      "grad_norm": 1.0788661241531372,
      "learning_rate": 0.00015284442856118086,
      "loss": 0.0357,
      "step": 67060
    },
    {
      "epoch": 19.657092614302464,
      "grad_norm": 0.389871209859848,
      "learning_rate": 0.0001527606322735925,
      "loss": 0.0548,
      "step": 67070
    },
    {
      "epoch": 19.66002344665885,
      "grad_norm": 0.5057292580604553,
      "learning_rate": 0.00015267683598600414,
      "loss": 0.0505,
      "step": 67080
    },
    {
      "epoch": 19.66295427901524,
      "grad_norm": 0.31689944863319397,
      "learning_rate": 0.00015259303969841577,
      "loss": 0.0542,
      "step": 67090
    },
    {
      "epoch": 19.66588511137163,
      "grad_norm": 1.209022045135498,
      "learning_rate": 0.0001525092434108274,
      "loss": 0.0591,
      "step": 67100
    },
    {
      "epoch": 19.66881594372802,
      "grad_norm": 1.2798902988433838,
      "learning_rate": 0.00015242544712323905,
      "loss": 0.055,
      "step": 67110
    },
    {
      "epoch": 19.671746776084408,
      "grad_norm": 0.9516874551773071,
      "learning_rate": 0.00015234165083565066,
      "loss": 0.0576,
      "step": 67120
    },
    {
      "epoch": 19.674677608440795,
      "grad_norm": 0.21559353172779083,
      "learning_rate": 0.00015225785454806232,
      "loss": 0.0326,
      "step": 67130
    },
    {
      "epoch": 19.677608440797187,
      "grad_norm": 1.6160051822662354,
      "learning_rate": 0.00015217405826047396,
      "loss": 0.0503,
      "step": 67140
    },
    {
      "epoch": 19.680539273153574,
      "grad_norm": 0.5828385353088379,
      "learning_rate": 0.00015209026197288557,
      "loss": 0.0396,
      "step": 67150
    },
    {
      "epoch": 19.683470105509965,
      "grad_norm": 2.0186245441436768,
      "learning_rate": 0.0001520064656852972,
      "loss": 0.0347,
      "step": 67160
    },
    {
      "epoch": 19.686400937866352,
      "grad_norm": 0.9094224572181702,
      "learning_rate": 0.00015192266939770884,
      "loss": 0.0438,
      "step": 67170
    },
    {
      "epoch": 19.689331770222744,
      "grad_norm": 1.4547820091247559,
      "learning_rate": 0.00015183887311012048,
      "loss": 0.0474,
      "step": 67180
    },
    {
      "epoch": 19.69226260257913,
      "grad_norm": 0.5540278553962708,
      "learning_rate": 0.00015175507682253211,
      "loss": 0.0511,
      "step": 67190
    },
    {
      "epoch": 19.695193434935522,
      "grad_norm": 1.526160478591919,
      "learning_rate": 0.00015167128053494375,
      "loss": 0.0447,
      "step": 67200
    },
    {
      "epoch": 19.69812426729191,
      "grad_norm": 1.5170979499816895,
      "learning_rate": 0.00015158748424735536,
      "loss": 0.0626,
      "step": 67210
    },
    {
      "epoch": 19.7010550996483,
      "grad_norm": 1.3287034034729004,
      "learning_rate": 0.00015150368795976702,
      "loss": 0.0378,
      "step": 67220
    },
    {
      "epoch": 19.703985932004688,
      "grad_norm": 0.804007887840271,
      "learning_rate": 0.00015141989167217863,
      "loss": 0.0341,
      "step": 67230
    },
    {
      "epoch": 19.70691676436108,
      "grad_norm": 1.24293053150177,
      "learning_rate": 0.00015133609538459027,
      "loss": 0.0409,
      "step": 67240
    },
    {
      "epoch": 19.709847596717466,
      "grad_norm": 1.9574698209762573,
      "learning_rate": 0.0001512522990970019,
      "loss": 0.0229,
      "step": 67250
    },
    {
      "epoch": 19.712778429073857,
      "grad_norm": 0.9463189840316772,
      "learning_rate": 0.00015116850280941354,
      "loss": 0.0317,
      "step": 67260
    },
    {
      "epoch": 19.715709261430245,
      "grad_norm": 0.898838996887207,
      "learning_rate": 0.00015108470652182518,
      "loss": 0.041,
      "step": 67270
    },
    {
      "epoch": 19.718640093786636,
      "grad_norm": 3.556133985519409,
      "learning_rate": 0.00015100091023423682,
      "loss": 0.0358,
      "step": 67280
    },
    {
      "epoch": 19.721570926143023,
      "grad_norm": 0.4396063983440399,
      "learning_rate": 0.00015091711394664843,
      "loss": 0.041,
      "step": 67290
    },
    {
      "epoch": 19.724501758499414,
      "grad_norm": 0.4566623270511627,
      "learning_rate": 0.00015083331765906006,
      "loss": 0.0407,
      "step": 67300
    },
    {
      "epoch": 19.727432590855802,
      "grad_norm": 1.6826856136322021,
      "learning_rate": 0.00015074952137147173,
      "loss": 0.0481,
      "step": 67310
    },
    {
      "epoch": 19.730363423212193,
      "grad_norm": 0.7342222929000854,
      "learning_rate": 0.00015066572508388334,
      "loss": 0.0341,
      "step": 67320
    },
    {
      "epoch": 19.73329425556858,
      "grad_norm": 0.6809529066085815,
      "learning_rate": 0.00015058192879629497,
      "loss": 0.0226,
      "step": 67330
    },
    {
      "epoch": 19.73622508792497,
      "grad_norm": 1.8789167404174805,
      "learning_rate": 0.0001504981325087066,
      "loss": 0.0338,
      "step": 67340
    },
    {
      "epoch": 19.73915592028136,
      "grad_norm": 0.9766175150871277,
      "learning_rate": 0.00015041433622111825,
      "loss": 0.0256,
      "step": 67350
    },
    {
      "epoch": 19.74208675263775,
      "grad_norm": 0.6281819343566895,
      "learning_rate": 0.00015033053993352989,
      "loss": 0.0321,
      "step": 67360
    },
    {
      "epoch": 19.745017584994137,
      "grad_norm": 0.9666949510574341,
      "learning_rate": 0.0001502467436459415,
      "loss": 0.0571,
      "step": 67370
    },
    {
      "epoch": 19.74794841735053,
      "grad_norm": 0.8637294173240662,
      "learning_rate": 0.00015016294735835313,
      "loss": 0.0511,
      "step": 67380
    },
    {
      "epoch": 19.750879249706916,
      "grad_norm": 0.852362871170044,
      "learning_rate": 0.00015007915107076477,
      "loss": 0.0437,
      "step": 67390
    },
    {
      "epoch": 19.753810082063307,
      "grad_norm": 0.12091756612062454,
      "learning_rate": 0.0001499953547831764,
      "loss": 0.0289,
      "step": 67400
    },
    {
      "epoch": 19.756740914419694,
      "grad_norm": 1.8380593061447144,
      "learning_rate": 0.00014991155849558804,
      "loss": 0.057,
      "step": 67410
    },
    {
      "epoch": 19.759671746776085,
      "grad_norm": 0.22651448845863342,
      "learning_rate": 0.00014982776220799968,
      "loss": 0.0599,
      "step": 67420
    },
    {
      "epoch": 19.762602579132473,
      "grad_norm": 0.7453606128692627,
      "learning_rate": 0.0001497439659204113,
      "loss": 0.0383,
      "step": 67430
    },
    {
      "epoch": 19.765533411488864,
      "grad_norm": 1.0493206977844238,
      "learning_rate": 0.00014966016963282295,
      "loss": 0.0326,
      "step": 67440
    },
    {
      "epoch": 19.76846424384525,
      "grad_norm": 1.0116814374923706,
      "learning_rate": 0.0001495763733452346,
      "loss": 0.0403,
      "step": 67450
    },
    {
      "epoch": 19.771395076201642,
      "grad_norm": 0.09971810132265091,
      "learning_rate": 0.0001494925770576462,
      "loss": 0.0546,
      "step": 67460
    },
    {
      "epoch": 19.77432590855803,
      "grad_norm": 1.8747791051864624,
      "learning_rate": 0.00014940878077005784,
      "loss": 0.0503,
      "step": 67470
    },
    {
      "epoch": 19.77725674091442,
      "grad_norm": 0.5217819213867188,
      "learning_rate": 0.00014932498448246947,
      "loss": 0.0377,
      "step": 67480
    },
    {
      "epoch": 19.78018757327081,
      "grad_norm": 1.1618704795837402,
      "learning_rate": 0.0001492411881948811,
      "loss": 0.0404,
      "step": 67490
    },
    {
      "epoch": 19.7831184056272,
      "grad_norm": 1.4975214004516602,
      "learning_rate": 0.00014915739190729275,
      "loss": 0.044,
      "step": 67500
    },
    {
      "epoch": 19.786049237983587,
      "grad_norm": 1.903483271598816,
      "learning_rate": 0.00014907359561970438,
      "loss": 0.05,
      "step": 67510
    },
    {
      "epoch": 19.788980070339978,
      "grad_norm": 0.16262482106685638,
      "learning_rate": 0.000148989799332116,
      "loss": 0.0479,
      "step": 67520
    },
    {
      "epoch": 19.791910902696365,
      "grad_norm": 0.5701009035110474,
      "learning_rate": 0.00014890600304452766,
      "loss": 0.0548,
      "step": 67530
    },
    {
      "epoch": 19.794841735052756,
      "grad_norm": 2.527402639389038,
      "learning_rate": 0.00014882220675693927,
      "loss": 0.0596,
      "step": 67540
    },
    {
      "epoch": 19.797772567409144,
      "grad_norm": 1.1896227598190308,
      "learning_rate": 0.0001487384104693509,
      "loss": 0.0389,
      "step": 67550
    },
    {
      "epoch": 19.800703399765535,
      "grad_norm": 1.637181282043457,
      "learning_rate": 0.00014865461418176254,
      "loss": 0.0589,
      "step": 67560
    },
    {
      "epoch": 19.803634232121922,
      "grad_norm": 0.1986645609140396,
      "learning_rate": 0.00014857081789417418,
      "loss": 0.0388,
      "step": 67570
    },
    {
      "epoch": 19.806565064478313,
      "grad_norm": 0.266731858253479,
      "learning_rate": 0.0001484870216065858,
      "loss": 0.0336,
      "step": 67580
    },
    {
      "epoch": 19.8094958968347,
      "grad_norm": 0.7067108750343323,
      "learning_rate": 0.00014840322531899745,
      "loss": 0.0268,
      "step": 67590
    },
    {
      "epoch": 19.812426729191092,
      "grad_norm": 1.1469461917877197,
      "learning_rate": 0.00014831942903140906,
      "loss": 0.0427,
      "step": 67600
    },
    {
      "epoch": 19.81535756154748,
      "grad_norm": 0.6395451426506042,
      "learning_rate": 0.0001482356327438207,
      "loss": 0.0351,
      "step": 67610
    },
    {
      "epoch": 19.81828839390387,
      "grad_norm": 3.541203498840332,
      "learning_rate": 0.00014815183645623236,
      "loss": 0.0553,
      "step": 67620
    },
    {
      "epoch": 19.821219226260258,
      "grad_norm": 0.8051596879959106,
      "learning_rate": 0.00014806804016864397,
      "loss": 0.0527,
      "step": 67630
    },
    {
      "epoch": 19.82415005861665,
      "grad_norm": 1.231253981590271,
      "learning_rate": 0.0001479842438810556,
      "loss": 0.0363,
      "step": 67640
    },
    {
      "epoch": 19.827080890973036,
      "grad_norm": 0.36885401606559753,
      "learning_rate": 0.00014790044759346724,
      "loss": 0.0499,
      "step": 67650
    },
    {
      "epoch": 19.830011723329427,
      "grad_norm": 1.00080406665802,
      "learning_rate": 0.00014781665130587888,
      "loss": 0.0357,
      "step": 67660
    },
    {
      "epoch": 19.832942555685815,
      "grad_norm": 0.6937059760093689,
      "learning_rate": 0.00014773285501829052,
      "loss": 0.0293,
      "step": 67670
    },
    {
      "epoch": 19.835873388042202,
      "grad_norm": 2.146501064300537,
      "learning_rate": 0.00014764905873070213,
      "loss": 0.0426,
      "step": 67680
    },
    {
      "epoch": 19.838804220398593,
      "grad_norm": 0.9258914589881897,
      "learning_rate": 0.00014756526244311376,
      "loss": 0.0476,
      "step": 67690
    },
    {
      "epoch": 19.84173505275498,
      "grad_norm": 0.9438823461532593,
      "learning_rate": 0.0001474814661555254,
      "loss": 0.0554,
      "step": 67700
    },
    {
      "epoch": 19.84466588511137,
      "grad_norm": 1.2047128677368164,
      "learning_rate": 0.00014739766986793704,
      "loss": 0.0438,
      "step": 67710
    },
    {
      "epoch": 19.84759671746776,
      "grad_norm": 0.7325015664100647,
      "learning_rate": 0.00014731387358034867,
      "loss": 0.036,
      "step": 67720
    },
    {
      "epoch": 19.85052754982415,
      "grad_norm": 1.053489327430725,
      "learning_rate": 0.0001472300772927603,
      "loss": 0.0389,
      "step": 67730
    },
    {
      "epoch": 19.853458382180538,
      "grad_norm": 1.2317408323287964,
      "learning_rate": 0.00014714628100517192,
      "loss": 0.036,
      "step": 67740
    },
    {
      "epoch": 19.85638921453693,
      "grad_norm": 1.7665717601776123,
      "learning_rate": 0.00014706248471758359,
      "loss": 0.0352,
      "step": 67750
    },
    {
      "epoch": 19.859320046893316,
      "grad_norm": 2.129838466644287,
      "learning_rate": 0.00014697868842999522,
      "loss": 0.025,
      "step": 67760
    },
    {
      "epoch": 19.862250879249707,
      "grad_norm": 0.37420639395713806,
      "learning_rate": 0.00014689489214240683,
      "loss": 0.0316,
      "step": 67770
    },
    {
      "epoch": 19.865181711606095,
      "grad_norm": 2.3539791107177734,
      "learning_rate": 0.00014681109585481847,
      "loss": 0.0501,
      "step": 67780
    },
    {
      "epoch": 19.868112543962486,
      "grad_norm": 0.22751881182193756,
      "learning_rate": 0.0001467272995672301,
      "loss": 0.0467,
      "step": 67790
    },
    {
      "epoch": 19.871043376318873,
      "grad_norm": 1.311574101448059,
      "learning_rate": 0.00014664350327964174,
      "loss": 0.0584,
      "step": 67800
    },
    {
      "epoch": 19.873974208675264,
      "grad_norm": 0.8968921899795532,
      "learning_rate": 0.00014655970699205338,
      "loss": 0.0375,
      "step": 67810
    },
    {
      "epoch": 19.87690504103165,
      "grad_norm": 2.00022292137146,
      "learning_rate": 0.00014647591070446502,
      "loss": 0.0371,
      "step": 67820
    },
    {
      "epoch": 19.879835873388043,
      "grad_norm": 0.09551993012428284,
      "learning_rate": 0.00014639211441687663,
      "loss": 0.0427,
      "step": 67830
    },
    {
      "epoch": 19.88276670574443,
      "grad_norm": 1.4903696775436401,
      "learning_rate": 0.0001463083181292883,
      "loss": 0.0325,
      "step": 67840
    },
    {
      "epoch": 19.88569753810082,
      "grad_norm": 1.0364940166473389,
      "learning_rate": 0.0001462245218416999,
      "loss": 0.0387,
      "step": 67850
    },
    {
      "epoch": 19.88862837045721,
      "grad_norm": 2.006194829940796,
      "learning_rate": 0.00014614072555411154,
      "loss": 0.0461,
      "step": 67860
    },
    {
      "epoch": 19.8915592028136,
      "grad_norm": 1.3088595867156982,
      "learning_rate": 0.00014605692926652317,
      "loss": 0.0514,
      "step": 67870
    },
    {
      "epoch": 19.894490035169987,
      "grad_norm": 1.5899204015731812,
      "learning_rate": 0.0001459731329789348,
      "loss": 0.0653,
      "step": 67880
    },
    {
      "epoch": 19.897420867526378,
      "grad_norm": 0.7284855842590332,
      "learning_rate": 0.00014588933669134645,
      "loss": 0.0419,
      "step": 67890
    },
    {
      "epoch": 19.900351699882766,
      "grad_norm": 0.2978993058204651,
      "learning_rate": 0.00014580554040375808,
      "loss": 0.0544,
      "step": 67900
    },
    {
      "epoch": 19.903282532239157,
      "grad_norm": 0.14782042801380157,
      "learning_rate": 0.0001457217441161697,
      "loss": 0.0365,
      "step": 67910
    },
    {
      "epoch": 19.906213364595544,
      "grad_norm": 2.126652240753174,
      "learning_rate": 0.00014563794782858133,
      "loss": 0.0464,
      "step": 67920
    },
    {
      "epoch": 19.909144196951935,
      "grad_norm": 0.08294807374477386,
      "learning_rate": 0.000145554151540993,
      "loss": 0.0591,
      "step": 67930
    },
    {
      "epoch": 19.912075029308323,
      "grad_norm": 0.44555947184562683,
      "learning_rate": 0.0001454703552534046,
      "loss": 0.0219,
      "step": 67940
    },
    {
      "epoch": 19.915005861664714,
      "grad_norm": 0.45859986543655396,
      "learning_rate": 0.00014538655896581624,
      "loss": 0.0567,
      "step": 67950
    },
    {
      "epoch": 19.9179366940211,
      "grad_norm": 1.3367959260940552,
      "learning_rate": 0.00014530276267822788,
      "loss": 0.0366,
      "step": 67960
    },
    {
      "epoch": 19.920867526377492,
      "grad_norm": 1.7132760286331177,
      "learning_rate": 0.0001452189663906395,
      "loss": 0.0631,
      "step": 67970
    },
    {
      "epoch": 19.92379835873388,
      "grad_norm": 0.8746137619018555,
      "learning_rate": 0.00014513517010305115,
      "loss": 0.0491,
      "step": 67980
    },
    {
      "epoch": 19.92672919109027,
      "grad_norm": 0.4940956234931946,
      "learning_rate": 0.00014505137381546276,
      "loss": 0.0442,
      "step": 67990
    },
    {
      "epoch": 19.929660023446658,
      "grad_norm": 0.6104592084884644,
      "learning_rate": 0.0001449675775278744,
      "loss": 0.0387,
      "step": 68000
    },
    {
      "epoch": 19.93259085580305,
      "grad_norm": 1.433945894241333,
      "learning_rate": 0.00014488378124028603,
      "loss": 0.0474,
      "step": 68010
    },
    {
      "epoch": 19.935521688159437,
      "grad_norm": 0.862643301486969,
      "learning_rate": 0.00014479998495269767,
      "loss": 0.0481,
      "step": 68020
    },
    {
      "epoch": 19.938452520515828,
      "grad_norm": 1.0259568691253662,
      "learning_rate": 0.0001447161886651093,
      "loss": 0.0398,
      "step": 68030
    },
    {
      "epoch": 19.941383352872215,
      "grad_norm": 0.3700338900089264,
      "learning_rate": 0.00014463239237752094,
      "loss": 0.0329,
      "step": 68040
    },
    {
      "epoch": 19.944314185228606,
      "grad_norm": 1.9515119791030884,
      "learning_rate": 0.00014454859608993255,
      "loss": 0.0481,
      "step": 68050
    },
    {
      "epoch": 19.947245017584994,
      "grad_norm": 0.6663853526115417,
      "learning_rate": 0.00014446479980234422,
      "loss": 0.0545,
      "step": 68060
    },
    {
      "epoch": 19.950175849941385,
      "grad_norm": 0.785936176776886,
      "learning_rate": 0.00014438100351475585,
      "loss": 0.0483,
      "step": 68070
    },
    {
      "epoch": 19.953106682297772,
      "grad_norm": 0.7969802618026733,
      "learning_rate": 0.00014429720722716746,
      "loss": 0.0416,
      "step": 68080
    },
    {
      "epoch": 19.956037514654163,
      "grad_norm": 0.09723832458257675,
      "learning_rate": 0.0001442134109395791,
      "loss": 0.0657,
      "step": 68090
    },
    {
      "epoch": 19.95896834701055,
      "grad_norm": 2.7556347846984863,
      "learning_rate": 0.00014412961465199074,
      "loss": 0.0413,
      "step": 68100
    },
    {
      "epoch": 19.96189917936694,
      "grad_norm": 1.8984242677688599,
      "learning_rate": 0.00014404581836440237,
      "loss": 0.0804,
      "step": 68110
    },
    {
      "epoch": 19.96483001172333,
      "grad_norm": 1.2490298748016357,
      "learning_rate": 0.000143962022076814,
      "loss": 0.0453,
      "step": 68120
    },
    {
      "epoch": 19.96776084407972,
      "grad_norm": 0.4661561846733093,
      "learning_rate": 0.00014387822578922565,
      "loss": 0.0363,
      "step": 68130
    },
    {
      "epoch": 19.970691676436108,
      "grad_norm": 0.9489818215370178,
      "learning_rate": 0.00014379442950163726,
      "loss": 0.0552,
      "step": 68140
    },
    {
      "epoch": 19.9736225087925,
      "grad_norm": 0.13850070536136627,
      "learning_rate": 0.00014371063321404892,
      "loss": 0.0327,
      "step": 68150
    },
    {
      "epoch": 19.976553341148886,
      "grad_norm": 1.867905855178833,
      "learning_rate": 0.00014362683692646053,
      "loss": 0.035,
      "step": 68160
    },
    {
      "epoch": 19.979484173505277,
      "grad_norm": 0.63496333360672,
      "learning_rate": 0.00014354304063887217,
      "loss": 0.0592,
      "step": 68170
    },
    {
      "epoch": 19.982415005861665,
      "grad_norm": 2.3404483795166016,
      "learning_rate": 0.0001434592443512838,
      "loss": 0.0562,
      "step": 68180
    },
    {
      "epoch": 19.985345838218056,
      "grad_norm": 0.49226048588752747,
      "learning_rate": 0.00014337544806369544,
      "loss": 0.0559,
      "step": 68190
    },
    {
      "epoch": 19.988276670574443,
      "grad_norm": 1.7372194528579712,
      "learning_rate": 0.00014329165177610708,
      "loss": 0.0494,
      "step": 68200
    },
    {
      "epoch": 19.991207502930834,
      "grad_norm": 0.09922665357589722,
      "learning_rate": 0.00014320785548851872,
      "loss": 0.0569,
      "step": 68210
    },
    {
      "epoch": 19.99413833528722,
      "grad_norm": 1.06900954246521,
      "learning_rate": 0.00014312405920093033,
      "loss": 0.0476,
      "step": 68220
    },
    {
      "epoch": 19.99706916764361,
      "grad_norm": 0.1956796795129776,
      "learning_rate": 0.00014304026291334196,
      "loss": 0.0313,
      "step": 68230
    },
    {
      "epoch": 20.0,
      "grad_norm": 3.505960464477539,
      "learning_rate": 0.0001429564666257536,
      "loss": 0.0461,
      "step": 68240
    },
    {
      "epoch": 20.0,
      "eval_accuracy": 0.7992087042532147,
      "eval_f1_macro": 0.8442337733014769,
      "eval_f1_micro": 0.8617910447761195,
      "eval_f1_weighted": 0.8594852904694286,
      "eval_loss": 0.06956317275762558,
      "eval_roc_auc": 0.9161594170752255,
      "eval_runtime": 141.6785,
      "eval_samples_per_second": 21.408,
      "eval_steps_per_second": 2.682,
      "step": 68240
    },
    {
      "epoch": 20.002930832356387,
      "grad_norm": 1.353389859199524,
      "learning_rate": 0.00014287267033816524,
      "loss": 0.0324,
      "step": 68250
    },
    {
      "epoch": 20.00586166471278,
      "grad_norm": 1.0352920293807983,
      "learning_rate": 0.00014278887405057687,
      "loss": 0.0448,
      "step": 68260
    },
    {
      "epoch": 20.008792497069166,
      "grad_norm": 0.5364607572555542,
      "learning_rate": 0.0001427050777629885,
      "loss": 0.0406,
      "step": 68270
    },
    {
      "epoch": 20.011723329425557,
      "grad_norm": 1.1069896221160889,
      "learning_rate": 0.00014262128147540015,
      "loss": 0.04,
      "step": 68280
    },
    {
      "epoch": 20.014654161781944,
      "grad_norm": 0.5650086998939514,
      "learning_rate": 0.00014253748518781178,
      "loss": 0.0409,
      "step": 68290
    },
    {
      "epoch": 20.017584994138335,
      "grad_norm": 1.8328503370285034,
      "learning_rate": 0.0001424536889002234,
      "loss": 0.0319,
      "step": 68300
    },
    {
      "epoch": 20.020515826494723,
      "grad_norm": 0.6734155416488647,
      "learning_rate": 0.00014236989261263503,
      "loss": 0.0254,
      "step": 68310
    },
    {
      "epoch": 20.023446658851114,
      "grad_norm": 1.1291143894195557,
      "learning_rate": 0.00014228609632504667,
      "loss": 0.0233,
      "step": 68320
    },
    {
      "epoch": 20.0263774912075,
      "grad_norm": 2.451831579208374,
      "learning_rate": 0.0001422023000374583,
      "loss": 0.0363,
      "step": 68330
    },
    {
      "epoch": 20.029308323563892,
      "grad_norm": 0.47248440980911255,
      "learning_rate": 0.00014211850374986994,
      "loss": 0.0456,
      "step": 68340
    },
    {
      "epoch": 20.03223915592028,
      "grad_norm": 0.600322425365448,
      "learning_rate": 0.00014203470746228158,
      "loss": 0.0585,
      "step": 68350
    },
    {
      "epoch": 20.03516998827667,
      "grad_norm": 2.6750569343566895,
      "learning_rate": 0.00014195091117469319,
      "loss": 0.0466,
      "step": 68360
    },
    {
      "epoch": 20.03810082063306,
      "grad_norm": 1.251874327659607,
      "learning_rate": 0.00014186711488710485,
      "loss": 0.0286,
      "step": 68370
    },
    {
      "epoch": 20.04103165298945,
      "grad_norm": 0.9304414987564087,
      "learning_rate": 0.0001417833185995165,
      "loss": 0.0345,
      "step": 68380
    },
    {
      "epoch": 20.043962485345837,
      "grad_norm": 0.15616554021835327,
      "learning_rate": 0.0001416995223119281,
      "loss": 0.0325,
      "step": 68390
    },
    {
      "epoch": 20.046893317702228,
      "grad_norm": 0.5945596098899841,
      "learning_rate": 0.00014161572602433973,
      "loss": 0.0319,
      "step": 68400
    },
    {
      "epoch": 20.049824150058615,
      "grad_norm": 0.8215764164924622,
      "learning_rate": 0.00014153192973675137,
      "loss": 0.0459,
      "step": 68410
    },
    {
      "epoch": 20.052754982415006,
      "grad_norm": 1.4125781059265137,
      "learning_rate": 0.000141448133449163,
      "loss": 0.0385,
      "step": 68420
    },
    {
      "epoch": 20.055685814771394,
      "grad_norm": 2.246811628341675,
      "learning_rate": 0.00014136433716157464,
      "loss": 0.0474,
      "step": 68430
    },
    {
      "epoch": 20.058616647127785,
      "grad_norm": 0.43453699350357056,
      "learning_rate": 0.00014128054087398628,
      "loss": 0.0493,
      "step": 68440
    },
    {
      "epoch": 20.061547479484172,
      "grad_norm": 0.8110420107841492,
      "learning_rate": 0.0001411967445863979,
      "loss": 0.0428,
      "step": 68450
    },
    {
      "epoch": 20.064478311840563,
      "grad_norm": 0.6342328786849976,
      "learning_rate": 0.00014111294829880955,
      "loss": 0.0461,
      "step": 68460
    },
    {
      "epoch": 20.06740914419695,
      "grad_norm": 0.6845846772193909,
      "learning_rate": 0.00014102915201122116,
      "loss": 0.0472,
      "step": 68470
    },
    {
      "epoch": 20.070339976553342,
      "grad_norm": 0.41554197669029236,
      "learning_rate": 0.0001409453557236328,
      "loss": 0.0448,
      "step": 68480
    },
    {
      "epoch": 20.07327080890973,
      "grad_norm": 1.2237294912338257,
      "learning_rate": 0.00014086155943604444,
      "loss": 0.0393,
      "step": 68490
    },
    {
      "epoch": 20.07620164126612,
      "grad_norm": 0.38268008828163147,
      "learning_rate": 0.00014077776314845607,
      "loss": 0.0504,
      "step": 68500
    },
    {
      "epoch": 20.079132473622508,
      "grad_norm": 2.2577977180480957,
      "learning_rate": 0.0001406939668608677,
      "loss": 0.0518,
      "step": 68510
    },
    {
      "epoch": 20.0820633059789,
      "grad_norm": 0.6621683835983276,
      "learning_rate": 0.00014061017057327935,
      "loss": 0.0543,
      "step": 68520
    },
    {
      "epoch": 20.084994138335286,
      "grad_norm": 0.2747795581817627,
      "learning_rate": 0.00014052637428569096,
      "loss": 0.0302,
      "step": 68530
    },
    {
      "epoch": 20.087924970691677,
      "grad_norm": 1.9435657262802124,
      "learning_rate": 0.0001404425779981026,
      "loss": 0.0433,
      "step": 68540
    },
    {
      "epoch": 20.090855803048065,
      "grad_norm": 1.0684183835983276,
      "learning_rate": 0.00014035878171051423,
      "loss": 0.0583,
      "step": 68550
    },
    {
      "epoch": 20.093786635404456,
      "grad_norm": 1.1774389743804932,
      "learning_rate": 0.00014027498542292587,
      "loss": 0.0394,
      "step": 68560
    },
    {
      "epoch": 20.096717467760843,
      "grad_norm": 0.8113527894020081,
      "learning_rate": 0.0001401911891353375,
      "loss": 0.0666,
      "step": 68570
    },
    {
      "epoch": 20.099648300117234,
      "grad_norm": 0.31577029824256897,
      "learning_rate": 0.00014010739284774914,
      "loss": 0.041,
      "step": 68580
    },
    {
      "epoch": 20.102579132473622,
      "grad_norm": 1.0028996467590332,
      "learning_rate": 0.00014002359656016078,
      "loss": 0.0365,
      "step": 68590
    },
    {
      "epoch": 20.105509964830013,
      "grad_norm": 0.22481290996074677,
      "learning_rate": 0.00013993980027257242,
      "loss": 0.0215,
      "step": 68600
    },
    {
      "epoch": 20.1084407971864,
      "grad_norm": 0.6547067165374756,
      "learning_rate": 0.00013985600398498403,
      "loss": 0.0266,
      "step": 68610
    },
    {
      "epoch": 20.11137162954279,
      "grad_norm": 0.3886353373527527,
      "learning_rate": 0.00013977220769739566,
      "loss": 0.0541,
      "step": 68620
    },
    {
      "epoch": 20.11430246189918,
      "grad_norm": 0.21409468352794647,
      "learning_rate": 0.0001396884114098073,
      "loss": 0.0252,
      "step": 68630
    },
    {
      "epoch": 20.11723329425557,
      "grad_norm": 0.5080288052558899,
      "learning_rate": 0.00013960461512221894,
      "loss": 0.0537,
      "step": 68640
    },
    {
      "epoch": 20.120164126611957,
      "grad_norm": 0.7665597796440125,
      "learning_rate": 0.00013952081883463057,
      "loss": 0.0337,
      "step": 68650
    },
    {
      "epoch": 20.12309495896835,
      "grad_norm": 1.212605595588684,
      "learning_rate": 0.0001394370225470422,
      "loss": 0.0345,
      "step": 68660
    },
    {
      "epoch": 20.126025791324736,
      "grad_norm": 0.5225138664245605,
      "learning_rate": 0.00013935322625945382,
      "loss": 0.035,
      "step": 68670
    },
    {
      "epoch": 20.128956623681127,
      "grad_norm": 0.9896294474601746,
      "learning_rate": 0.00013926942997186548,
      "loss": 0.0347,
      "step": 68680
    },
    {
      "epoch": 20.131887456037514,
      "grad_norm": 0.21161068975925446,
      "learning_rate": 0.00013918563368427712,
      "loss": 0.0283,
      "step": 68690
    },
    {
      "epoch": 20.134818288393905,
      "grad_norm": 0.8375950455665588,
      "learning_rate": 0.00013910183739668873,
      "loss": 0.0413,
      "step": 68700
    },
    {
      "epoch": 20.137749120750293,
      "grad_norm": 1.132400631904602,
      "learning_rate": 0.00013901804110910037,
      "loss": 0.031,
      "step": 68710
    },
    {
      "epoch": 20.140679953106684,
      "grad_norm": 2.5260939598083496,
      "learning_rate": 0.000138934244821512,
      "loss": 0.0525,
      "step": 68720
    },
    {
      "epoch": 20.14361078546307,
      "grad_norm": 2.281830072402954,
      "learning_rate": 0.00013885044853392364,
      "loss": 0.0464,
      "step": 68730
    },
    {
      "epoch": 20.146541617819462,
      "grad_norm": 1.1442970037460327,
      "learning_rate": 0.00013876665224633528,
      "loss": 0.0437,
      "step": 68740
    },
    {
      "epoch": 20.14947245017585,
      "grad_norm": 2.224468469619751,
      "learning_rate": 0.00013868285595874689,
      "loss": 0.0366,
      "step": 68750
    },
    {
      "epoch": 20.15240328253224,
      "grad_norm": 0.5923691391944885,
      "learning_rate": 0.00013859905967115852,
      "loss": 0.0355,
      "step": 68760
    },
    {
      "epoch": 20.15533411488863,
      "grad_norm": 1.803719401359558,
      "learning_rate": 0.00013851526338357016,
      "loss": 0.0514,
      "step": 68770
    },
    {
      "epoch": 20.15826494724502,
      "grad_norm": 2.153970956802368,
      "learning_rate": 0.0001384314670959818,
      "loss": 0.032,
      "step": 68780
    },
    {
      "epoch": 20.161195779601407,
      "grad_norm": 0.7750343084335327,
      "learning_rate": 0.00013834767080839343,
      "loss": 0.0339,
      "step": 68790
    },
    {
      "epoch": 20.164126611957798,
      "grad_norm": 0.8913909792900085,
      "learning_rate": 0.00013826387452080507,
      "loss": 0.0398,
      "step": 68800
    },
    {
      "epoch": 20.167057444314185,
      "grad_norm": 0.1013428270816803,
      "learning_rate": 0.0001381800782332167,
      "loss": 0.048,
      "step": 68810
    },
    {
      "epoch": 20.169988276670573,
      "grad_norm": 1.1340292692184448,
      "learning_rate": 0.00013809628194562834,
      "loss": 0.0413,
      "step": 68820
    },
    {
      "epoch": 20.172919109026964,
      "grad_norm": 1.638576626777649,
      "learning_rate": 0.00013801248565803998,
      "loss": 0.0446,
      "step": 68830
    },
    {
      "epoch": 20.17584994138335,
      "grad_norm": 0.9599676728248596,
      "learning_rate": 0.0001379286893704516,
      "loss": 0.0419,
      "step": 68840
    },
    {
      "epoch": 20.178780773739742,
      "grad_norm": 0.7463927865028381,
      "learning_rate": 0.00013784489308286323,
      "loss": 0.0425,
      "step": 68850
    },
    {
      "epoch": 20.18171160609613,
      "grad_norm": 0.957107424736023,
      "learning_rate": 0.00013776109679527486,
      "loss": 0.0415,
      "step": 68860
    },
    {
      "epoch": 20.18464243845252,
      "grad_norm": 0.7088214159011841,
      "learning_rate": 0.0001376773005076865,
      "loss": 0.0341,
      "step": 68870
    },
    {
      "epoch": 20.187573270808908,
      "grad_norm": 0.6861928105354309,
      "learning_rate": 0.00013759350422009814,
      "loss": 0.0404,
      "step": 68880
    },
    {
      "epoch": 20.1905041031653,
      "grad_norm": 0.675982654094696,
      "learning_rate": 0.00013750970793250977,
      "loss": 0.0307,
      "step": 68890
    },
    {
      "epoch": 20.193434935521687,
      "grad_norm": 1.4828295707702637,
      "learning_rate": 0.0001374259116449214,
      "loss": 0.0445,
      "step": 68900
    },
    {
      "epoch": 20.196365767878078,
      "grad_norm": 0.6535727381706238,
      "learning_rate": 0.00013734211535733305,
      "loss": 0.0465,
      "step": 68910
    },
    {
      "epoch": 20.199296600234465,
      "grad_norm": 0.7771033048629761,
      "learning_rate": 0.00013725831906974466,
      "loss": 0.0268,
      "step": 68920
    },
    {
      "epoch": 20.202227432590856,
      "grad_norm": 1.8736302852630615,
      "learning_rate": 0.0001371745227821563,
      "loss": 0.0542,
      "step": 68930
    },
    {
      "epoch": 20.205158264947244,
      "grad_norm": 1.3095225095748901,
      "learning_rate": 0.00013709072649456793,
      "loss": 0.0372,
      "step": 68940
    },
    {
      "epoch": 20.208089097303635,
      "grad_norm": 0.8191419243812561,
      "learning_rate": 0.00013700693020697957,
      "loss": 0.0481,
      "step": 68950
    },
    {
      "epoch": 20.211019929660022,
      "grad_norm": 2.379624128341675,
      "learning_rate": 0.0001369231339193912,
      "loss": 0.0357,
      "step": 68960
    },
    {
      "epoch": 20.213950762016413,
      "grad_norm": 3.783384323120117,
      "learning_rate": 0.00013683933763180284,
      "loss": 0.048,
      "step": 68970
    },
    {
      "epoch": 20.2168815943728,
      "grad_norm": 1.2832112312316895,
      "learning_rate": 0.00013675554134421445,
      "loss": 0.0369,
      "step": 68980
    },
    {
      "epoch": 20.21981242672919,
      "grad_norm": 0.20521721243858337,
      "learning_rate": 0.0001366717450566261,
      "loss": 0.0422,
      "step": 68990
    },
    {
      "epoch": 20.22274325908558,
      "grad_norm": 1.04669189453125,
      "learning_rate": 0.00013658794876903775,
      "loss": 0.036,
      "step": 69000
    },
    {
      "epoch": 20.22567409144197,
      "grad_norm": 0.5369879007339478,
      "learning_rate": 0.00013650415248144936,
      "loss": 0.0262,
      "step": 69010
    },
    {
      "epoch": 20.228604923798358,
      "grad_norm": 1.8576669692993164,
      "learning_rate": 0.000136420356193861,
      "loss": 0.0342,
      "step": 69020
    },
    {
      "epoch": 20.23153575615475,
      "grad_norm": 1.4079338312149048,
      "learning_rate": 0.00013633655990627264,
      "loss": 0.061,
      "step": 69030
    },
    {
      "epoch": 20.234466588511136,
      "grad_norm": 1.5308151245117188,
      "learning_rate": 0.00013625276361868427,
      "loss": 0.0432,
      "step": 69040
    },
    {
      "epoch": 20.237397420867527,
      "grad_norm": 0.8615121841430664,
      "learning_rate": 0.0001361689673310959,
      "loss": 0.0226,
      "step": 69050
    },
    {
      "epoch": 20.240328253223915,
      "grad_norm": 1.953554391860962,
      "learning_rate": 0.00013608517104350752,
      "loss": 0.0443,
      "step": 69060
    },
    {
      "epoch": 20.243259085580306,
      "grad_norm": 1.2472633123397827,
      "learning_rate": 0.00013600137475591916,
      "loss": 0.0552,
      "step": 69070
    },
    {
      "epoch": 20.246189917936693,
      "grad_norm": 0.6411824226379395,
      "learning_rate": 0.0001359175784683308,
      "loss": 0.0419,
      "step": 69080
    },
    {
      "epoch": 20.249120750293084,
      "grad_norm": 1.0651670694351196,
      "learning_rate": 0.00013583378218074243,
      "loss": 0.0482,
      "step": 69090
    },
    {
      "epoch": 20.25205158264947,
      "grad_norm": 0.8199476003646851,
      "learning_rate": 0.00013574998589315407,
      "loss": 0.0402,
      "step": 69100
    },
    {
      "epoch": 20.254982415005863,
      "grad_norm": 2.4048421382904053,
      "learning_rate": 0.0001356661896055657,
      "loss": 0.0399,
      "step": 69110
    },
    {
      "epoch": 20.25791324736225,
      "grad_norm": 1.9682344198226929,
      "learning_rate": 0.00013558239331797734,
      "loss": 0.0413,
      "step": 69120
    },
    {
      "epoch": 20.26084407971864,
      "grad_norm": 0.7959736585617065,
      "learning_rate": 0.00013549859703038898,
      "loss": 0.024,
      "step": 69130
    },
    {
      "epoch": 20.26377491207503,
      "grad_norm": 0.6918814182281494,
      "learning_rate": 0.0001354148007428006,
      "loss": 0.0537,
      "step": 69140
    },
    {
      "epoch": 20.26670574443142,
      "grad_norm": 0.7920053005218506,
      "learning_rate": 0.00013533100445521222,
      "loss": 0.073,
      "step": 69150
    },
    {
      "epoch": 20.269636576787807,
      "grad_norm": 1.5823454856872559,
      "learning_rate": 0.00013524720816762386,
      "loss": 0.0381,
      "step": 69160
    },
    {
      "epoch": 20.272567409144198,
      "grad_norm": 0.059979550540447235,
      "learning_rate": 0.0001351634118800355,
      "loss": 0.0263,
      "step": 69170
    },
    {
      "epoch": 20.275498241500586,
      "grad_norm": 1.7331804037094116,
      "learning_rate": 0.00013507961559244713,
      "loss": 0.0389,
      "step": 69180
    },
    {
      "epoch": 20.278429073856977,
      "grad_norm": 0.641001284122467,
      "learning_rate": 0.00013499581930485877,
      "loss": 0.0248,
      "step": 69190
    },
    {
      "epoch": 20.281359906213364,
      "grad_norm": 1.0626459121704102,
      "learning_rate": 0.0001349120230172704,
      "loss": 0.0356,
      "step": 69200
    },
    {
      "epoch": 20.284290738569755,
      "grad_norm": 0.9267736673355103,
      "learning_rate": 0.00013482822672968204,
      "loss": 0.0371,
      "step": 69210
    },
    {
      "epoch": 20.287221570926143,
      "grad_norm": 0.6256147027015686,
      "learning_rate": 0.00013474443044209368,
      "loss": 0.0254,
      "step": 69220
    },
    {
      "epoch": 20.290152403282534,
      "grad_norm": 0.17884714901447296,
      "learning_rate": 0.0001346606341545053,
      "loss": 0.041,
      "step": 69230
    },
    {
      "epoch": 20.29308323563892,
      "grad_norm": 0.6289978623390198,
      "learning_rate": 0.00013457683786691693,
      "loss": 0.0445,
      "step": 69240
    },
    {
      "epoch": 20.296014067995312,
      "grad_norm": 1.889480710029602,
      "learning_rate": 0.00013449304157932856,
      "loss": 0.0341,
      "step": 69250
    },
    {
      "epoch": 20.2989449003517,
      "grad_norm": 1.4557379484176636,
      "learning_rate": 0.0001344092452917402,
      "loss": 0.0246,
      "step": 69260
    },
    {
      "epoch": 20.30187573270809,
      "grad_norm": 1.3188633918762207,
      "learning_rate": 0.00013432544900415184,
      "loss": 0.0315,
      "step": 69270
    },
    {
      "epoch": 20.304806565064478,
      "grad_norm": 0.12144853919744492,
      "learning_rate": 0.00013424165271656347,
      "loss": 0.0187,
      "step": 69280
    },
    {
      "epoch": 20.30773739742087,
      "grad_norm": 1.526610255241394,
      "learning_rate": 0.00013415785642897508,
      "loss": 0.0309,
      "step": 69290
    },
    {
      "epoch": 20.310668229777256,
      "grad_norm": 0.5597758293151855,
      "learning_rate": 0.00013407406014138672,
      "loss": 0.0497,
      "step": 69300
    },
    {
      "epoch": 20.313599062133648,
      "grad_norm": 0.37851953506469727,
      "learning_rate": 0.00013399026385379838,
      "loss": 0.0536,
      "step": 69310
    },
    {
      "epoch": 20.316529894490035,
      "grad_norm": 1.0245716571807861,
      "learning_rate": 0.00013390646756621,
      "loss": 0.0428,
      "step": 69320
    },
    {
      "epoch": 20.319460726846426,
      "grad_norm": 0.9238138198852539,
      "learning_rate": 0.00013382267127862163,
      "loss": 0.0472,
      "step": 69330
    },
    {
      "epoch": 20.322391559202813,
      "grad_norm": 1.0544344186782837,
      "learning_rate": 0.00013373887499103327,
      "loss": 0.0439,
      "step": 69340
    },
    {
      "epoch": 20.325322391559205,
      "grad_norm": 0.34005117416381836,
      "learning_rate": 0.0001336550787034449,
      "loss": 0.0296,
      "step": 69350
    },
    {
      "epoch": 20.328253223915592,
      "grad_norm": 1.3688288927078247,
      "learning_rate": 0.00013357128241585654,
      "loss": 0.0495,
      "step": 69360
    },
    {
      "epoch": 20.33118405627198,
      "grad_norm": 0.9963445663452148,
      "learning_rate": 0.00013348748612826815,
      "loss": 0.05,
      "step": 69370
    },
    {
      "epoch": 20.33411488862837,
      "grad_norm": 0.7314379811286926,
      "learning_rate": 0.0001334036898406798,
      "loss": 0.0296,
      "step": 69380
    },
    {
      "epoch": 20.337045720984758,
      "grad_norm": 0.46546483039855957,
      "learning_rate": 0.00013331989355309142,
      "loss": 0.0648,
      "step": 69390
    },
    {
      "epoch": 20.33997655334115,
      "grad_norm": 0.1570601612329483,
      "learning_rate": 0.00013323609726550306,
      "loss": 0.0442,
      "step": 69400
    },
    {
      "epoch": 20.342907385697536,
      "grad_norm": 0.36855730414390564,
      "learning_rate": 0.0001331523009779147,
      "loss": 0.0333,
      "step": 69410
    },
    {
      "epoch": 20.345838218053927,
      "grad_norm": 1.3044731616973877,
      "learning_rate": 0.00013306850469032634,
      "loss": 0.0592,
      "step": 69420
    },
    {
      "epoch": 20.348769050410315,
      "grad_norm": 0.6803382039070129,
      "learning_rate": 0.00013298470840273797,
      "loss": 0.0325,
      "step": 69430
    },
    {
      "epoch": 20.351699882766706,
      "grad_norm": 1.378997802734375,
      "learning_rate": 0.0001329009121151496,
      "loss": 0.0392,
      "step": 69440
    },
    {
      "epoch": 20.354630715123093,
      "grad_norm": 0.5801267623901367,
      "learning_rate": 0.00013281711582756125,
      "loss": 0.0338,
      "step": 69450
    },
    {
      "epoch": 20.357561547479484,
      "grad_norm": 3.6535918712615967,
      "learning_rate": 0.00013273331953997286,
      "loss": 0.0549,
      "step": 69460
    },
    {
      "epoch": 20.360492379835872,
      "grad_norm": 1.531502604484558,
      "learning_rate": 0.0001326495232523845,
      "loss": 0.0571,
      "step": 69470
    },
    {
      "epoch": 20.363423212192263,
      "grad_norm": 0.3505154252052307,
      "learning_rate": 0.00013256572696479613,
      "loss": 0.0294,
      "step": 69480
    },
    {
      "epoch": 20.36635404454865,
      "grad_norm": 1.3302738666534424,
      "learning_rate": 0.00013248193067720777,
      "loss": 0.0623,
      "step": 69490
    },
    {
      "epoch": 20.36928487690504,
      "grad_norm": 2.597379207611084,
      "learning_rate": 0.0001323981343896194,
      "loss": 0.0718,
      "step": 69500
    },
    {
      "epoch": 20.37221570926143,
      "grad_norm": 1.1276053190231323,
      "learning_rate": 0.00013231433810203104,
      "loss": 0.0221,
      "step": 69510
    },
    {
      "epoch": 20.37514654161782,
      "grad_norm": 0.6385917663574219,
      "learning_rate": 0.00013223054181444265,
      "loss": 0.0273,
      "step": 69520
    },
    {
      "epoch": 20.378077373974207,
      "grad_norm": 0.03231051191687584,
      "learning_rate": 0.0001321467455268543,
      "loss": 0.034,
      "step": 69530
    },
    {
      "epoch": 20.3810082063306,
      "grad_norm": 0.44974714517593384,
      "learning_rate": 0.00013206294923926592,
      "loss": 0.027,
      "step": 69540
    },
    {
      "epoch": 20.383939038686986,
      "grad_norm": 1.4019732475280762,
      "learning_rate": 0.00013197915295167756,
      "loss": 0.0263,
      "step": 69550
    },
    {
      "epoch": 20.386869871043377,
      "grad_norm": 1.100238561630249,
      "learning_rate": 0.0001318953566640892,
      "loss": 0.043,
      "step": 69560
    },
    {
      "epoch": 20.389800703399764,
      "grad_norm": 0.9700995087623596,
      "learning_rate": 0.00013181156037650083,
      "loss": 0.0448,
      "step": 69570
    },
    {
      "epoch": 20.392731535756155,
      "grad_norm": 0.5828493237495422,
      "learning_rate": 0.00013172776408891247,
      "loss": 0.032,
      "step": 69580
    },
    {
      "epoch": 20.395662368112543,
      "grad_norm": 2.26412034034729,
      "learning_rate": 0.0001316439678013241,
      "loss": 0.0434,
      "step": 69590
    },
    {
      "epoch": 20.398593200468934,
      "grad_norm": 1.1686136722564697,
      "learning_rate": 0.00013156017151373572,
      "loss": 0.051,
      "step": 69600
    },
    {
      "epoch": 20.40152403282532,
      "grad_norm": 0.7541450262069702,
      "learning_rate": 0.00013147637522614735,
      "loss": 0.0275,
      "step": 69610
    },
    {
      "epoch": 20.404454865181712,
      "grad_norm": 0.458760529756546,
      "learning_rate": 0.00013139257893855902,
      "loss": 0.0383,
      "step": 69620
    },
    {
      "epoch": 20.4073856975381,
      "grad_norm": 0.02890508994460106,
      "learning_rate": 0.00013130878265097063,
      "loss": 0.0466,
      "step": 69630
    },
    {
      "epoch": 20.41031652989449,
      "grad_norm": 1.388089895248413,
      "learning_rate": 0.00013122498636338226,
      "loss": 0.0337,
      "step": 69640
    },
    {
      "epoch": 20.41324736225088,
      "grad_norm": 0.44780850410461426,
      "learning_rate": 0.0001311411900757939,
      "loss": 0.0438,
      "step": 69650
    },
    {
      "epoch": 20.41617819460727,
      "grad_norm": 0.6282414793968201,
      "learning_rate": 0.00013105739378820554,
      "loss": 0.041,
      "step": 69660
    },
    {
      "epoch": 20.419109026963657,
      "grad_norm": 1.854477882385254,
      "learning_rate": 0.00013097359750061717,
      "loss": 0.0467,
      "step": 69670
    },
    {
      "epoch": 20.422039859320048,
      "grad_norm": 0.5380657315254211,
      "learning_rate": 0.00013088980121302878,
      "loss": 0.0192,
      "step": 69680
    },
    {
      "epoch": 20.424970691676435,
      "grad_norm": 1.0149612426757812,
      "learning_rate": 0.00013080600492544042,
      "loss": 0.0461,
      "step": 69690
    },
    {
      "epoch": 20.427901524032826,
      "grad_norm": 0.06527762115001678,
      "learning_rate": 0.00013072220863785206,
      "loss": 0.0412,
      "step": 69700
    },
    {
      "epoch": 20.430832356389214,
      "grad_norm": 0.5552666783332825,
      "learning_rate": 0.0001306384123502637,
      "loss": 0.0343,
      "step": 69710
    },
    {
      "epoch": 20.433763188745605,
      "grad_norm": 0.5150198936462402,
      "learning_rate": 0.00013055461606267533,
      "loss": 0.0328,
      "step": 69720
    },
    {
      "epoch": 20.436694021101992,
      "grad_norm": 1.2829707860946655,
      "learning_rate": 0.00013047081977508697,
      "loss": 0.0423,
      "step": 69730
    },
    {
      "epoch": 20.439624853458383,
      "grad_norm": 0.6276185512542725,
      "learning_rate": 0.00013038702348749858,
      "loss": 0.0381,
      "step": 69740
    },
    {
      "epoch": 20.44255568581477,
      "grad_norm": 1.6655346155166626,
      "learning_rate": 0.00013030322719991024,
      "loss": 0.0411,
      "step": 69750
    },
    {
      "epoch": 20.445486518171162,
      "grad_norm": 1.3457869291305542,
      "learning_rate": 0.00013021943091232188,
      "loss": 0.0336,
      "step": 69760
    },
    {
      "epoch": 20.44841735052755,
      "grad_norm": 1.2776427268981934,
      "learning_rate": 0.0001301356346247335,
      "loss": 0.0401,
      "step": 69770
    },
    {
      "epoch": 20.45134818288394,
      "grad_norm": 0.596689760684967,
      "learning_rate": 0.00013005183833714512,
      "loss": 0.0214,
      "step": 69780
    },
    {
      "epoch": 20.454279015240328,
      "grad_norm": 1.045940637588501,
      "learning_rate": 0.00012996804204955676,
      "loss": 0.0453,
      "step": 69790
    },
    {
      "epoch": 20.45720984759672,
      "grad_norm": 1.0978883504867554,
      "learning_rate": 0.0001298842457619684,
      "loss": 0.0452,
      "step": 69800
    },
    {
      "epoch": 20.460140679953106,
      "grad_norm": 0.034147728234529495,
      "learning_rate": 0.00012980044947438004,
      "loss": 0.0412,
      "step": 69810
    },
    {
      "epoch": 20.463071512309497,
      "grad_norm": 0.7514634728431702,
      "learning_rate": 0.00012971665318679167,
      "loss": 0.0359,
      "step": 69820
    },
    {
      "epoch": 20.466002344665885,
      "grad_norm": 0.8196475505828857,
      "learning_rate": 0.00012963285689920328,
      "loss": 0.0378,
      "step": 69830
    },
    {
      "epoch": 20.468933177022276,
      "grad_norm": 2.1364622116088867,
      "learning_rate": 0.00012954906061161495,
      "loss": 0.0415,
      "step": 69840
    },
    {
      "epoch": 20.471864009378663,
      "grad_norm": 0.7930635213851929,
      "learning_rate": 0.00012946526432402656,
      "loss": 0.025,
      "step": 69850
    },
    {
      "epoch": 20.474794841735054,
      "grad_norm": 1.2924710512161255,
      "learning_rate": 0.0001293814680364382,
      "loss": 0.0376,
      "step": 69860
    },
    {
      "epoch": 20.47772567409144,
      "grad_norm": 0.6723520159721375,
      "learning_rate": 0.00012929767174884983,
      "loss": 0.0327,
      "step": 69870
    },
    {
      "epoch": 20.480656506447833,
      "grad_norm": 0.9838013648986816,
      "learning_rate": 0.00012921387546126147,
      "loss": 0.0402,
      "step": 69880
    },
    {
      "epoch": 20.48358733880422,
      "grad_norm": 0.5253971815109253,
      "learning_rate": 0.0001291300791736731,
      "loss": 0.0443,
      "step": 69890
    },
    {
      "epoch": 20.48651817116061,
      "grad_norm": 0.5994189977645874,
      "learning_rate": 0.00012904628288608474,
      "loss": 0.0351,
      "step": 69900
    },
    {
      "epoch": 20.489449003517,
      "grad_norm": 1.558596134185791,
      "learning_rate": 0.00012896248659849635,
      "loss": 0.0485,
      "step": 69910
    },
    {
      "epoch": 20.49237983587339,
      "grad_norm": 1.4704258441925049,
      "learning_rate": 0.00012887869031090799,
      "loss": 0.0315,
      "step": 69920
    },
    {
      "epoch": 20.495310668229777,
      "grad_norm": 0.2567942142486572,
      "learning_rate": 0.00012879489402331965,
      "loss": 0.0407,
      "step": 69930
    },
    {
      "epoch": 20.49824150058617,
      "grad_norm": 1.180918574333191,
      "learning_rate": 0.00012871109773573126,
      "loss": 0.0505,
      "step": 69940
    },
    {
      "epoch": 20.501172332942556,
      "grad_norm": 0.5744313597679138,
      "learning_rate": 0.0001286273014481429,
      "loss": 0.0378,
      "step": 69950
    },
    {
      "epoch": 20.504103165298943,
      "grad_norm": 0.09986551851034164,
      "learning_rate": 0.00012854350516055453,
      "loss": 0.0207,
      "step": 69960
    },
    {
      "epoch": 20.507033997655334,
      "grad_norm": 0.19048915803432465,
      "learning_rate": 0.00012845970887296617,
      "loss": 0.0476,
      "step": 69970
    },
    {
      "epoch": 20.50996483001172,
      "grad_norm": 1.930239200592041,
      "learning_rate": 0.0001283759125853778,
      "loss": 0.0424,
      "step": 69980
    },
    {
      "epoch": 20.512895662368113,
      "grad_norm": 0.5998212695121765,
      "learning_rate": 0.00012829211629778942,
      "loss": 0.0452,
      "step": 69990
    },
    {
      "epoch": 20.5158264947245,
      "grad_norm": 0.44083648920059204,
      "learning_rate": 0.00012820832001020105,
      "loss": 0.0384,
      "step": 70000
    },
    {
      "epoch": 20.51875732708089,
      "grad_norm": 3.4644930362701416,
      "learning_rate": 0.0001281245237226127,
      "loss": 0.0396,
      "step": 70010
    },
    {
      "epoch": 20.52168815943728,
      "grad_norm": 1.5308476686477661,
      "learning_rate": 0.00012804072743502433,
      "loss": 0.0524,
      "step": 70020
    },
    {
      "epoch": 20.52461899179367,
      "grad_norm": 0.2455011010169983,
      "learning_rate": 0.00012795693114743596,
      "loss": 0.0402,
      "step": 70030
    },
    {
      "epoch": 20.527549824150057,
      "grad_norm": 1.137001872062683,
      "learning_rate": 0.0001278731348598476,
      "loss": 0.0403,
      "step": 70040
    },
    {
      "epoch": 20.530480656506448,
      "grad_norm": 2.3882601261138916,
      "learning_rate": 0.0001277893385722592,
      "loss": 0.0555,
      "step": 70050
    },
    {
      "epoch": 20.533411488862836,
      "grad_norm": 0.10428991168737411,
      "learning_rate": 0.00012770554228467087,
      "loss": 0.035,
      "step": 70060
    },
    {
      "epoch": 20.536342321219227,
      "grad_norm": 0.382045179605484,
      "learning_rate": 0.0001276217459970825,
      "loss": 0.0432,
      "step": 70070
    },
    {
      "epoch": 20.539273153575614,
      "grad_norm": 1.066462516784668,
      "learning_rate": 0.00012753794970949412,
      "loss": 0.0224,
      "step": 70080
    },
    {
      "epoch": 20.542203985932005,
      "grad_norm": 1.9785188436508179,
      "learning_rate": 0.00012745415342190576,
      "loss": 0.0477,
      "step": 70090
    },
    {
      "epoch": 20.545134818288393,
      "grad_norm": 0.3923071324825287,
      "learning_rate": 0.0001273703571343174,
      "loss": 0.0408,
      "step": 70100
    },
    {
      "epoch": 20.548065650644784,
      "grad_norm": 1.6035077571868896,
      "learning_rate": 0.00012728656084672903,
      "loss": 0.0251,
      "step": 70110
    },
    {
      "epoch": 20.55099648300117,
      "grad_norm": 1.0352727174758911,
      "learning_rate": 0.00012720276455914067,
      "loss": 0.0442,
      "step": 70120
    },
    {
      "epoch": 20.553927315357562,
      "grad_norm": 0.9427557587623596,
      "learning_rate": 0.0001271189682715523,
      "loss": 0.0357,
      "step": 70130
    },
    {
      "epoch": 20.55685814771395,
      "grad_norm": 0.6178816556930542,
      "learning_rate": 0.00012703517198396391,
      "loss": 0.0303,
      "step": 70140
    },
    {
      "epoch": 20.55978898007034,
      "grad_norm": 0.14076213538646698,
      "learning_rate": 0.00012695137569637558,
      "loss": 0.0496,
      "step": 70150
    },
    {
      "epoch": 20.562719812426728,
      "grad_norm": 0.061753805726766586,
      "learning_rate": 0.0001268675794087872,
      "loss": 0.0271,
      "step": 70160
    },
    {
      "epoch": 20.56565064478312,
      "grad_norm": 0.8103387355804443,
      "learning_rate": 0.00012678378312119882,
      "loss": 0.0468,
      "step": 70170
    },
    {
      "epoch": 20.568581477139507,
      "grad_norm": 0.7156089544296265,
      "learning_rate": 0.00012669998683361046,
      "loss": 0.031,
      "step": 70180
    },
    {
      "epoch": 20.571512309495898,
      "grad_norm": 0.76472008228302,
      "learning_rate": 0.0001266161905460221,
      "loss": 0.045,
      "step": 70190
    },
    {
      "epoch": 20.574443141852285,
      "grad_norm": 0.39502906799316406,
      "learning_rate": 0.00012653239425843374,
      "loss": 0.0298,
      "step": 70200
    },
    {
      "epoch": 20.577373974208676,
      "grad_norm": 0.9082567691802979,
      "learning_rate": 0.00012644859797084537,
      "loss": 0.0582,
      "step": 70210
    },
    {
      "epoch": 20.580304806565064,
      "grad_norm": 0.6511194109916687,
      "learning_rate": 0.00012636480168325698,
      "loss": 0.0309,
      "step": 70220
    },
    {
      "epoch": 20.583235638921455,
      "grad_norm": 0.46937307715415955,
      "learning_rate": 0.00012628100539566862,
      "loss": 0.048,
      "step": 70230
    },
    {
      "epoch": 20.586166471277842,
      "grad_norm": 0.777626633644104,
      "learning_rate": 0.00012619720910808028,
      "loss": 0.0461,
      "step": 70240
    },
    {
      "epoch": 20.589097303634233,
      "grad_norm": 1.4398053884506226,
      "learning_rate": 0.0001261134128204919,
      "loss": 0.0474,
      "step": 70250
    },
    {
      "epoch": 20.59202813599062,
      "grad_norm": 0.43413034081459045,
      "learning_rate": 0.00012602961653290353,
      "loss": 0.0449,
      "step": 70260
    },
    {
      "epoch": 20.59495896834701,
      "grad_norm": 2.6012070178985596,
      "learning_rate": 0.00012594582024531517,
      "loss": 0.0517,
      "step": 70270
    },
    {
      "epoch": 20.5978898007034,
      "grad_norm": 6.7617411613464355,
      "learning_rate": 0.0001258620239577268,
      "loss": 0.0388,
      "step": 70280
    },
    {
      "epoch": 20.60082063305979,
      "grad_norm": 0.7560275793075562,
      "learning_rate": 0.00012577822767013844,
      "loss": 0.0414,
      "step": 70290
    },
    {
      "epoch": 20.603751465416178,
      "grad_norm": 0.6553523540496826,
      "learning_rate": 0.00012569443138255005,
      "loss": 0.0397,
      "step": 70300
    },
    {
      "epoch": 20.60668229777257,
      "grad_norm": 0.28047817945480347,
      "learning_rate": 0.00012561063509496169,
      "loss": 0.0411,
      "step": 70310
    },
    {
      "epoch": 20.609613130128956,
      "grad_norm": 0.24300436675548553,
      "learning_rate": 0.00012552683880737332,
      "loss": 0.0551,
      "step": 70320
    },
    {
      "epoch": 20.612543962485347,
      "grad_norm": 0.766708493232727,
      "learning_rate": 0.00012544304251978496,
      "loss": 0.0398,
      "step": 70330
    },
    {
      "epoch": 20.615474794841735,
      "grad_norm": 0.6513471007347107,
      "learning_rate": 0.0001253592462321966,
      "loss": 0.0518,
      "step": 70340
    },
    {
      "epoch": 20.618405627198126,
      "grad_norm": 0.3570600152015686,
      "learning_rate": 0.00012527544994460823,
      "loss": 0.0285,
      "step": 70350
    },
    {
      "epoch": 20.621336459554513,
      "grad_norm": 0.43533989787101746,
      "learning_rate": 0.00012519165365701984,
      "loss": 0.0354,
      "step": 70360
    },
    {
      "epoch": 20.624267291910904,
      "grad_norm": 0.4546496868133545,
      "learning_rate": 0.0001251078573694315,
      "loss": 0.0409,
      "step": 70370
    },
    {
      "epoch": 20.62719812426729,
      "grad_norm": 1.9292782545089722,
      "learning_rate": 0.00012502406108184314,
      "loss": 0.0553,
      "step": 70380
    },
    {
      "epoch": 20.630128956623683,
      "grad_norm": 0.038255076855421066,
      "learning_rate": 0.00012494026479425475,
      "loss": 0.0135,
      "step": 70390
    },
    {
      "epoch": 20.63305978898007,
      "grad_norm": 0.7652891278266907,
      "learning_rate": 0.0001248564685066664,
      "loss": 0.0291,
      "step": 70400
    },
    {
      "epoch": 20.63599062133646,
      "grad_norm": 1.3953789472579956,
      "learning_rate": 0.00012477267221907803,
      "loss": 0.0493,
      "step": 70410
    },
    {
      "epoch": 20.63892145369285,
      "grad_norm": 2.129368543624878,
      "learning_rate": 0.00012468887593148966,
      "loss": 0.0493,
      "step": 70420
    },
    {
      "epoch": 20.64185228604924,
      "grad_norm": 0.5878951549530029,
      "learning_rate": 0.0001246050796439013,
      "loss": 0.0373,
      "step": 70430
    },
    {
      "epoch": 20.644783118405627,
      "grad_norm": 1.2632547616958618,
      "learning_rate": 0.00012452128335631294,
      "loss": 0.0413,
      "step": 70440
    },
    {
      "epoch": 20.647713950762018,
      "grad_norm": 0.5957069993019104,
      "learning_rate": 0.00012443748706872455,
      "loss": 0.0405,
      "step": 70450
    },
    {
      "epoch": 20.650644783118405,
      "grad_norm": 0.740143358707428,
      "learning_rate": 0.0001243536907811362,
      "loss": 0.0351,
      "step": 70460
    },
    {
      "epoch": 20.653575615474796,
      "grad_norm": 0.09896552562713623,
      "learning_rate": 0.00012426989449354782,
      "loss": 0.0406,
      "step": 70470
    },
    {
      "epoch": 20.656506447831184,
      "grad_norm": 1.2274506092071533,
      "learning_rate": 0.00012418609820595946,
      "loss": 0.0363,
      "step": 70480
    },
    {
      "epoch": 20.659437280187575,
      "grad_norm": 0.4612162411212921,
      "learning_rate": 0.0001241023019183711,
      "loss": 0.0352,
      "step": 70490
    },
    {
      "epoch": 20.662368112543962,
      "grad_norm": 1.8577085733413696,
      "learning_rate": 0.00012401850563078273,
      "loss": 0.055,
      "step": 70500
    },
    {
      "epoch": 20.66529894490035,
      "grad_norm": 0.22635984420776367,
      "learning_rate": 0.00012393470934319437,
      "loss": 0.0342,
      "step": 70510
    },
    {
      "epoch": 20.66822977725674,
      "grad_norm": 1.267382264137268,
      "learning_rate": 0.000123850913055606,
      "loss": 0.0469,
      "step": 70520
    },
    {
      "epoch": 20.671160609613132,
      "grad_norm": 0.40911731123924255,
      "learning_rate": 0.00012376711676801761,
      "loss": 0.0315,
      "step": 70530
    },
    {
      "epoch": 20.67409144196952,
      "grad_norm": 0.1863158494234085,
      "learning_rate": 0.00012368332048042925,
      "loss": 0.0333,
      "step": 70540
    },
    {
      "epoch": 20.677022274325907,
      "grad_norm": 1.7372844219207764,
      "learning_rate": 0.00012359952419284092,
      "loss": 0.046,
      "step": 70550
    },
    {
      "epoch": 20.679953106682298,
      "grad_norm": 0.42368704080581665,
      "learning_rate": 0.00012351572790525252,
      "loss": 0.0348,
      "step": 70560
    },
    {
      "epoch": 20.682883939038685,
      "grad_norm": 0.9264776706695557,
      "learning_rate": 0.00012343193161766416,
      "loss": 0.0584,
      "step": 70570
    },
    {
      "epoch": 20.685814771395076,
      "grad_norm": 2.0549304485321045,
      "learning_rate": 0.0001233481353300758,
      "loss": 0.057,
      "step": 70580
    },
    {
      "epoch": 20.688745603751464,
      "grad_norm": 2.604647636413574,
      "learning_rate": 0.00012326433904248744,
      "loss": 0.0395,
      "step": 70590
    },
    {
      "epoch": 20.691676436107855,
      "grad_norm": 0.22302044928073883,
      "learning_rate": 0.00012318054275489907,
      "loss": 0.0259,
      "step": 70600
    },
    {
      "epoch": 20.694607268464242,
      "grad_norm": 0.6649948358535767,
      "learning_rate": 0.00012309674646731068,
      "loss": 0.0444,
      "step": 70610
    },
    {
      "epoch": 20.697538100820633,
      "grad_norm": 1.160809874534607,
      "learning_rate": 0.00012301295017972232,
      "loss": 0.0445,
      "step": 70620
    },
    {
      "epoch": 20.70046893317702,
      "grad_norm": 0.6903874278068542,
      "learning_rate": 0.00012292915389213396,
      "loss": 0.0388,
      "step": 70630
    },
    {
      "epoch": 20.703399765533412,
      "grad_norm": 0.43670907616615295,
      "learning_rate": 0.0001228453576045456,
      "loss": 0.0314,
      "step": 70640
    },
    {
      "epoch": 20.7063305978898,
      "grad_norm": 1.1962465047836304,
      "learning_rate": 0.00012276156131695723,
      "loss": 0.0468,
      "step": 70650
    },
    {
      "epoch": 20.70926143024619,
      "grad_norm": 0.5297024846076965,
      "learning_rate": 0.00012267776502936887,
      "loss": 0.0436,
      "step": 70660
    },
    {
      "epoch": 20.712192262602578,
      "grad_norm": 0.5336617231369019,
      "learning_rate": 0.00012259396874178048,
      "loss": 0.0299,
      "step": 70670
    },
    {
      "epoch": 20.71512309495897,
      "grad_norm": 2.0874783992767334,
      "learning_rate": 0.00012251017245419214,
      "loss": 0.0396,
      "step": 70680
    },
    {
      "epoch": 20.718053927315356,
      "grad_norm": 0.2971891462802887,
      "learning_rate": 0.00012242637616660378,
      "loss": 0.0396,
      "step": 70690
    },
    {
      "epoch": 20.720984759671747,
      "grad_norm": 0.9167399406433105,
      "learning_rate": 0.00012234257987901539,
      "loss": 0.0233,
      "step": 70700
    },
    {
      "epoch": 20.723915592028135,
      "grad_norm": 0.09640965610742569,
      "learning_rate": 0.00012225878359142702,
      "loss": 0.0484,
      "step": 70710
    },
    {
      "epoch": 20.726846424384526,
      "grad_norm": 0.9563041925430298,
      "learning_rate": 0.00012217498730383866,
      "loss": 0.0316,
      "step": 70720
    },
    {
      "epoch": 20.729777256740913,
      "grad_norm": 1.120168685913086,
      "learning_rate": 0.0001220911910162503,
      "loss": 0.029,
      "step": 70730
    },
    {
      "epoch": 20.732708089097304,
      "grad_norm": 0.16447116434574127,
      "learning_rate": 0.00012200739472866193,
      "loss": 0.0449,
      "step": 70740
    },
    {
      "epoch": 20.735638921453692,
      "grad_norm": 1.0752657651901245,
      "learning_rate": 0.00012192359844107356,
      "loss": 0.0539,
      "step": 70750
    },
    {
      "epoch": 20.738569753810083,
      "grad_norm": 0.24442000687122345,
      "learning_rate": 0.00012183980215348519,
      "loss": 0.0283,
      "step": 70760
    },
    {
      "epoch": 20.74150058616647,
      "grad_norm": 0.6878024935722351,
      "learning_rate": 0.00012175600586589683,
      "loss": 0.0538,
      "step": 70770
    },
    {
      "epoch": 20.74443141852286,
      "grad_norm": 0.6384059190750122,
      "learning_rate": 0.00012167220957830845,
      "loss": 0.0259,
      "step": 70780
    },
    {
      "epoch": 20.74736225087925,
      "grad_norm": 0.840072751045227,
      "learning_rate": 0.00012158841329072009,
      "loss": 0.0527,
      "step": 70790
    },
    {
      "epoch": 20.75029308323564,
      "grad_norm": 0.3699638843536377,
      "learning_rate": 0.00012150461700313173,
      "loss": 0.0237,
      "step": 70800
    },
    {
      "epoch": 20.753223915592027,
      "grad_norm": 0.18220952153205872,
      "learning_rate": 0.00012142082071554335,
      "loss": 0.0291,
      "step": 70810
    },
    {
      "epoch": 20.75615474794842,
      "grad_norm": 0.5265595316886902,
      "learning_rate": 0.00012133702442795499,
      "loss": 0.0467,
      "step": 70820
    },
    {
      "epoch": 20.759085580304806,
      "grad_norm": 1.4828641414642334,
      "learning_rate": 0.00012125322814036664,
      "loss": 0.03,
      "step": 70830
    },
    {
      "epoch": 20.762016412661197,
      "grad_norm": 2.026888847351074,
      "learning_rate": 0.00012116943185277826,
      "loss": 0.0268,
      "step": 70840
    },
    {
      "epoch": 20.764947245017584,
      "grad_norm": 0.5890913009643555,
      "learning_rate": 0.0001210856355651899,
      "loss": 0.0425,
      "step": 70850
    },
    {
      "epoch": 20.767878077373975,
      "grad_norm": 0.29568204283714294,
      "learning_rate": 0.00012100183927760153,
      "loss": 0.0598,
      "step": 70860
    },
    {
      "epoch": 20.770808909730363,
      "grad_norm": 5.933411121368408,
      "learning_rate": 0.00012091804299001316,
      "loss": 0.0647,
      "step": 70870
    },
    {
      "epoch": 20.773739742086754,
      "grad_norm": 1.349892258644104,
      "learning_rate": 0.0001208342467024248,
      "loss": 0.0415,
      "step": 70880
    },
    {
      "epoch": 20.77667057444314,
      "grad_norm": 0.13012386858463287,
      "learning_rate": 0.00012075045041483643,
      "loss": 0.0592,
      "step": 70890
    },
    {
      "epoch": 20.779601406799532,
      "grad_norm": 0.8608554005622864,
      "learning_rate": 0.00012066665412724805,
      "loss": 0.0331,
      "step": 70900
    },
    {
      "epoch": 20.78253223915592,
      "grad_norm": 1.4981704950332642,
      "learning_rate": 0.00012058285783965969,
      "loss": 0.0835,
      "step": 70910
    },
    {
      "epoch": 20.78546307151231,
      "grad_norm": 0.10879295319318771,
      "learning_rate": 0.00012049906155207131,
      "loss": 0.0466,
      "step": 70920
    },
    {
      "epoch": 20.7883939038687,
      "grad_norm": 0.027470756322145462,
      "learning_rate": 0.00012041526526448295,
      "loss": 0.0431,
      "step": 70930
    },
    {
      "epoch": 20.79132473622509,
      "grad_norm": 0.532998263835907,
      "learning_rate": 0.0001203314689768946,
      "loss": 0.0328,
      "step": 70940
    },
    {
      "epoch": 20.794255568581477,
      "grad_norm": 1.8702459335327148,
      "learning_rate": 0.00012024767268930622,
      "loss": 0.0314,
      "step": 70950
    },
    {
      "epoch": 20.797186400937868,
      "grad_norm": 1.1253459453582764,
      "learning_rate": 0.00012016387640171786,
      "loss": 0.035,
      "step": 70960
    },
    {
      "epoch": 20.800117233294255,
      "grad_norm": 0.47579503059387207,
      "learning_rate": 0.0001200800801141295,
      "loss": 0.0242,
      "step": 70970
    },
    {
      "epoch": 20.803048065650646,
      "grad_norm": 3.2288410663604736,
      "learning_rate": 0.00011999628382654112,
      "loss": 0.0548,
      "step": 70980
    },
    {
      "epoch": 20.805978898007034,
      "grad_norm": 0.2999367415904999,
      "learning_rate": 0.00011991248753895276,
      "loss": 0.0224,
      "step": 70990
    },
    {
      "epoch": 20.808909730363425,
      "grad_norm": 1.9446847438812256,
      "learning_rate": 0.0001198286912513644,
      "loss": 0.0373,
      "step": 71000
    },
    {
      "epoch": 20.811840562719812,
      "grad_norm": 0.278628408908844,
      "learning_rate": 0.00011974489496377602,
      "loss": 0.0389,
      "step": 71010
    },
    {
      "epoch": 20.814771395076203,
      "grad_norm": 0.8864014744758606,
      "learning_rate": 0.00011966109867618766,
      "loss": 0.042,
      "step": 71020
    },
    {
      "epoch": 20.81770222743259,
      "grad_norm": 0.9327709674835205,
      "learning_rate": 0.0001195773023885993,
      "loss": 0.0385,
      "step": 71030
    },
    {
      "epoch": 20.82063305978898,
      "grad_norm": 1.399207592010498,
      "learning_rate": 0.00011949350610101092,
      "loss": 0.0432,
      "step": 71040
    },
    {
      "epoch": 20.82356389214537,
      "grad_norm": 1.3328347206115723,
      "learning_rate": 0.00011940970981342257,
      "loss": 0.0539,
      "step": 71050
    },
    {
      "epoch": 20.82649472450176,
      "grad_norm": 0.571342945098877,
      "learning_rate": 0.00011932591352583419,
      "loss": 0.0436,
      "step": 71060
    },
    {
      "epoch": 20.829425556858148,
      "grad_norm": 0.6511664390563965,
      "learning_rate": 0.00011924211723824583,
      "loss": 0.0616,
      "step": 71070
    },
    {
      "epoch": 20.83235638921454,
      "grad_norm": 0.5048885345458984,
      "learning_rate": 0.00011915832095065746,
      "loss": 0.0165,
      "step": 71080
    },
    {
      "epoch": 20.835287221570926,
      "grad_norm": 1.119186520576477,
      "learning_rate": 0.00011907452466306909,
      "loss": 0.0329,
      "step": 71090
    },
    {
      "epoch": 20.838218053927314,
      "grad_norm": 0.09841661900281906,
      "learning_rate": 0.00011899072837548072,
      "loss": 0.0377,
      "step": 71100
    },
    {
      "epoch": 20.841148886283705,
      "grad_norm": 0.21429190039634705,
      "learning_rate": 0.00011890693208789236,
      "loss": 0.0298,
      "step": 71110
    },
    {
      "epoch": 20.844079718640092,
      "grad_norm": 0.9853460788726807,
      "learning_rate": 0.00011882313580030398,
      "loss": 0.0643,
      "step": 71120
    },
    {
      "epoch": 20.847010550996483,
      "grad_norm": 0.3506454527378082,
      "learning_rate": 0.00011873933951271562,
      "loss": 0.0286,
      "step": 71130
    },
    {
      "epoch": 20.84994138335287,
      "grad_norm": 0.4036330580711365,
      "learning_rate": 0.00011865554322512727,
      "loss": 0.0372,
      "step": 71140
    },
    {
      "epoch": 20.85287221570926,
      "grad_norm": 1.6786683797836304,
      "learning_rate": 0.00011857174693753888,
      "loss": 0.0456,
      "step": 71150
    },
    {
      "epoch": 20.85580304806565,
      "grad_norm": 0.4579988121986389,
      "learning_rate": 0.00011848795064995053,
      "loss": 0.0464,
      "step": 71160
    },
    {
      "epoch": 20.85873388042204,
      "grad_norm": 0.20343391597270966,
      "learning_rate": 0.00011840415436236217,
      "loss": 0.0262,
      "step": 71170
    },
    {
      "epoch": 20.861664712778428,
      "grad_norm": 0.4275149405002594,
      "learning_rate": 0.00011832035807477379,
      "loss": 0.0314,
      "step": 71180
    },
    {
      "epoch": 20.86459554513482,
      "grad_norm": 0.7983660101890564,
      "learning_rate": 0.00011823656178718543,
      "loss": 0.0329,
      "step": 71190
    },
    {
      "epoch": 20.867526377491206,
      "grad_norm": 0.43072065711021423,
      "learning_rate": 0.00011815276549959706,
      "loss": 0.0511,
      "step": 71200
    },
    {
      "epoch": 20.870457209847597,
      "grad_norm": 0.7454724311828613,
      "learning_rate": 0.00011806896921200869,
      "loss": 0.0483,
      "step": 71210
    },
    {
      "epoch": 20.873388042203985,
      "grad_norm": 3.483010768890381,
      "learning_rate": 0.00011798517292442032,
      "loss": 0.0523,
      "step": 71220
    },
    {
      "epoch": 20.876318874560376,
      "grad_norm": 0.13620586693286896,
      "learning_rate": 0.00011790137663683195,
      "loss": 0.0278,
      "step": 71230
    },
    {
      "epoch": 20.879249706916763,
      "grad_norm": 0.6617693901062012,
      "learning_rate": 0.00011781758034924358,
      "loss": 0.0484,
      "step": 71240
    },
    {
      "epoch": 20.882180539273154,
      "grad_norm": 1.8809837102890015,
      "learning_rate": 0.00011773378406165523,
      "loss": 0.0363,
      "step": 71250
    },
    {
      "epoch": 20.88511137162954,
      "grad_norm": 0.34528517723083496,
      "learning_rate": 0.00011764998777406686,
      "loss": 0.0329,
      "step": 71260
    },
    {
      "epoch": 20.888042203985933,
      "grad_norm": 1.282517433166504,
      "learning_rate": 0.0001175661914864785,
      "loss": 0.0528,
      "step": 71270
    },
    {
      "epoch": 20.89097303634232,
      "grad_norm": 0.1803731620311737,
      "learning_rate": 0.00011748239519889013,
      "loss": 0.0255,
      "step": 71280
    },
    {
      "epoch": 20.89390386869871,
      "grad_norm": 3.016228437423706,
      "learning_rate": 0.00011739859891130175,
      "loss": 0.0359,
      "step": 71290
    },
    {
      "epoch": 20.8968347010551,
      "grad_norm": 1.6265792846679688,
      "learning_rate": 0.00011731480262371339,
      "loss": 0.053,
      "step": 71300
    },
    {
      "epoch": 20.89976553341149,
      "grad_norm": 0.8895305395126343,
      "learning_rate": 0.00011723100633612503,
      "loss": 0.0578,
      "step": 71310
    },
    {
      "epoch": 20.902696365767877,
      "grad_norm": 1.4218711853027344,
      "learning_rate": 0.00011714721004853665,
      "loss": 0.0338,
      "step": 71320
    },
    {
      "epoch": 20.905627198124268,
      "grad_norm": 0.3950820863246918,
      "learning_rate": 0.00011706341376094829,
      "loss": 0.0211,
      "step": 71330
    },
    {
      "epoch": 20.908558030480656,
      "grad_norm": 2.055903911590576,
      "learning_rate": 0.00011697961747335994,
      "loss": 0.0455,
      "step": 71340
    },
    {
      "epoch": 20.911488862837047,
      "grad_norm": 0.09616030752658844,
      "learning_rate": 0.00011689582118577155,
      "loss": 0.0434,
      "step": 71350
    },
    {
      "epoch": 20.914419695193434,
      "grad_norm": 0.496908962726593,
      "learning_rate": 0.0001168120248981832,
      "loss": 0.0299,
      "step": 71360
    },
    {
      "epoch": 20.917350527549825,
      "grad_norm": 3.127680540084839,
      "learning_rate": 0.00011672822861059482,
      "loss": 0.0357,
      "step": 71370
    },
    {
      "epoch": 20.920281359906213,
      "grad_norm": 0.9625835418701172,
      "learning_rate": 0.00011664443232300646,
      "loss": 0.0417,
      "step": 71380
    },
    {
      "epoch": 20.923212192262604,
      "grad_norm": 1.8086411952972412,
      "learning_rate": 0.0001165606360354181,
      "loss": 0.0375,
      "step": 71390
    },
    {
      "epoch": 20.92614302461899,
      "grad_norm": 1.5584677457809448,
      "learning_rate": 0.00011647683974782972,
      "loss": 0.0457,
      "step": 71400
    },
    {
      "epoch": 20.929073856975382,
      "grad_norm": 1.7917897701263428,
      "learning_rate": 0.00011639304346024136,
      "loss": 0.0353,
      "step": 71410
    },
    {
      "epoch": 20.93200468933177,
      "grad_norm": 2.480126142501831,
      "learning_rate": 0.00011630924717265299,
      "loss": 0.0304,
      "step": 71420
    },
    {
      "epoch": 20.93493552168816,
      "grad_norm": 0.8072903752326965,
      "learning_rate": 0.00011622545088506462,
      "loss": 0.0457,
      "step": 71430
    },
    {
      "epoch": 20.937866354044548,
      "grad_norm": 1.1558270454406738,
      "learning_rate": 0.00011614165459747625,
      "loss": 0.0305,
      "step": 71440
    },
    {
      "epoch": 20.94079718640094,
      "grad_norm": 0.5813876986503601,
      "learning_rate": 0.0001160578583098879,
      "loss": 0.0424,
      "step": 71450
    },
    {
      "epoch": 20.943728018757326,
      "grad_norm": 0.6585033535957336,
      "learning_rate": 0.00011597406202229951,
      "loss": 0.0298,
      "step": 71460
    },
    {
      "epoch": 20.946658851113718,
      "grad_norm": 1.0059032440185547,
      "learning_rate": 0.00011589026573471116,
      "loss": 0.0403,
      "step": 71470
    },
    {
      "epoch": 20.949589683470105,
      "grad_norm": 1.3106157779693604,
      "learning_rate": 0.0001158064694471228,
      "loss": 0.0281,
      "step": 71480
    },
    {
      "epoch": 20.952520515826496,
      "grad_norm": 0.5312106013298035,
      "learning_rate": 0.00011572267315953442,
      "loss": 0.04,
      "step": 71490
    },
    {
      "epoch": 20.955451348182883,
      "grad_norm": 0.5755655765533447,
      "learning_rate": 0.00011563887687194606,
      "loss": 0.041,
      "step": 71500
    },
    {
      "epoch": 20.958382180539274,
      "grad_norm": 0.5849356651306152,
      "learning_rate": 0.0001155550805843577,
      "loss": 0.0392,
      "step": 71510
    },
    {
      "epoch": 20.961313012895662,
      "grad_norm": 3.2984941005706787,
      "learning_rate": 0.00011547128429676932,
      "loss": 0.0412,
      "step": 71520
    },
    {
      "epoch": 20.964243845252053,
      "grad_norm": 1.230169415473938,
      "learning_rate": 0.00011538748800918096,
      "loss": 0.0369,
      "step": 71530
    },
    {
      "epoch": 20.96717467760844,
      "grad_norm": 1.7624720335006714,
      "learning_rate": 0.00011530369172159258,
      "loss": 0.0278,
      "step": 71540
    },
    {
      "epoch": 20.97010550996483,
      "grad_norm": 0.2952609956264496,
      "learning_rate": 0.00011521989543400422,
      "loss": 0.0444,
      "step": 71550
    },
    {
      "epoch": 20.97303634232122,
      "grad_norm": 0.4370947778224945,
      "learning_rate": 0.00011513609914641587,
      "loss": 0.0399,
      "step": 71560
    },
    {
      "epoch": 20.97596717467761,
      "grad_norm": 1.431603193283081,
      "learning_rate": 0.00011505230285882748,
      "loss": 0.0526,
      "step": 71570
    },
    {
      "epoch": 20.978898007033997,
      "grad_norm": 0.4584493637084961,
      "learning_rate": 0.00011496850657123913,
      "loss": 0.0437,
      "step": 71580
    },
    {
      "epoch": 20.98182883939039,
      "grad_norm": 1.3053631782531738,
      "learning_rate": 0.00011488471028365076,
      "loss": 0.0372,
      "step": 71590
    },
    {
      "epoch": 20.984759671746776,
      "grad_norm": 1.0017489194869995,
      "learning_rate": 0.00011480091399606239,
      "loss": 0.0708,
      "step": 71600
    },
    {
      "epoch": 20.987690504103167,
      "grad_norm": 0.2418079972267151,
      "learning_rate": 0.00011471711770847402,
      "loss": 0.0415,
      "step": 71610
    },
    {
      "epoch": 20.990621336459554,
      "grad_norm": 1.9054690599441528,
      "learning_rate": 0.00011463332142088566,
      "loss": 0.0648,
      "step": 71620
    },
    {
      "epoch": 20.993552168815945,
      "grad_norm": 2.178865432739258,
      "learning_rate": 0.00011454952513329728,
      "loss": 0.0504,
      "step": 71630
    },
    {
      "epoch": 20.996483001172333,
      "grad_norm": 1.4200674295425415,
      "learning_rate": 0.00011446572884570892,
      "loss": 0.0461,
      "step": 71640
    },
    {
      "epoch": 20.99941383352872,
      "grad_norm": 0.2627868056297302,
      "learning_rate": 0.00011438193255812057,
      "loss": 0.0376,
      "step": 71650
    },
    {
      "epoch": 21.0,
      "eval_accuracy": 0.811078140454995,
      "eval_f1_macro": 0.8488041986747853,
      "eval_f1_micro": 0.8660951817913094,
      "eval_f1_weighted": 0.8649727841086785,
      "eval_loss": 0.0701722651720047,
      "eval_roc_auc": 0.9221366807705982,
      "eval_runtime": 142.0008,
      "eval_samples_per_second": 21.359,
      "eval_steps_per_second": 2.676,
      "step": 71652
    },
    {
      "epoch": 21.00234466588511,
      "grad_norm": 0.8824422955513,
      "learning_rate": 0.00011429813627053218,
      "loss": 0.0385,
      "step": 71660
    },
    {
      "epoch": 21.0052754982415,
      "grad_norm": 0.6059803366661072,
      "learning_rate": 0.00011421433998294383,
      "loss": 0.0306,
      "step": 71670
    },
    {
      "epoch": 21.00820633059789,
      "grad_norm": 1.4278279542922974,
      "learning_rate": 0.00011413054369535544,
      "loss": 0.0437,
      "step": 71680
    },
    {
      "epoch": 21.011137162954277,
      "grad_norm": 1.1081386804580688,
      "learning_rate": 0.00011404674740776709,
      "loss": 0.0498,
      "step": 71690
    },
    {
      "epoch": 21.01406799531067,
      "grad_norm": 1.6599208116531372,
      "learning_rate": 0.00011396295112017873,
      "loss": 0.0514,
      "step": 71700
    },
    {
      "epoch": 21.016998827667056,
      "grad_norm": 1.9950156211853027,
      "learning_rate": 0.00011387915483259035,
      "loss": 0.0216,
      "step": 71710
    },
    {
      "epoch": 21.019929660023447,
      "grad_norm": 0.3488054871559143,
      "learning_rate": 0.00011379535854500199,
      "loss": 0.0291,
      "step": 71720
    },
    {
      "epoch": 21.022860492379834,
      "grad_norm": 0.9965963363647461,
      "learning_rate": 0.00011371156225741362,
      "loss": 0.0253,
      "step": 71730
    },
    {
      "epoch": 21.025791324736225,
      "grad_norm": 1.4099128246307373,
      "learning_rate": 0.00011362776596982525,
      "loss": 0.054,
      "step": 71740
    },
    {
      "epoch": 21.028722157092613,
      "grad_norm": 1.1411762237548828,
      "learning_rate": 0.00011354396968223688,
      "loss": 0.0375,
      "step": 71750
    },
    {
      "epoch": 21.031652989449004,
      "grad_norm": 1.246361255645752,
      "learning_rate": 0.00011346017339464853,
      "loss": 0.0365,
      "step": 71760
    },
    {
      "epoch": 21.03458382180539,
      "grad_norm": 0.0506553016602993,
      "learning_rate": 0.00011337637710706014,
      "loss": 0.0361,
      "step": 71770
    },
    {
      "epoch": 21.037514654161782,
      "grad_norm": 0.29801875352859497,
      "learning_rate": 0.0001132925808194718,
      "loss": 0.0278,
      "step": 71780
    },
    {
      "epoch": 21.04044548651817,
      "grad_norm": 0.23154325783252716,
      "learning_rate": 0.00011320878453188343,
      "loss": 0.0465,
      "step": 71790
    },
    {
      "epoch": 21.04337631887456,
      "grad_norm": 0.7502025961875916,
      "learning_rate": 0.00011312498824429505,
      "loss": 0.0537,
      "step": 71800
    },
    {
      "epoch": 21.04630715123095,
      "grad_norm": 1.904975414276123,
      "learning_rate": 0.00011304119195670669,
      "loss": 0.0427,
      "step": 71810
    },
    {
      "epoch": 21.04923798358734,
      "grad_norm": 0.7638176679611206,
      "learning_rate": 0.00011295739566911833,
      "loss": 0.0445,
      "step": 71820
    },
    {
      "epoch": 21.052168815943727,
      "grad_norm": 0.4064529240131378,
      "learning_rate": 0.00011287359938152995,
      "loss": 0.0417,
      "step": 71830
    },
    {
      "epoch": 21.055099648300118,
      "grad_norm": 2.4914281368255615,
      "learning_rate": 0.00011278980309394159,
      "loss": 0.0345,
      "step": 71840
    },
    {
      "epoch": 21.058030480656505,
      "grad_norm": 1.2144070863723755,
      "learning_rate": 0.00011270600680635321,
      "loss": 0.0303,
      "step": 71850
    },
    {
      "epoch": 21.060961313012896,
      "grad_norm": 0.33346888422966003,
      "learning_rate": 0.00011262221051876485,
      "loss": 0.0517,
      "step": 71860
    },
    {
      "epoch": 21.063892145369284,
      "grad_norm": 0.4871579706668854,
      "learning_rate": 0.0001125384142311765,
      "loss": 0.0316,
      "step": 71870
    },
    {
      "epoch": 21.066822977725675,
      "grad_norm": 0.3007061779499054,
      "learning_rate": 0.00011245461794358811,
      "loss": 0.0242,
      "step": 71880
    },
    {
      "epoch": 21.069753810082062,
      "grad_norm": 1.2818154096603394,
      "learning_rate": 0.00011237082165599976,
      "loss": 0.0192,
      "step": 71890
    },
    {
      "epoch": 21.072684642438453,
      "grad_norm": 2.790212869644165,
      "learning_rate": 0.0001122870253684114,
      "loss": 0.0447,
      "step": 71900
    },
    {
      "epoch": 21.07561547479484,
      "grad_norm": 0.9252834320068359,
      "learning_rate": 0.00011220322908082302,
      "loss": 0.0545,
      "step": 71910
    },
    {
      "epoch": 21.078546307151232,
      "grad_norm": 0.41445210576057434,
      "learning_rate": 0.00011211943279323466,
      "loss": 0.0369,
      "step": 71920
    },
    {
      "epoch": 21.08147713950762,
      "grad_norm": 0.43881210684776306,
      "learning_rate": 0.00011203563650564629,
      "loss": 0.0372,
      "step": 71930
    },
    {
      "epoch": 21.08440797186401,
      "grad_norm": 1.8511476516723633,
      "learning_rate": 0.00011195184021805792,
      "loss": 0.0351,
      "step": 71940
    },
    {
      "epoch": 21.087338804220398,
      "grad_norm": 1.1921817064285278,
      "learning_rate": 0.00011186804393046955,
      "loss": 0.0308,
      "step": 71950
    },
    {
      "epoch": 21.09026963657679,
      "grad_norm": 0.5576656460762024,
      "learning_rate": 0.0001117842476428812,
      "loss": 0.0226,
      "step": 71960
    },
    {
      "epoch": 21.093200468933176,
      "grad_norm": 1.3183294534683228,
      "learning_rate": 0.00011170045135529281,
      "loss": 0.0489,
      "step": 71970
    },
    {
      "epoch": 21.096131301289567,
      "grad_norm": 1.633333444595337,
      "learning_rate": 0.00011161665506770446,
      "loss": 0.0167,
      "step": 71980
    },
    {
      "epoch": 21.099062133645955,
      "grad_norm": 0.436987966299057,
      "learning_rate": 0.00011153285878011607,
      "loss": 0.0281,
      "step": 71990
    },
    {
      "epoch": 21.101992966002346,
      "grad_norm": 2.538651943206787,
      "learning_rate": 0.00011144906249252772,
      "loss": 0.0337,
      "step": 72000
    },
    {
      "epoch": 21.104923798358733,
      "grad_norm": 1.3553533554077148,
      "learning_rate": 0.00011136526620493936,
      "loss": 0.0545,
      "step": 72010
    },
    {
      "epoch": 21.107854630715124,
      "grad_norm": 1.3207710981369019,
      "learning_rate": 0.00011128146991735098,
      "loss": 0.0314,
      "step": 72020
    },
    {
      "epoch": 21.11078546307151,
      "grad_norm": 0.7849487662315369,
      "learning_rate": 0.00011119767362976262,
      "loss": 0.0531,
      "step": 72030
    },
    {
      "epoch": 21.113716295427903,
      "grad_norm": 0.7281244993209839,
      "learning_rate": 0.00011111387734217426,
      "loss": 0.0226,
      "step": 72040
    },
    {
      "epoch": 21.11664712778429,
      "grad_norm": 0.47852498292922974,
      "learning_rate": 0.00011103008105458588,
      "loss": 0.0363,
      "step": 72050
    },
    {
      "epoch": 21.11957796014068,
      "grad_norm": 0.8565724492073059,
      "learning_rate": 0.00011094628476699752,
      "loss": 0.0213,
      "step": 72060
    },
    {
      "epoch": 21.12250879249707,
      "grad_norm": 3.58815336227417,
      "learning_rate": 0.00011086248847940917,
      "loss": 0.0412,
      "step": 72070
    },
    {
      "epoch": 21.12543962485346,
      "grad_norm": 1.1302251815795898,
      "learning_rate": 0.00011077869219182078,
      "loss": 0.0303,
      "step": 72080
    },
    {
      "epoch": 21.128370457209847,
      "grad_norm": 2.560807466506958,
      "learning_rate": 0.00011069489590423243,
      "loss": 0.0494,
      "step": 72090
    },
    {
      "epoch": 21.131301289566238,
      "grad_norm": 0.19994059205055237,
      "learning_rate": 0.00011061109961664406,
      "loss": 0.0263,
      "step": 72100
    },
    {
      "epoch": 21.134232121922626,
      "grad_norm": 1.6728408336639404,
      "learning_rate": 0.00011052730332905569,
      "loss": 0.0318,
      "step": 72110
    },
    {
      "epoch": 21.137162954279017,
      "grad_norm": 1.5937179327011108,
      "learning_rate": 0.00011044350704146732,
      "loss": 0.0556,
      "step": 72120
    },
    {
      "epoch": 21.140093786635404,
      "grad_norm": 0.5670934319496155,
      "learning_rate": 0.00011035971075387896,
      "loss": 0.0222,
      "step": 72130
    },
    {
      "epoch": 21.143024618991795,
      "grad_norm": 0.6199095249176025,
      "learning_rate": 0.00011027591446629058,
      "loss": 0.0494,
      "step": 72140
    },
    {
      "epoch": 21.145955451348183,
      "grad_norm": 0.8386884331703186,
      "learning_rate": 0.00011019211817870222,
      "loss": 0.0202,
      "step": 72150
    },
    {
      "epoch": 21.148886283704574,
      "grad_norm": 0.05149554833769798,
      "learning_rate": 0.00011010832189111384,
      "loss": 0.0278,
      "step": 72160
    },
    {
      "epoch": 21.15181711606096,
      "grad_norm": 0.9891664385795593,
      "learning_rate": 0.00011002452560352548,
      "loss": 0.0326,
      "step": 72170
    },
    {
      "epoch": 21.154747948417352,
      "grad_norm": 1.586152195930481,
      "learning_rate": 0.00010994072931593713,
      "loss": 0.0462,
      "step": 72180
    },
    {
      "epoch": 21.15767878077374,
      "grad_norm": 2.132314443588257,
      "learning_rate": 0.00010985693302834874,
      "loss": 0.0503,
      "step": 72190
    },
    {
      "epoch": 21.16060961313013,
      "grad_norm": 1.5086407661437988,
      "learning_rate": 0.00010977313674076039,
      "loss": 0.0401,
      "step": 72200
    },
    {
      "epoch": 21.163540445486518,
      "grad_norm": 1.7967647314071655,
      "learning_rate": 0.00010968934045317203,
      "loss": 0.0157,
      "step": 72210
    },
    {
      "epoch": 21.16647127784291,
      "grad_norm": 0.2111743688583374,
      "learning_rate": 0.00010960554416558365,
      "loss": 0.0332,
      "step": 72220
    },
    {
      "epoch": 21.169402110199297,
      "grad_norm": 0.3623292148113251,
      "learning_rate": 0.00010952174787799529,
      "loss": 0.0276,
      "step": 72230
    },
    {
      "epoch": 21.172332942555684,
      "grad_norm": 0.9620206952095032,
      "learning_rate": 0.00010943795159040693,
      "loss": 0.0479,
      "step": 72240
    },
    {
      "epoch": 21.175263774912075,
      "grad_norm": 0.7946463227272034,
      "learning_rate": 0.00010935415530281855,
      "loss": 0.0441,
      "step": 72250
    },
    {
      "epoch": 21.178194607268463,
      "grad_norm": 1.22270667552948,
      "learning_rate": 0.00010927035901523019,
      "loss": 0.0396,
      "step": 72260
    },
    {
      "epoch": 21.181125439624854,
      "grad_norm": 0.11841408163309097,
      "learning_rate": 0.00010918656272764184,
      "loss": 0.0193,
      "step": 72270
    },
    {
      "epoch": 21.18405627198124,
      "grad_norm": 0.5838463306427002,
      "learning_rate": 0.00010910276644005345,
      "loss": 0.0217,
      "step": 72280
    },
    {
      "epoch": 21.186987104337632,
      "grad_norm": 2.07112455368042,
      "learning_rate": 0.0001090189701524651,
      "loss": 0.0457,
      "step": 72290
    },
    {
      "epoch": 21.18991793669402,
      "grad_norm": 0.6232526898384094,
      "learning_rate": 0.0001089351738648767,
      "loss": 0.0503,
      "step": 72300
    },
    {
      "epoch": 21.19284876905041,
      "grad_norm": 0.034887176007032394,
      "learning_rate": 0.00010885137757728836,
      "loss": 0.0505,
      "step": 72310
    },
    {
      "epoch": 21.195779601406798,
      "grad_norm": 0.18386822938919067,
      "learning_rate": 0.00010876758128969999,
      "loss": 0.0156,
      "step": 72320
    },
    {
      "epoch": 21.19871043376319,
      "grad_norm": 0.15030165016651154,
      "learning_rate": 0.00010868378500211162,
      "loss": 0.0463,
      "step": 72330
    },
    {
      "epoch": 21.201641266119577,
      "grad_norm": 0.8232146501541138,
      "learning_rate": 0.00010859998871452325,
      "loss": 0.036,
      "step": 72340
    },
    {
      "epoch": 21.204572098475968,
      "grad_norm": 0.19960667192935944,
      "learning_rate": 0.00010851619242693489,
      "loss": 0.0646,
      "step": 72350
    },
    {
      "epoch": 21.207502930832355,
      "grad_norm": 1.4844905138015747,
      "learning_rate": 0.00010843239613934651,
      "loss": 0.0289,
      "step": 72360
    },
    {
      "epoch": 21.210433763188746,
      "grad_norm": 1.2124725580215454,
      "learning_rate": 0.00010834859985175815,
      "loss": 0.051,
      "step": 72370
    },
    {
      "epoch": 21.213364595545134,
      "grad_norm": 0.4030625522136688,
      "learning_rate": 0.0001082648035641698,
      "loss": 0.0345,
      "step": 72380
    },
    {
      "epoch": 21.216295427901525,
      "grad_norm": 2.07151460647583,
      "learning_rate": 0.00010818100727658141,
      "loss": 0.0439,
      "step": 72390
    },
    {
      "epoch": 21.219226260257912,
      "grad_norm": 0.5922940373420715,
      "learning_rate": 0.00010809721098899306,
      "loss": 0.0235,
      "step": 72400
    },
    {
      "epoch": 21.222157092614303,
      "grad_norm": 0.18256500363349915,
      "learning_rate": 0.0001080134147014047,
      "loss": 0.0301,
      "step": 72410
    },
    {
      "epoch": 21.22508792497069,
      "grad_norm": 0.03983990475535393,
      "learning_rate": 0.00010792961841381632,
      "loss": 0.0289,
      "step": 72420
    },
    {
      "epoch": 21.22801875732708,
      "grad_norm": 1.4761754274368286,
      "learning_rate": 0.00010784582212622796,
      "loss": 0.0173,
      "step": 72430
    },
    {
      "epoch": 21.23094958968347,
      "grad_norm": 4.649366855621338,
      "learning_rate": 0.0001077620258386396,
      "loss": 0.0488,
      "step": 72440
    },
    {
      "epoch": 21.23388042203986,
      "grad_norm": 0.6001485586166382,
      "learning_rate": 0.00010767822955105122,
      "loss": 0.0421,
      "step": 72450
    },
    {
      "epoch": 21.236811254396248,
      "grad_norm": 0.3456995189189911,
      "learning_rate": 0.00010759443326346285,
      "loss": 0.032,
      "step": 72460
    },
    {
      "epoch": 21.23974208675264,
      "grad_norm": 0.7449782490730286,
      "learning_rate": 0.00010751063697587448,
      "loss": 0.0503,
      "step": 72470
    },
    {
      "epoch": 21.242672919109026,
      "grad_norm": 3.8827226161956787,
      "learning_rate": 0.00010742684068828611,
      "loss": 0.0547,
      "step": 72480
    },
    {
      "epoch": 21.245603751465417,
      "grad_norm": 0.3336099684238434,
      "learning_rate": 0.00010734304440069776,
      "loss": 0.05,
      "step": 72490
    },
    {
      "epoch": 21.248534583821804,
      "grad_norm": 0.8481994271278381,
      "learning_rate": 0.00010725924811310937,
      "loss": 0.0317,
      "step": 72500
    },
    {
      "epoch": 21.251465416178196,
      "grad_norm": 0.47416260838508606,
      "learning_rate": 0.00010717545182552102,
      "loss": 0.0467,
      "step": 72510
    },
    {
      "epoch": 21.254396248534583,
      "grad_norm": 0.7160341739654541,
      "learning_rate": 0.00010709165553793266,
      "loss": 0.0194,
      "step": 72520
    },
    {
      "epoch": 21.257327080890974,
      "grad_norm": 2.009168863296509,
      "learning_rate": 0.00010700785925034428,
      "loss": 0.0339,
      "step": 72530
    },
    {
      "epoch": 21.26025791324736,
      "grad_norm": 1.0255721807479858,
      "learning_rate": 0.00010692406296275592,
      "loss": 0.0379,
      "step": 72540
    },
    {
      "epoch": 21.263188745603752,
      "grad_norm": 0.9738671183586121,
      "learning_rate": 0.00010684026667516756,
      "loss": 0.023,
      "step": 72550
    },
    {
      "epoch": 21.26611957796014,
      "grad_norm": 0.7120407223701477,
      "learning_rate": 0.00010675647038757918,
      "loss": 0.0305,
      "step": 72560
    },
    {
      "epoch": 21.26905041031653,
      "grad_norm": 0.5883305072784424,
      "learning_rate": 0.00010667267409999082,
      "loss": 0.0466,
      "step": 72570
    },
    {
      "epoch": 21.27198124267292,
      "grad_norm": 0.5055590271949768,
      "learning_rate": 0.00010658887781240247,
      "loss": 0.0353,
      "step": 72580
    },
    {
      "epoch": 21.27491207502931,
      "grad_norm": 0.6251363754272461,
      "learning_rate": 0.00010650508152481408,
      "loss": 0.0295,
      "step": 72590
    },
    {
      "epoch": 21.277842907385697,
      "grad_norm": 1.4925103187561035,
      "learning_rate": 0.00010642128523722573,
      "loss": 0.0271,
      "step": 72600
    },
    {
      "epoch": 21.280773739742088,
      "grad_norm": 0.7537412047386169,
      "learning_rate": 0.00010633748894963734,
      "loss": 0.0461,
      "step": 72610
    },
    {
      "epoch": 21.283704572098475,
      "grad_norm": 0.4212029278278351,
      "learning_rate": 0.00010625369266204899,
      "loss": 0.0298,
      "step": 72620
    },
    {
      "epoch": 21.286635404454866,
      "grad_norm": 0.06536058336496353,
      "learning_rate": 0.00010616989637446063,
      "loss": 0.0396,
      "step": 72630
    },
    {
      "epoch": 21.289566236811254,
      "grad_norm": 0.8648067712783813,
      "learning_rate": 0.00010608610008687225,
      "loss": 0.0296,
      "step": 72640
    },
    {
      "epoch": 21.292497069167645,
      "grad_norm": 0.40772172808647156,
      "learning_rate": 0.00010600230379928389,
      "loss": 0.0446,
      "step": 72650
    },
    {
      "epoch": 21.295427901524032,
      "grad_norm": 1.2958766222000122,
      "learning_rate": 0.00010591850751169552,
      "loss": 0.0376,
      "step": 72660
    },
    {
      "epoch": 21.298358733880423,
      "grad_norm": 0.8610221743583679,
      "learning_rate": 0.00010583471122410715,
      "loss": 0.035,
      "step": 72670
    },
    {
      "epoch": 21.30128956623681,
      "grad_norm": 0.9973687529563904,
      "learning_rate": 0.00010575091493651878,
      "loss": 0.051,
      "step": 72680
    },
    {
      "epoch": 21.304220398593202,
      "grad_norm": 0.2940879166126251,
      "learning_rate": 0.00010566711864893043,
      "loss": 0.032,
      "step": 72690
    },
    {
      "epoch": 21.30715123094959,
      "grad_norm": 2.0029549598693848,
      "learning_rate": 0.00010558332236134204,
      "loss": 0.0418,
      "step": 72700
    },
    {
      "epoch": 21.31008206330598,
      "grad_norm": 2.051520347595215,
      "learning_rate": 0.00010549952607375369,
      "loss": 0.0423,
      "step": 72710
    },
    {
      "epoch": 21.313012895662368,
      "grad_norm": 1.4104183912277222,
      "learning_rate": 0.00010541572978616533,
      "loss": 0.045,
      "step": 72720
    },
    {
      "epoch": 21.31594372801876,
      "grad_norm": 1.5067555904388428,
      "learning_rate": 0.00010533193349857695,
      "loss": 0.0361,
      "step": 72730
    },
    {
      "epoch": 21.318874560375146,
      "grad_norm": 1.793685793876648,
      "learning_rate": 0.00010524813721098859,
      "loss": 0.053,
      "step": 72740
    },
    {
      "epoch": 21.321805392731537,
      "grad_norm": 0.7116531133651733,
      "learning_rate": 0.00010516434092340023,
      "loss": 0.0356,
      "step": 72750
    },
    {
      "epoch": 21.324736225087925,
      "grad_norm": 1.279983401298523,
      "learning_rate": 0.00010508054463581185,
      "loss": 0.0352,
      "step": 72760
    },
    {
      "epoch": 21.327667057444316,
      "grad_norm": 0.2560340464115143,
      "learning_rate": 0.00010499674834822349,
      "loss": 0.0221,
      "step": 72770
    },
    {
      "epoch": 21.330597889800703,
      "grad_norm": 1.122450828552246,
      "learning_rate": 0.00010491295206063511,
      "loss": 0.0452,
      "step": 72780
    },
    {
      "epoch": 21.33352872215709,
      "grad_norm": 2.477674961090088,
      "learning_rate": 0.00010482915577304675,
      "loss": 0.0333,
      "step": 72790
    },
    {
      "epoch": 21.336459554513482,
      "grad_norm": 2.3323893547058105,
      "learning_rate": 0.0001047453594854584,
      "loss": 0.0486,
      "step": 72800
    },
    {
      "epoch": 21.33939038686987,
      "grad_norm": 1.2117640972137451,
      "learning_rate": 0.00010466156319787,
      "loss": 0.0408,
      "step": 72810
    },
    {
      "epoch": 21.34232121922626,
      "grad_norm": 0.2747268080711365,
      "learning_rate": 0.00010457776691028166,
      "loss": 0.0289,
      "step": 72820
    },
    {
      "epoch": 21.345252051582648,
      "grad_norm": 0.2737002968788147,
      "learning_rate": 0.0001044939706226933,
      "loss": 0.0346,
      "step": 72830
    },
    {
      "epoch": 21.34818288393904,
      "grad_norm": 0.5022983551025391,
      "learning_rate": 0.00010441017433510492,
      "loss": 0.0408,
      "step": 72840
    },
    {
      "epoch": 21.351113716295426,
      "grad_norm": 0.7372909188270569,
      "learning_rate": 0.00010432637804751655,
      "loss": 0.0207,
      "step": 72850
    },
    {
      "epoch": 21.354044548651817,
      "grad_norm": 0.8389388918876648,
      "learning_rate": 0.00010424258175992819,
      "loss": 0.036,
      "step": 72860
    },
    {
      "epoch": 21.356975381008205,
      "grad_norm": 2.455695867538452,
      "learning_rate": 0.00010415878547233981,
      "loss": 0.0433,
      "step": 72870
    },
    {
      "epoch": 21.359906213364596,
      "grad_norm": 0.779458224773407,
      "learning_rate": 0.00010407498918475145,
      "loss": 0.0404,
      "step": 72880
    },
    {
      "epoch": 21.362837045720983,
      "grad_norm": 0.467878520488739,
      "learning_rate": 0.00010399119289716309,
      "loss": 0.0285,
      "step": 72890
    },
    {
      "epoch": 21.365767878077374,
      "grad_norm": 0.3154386878013611,
      "learning_rate": 0.00010390739660957471,
      "loss": 0.0447,
      "step": 72900
    },
    {
      "epoch": 21.368698710433762,
      "grad_norm": 0.7782100439071655,
      "learning_rate": 0.00010382360032198636,
      "loss": 0.0433,
      "step": 72910
    },
    {
      "epoch": 21.371629542790153,
      "grad_norm": 1.71051824092865,
      "learning_rate": 0.00010373980403439797,
      "loss": 0.0401,
      "step": 72920
    },
    {
      "epoch": 21.37456037514654,
      "grad_norm": 0.7955797910690308,
      "learning_rate": 0.00010365600774680962,
      "loss": 0.0257,
      "step": 72930
    },
    {
      "epoch": 21.37749120750293,
      "grad_norm": 1.5029484033584595,
      "learning_rate": 0.00010357221145922126,
      "loss": 0.0324,
      "step": 72940
    },
    {
      "epoch": 21.38042203985932,
      "grad_norm": 0.7775508165359497,
      "learning_rate": 0.00010348841517163288,
      "loss": 0.0495,
      "step": 72950
    },
    {
      "epoch": 21.38335287221571,
      "grad_norm": 0.404308557510376,
      "learning_rate": 0.00010340461888404452,
      "loss": 0.0451,
      "step": 72960
    },
    {
      "epoch": 21.386283704572097,
      "grad_norm": 0.8400343060493469,
      "learning_rate": 0.00010332082259645615,
      "loss": 0.0219,
      "step": 72970
    },
    {
      "epoch": 21.38921453692849,
      "grad_norm": 0.07733193784952164,
      "learning_rate": 0.00010323702630886778,
      "loss": 0.0465,
      "step": 72980
    },
    {
      "epoch": 21.392145369284876,
      "grad_norm": 0.7523190379142761,
      "learning_rate": 0.00010315323002127941,
      "loss": 0.037,
      "step": 72990
    },
    {
      "epoch": 21.395076201641267,
      "grad_norm": 0.6002689599990845,
      "learning_rate": 0.00010306943373369107,
      "loss": 0.034,
      "step": 73000
    },
    {
      "epoch": 21.398007033997654,
      "grad_norm": 0.5186640024185181,
      "learning_rate": 0.00010298563744610267,
      "loss": 0.0442,
      "step": 73010
    },
    {
      "epoch": 21.400937866354045,
      "grad_norm": 0.9688014388084412,
      "learning_rate": 0.00010290184115851433,
      "loss": 0.0464,
      "step": 73020
    },
    {
      "epoch": 21.403868698710433,
      "grad_norm": 0.6393700838088989,
      "learning_rate": 0.00010281804487092596,
      "loss": 0.0421,
      "step": 73030
    },
    {
      "epoch": 21.406799531066824,
      "grad_norm": 0.9569377899169922,
      "learning_rate": 0.00010273424858333759,
      "loss": 0.0283,
      "step": 73040
    },
    {
      "epoch": 21.40973036342321,
      "grad_norm": 0.4876835346221924,
      "learning_rate": 0.00010265045229574922,
      "loss": 0.0436,
      "step": 73050
    },
    {
      "epoch": 21.412661195779602,
      "grad_norm": 2.4951181411743164,
      "learning_rate": 0.00010256665600816085,
      "loss": 0.0558,
      "step": 73060
    },
    {
      "epoch": 21.41559202813599,
      "grad_norm": 2.747535228729248,
      "learning_rate": 0.00010248285972057248,
      "loss": 0.0359,
      "step": 73070
    },
    {
      "epoch": 21.41852286049238,
      "grad_norm": 0.3560982048511505,
      "learning_rate": 0.00010239906343298412,
      "loss": 0.0467,
      "step": 73080
    },
    {
      "epoch": 21.421453692848768,
      "grad_norm": 1.5358577966690063,
      "learning_rate": 0.00010231526714539574,
      "loss": 0.0357,
      "step": 73090
    },
    {
      "epoch": 21.42438452520516,
      "grad_norm": 0.46733346581459045,
      "learning_rate": 0.00010223147085780738,
      "loss": 0.0449,
      "step": 73100
    },
    {
      "epoch": 21.427315357561547,
      "grad_norm": 0.7971773743629456,
      "learning_rate": 0.00010214767457021903,
      "loss": 0.0312,
      "step": 73110
    },
    {
      "epoch": 21.430246189917938,
      "grad_norm": 1.2017908096313477,
      "learning_rate": 0.00010206387828263064,
      "loss": 0.0242,
      "step": 73120
    },
    {
      "epoch": 21.433177022274325,
      "grad_norm": 0.3892443776130676,
      "learning_rate": 0.00010198008199504229,
      "loss": 0.0273,
      "step": 73130
    },
    {
      "epoch": 21.436107854630716,
      "grad_norm": 0.5293996930122375,
      "learning_rate": 0.00010189628570745393,
      "loss": 0.0202,
      "step": 73140
    },
    {
      "epoch": 21.439038686987104,
      "grad_norm": 1.2936689853668213,
      "learning_rate": 0.00010181248941986555,
      "loss": 0.0515,
      "step": 73150
    },
    {
      "epoch": 21.441969519343495,
      "grad_norm": 0.869368851184845,
      "learning_rate": 0.00010172869313227719,
      "loss": 0.0236,
      "step": 73160
    },
    {
      "epoch": 21.444900351699882,
      "grad_norm": 0.29548320174217224,
      "learning_rate": 0.00010164489684468882,
      "loss": 0.018,
      "step": 73170
    },
    {
      "epoch": 21.447831184056273,
      "grad_norm": 0.36408790946006775,
      "learning_rate": 0.00010156110055710045,
      "loss": 0.0191,
      "step": 73180
    },
    {
      "epoch": 21.45076201641266,
      "grad_norm": 1.437907338142395,
      "learning_rate": 0.00010147730426951208,
      "loss": 0.0414,
      "step": 73190
    },
    {
      "epoch": 21.45369284876905,
      "grad_norm": 0.38196563720703125,
      "learning_rate": 0.00010139350798192372,
      "loss": 0.0431,
      "step": 73200
    },
    {
      "epoch": 21.45662368112544,
      "grad_norm": 1.1342320442199707,
      "learning_rate": 0.00010130971169433534,
      "loss": 0.0298,
      "step": 73210
    },
    {
      "epoch": 21.45955451348183,
      "grad_norm": 0.4358827769756317,
      "learning_rate": 0.000101225915406747,
      "loss": 0.0474,
      "step": 73220
    },
    {
      "epoch": 21.462485345838218,
      "grad_norm": 0.9299756288528442,
      "learning_rate": 0.0001011421191191586,
      "loss": 0.0271,
      "step": 73230
    },
    {
      "epoch": 21.46541617819461,
      "grad_norm": 1.4121754169464111,
      "learning_rate": 0.00010105832283157025,
      "loss": 0.0526,
      "step": 73240
    },
    {
      "epoch": 21.468347010550996,
      "grad_norm": 0.434284508228302,
      "learning_rate": 0.00010097452654398189,
      "loss": 0.0351,
      "step": 73250
    },
    {
      "epoch": 21.471277842907387,
      "grad_norm": 0.6461610794067383,
      "learning_rate": 0.00010089073025639351,
      "loss": 0.0378,
      "step": 73260
    },
    {
      "epoch": 21.474208675263775,
      "grad_norm": 1.2490959167480469,
      "learning_rate": 0.00010080693396880515,
      "loss": 0.0599,
      "step": 73270
    },
    {
      "epoch": 21.477139507620166,
      "grad_norm": 0.14626365900039673,
      "learning_rate": 0.00010072313768121679,
      "loss": 0.0257,
      "step": 73280
    },
    {
      "epoch": 21.480070339976553,
      "grad_norm": 0.06559721380472183,
      "learning_rate": 0.00010063934139362841,
      "loss": 0.0277,
      "step": 73290
    },
    {
      "epoch": 21.483001172332944,
      "grad_norm": 1.3525763750076294,
      "learning_rate": 0.00010055554510604005,
      "loss": 0.0316,
      "step": 73300
    },
    {
      "epoch": 21.48593200468933,
      "grad_norm": 0.8430867791175842,
      "learning_rate": 0.00010047174881845168,
      "loss": 0.0298,
      "step": 73310
    },
    {
      "epoch": 21.488862837045723,
      "grad_norm": 0.15602539479732513,
      "learning_rate": 0.00010038795253086331,
      "loss": 0.0518,
      "step": 73320
    },
    {
      "epoch": 21.49179366940211,
      "grad_norm": 0.6828360557556152,
      "learning_rate": 0.00010030415624327496,
      "loss": 0.0275,
      "step": 73330
    },
    {
      "epoch": 21.4947245017585,
      "grad_norm": 1.6067770719528198,
      "learning_rate": 0.0001002203599556866,
      "loss": 0.0378,
      "step": 73340
    },
    {
      "epoch": 21.49765533411489,
      "grad_norm": 0.7936879992485046,
      "learning_rate": 0.00010013656366809822,
      "loss": 0.038,
      "step": 73350
    },
    {
      "epoch": 21.50058616647128,
      "grad_norm": 0.39333978295326233,
      "learning_rate": 0.00010005276738050985,
      "loss": 0.0277,
      "step": 73360
    },
    {
      "epoch": 21.503516998827667,
      "grad_norm": 0.9381083250045776,
      "learning_rate": 9.996897109292148e-05,
      "loss": 0.0262,
      "step": 73370
    },
    {
      "epoch": 21.506447831184055,
      "grad_norm": 1.0315794944763184,
      "learning_rate": 9.988517480533311e-05,
      "loss": 0.0279,
      "step": 73380
    },
    {
      "epoch": 21.509378663540446,
      "grad_norm": 0.5776851773262024,
      "learning_rate": 9.980137851774475e-05,
      "loss": 0.0363,
      "step": 73390
    },
    {
      "epoch": 21.512309495896833,
      "grad_norm": 1.077719807624817,
      "learning_rate": 9.971758223015637e-05,
      "loss": 0.0254,
      "step": 73400
    },
    {
      "epoch": 21.515240328253224,
      "grad_norm": 1.8168553113937378,
      "learning_rate": 9.963378594256801e-05,
      "loss": 0.0305,
      "step": 73410
    },
    {
      "epoch": 21.51817116060961,
      "grad_norm": 1.7813029289245605,
      "learning_rate": 9.954998965497965e-05,
      "loss": 0.0341,
      "step": 73420
    },
    {
      "epoch": 21.521101992966003,
      "grad_norm": 1.0074057579040527,
      "learning_rate": 9.946619336739127e-05,
      "loss": 0.0191,
      "step": 73430
    },
    {
      "epoch": 21.52403282532239,
      "grad_norm": 1.697406530380249,
      "learning_rate": 9.938239707980292e-05,
      "loss": 0.0258,
      "step": 73440
    },
    {
      "epoch": 21.52696365767878,
      "grad_norm": 0.6219547986984253,
      "learning_rate": 9.929860079221456e-05,
      "loss": 0.0453,
      "step": 73450
    },
    {
      "epoch": 21.52989449003517,
      "grad_norm": 1.7124192714691162,
      "learning_rate": 9.921480450462618e-05,
      "loss": 0.0514,
      "step": 73460
    },
    {
      "epoch": 21.53282532239156,
      "grad_norm": 1.2699891328811646,
      "learning_rate": 9.913100821703782e-05,
      "loss": 0.0344,
      "step": 73470
    },
    {
      "epoch": 21.535756154747947,
      "grad_norm": 0.43425658345222473,
      "learning_rate": 9.904721192944946e-05,
      "loss": 0.0489,
      "step": 73480
    },
    {
      "epoch": 21.538686987104338,
      "grad_norm": 1.0271620750427246,
      "learning_rate": 9.896341564186108e-05,
      "loss": 0.0249,
      "step": 73490
    },
    {
      "epoch": 21.541617819460726,
      "grad_norm": 0.3329522907733917,
      "learning_rate": 9.887961935427272e-05,
      "loss": 0.0369,
      "step": 73500
    },
    {
      "epoch": 21.544548651817117,
      "grad_norm": 0.9443899989128113,
      "learning_rate": 9.879582306668435e-05,
      "loss": 0.0424,
      "step": 73510
    },
    {
      "epoch": 21.547479484173504,
      "grad_norm": 0.5298452377319336,
      "learning_rate": 9.871202677909598e-05,
      "loss": 0.0545,
      "step": 73520
    },
    {
      "epoch": 21.550410316529895,
      "grad_norm": 2.027852773666382,
      "learning_rate": 9.862823049150761e-05,
      "loss": 0.0505,
      "step": 73530
    },
    {
      "epoch": 21.553341148886282,
      "grad_norm": 0.6255384683609009,
      "learning_rate": 9.854443420391924e-05,
      "loss": 0.0365,
      "step": 73540
    },
    {
      "epoch": 21.556271981242674,
      "grad_norm": 1.5217641592025757,
      "learning_rate": 9.846063791633089e-05,
      "loss": 0.0362,
      "step": 73550
    },
    {
      "epoch": 21.55920281359906,
      "grad_norm": 0.28089097142219543,
      "learning_rate": 9.837684162874252e-05,
      "loss": 0.048,
      "step": 73560
    },
    {
      "epoch": 21.562133645955452,
      "grad_norm": 0.8494806289672852,
      "learning_rate": 9.829304534115415e-05,
      "loss": 0.0298,
      "step": 73570
    },
    {
      "epoch": 21.56506447831184,
      "grad_norm": 0.34304508566856384,
      "learning_rate": 9.820924905356578e-05,
      "loss": 0.0374,
      "step": 73580
    },
    {
      "epoch": 21.56799531066823,
      "grad_norm": 1.3597362041473389,
      "learning_rate": 9.812545276597742e-05,
      "loss": 0.0244,
      "step": 73590
    },
    {
      "epoch": 21.570926143024618,
      "grad_norm": 0.49596887826919556,
      "learning_rate": 9.804165647838904e-05,
      "loss": 0.0343,
      "step": 73600
    },
    {
      "epoch": 21.57385697538101,
      "grad_norm": 0.1335466355085373,
      "learning_rate": 9.795786019080068e-05,
      "loss": 0.0418,
      "step": 73610
    },
    {
      "epoch": 21.576787807737396,
      "grad_norm": 0.20601259171962738,
      "learning_rate": 9.787406390321232e-05,
      "loss": 0.0429,
      "step": 73620
    },
    {
      "epoch": 21.579718640093787,
      "grad_norm": 0.3325166702270508,
      "learning_rate": 9.779026761562394e-05,
      "loss": 0.038,
      "step": 73630
    },
    {
      "epoch": 21.582649472450175,
      "grad_norm": 0.6270747780799866,
      "learning_rate": 9.770647132803558e-05,
      "loss": 0.04,
      "step": 73640
    },
    {
      "epoch": 21.585580304806566,
      "grad_norm": 1.03115975856781,
      "learning_rate": 9.762267504044723e-05,
      "loss": 0.0563,
      "step": 73650
    },
    {
      "epoch": 21.588511137162953,
      "grad_norm": 0.8253624439239502,
      "learning_rate": 9.753887875285885e-05,
      "loss": 0.0293,
      "step": 73660
    },
    {
      "epoch": 21.591441969519344,
      "grad_norm": 1.863590121269226,
      "learning_rate": 9.745508246527049e-05,
      "loss": 0.0346,
      "step": 73670
    },
    {
      "epoch": 21.594372801875732,
      "grad_norm": 0.5201289057731628,
      "learning_rate": 9.737128617768211e-05,
      "loss": 0.0319,
      "step": 73680
    },
    {
      "epoch": 21.597303634232123,
      "grad_norm": 0.5681307911872864,
      "learning_rate": 9.728748989009375e-05,
      "loss": 0.037,
      "step": 73690
    },
    {
      "epoch": 21.60023446658851,
      "grad_norm": 0.17935606837272644,
      "learning_rate": 9.720369360250538e-05,
      "loss": 0.0295,
      "step": 73700
    },
    {
      "epoch": 21.6031652989449,
      "grad_norm": 0.6788191795349121,
      "learning_rate": 9.711989731491701e-05,
      "loss": 0.0389,
      "step": 73710
    },
    {
      "epoch": 21.60609613130129,
      "grad_norm": 0.24116791784763336,
      "learning_rate": 9.703610102732864e-05,
      "loss": 0.0276,
      "step": 73720
    },
    {
      "epoch": 21.60902696365768,
      "grad_norm": 0.7182139754295349,
      "learning_rate": 9.695230473974028e-05,
      "loss": 0.0264,
      "step": 73730
    },
    {
      "epoch": 21.611957796014067,
      "grad_norm": 0.562654435634613,
      "learning_rate": 9.68685084521519e-05,
      "loss": 0.0317,
      "step": 73740
    },
    {
      "epoch": 21.61488862837046,
      "grad_norm": 0.8011791706085205,
      "learning_rate": 9.678471216456355e-05,
      "loss": 0.032,
      "step": 73750
    },
    {
      "epoch": 21.617819460726846,
      "grad_norm": 1.4794256687164307,
      "learning_rate": 9.670091587697519e-05,
      "loss": 0.029,
      "step": 73760
    },
    {
      "epoch": 21.620750293083237,
      "grad_norm": 2.492511749267578,
      "learning_rate": 9.661711958938681e-05,
      "loss": 0.0414,
      "step": 73770
    },
    {
      "epoch": 21.623681125439624,
      "grad_norm": 0.7441529035568237,
      "learning_rate": 9.653332330179845e-05,
      "loss": 0.0402,
      "step": 73780
    },
    {
      "epoch": 21.626611957796015,
      "grad_norm": 1.1303222179412842,
      "learning_rate": 9.644952701421009e-05,
      "loss": 0.036,
      "step": 73790
    },
    {
      "epoch": 21.629542790152403,
      "grad_norm": 1.252740502357483,
      "learning_rate": 9.636573072662171e-05,
      "loss": 0.0483,
      "step": 73800
    },
    {
      "epoch": 21.632473622508794,
      "grad_norm": 0.08235644549131393,
      "learning_rate": 9.628193443903335e-05,
      "loss": 0.0271,
      "step": 73810
    },
    {
      "epoch": 21.63540445486518,
      "grad_norm": 0.27520182728767395,
      "learning_rate": 9.619813815144499e-05,
      "loss": 0.0247,
      "step": 73820
    },
    {
      "epoch": 21.638335287221572,
      "grad_norm": 1.6673097610473633,
      "learning_rate": 9.611434186385661e-05,
      "loss": 0.0422,
      "step": 73830
    },
    {
      "epoch": 21.64126611957796,
      "grad_norm": 1.0821921825408936,
      "learning_rate": 9.603054557626825e-05,
      "loss": 0.0443,
      "step": 73840
    },
    {
      "epoch": 21.64419695193435,
      "grad_norm": 0.8952640891075134,
      "learning_rate": 9.594674928867987e-05,
      "loss": 0.0379,
      "step": 73850
    },
    {
      "epoch": 21.64712778429074,
      "grad_norm": 0.5649998784065247,
      "learning_rate": 9.586295300109152e-05,
      "loss": 0.0401,
      "step": 73860
    },
    {
      "epoch": 21.65005861664713,
      "grad_norm": 0.6886745095252991,
      "learning_rate": 9.577915671350316e-05,
      "loss": 0.0419,
      "step": 73870
    },
    {
      "epoch": 21.652989449003517,
      "grad_norm": 2.0123443603515625,
      "learning_rate": 9.569536042591478e-05,
      "loss": 0.0341,
      "step": 73880
    },
    {
      "epoch": 21.655920281359908,
      "grad_norm": 1.2607862949371338,
      "learning_rate": 9.561156413832642e-05,
      "loss": 0.04,
      "step": 73890
    },
    {
      "epoch": 21.658851113716295,
      "grad_norm": 0.3166132867336273,
      "learning_rate": 9.552776785073805e-05,
      "loss": 0.0175,
      "step": 73900
    },
    {
      "epoch": 21.661781946072686,
      "grad_norm": 0.9054419994354248,
      "learning_rate": 9.544397156314968e-05,
      "loss": 0.0286,
      "step": 73910
    },
    {
      "epoch": 21.664712778429074,
      "grad_norm": 0.09646642953157425,
      "learning_rate": 9.536017527556131e-05,
      "loss": 0.0187,
      "step": 73920
    },
    {
      "epoch": 21.66764361078546,
      "grad_norm": 0.5026487112045288,
      "learning_rate": 9.527637898797295e-05,
      "loss": 0.0286,
      "step": 73930
    },
    {
      "epoch": 21.670574443141852,
      "grad_norm": 1.504767894744873,
      "learning_rate": 9.519258270038457e-05,
      "loss": 0.029,
      "step": 73940
    },
    {
      "epoch": 21.67350527549824,
      "grad_norm": 0.24826499819755554,
      "learning_rate": 9.510878641279621e-05,
      "loss": 0.04,
      "step": 73950
    },
    {
      "epoch": 21.67643610785463,
      "grad_norm": 1.7707595825195312,
      "learning_rate": 9.502499012520786e-05,
      "loss": 0.0316,
      "step": 73960
    },
    {
      "epoch": 21.67936694021102,
      "grad_norm": 2.3506245613098145,
      "learning_rate": 9.494119383761948e-05,
      "loss": 0.0372,
      "step": 73970
    },
    {
      "epoch": 21.68229777256741,
      "grad_norm": 1.5435320138931274,
      "learning_rate": 9.485739755003112e-05,
      "loss": 0.0314,
      "step": 73980
    },
    {
      "epoch": 21.685228604923797,
      "grad_norm": 0.34599822759628296,
      "learning_rate": 9.477360126244274e-05,
      "loss": 0.063,
      "step": 73990
    },
    {
      "epoch": 21.688159437280188,
      "grad_norm": 0.9819491505622864,
      "learning_rate": 9.468980497485438e-05,
      "loss": 0.0336,
      "step": 74000
    },
    {
      "epoch": 21.691090269636575,
      "grad_norm": 0.4115633964538574,
      "learning_rate": 9.460600868726602e-05,
      "loss": 0.052,
      "step": 74010
    },
    {
      "epoch": 21.694021101992966,
      "grad_norm": 0.7627604007720947,
      "learning_rate": 9.452221239967764e-05,
      "loss": 0.0603,
      "step": 74020
    },
    {
      "epoch": 21.696951934349354,
      "grad_norm": 0.029875177890062332,
      "learning_rate": 9.443841611208928e-05,
      "loss": 0.0512,
      "step": 74030
    },
    {
      "epoch": 21.699882766705745,
      "grad_norm": 2.3880059719085693,
      "learning_rate": 9.435461982450091e-05,
      "loss": 0.0363,
      "step": 74040
    },
    {
      "epoch": 21.702813599062132,
      "grad_norm": 1.251740574836731,
      "learning_rate": 9.427082353691254e-05,
      "loss": 0.0333,
      "step": 74050
    },
    {
      "epoch": 21.705744431418523,
      "grad_norm": 0.506783664226532,
      "learning_rate": 9.418702724932417e-05,
      "loss": 0.0299,
      "step": 74060
    },
    {
      "epoch": 21.70867526377491,
      "grad_norm": 0.8009504675865173,
      "learning_rate": 9.410323096173582e-05,
      "loss": 0.0449,
      "step": 74070
    },
    {
      "epoch": 21.7116060961313,
      "grad_norm": 0.7885809540748596,
      "learning_rate": 9.401943467414745e-05,
      "loss": 0.0276,
      "step": 74080
    },
    {
      "epoch": 21.71453692848769,
      "grad_norm": 0.95485520362854,
      "learning_rate": 9.393563838655908e-05,
      "loss": 0.0315,
      "step": 74090
    },
    {
      "epoch": 21.71746776084408,
      "grad_norm": 0.7760622501373291,
      "learning_rate": 9.385184209897072e-05,
      "loss": 0.046,
      "step": 74100
    },
    {
      "epoch": 21.720398593200468,
      "grad_norm": 1.4825881719589233,
      "learning_rate": 9.376804581138234e-05,
      "loss": 0.0283,
      "step": 74110
    },
    {
      "epoch": 21.72332942555686,
      "grad_norm": 0.8261870741844177,
      "learning_rate": 9.368424952379398e-05,
      "loss": 0.0231,
      "step": 74120
    },
    {
      "epoch": 21.726260257913246,
      "grad_norm": 1.8269208669662476,
      "learning_rate": 9.360045323620562e-05,
      "loss": 0.0315,
      "step": 74130
    },
    {
      "epoch": 21.729191090269637,
      "grad_norm": 0.638530969619751,
      "learning_rate": 9.351665694861724e-05,
      "loss": 0.0296,
      "step": 74140
    },
    {
      "epoch": 21.732121922626025,
      "grad_norm": 0.6787648797035217,
      "learning_rate": 9.343286066102888e-05,
      "loss": 0.0262,
      "step": 74150
    },
    {
      "epoch": 21.735052754982416,
      "grad_norm": 0.3851892352104187,
      "learning_rate": 9.33490643734405e-05,
      "loss": 0.0209,
      "step": 74160
    },
    {
      "epoch": 21.737983587338803,
      "grad_norm": 1.3509684801101685,
      "learning_rate": 9.326526808585214e-05,
      "loss": 0.0254,
      "step": 74170
    },
    {
      "epoch": 21.740914419695194,
      "grad_norm": 0.5918715000152588,
      "learning_rate": 9.318147179826379e-05,
      "loss": 0.0441,
      "step": 74180
    },
    {
      "epoch": 21.74384525205158,
      "grad_norm": 1.3668134212493896,
      "learning_rate": 9.309767551067541e-05,
      "loss": 0.0326,
      "step": 74190
    },
    {
      "epoch": 21.746776084407973,
      "grad_norm": 1.6197350025177002,
      "learning_rate": 9.301387922308705e-05,
      "loss": 0.0391,
      "step": 74200
    },
    {
      "epoch": 21.74970691676436,
      "grad_norm": 0.6863899230957031,
      "learning_rate": 9.293008293549869e-05,
      "loss": 0.0233,
      "step": 74210
    },
    {
      "epoch": 21.75263774912075,
      "grad_norm": 0.6647087335586548,
      "learning_rate": 9.284628664791031e-05,
      "loss": 0.0371,
      "step": 74220
    },
    {
      "epoch": 21.75556858147714,
      "grad_norm": 0.9160662889480591,
      "learning_rate": 9.276249036032195e-05,
      "loss": 0.0415,
      "step": 74230
    },
    {
      "epoch": 21.75849941383353,
      "grad_norm": 0.879748523235321,
      "learning_rate": 9.267869407273358e-05,
      "loss": 0.0292,
      "step": 74240
    },
    {
      "epoch": 21.761430246189917,
      "grad_norm": 3.543102502822876,
      "learning_rate": 9.25948977851452e-05,
      "loss": 0.0364,
      "step": 74250
    },
    {
      "epoch": 21.764361078546308,
      "grad_norm": 0.6616957187652588,
      "learning_rate": 9.251110149755684e-05,
      "loss": 0.0407,
      "step": 74260
    },
    {
      "epoch": 21.767291910902696,
      "grad_norm": 0.4234829843044281,
      "learning_rate": 9.242730520996849e-05,
      "loss": 0.0432,
      "step": 74270
    },
    {
      "epoch": 21.770222743259087,
      "grad_norm": 0.6244150400161743,
      "learning_rate": 9.23435089223801e-05,
      "loss": 0.0236,
      "step": 74280
    },
    {
      "epoch": 21.773153575615474,
      "grad_norm": 0.4972705841064453,
      "learning_rate": 9.225971263479175e-05,
      "loss": 0.0412,
      "step": 74290
    },
    {
      "epoch": 21.776084407971865,
      "grad_norm": 0.34428977966308594,
      "learning_rate": 9.217591634720338e-05,
      "loss": 0.0276,
      "step": 74300
    },
    {
      "epoch": 21.779015240328253,
      "grad_norm": 0.8788637518882751,
      "learning_rate": 9.209212005961501e-05,
      "loss": 0.029,
      "step": 74310
    },
    {
      "epoch": 21.781946072684644,
      "grad_norm": 0.10239484161138535,
      "learning_rate": 9.200832377202665e-05,
      "loss": 0.0451,
      "step": 74320
    },
    {
      "epoch": 21.78487690504103,
      "grad_norm": 0.652712881565094,
      "learning_rate": 9.192452748443827e-05,
      "loss": 0.0374,
      "step": 74330
    },
    {
      "epoch": 21.787807737397422,
      "grad_norm": 1.2591527700424194,
      "learning_rate": 9.184073119684991e-05,
      "loss": 0.0241,
      "step": 74340
    },
    {
      "epoch": 21.79073856975381,
      "grad_norm": 1.7869378328323364,
      "learning_rate": 9.175693490926155e-05,
      "loss": 0.0413,
      "step": 74350
    },
    {
      "epoch": 21.7936694021102,
      "grad_norm": 0.9575565457344055,
      "learning_rate": 9.167313862167317e-05,
      "loss": 0.0312,
      "step": 74360
    },
    {
      "epoch": 21.796600234466588,
      "grad_norm": 0.08285575360059738,
      "learning_rate": 9.15893423340848e-05,
      "loss": 0.0357,
      "step": 74370
    },
    {
      "epoch": 21.79953106682298,
      "grad_norm": 2.773081064224243,
      "learning_rate": 9.150554604649646e-05,
      "loss": 0.0335,
      "step": 74380
    },
    {
      "epoch": 21.802461899179367,
      "grad_norm": 1.564465045928955,
      "learning_rate": 9.142174975890807e-05,
      "loss": 0.0465,
      "step": 74390
    },
    {
      "epoch": 21.805392731535758,
      "grad_norm": 0.47704827785491943,
      "learning_rate": 9.133795347131972e-05,
      "loss": 0.0392,
      "step": 74400
    },
    {
      "epoch": 21.808323563892145,
      "grad_norm": 0.50577712059021,
      "learning_rate": 9.125415718373135e-05,
      "loss": 0.0345,
      "step": 74410
    },
    {
      "epoch": 21.811254396248536,
      "grad_norm": 2.9404122829437256,
      "learning_rate": 9.117036089614298e-05,
      "loss": 0.0502,
      "step": 74420
    },
    {
      "epoch": 21.814185228604924,
      "grad_norm": 0.5434615015983582,
      "learning_rate": 9.108656460855461e-05,
      "loss": 0.0282,
      "step": 74430
    },
    {
      "epoch": 21.817116060961315,
      "grad_norm": 0.9472388625144958,
      "learning_rate": 9.100276832096625e-05,
      "loss": 0.0429,
      "step": 74440
    },
    {
      "epoch": 21.820046893317702,
      "grad_norm": 0.832606852054596,
      "learning_rate": 9.091897203337787e-05,
      "loss": 0.0299,
      "step": 74450
    },
    {
      "epoch": 21.822977725674093,
      "grad_norm": 0.8173126578330994,
      "learning_rate": 9.083517574578951e-05,
      "loss": 0.0353,
      "step": 74460
    },
    {
      "epoch": 21.82590855803048,
      "grad_norm": 1.011908769607544,
      "learning_rate": 9.075137945820113e-05,
      "loss": 0.0356,
      "step": 74470
    },
    {
      "epoch": 21.828839390386868,
      "grad_norm": 0.3300301134586334,
      "learning_rate": 9.066758317061277e-05,
      "loss": 0.0242,
      "step": 74480
    },
    {
      "epoch": 21.83177022274326,
      "grad_norm": 0.9169202446937561,
      "learning_rate": 9.058378688302442e-05,
      "loss": 0.0286,
      "step": 74490
    },
    {
      "epoch": 21.83470105509965,
      "grad_norm": 1.2136304378509521,
      "learning_rate": 9.049999059543604e-05,
      "loss": 0.0294,
      "step": 74500
    },
    {
      "epoch": 21.837631887456038,
      "grad_norm": 0.04736838862299919,
      "learning_rate": 9.041619430784768e-05,
      "loss": 0.0227,
      "step": 74510
    },
    {
      "epoch": 21.840562719812425,
      "grad_norm": 1.5880281925201416,
      "learning_rate": 9.033239802025932e-05,
      "loss": 0.041,
      "step": 74520
    },
    {
      "epoch": 21.843493552168816,
      "grad_norm": 1.033638596534729,
      "learning_rate": 9.024860173267094e-05,
      "loss": 0.0443,
      "step": 74530
    },
    {
      "epoch": 21.846424384525204,
      "grad_norm": 1.2352157831192017,
      "learning_rate": 9.016480544508258e-05,
      "loss": 0.0444,
      "step": 74540
    },
    {
      "epoch": 21.849355216881595,
      "grad_norm": 0.5174946784973145,
      "learning_rate": 9.008100915749421e-05,
      "loss": 0.0313,
      "step": 74550
    },
    {
      "epoch": 21.852286049237982,
      "grad_norm": 1.4725180864334106,
      "learning_rate": 8.999721286990584e-05,
      "loss": 0.0462,
      "step": 74560
    },
    {
      "epoch": 21.855216881594373,
      "grad_norm": 3.283094882965088,
      "learning_rate": 8.991341658231747e-05,
      "loss": 0.0669,
      "step": 74570
    },
    {
      "epoch": 21.85814771395076,
      "grad_norm": 0.8821629285812378,
      "learning_rate": 8.982962029472912e-05,
      "loss": 0.044,
      "step": 74580
    },
    {
      "epoch": 21.86107854630715,
      "grad_norm": 1.025992512702942,
      "learning_rate": 8.974582400714073e-05,
      "loss": 0.0331,
      "step": 74590
    },
    {
      "epoch": 21.86400937866354,
      "grad_norm": 0.15291666984558105,
      "learning_rate": 8.966202771955238e-05,
      "loss": 0.0291,
      "step": 74600
    },
    {
      "epoch": 21.86694021101993,
      "grad_norm": 0.9689904451370239,
      "learning_rate": 8.957823143196401e-05,
      "loss": 0.0429,
      "step": 74610
    },
    {
      "epoch": 21.869871043376317,
      "grad_norm": 0.2210911363363266,
      "learning_rate": 8.949443514437565e-05,
      "loss": 0.0341,
      "step": 74620
    },
    {
      "epoch": 21.87280187573271,
      "grad_norm": 0.8743101358413696,
      "learning_rate": 8.941063885678728e-05,
      "loss": 0.0325,
      "step": 74630
    },
    {
      "epoch": 21.875732708089096,
      "grad_norm": 1.5661816596984863,
      "learning_rate": 8.932684256919892e-05,
      "loss": 0.0488,
      "step": 74640
    },
    {
      "epoch": 21.878663540445487,
      "grad_norm": 0.4956940710544586,
      "learning_rate": 8.924304628161054e-05,
      "loss": 0.0259,
      "step": 74650
    },
    {
      "epoch": 21.881594372801874,
      "grad_norm": 2.2744810581207275,
      "learning_rate": 8.915924999402218e-05,
      "loss": 0.0192,
      "step": 74660
    },
    {
      "epoch": 21.884525205158265,
      "grad_norm": 0.35186100006103516,
      "learning_rate": 8.907545370643382e-05,
      "loss": 0.0465,
      "step": 74670
    },
    {
      "epoch": 21.887456037514653,
      "grad_norm": 0.22553156316280365,
      "learning_rate": 8.899165741884544e-05,
      "loss": 0.023,
      "step": 74680
    },
    {
      "epoch": 21.890386869871044,
      "grad_norm": 0.6167382597923279,
      "learning_rate": 8.890786113125708e-05,
      "loss": 0.0377,
      "step": 74690
    },
    {
      "epoch": 21.89331770222743,
      "grad_norm": 0.43178582191467285,
      "learning_rate": 8.882406484366871e-05,
      "loss": 0.0453,
      "step": 74700
    },
    {
      "epoch": 21.896248534583822,
      "grad_norm": 0.9693524241447449,
      "learning_rate": 8.874026855608035e-05,
      "loss": 0.0435,
      "step": 74710
    },
    {
      "epoch": 21.89917936694021,
      "grad_norm": 0.9715001583099365,
      "learning_rate": 8.865647226849197e-05,
      "loss": 0.0447,
      "step": 74720
    },
    {
      "epoch": 21.9021101992966,
      "grad_norm": 1.3395766019821167,
      "learning_rate": 8.857267598090361e-05,
      "loss": 0.0213,
      "step": 74730
    },
    {
      "epoch": 21.90504103165299,
      "grad_norm": 1.3764399290084839,
      "learning_rate": 8.848887969331525e-05,
      "loss": 0.0238,
      "step": 74740
    },
    {
      "epoch": 21.90797186400938,
      "grad_norm": 0.2828413248062134,
      "learning_rate": 8.840508340572688e-05,
      "loss": 0.062,
      "step": 74750
    },
    {
      "epoch": 21.910902696365767,
      "grad_norm": 0.5491711497306824,
      "learning_rate": 8.83212871181385e-05,
      "loss": 0.0325,
      "step": 74760
    },
    {
      "epoch": 21.913833528722158,
      "grad_norm": 0.737255871295929,
      "learning_rate": 8.823749083055014e-05,
      "loss": 0.0331,
      "step": 74770
    },
    {
      "epoch": 21.916764361078545,
      "grad_norm": 0.42613762617111206,
      "learning_rate": 8.815369454296178e-05,
      "loss": 0.0337,
      "step": 74780
    },
    {
      "epoch": 21.919695193434936,
      "grad_norm": 0.6436436176300049,
      "learning_rate": 8.80698982553734e-05,
      "loss": 0.0295,
      "step": 74790
    },
    {
      "epoch": 21.922626025791324,
      "grad_norm": 0.7278363704681396,
      "learning_rate": 8.798610196778504e-05,
      "loss": 0.0235,
      "step": 74800
    },
    {
      "epoch": 21.925556858147715,
      "grad_norm": 0.06654568761587143,
      "learning_rate": 8.790230568019668e-05,
      "loss": 0.0379,
      "step": 74810
    },
    {
      "epoch": 21.928487690504102,
      "grad_norm": 4.158542633056641,
      "learning_rate": 8.781850939260831e-05,
      "loss": 0.0488,
      "step": 74820
    },
    {
      "epoch": 21.931418522860493,
      "grad_norm": 1.4625359773635864,
      "learning_rate": 8.773471310501994e-05,
      "loss": 0.0336,
      "step": 74830
    },
    {
      "epoch": 21.93434935521688,
      "grad_norm": 1.234662413597107,
      "learning_rate": 8.765091681743159e-05,
      "loss": 0.0389,
      "step": 74840
    },
    {
      "epoch": 21.937280187573272,
      "grad_norm": 0.060323070734739304,
      "learning_rate": 8.756712052984321e-05,
      "loss": 0.0423,
      "step": 74850
    },
    {
      "epoch": 21.94021101992966,
      "grad_norm": 0.3671833276748657,
      "learning_rate": 8.748332424225485e-05,
      "loss": 0.0234,
      "step": 74860
    },
    {
      "epoch": 21.94314185228605,
      "grad_norm": 1.2671315670013428,
      "learning_rate": 8.739952795466647e-05,
      "loss": 0.0419,
      "step": 74870
    },
    {
      "epoch": 21.946072684642438,
      "grad_norm": 1.7018746137619019,
      "learning_rate": 8.731573166707811e-05,
      "loss": 0.0216,
      "step": 74880
    },
    {
      "epoch": 21.94900351699883,
      "grad_norm": 1.343760371208191,
      "learning_rate": 8.723193537948974e-05,
      "loss": 0.038,
      "step": 74890
    },
    {
      "epoch": 21.951934349355216,
      "grad_norm": 0.8753105998039246,
      "learning_rate": 8.714813909190137e-05,
      "loss": 0.0281,
      "step": 74900
    },
    {
      "epoch": 21.954865181711607,
      "grad_norm": 2.138256549835205,
      "learning_rate": 8.706434280431302e-05,
      "loss": 0.0189,
      "step": 74910
    },
    {
      "epoch": 21.957796014067995,
      "grad_norm": 1.3116658926010132,
      "learning_rate": 8.698054651672464e-05,
      "loss": 0.0315,
      "step": 74920
    },
    {
      "epoch": 21.960726846424386,
      "grad_norm": 0.4148126244544983,
      "learning_rate": 8.689675022913628e-05,
      "loss": 0.038,
      "step": 74930
    },
    {
      "epoch": 21.963657678780773,
      "grad_norm": 1.3681809902191162,
      "learning_rate": 8.681295394154791e-05,
      "loss": 0.0661,
      "step": 74940
    },
    {
      "epoch": 21.966588511137164,
      "grad_norm": 0.4210655093193054,
      "learning_rate": 8.672915765395955e-05,
      "loss": 0.023,
      "step": 74950
    },
    {
      "epoch": 21.969519343493552,
      "grad_norm": 0.8847641348838806,
      "learning_rate": 8.664536136637117e-05,
      "loss": 0.0208,
      "step": 74960
    },
    {
      "epoch": 21.972450175849943,
      "grad_norm": 1.3656294345855713,
      "learning_rate": 8.656156507878281e-05,
      "loss": 0.0369,
      "step": 74970
    },
    {
      "epoch": 21.97538100820633,
      "grad_norm": 0.4030349850654602,
      "learning_rate": 8.647776879119445e-05,
      "loss": 0.0194,
      "step": 74980
    },
    {
      "epoch": 21.97831184056272,
      "grad_norm": 0.7210498452186584,
      "learning_rate": 8.639397250360607e-05,
      "loss": 0.0394,
      "step": 74990
    },
    {
      "epoch": 21.98124267291911,
      "grad_norm": 0.5166829228401184,
      "learning_rate": 8.631017621601771e-05,
      "loss": 0.0327,
      "step": 75000
    },
    {
      "epoch": 21.9841735052755,
      "grad_norm": 1.1348377466201782,
      "learning_rate": 8.622637992842934e-05,
      "loss": 0.0408,
      "step": 75010
    },
    {
      "epoch": 21.987104337631887,
      "grad_norm": 0.8117797374725342,
      "learning_rate": 8.614258364084098e-05,
      "loss": 0.0218,
      "step": 75020
    },
    {
      "epoch": 21.99003516998828,
      "grad_norm": 2.522592782974243,
      "learning_rate": 8.60587873532526e-05,
      "loss": 0.0266,
      "step": 75030
    },
    {
      "epoch": 21.992966002344666,
      "grad_norm": 1.2080484628677368,
      "learning_rate": 8.597499106566424e-05,
      "loss": 0.0436,
      "step": 75040
    },
    {
      "epoch": 21.995896834701057,
      "grad_norm": 0.4244738519191742,
      "learning_rate": 8.589119477807588e-05,
      "loss": 0.0293,
      "step": 75050
    },
    {
      "epoch": 21.998827667057444,
      "grad_norm": 1.783116102218628,
      "learning_rate": 8.580739849048752e-05,
      "loss": 0.039,
      "step": 75060
    },
    {
      "epoch": 22.0,
      "eval_accuracy": 0.8229475766567754,
      "eval_f1_macro": 0.8690965359801605,
      "eval_f1_micro": 0.8758949880668258,
      "eval_f1_weighted": 0.873810575515685,
      "eval_loss": 0.06570787727832794,
      "eval_roc_auc": 0.9238828621357474,
      "eval_runtime": 147.264,
      "eval_samples_per_second": 20.596,
      "eval_steps_per_second": 2.58,
      "step": 75064
    },
    {
      "epoch": 22.00175849941383,
      "grad_norm": 1.7503418922424316,
      "learning_rate": 8.572360220289914e-05,
      "loss": 0.0261,
      "step": 75070
    },
    {
      "epoch": 22.004689331770223,
      "grad_norm": 0.69688880443573,
      "learning_rate": 8.563980591531078e-05,
      "loss": 0.0288,
      "step": 75080
    },
    {
      "epoch": 22.00762016412661,
      "grad_norm": 1.1255624294281006,
      "learning_rate": 8.555600962772241e-05,
      "loss": 0.04,
      "step": 75090
    },
    {
      "epoch": 22.010550996483,
      "grad_norm": 0.3479914963245392,
      "learning_rate": 8.547221334013404e-05,
      "loss": 0.0192,
      "step": 75100
    },
    {
      "epoch": 22.01348182883939,
      "grad_norm": 1.0546977519989014,
      "learning_rate": 8.538841705254567e-05,
      "loss": 0.0485,
      "step": 75110
    },
    {
      "epoch": 22.01641266119578,
      "grad_norm": 0.25717803835868835,
      "learning_rate": 8.530462076495731e-05,
      "loss": 0.0336,
      "step": 75120
    },
    {
      "epoch": 22.019343493552167,
      "grad_norm": 0.43356016278266907,
      "learning_rate": 8.522082447736895e-05,
      "loss": 0.0311,
      "step": 75130
    },
    {
      "epoch": 22.02227432590856,
      "grad_norm": 0.820404052734375,
      "learning_rate": 8.513702818978057e-05,
      "loss": 0.0329,
      "step": 75140
    },
    {
      "epoch": 22.025205158264946,
      "grad_norm": 1.2748401165008545,
      "learning_rate": 8.50532319021922e-05,
      "loss": 0.0247,
      "step": 75150
    },
    {
      "epoch": 22.028135990621337,
      "grad_norm": 0.23594482243061066,
      "learning_rate": 8.496943561460384e-05,
      "loss": 0.0242,
      "step": 75160
    },
    {
      "epoch": 22.031066822977724,
      "grad_norm": 2.031890392303467,
      "learning_rate": 8.488563932701548e-05,
      "loss": 0.0364,
      "step": 75170
    },
    {
      "epoch": 22.033997655334115,
      "grad_norm": 0.4894753694534302,
      "learning_rate": 8.48018430394271e-05,
      "loss": 0.0182,
      "step": 75180
    },
    {
      "epoch": 22.036928487690503,
      "grad_norm": 0.8516810536384583,
      "learning_rate": 8.471804675183874e-05,
      "loss": 0.0266,
      "step": 75190
    },
    {
      "epoch": 22.039859320046894,
      "grad_norm": 0.16123327612876892,
      "learning_rate": 8.463425046425038e-05,
      "loss": 0.0356,
      "step": 75200
    },
    {
      "epoch": 22.04279015240328,
      "grad_norm": 5.388742446899414,
      "learning_rate": 8.4550454176662e-05,
      "loss": 0.032,
      "step": 75210
    },
    {
      "epoch": 22.045720984759672,
      "grad_norm": 0.05179520323872566,
      "learning_rate": 8.446665788907365e-05,
      "loss": 0.019,
      "step": 75220
    },
    {
      "epoch": 22.04865181711606,
      "grad_norm": 0.2734701633453369,
      "learning_rate": 8.438286160148527e-05,
      "loss": 0.0206,
      "step": 75230
    },
    {
      "epoch": 22.05158264947245,
      "grad_norm": 0.4495503306388855,
      "learning_rate": 8.429906531389691e-05,
      "loss": 0.0307,
      "step": 75240
    },
    {
      "epoch": 22.054513481828838,
      "grad_norm": 0.2526187598705292,
      "learning_rate": 8.421526902630855e-05,
      "loss": 0.0552,
      "step": 75250
    },
    {
      "epoch": 22.05744431418523,
      "grad_norm": 2.3591468334198,
      "learning_rate": 8.413147273872018e-05,
      "loss": 0.0312,
      "step": 75260
    },
    {
      "epoch": 22.060375146541617,
      "grad_norm": 2.3528711795806885,
      "learning_rate": 8.404767645113181e-05,
      "loss": 0.0358,
      "step": 75270
    },
    {
      "epoch": 22.063305978898008,
      "grad_norm": 1.291551113128662,
      "learning_rate": 8.396388016354344e-05,
      "loss": 0.0262,
      "step": 75280
    },
    {
      "epoch": 22.066236811254395,
      "grad_norm": 0.985241174697876,
      "learning_rate": 8.388008387595508e-05,
      "loss": 0.0206,
      "step": 75290
    },
    {
      "epoch": 22.069167643610786,
      "grad_norm": 0.16597816348075867,
      "learning_rate": 8.37962875883667e-05,
      "loss": 0.0315,
      "step": 75300
    },
    {
      "epoch": 22.072098475967174,
      "grad_norm": 2.2455615997314453,
      "learning_rate": 8.371249130077834e-05,
      "loss": 0.0347,
      "step": 75310
    },
    {
      "epoch": 22.075029308323565,
      "grad_norm": 0.4499487280845642,
      "learning_rate": 8.362869501318998e-05,
      "loss": 0.03,
      "step": 75320
    },
    {
      "epoch": 22.077960140679952,
      "grad_norm": 3.2113142013549805,
      "learning_rate": 8.354489872560161e-05,
      "loss": 0.0177,
      "step": 75330
    },
    {
      "epoch": 22.080890973036343,
      "grad_norm": 0.8016000986099243,
      "learning_rate": 8.346110243801324e-05,
      "loss": 0.0335,
      "step": 75340
    },
    {
      "epoch": 22.08382180539273,
      "grad_norm": 0.8540802597999573,
      "learning_rate": 8.337730615042487e-05,
      "loss": 0.0257,
      "step": 75350
    },
    {
      "epoch": 22.08675263774912,
      "grad_norm": 0.03946899250149727,
      "learning_rate": 8.329350986283651e-05,
      "loss": 0.023,
      "step": 75360
    },
    {
      "epoch": 22.08968347010551,
      "grad_norm": 0.44467034935951233,
      "learning_rate": 8.320971357524815e-05,
      "loss": 0.0308,
      "step": 75370
    },
    {
      "epoch": 22.0926143024619,
      "grad_norm": 0.24317802488803864,
      "learning_rate": 8.312591728765977e-05,
      "loss": 0.0474,
      "step": 75380
    },
    {
      "epoch": 22.095545134818288,
      "grad_norm": 0.09977050870656967,
      "learning_rate": 8.304212100007141e-05,
      "loss": 0.0343,
      "step": 75390
    },
    {
      "epoch": 22.09847596717468,
      "grad_norm": 0.7061604261398315,
      "learning_rate": 8.295832471248304e-05,
      "loss": 0.0302,
      "step": 75400
    },
    {
      "epoch": 22.101406799531066,
      "grad_norm": 0.41760846972465515,
      "learning_rate": 8.287452842489467e-05,
      "loss": 0.0338,
      "step": 75410
    },
    {
      "epoch": 22.104337631887457,
      "grad_norm": 1.071229338645935,
      "learning_rate": 8.27907321373063e-05,
      "loss": 0.037,
      "step": 75420
    },
    {
      "epoch": 22.107268464243845,
      "grad_norm": 0.04941257834434509,
      "learning_rate": 8.270693584971794e-05,
      "loss": 0.025,
      "step": 75430
    },
    {
      "epoch": 22.110199296600236,
      "grad_norm": 1.5815255641937256,
      "learning_rate": 8.262313956212958e-05,
      "loss": 0.0246,
      "step": 75440
    },
    {
      "epoch": 22.113130128956623,
      "grad_norm": 0.724801778793335,
      "learning_rate": 8.25393432745412e-05,
      "loss": 0.0431,
      "step": 75450
    },
    {
      "epoch": 22.116060961313014,
      "grad_norm": 3.5590763092041016,
      "learning_rate": 8.245554698695284e-05,
      "loss": 0.035,
      "step": 75460
    },
    {
      "epoch": 22.1189917936694,
      "grad_norm": 0.7690246105194092,
      "learning_rate": 8.237175069936448e-05,
      "loss": 0.0467,
      "step": 75470
    },
    {
      "epoch": 22.121922626025793,
      "grad_norm": 0.1123523935675621,
      "learning_rate": 8.228795441177611e-05,
      "loss": 0.0296,
      "step": 75480
    },
    {
      "epoch": 22.12485345838218,
      "grad_norm": 0.7478750348091125,
      "learning_rate": 8.220415812418774e-05,
      "loss": 0.0227,
      "step": 75490
    },
    {
      "epoch": 22.12778429073857,
      "grad_norm": 0.8733584880828857,
      "learning_rate": 8.212036183659937e-05,
      "loss": 0.0205,
      "step": 75500
    },
    {
      "epoch": 22.13071512309496,
      "grad_norm": 0.874546468257904,
      "learning_rate": 8.203656554901101e-05,
      "loss": 0.018,
      "step": 75510
    },
    {
      "epoch": 22.13364595545135,
      "grad_norm": 0.12328355014324188,
      "learning_rate": 8.195276926142263e-05,
      "loss": 0.0241,
      "step": 75520
    },
    {
      "epoch": 22.136576787807737,
      "grad_norm": 0.10110944509506226,
      "learning_rate": 8.186897297383428e-05,
      "loss": 0.0305,
      "step": 75530
    },
    {
      "epoch": 22.139507620164128,
      "grad_norm": 0.037693317979574203,
      "learning_rate": 8.17851766862459e-05,
      "loss": 0.0388,
      "step": 75540
    },
    {
      "epoch": 22.142438452520516,
      "grad_norm": 0.4752597212791443,
      "learning_rate": 8.170138039865754e-05,
      "loss": 0.0304,
      "step": 75550
    },
    {
      "epoch": 22.145369284876907,
      "grad_norm": 0.8003637790679932,
      "learning_rate": 8.161758411106918e-05,
      "loss": 0.0247,
      "step": 75560
    },
    {
      "epoch": 22.148300117233294,
      "grad_norm": 0.23388731479644775,
      "learning_rate": 8.15337878234808e-05,
      "loss": 0.0201,
      "step": 75570
    },
    {
      "epoch": 22.151230949589685,
      "grad_norm": 0.7480719685554504,
      "learning_rate": 8.144999153589244e-05,
      "loss": 0.0191,
      "step": 75580
    },
    {
      "epoch": 22.154161781946073,
      "grad_norm": 0.42378634214401245,
      "learning_rate": 8.136619524830408e-05,
      "loss": 0.0408,
      "step": 75590
    },
    {
      "epoch": 22.157092614302464,
      "grad_norm": 1.8859044313430786,
      "learning_rate": 8.128239896071571e-05,
      "loss": 0.0265,
      "step": 75600
    },
    {
      "epoch": 22.16002344665885,
      "grad_norm": 2.9311723709106445,
      "learning_rate": 8.119860267312734e-05,
      "loss": 0.028,
      "step": 75610
    },
    {
      "epoch": 22.162954279015242,
      "grad_norm": 0.41380536556243896,
      "learning_rate": 8.111480638553897e-05,
      "loss": 0.0171,
      "step": 75620
    },
    {
      "epoch": 22.16588511137163,
      "grad_norm": 2.542088270187378,
      "learning_rate": 8.103101009795061e-05,
      "loss": 0.0382,
      "step": 75630
    },
    {
      "epoch": 22.168815943728017,
      "grad_norm": 1.0865947008132935,
      "learning_rate": 8.094721381036225e-05,
      "loss": 0.0241,
      "step": 75640
    },
    {
      "epoch": 22.171746776084408,
      "grad_norm": 1.761199951171875,
      "learning_rate": 8.086341752277387e-05,
      "loss": 0.0385,
      "step": 75650
    },
    {
      "epoch": 22.174677608440795,
      "grad_norm": 0.04209691286087036,
      "learning_rate": 8.077962123518551e-05,
      "loss": 0.0307,
      "step": 75660
    },
    {
      "epoch": 22.177608440797187,
      "grad_norm": 0.1633070707321167,
      "learning_rate": 8.069582494759714e-05,
      "loss": 0.0434,
      "step": 75670
    },
    {
      "epoch": 22.180539273153574,
      "grad_norm": 1.249821424484253,
      "learning_rate": 8.061202866000877e-05,
      "loss": 0.0315,
      "step": 75680
    },
    {
      "epoch": 22.183470105509965,
      "grad_norm": 0.839280366897583,
      "learning_rate": 8.05282323724204e-05,
      "loss": 0.0203,
      "step": 75690
    },
    {
      "epoch": 22.186400937866352,
      "grad_norm": 0.038396548479795456,
      "learning_rate": 8.044443608483204e-05,
      "loss": 0.0228,
      "step": 75700
    },
    {
      "epoch": 22.189331770222744,
      "grad_norm": 1.463788390159607,
      "learning_rate": 8.036063979724368e-05,
      "loss": 0.0367,
      "step": 75710
    },
    {
      "epoch": 22.19226260257913,
      "grad_norm": 0.21585096418857574,
      "learning_rate": 8.02768435096553e-05,
      "loss": 0.0241,
      "step": 75720
    },
    {
      "epoch": 22.195193434935522,
      "grad_norm": 0.14587725698947906,
      "learning_rate": 8.019304722206694e-05,
      "loss": 0.0271,
      "step": 75730
    },
    {
      "epoch": 22.19812426729191,
      "grad_norm": 3.4070777893066406,
      "learning_rate": 8.010925093447857e-05,
      "loss": 0.032,
      "step": 75740
    },
    {
      "epoch": 22.2010550996483,
      "grad_norm": 1.4570797681808472,
      "learning_rate": 8.002545464689021e-05,
      "loss": 0.0307,
      "step": 75750
    },
    {
      "epoch": 22.203985932004688,
      "grad_norm": 0.6686009168624878,
      "learning_rate": 7.994165835930183e-05,
      "loss": 0.0402,
      "step": 75760
    },
    {
      "epoch": 22.20691676436108,
      "grad_norm": 0.7267662286758423,
      "learning_rate": 7.985786207171347e-05,
      "loss": 0.0476,
      "step": 75770
    },
    {
      "epoch": 22.209847596717466,
      "grad_norm": 0.45270663499832153,
      "learning_rate": 7.977406578412511e-05,
      "loss": 0.0414,
      "step": 75780
    },
    {
      "epoch": 22.212778429073857,
      "grad_norm": 0.16937090456485748,
      "learning_rate": 7.969026949653673e-05,
      "loss": 0.0307,
      "step": 75790
    },
    {
      "epoch": 22.215709261430245,
      "grad_norm": 1.959559679031372,
      "learning_rate": 7.960647320894837e-05,
      "loss": 0.0409,
      "step": 75800
    },
    {
      "epoch": 22.218640093786636,
      "grad_norm": 0.4055796265602112,
      "learning_rate": 7.952267692136e-05,
      "loss": 0.0367,
      "step": 75810
    },
    {
      "epoch": 22.221570926143023,
      "grad_norm": 1.1365033388137817,
      "learning_rate": 7.943888063377164e-05,
      "loss": 0.0395,
      "step": 75820
    },
    {
      "epoch": 22.224501758499414,
      "grad_norm": 0.5484488606452942,
      "learning_rate": 7.935508434618326e-05,
      "loss": 0.024,
      "step": 75830
    },
    {
      "epoch": 22.227432590855802,
      "grad_norm": 1.4908692836761475,
      "learning_rate": 7.927128805859492e-05,
      "loss": 0.0273,
      "step": 75840
    },
    {
      "epoch": 22.230363423212193,
      "grad_norm": 0.5395707488059998,
      "learning_rate": 7.918749177100654e-05,
      "loss": 0.0362,
      "step": 75850
    },
    {
      "epoch": 22.23329425556858,
      "grad_norm": 0.7611168622970581,
      "learning_rate": 7.910369548341818e-05,
      "loss": 0.0168,
      "step": 75860
    },
    {
      "epoch": 22.23622508792497,
      "grad_norm": 1.6296411752700806,
      "learning_rate": 7.90198991958298e-05,
      "loss": 0.0381,
      "step": 75870
    },
    {
      "epoch": 22.23915592028136,
      "grad_norm": 0.3449481725692749,
      "learning_rate": 7.893610290824144e-05,
      "loss": 0.0334,
      "step": 75880
    },
    {
      "epoch": 22.24208675263775,
      "grad_norm": 0.2723242938518524,
      "learning_rate": 7.885230662065307e-05,
      "loss": 0.0408,
      "step": 75890
    },
    {
      "epoch": 22.245017584994137,
      "grad_norm": 1.5645406246185303,
      "learning_rate": 7.87685103330647e-05,
      "loss": 0.0366,
      "step": 75900
    },
    {
      "epoch": 22.24794841735053,
      "grad_norm": 0.20532652735710144,
      "learning_rate": 7.868471404547635e-05,
      "loss": 0.0326,
      "step": 75910
    },
    {
      "epoch": 22.250879249706916,
      "grad_norm": 0.6544132828712463,
      "learning_rate": 7.860091775788797e-05,
      "loss": 0.0236,
      "step": 75920
    },
    {
      "epoch": 22.253810082063307,
      "grad_norm": 0.23316590487957,
      "learning_rate": 7.85171214702996e-05,
      "loss": 0.0306,
      "step": 75930
    },
    {
      "epoch": 22.256740914419694,
      "grad_norm": 0.6673895120620728,
      "learning_rate": 7.843332518271124e-05,
      "loss": 0.0325,
      "step": 75940
    },
    {
      "epoch": 22.259671746776085,
      "grad_norm": 1.9533708095550537,
      "learning_rate": 7.834952889512288e-05,
      "loss": 0.0314,
      "step": 75950
    },
    {
      "epoch": 22.262602579132473,
      "grad_norm": 0.6431116461753845,
      "learning_rate": 7.82657326075345e-05,
      "loss": 0.0267,
      "step": 75960
    },
    {
      "epoch": 22.265533411488864,
      "grad_norm": 3.1747326850891113,
      "learning_rate": 7.818193631994614e-05,
      "loss": 0.032,
      "step": 75970
    },
    {
      "epoch": 22.26846424384525,
      "grad_norm": 0.11872217059135437,
      "learning_rate": 7.809814003235778e-05,
      "loss": 0.0389,
      "step": 75980
    },
    {
      "epoch": 22.271395076201642,
      "grad_norm": 1.9159250259399414,
      "learning_rate": 7.80143437447694e-05,
      "loss": 0.023,
      "step": 75990
    },
    {
      "epoch": 22.27432590855803,
      "grad_norm": 0.11063407361507416,
      "learning_rate": 7.793054745718104e-05,
      "loss": 0.0428,
      "step": 76000
    },
    {
      "epoch": 22.27725674091442,
      "grad_norm": 1.698638916015625,
      "learning_rate": 7.784675116959267e-05,
      "loss": 0.0374,
      "step": 76010
    },
    {
      "epoch": 22.28018757327081,
      "grad_norm": 0.8835564851760864,
      "learning_rate": 7.776295488200431e-05,
      "loss": 0.042,
      "step": 76020
    },
    {
      "epoch": 22.2831184056272,
      "grad_norm": 1.6636614799499512,
      "learning_rate": 7.767915859441593e-05,
      "loss": 0.0422,
      "step": 76030
    },
    {
      "epoch": 22.286049237983587,
      "grad_norm": 2.0592470169067383,
      "learning_rate": 7.759536230682757e-05,
      "loss": 0.0486,
      "step": 76040
    },
    {
      "epoch": 22.288980070339978,
      "grad_norm": 0.40672147274017334,
      "learning_rate": 7.751156601923921e-05,
      "loss": 0.0223,
      "step": 76050
    },
    {
      "epoch": 22.291910902696365,
      "grad_norm": 0.5293249487876892,
      "learning_rate": 7.742776973165084e-05,
      "loss": 0.0177,
      "step": 76060
    },
    {
      "epoch": 22.294841735052756,
      "grad_norm": 0.17273780703544617,
      "learning_rate": 7.734397344406247e-05,
      "loss": 0.0207,
      "step": 76070
    },
    {
      "epoch": 22.297772567409144,
      "grad_norm": 1.0000765323638916,
      "learning_rate": 7.72601771564741e-05,
      "loss": 0.0236,
      "step": 76080
    },
    {
      "epoch": 22.300703399765535,
      "grad_norm": 0.533543586730957,
      "learning_rate": 7.717638086888574e-05,
      "loss": 0.017,
      "step": 76090
    },
    {
      "epoch": 22.303634232121922,
      "grad_norm": 1.3059470653533936,
      "learning_rate": 7.709258458129736e-05,
      "loss": 0.0271,
      "step": 76100
    },
    {
      "epoch": 22.306565064478313,
      "grad_norm": 0.10583193600177765,
      "learning_rate": 7.7008788293709e-05,
      "loss": 0.0422,
      "step": 76110
    },
    {
      "epoch": 22.3094958968347,
      "grad_norm": 2.05124831199646,
      "learning_rate": 7.692499200612064e-05,
      "loss": 0.041,
      "step": 76120
    },
    {
      "epoch": 22.312426729191092,
      "grad_norm": 0.05568552389740944,
      "learning_rate": 7.684119571853227e-05,
      "loss": 0.0397,
      "step": 76130
    },
    {
      "epoch": 22.31535756154748,
      "grad_norm": 0.7096067070960999,
      "learning_rate": 7.67573994309439e-05,
      "loss": 0.0274,
      "step": 76140
    },
    {
      "epoch": 22.31828839390387,
      "grad_norm": 1.1543762683868408,
      "learning_rate": 7.667360314335555e-05,
      "loss": 0.0449,
      "step": 76150
    },
    {
      "epoch": 22.321219226260258,
      "grad_norm": 1.1087201833724976,
      "learning_rate": 7.658980685576717e-05,
      "loss": 0.0417,
      "step": 76160
    },
    {
      "epoch": 22.32415005861665,
      "grad_norm": 0.9311825633049011,
      "learning_rate": 7.650601056817881e-05,
      "loss": 0.0383,
      "step": 76170
    },
    {
      "epoch": 22.327080890973036,
      "grad_norm": 1.1945875883102417,
      "learning_rate": 7.642221428059043e-05,
      "loss": 0.034,
      "step": 76180
    },
    {
      "epoch": 22.330011723329427,
      "grad_norm": 0.38980498909950256,
      "learning_rate": 7.633841799300207e-05,
      "loss": 0.0285,
      "step": 76190
    },
    {
      "epoch": 22.332942555685815,
      "grad_norm": 1.222140908241272,
      "learning_rate": 7.62546217054137e-05,
      "loss": 0.0353,
      "step": 76200
    },
    {
      "epoch": 22.335873388042202,
      "grad_norm": 0.4862479865550995,
      "learning_rate": 7.617082541782533e-05,
      "loss": 0.0107,
      "step": 76210
    },
    {
      "epoch": 22.338804220398593,
      "grad_norm": 1.0557374954223633,
      "learning_rate": 7.608702913023698e-05,
      "loss": 0.0264,
      "step": 76220
    },
    {
      "epoch": 22.34173505275498,
      "grad_norm": 1.730442762374878,
      "learning_rate": 7.60032328426486e-05,
      "loss": 0.0386,
      "step": 76230
    },
    {
      "epoch": 22.34466588511137,
      "grad_norm": 1.6484477519989014,
      "learning_rate": 7.591943655506024e-05,
      "loss": 0.0395,
      "step": 76240
    },
    {
      "epoch": 22.34759671746776,
      "grad_norm": 2.836552858352661,
      "learning_rate": 7.583564026747188e-05,
      "loss": 0.0389,
      "step": 76250
    },
    {
      "epoch": 22.35052754982415,
      "grad_norm": 0.17940160632133484,
      "learning_rate": 7.575184397988351e-05,
      "loss": 0.021,
      "step": 76260
    },
    {
      "epoch": 22.353458382180538,
      "grad_norm": 1.1971005201339722,
      "learning_rate": 7.566804769229514e-05,
      "loss": 0.0349,
      "step": 76270
    },
    {
      "epoch": 22.35638921453693,
      "grad_norm": 0.5097203850746155,
      "learning_rate": 7.558425140470677e-05,
      "loss": 0.0313,
      "step": 76280
    },
    {
      "epoch": 22.359320046893316,
      "grad_norm": 0.8673688769340515,
      "learning_rate": 7.550045511711841e-05,
      "loss": 0.0412,
      "step": 76290
    },
    {
      "epoch": 22.362250879249707,
      "grad_norm": 1.517547845840454,
      "learning_rate": 7.541665882953003e-05,
      "loss": 0.0499,
      "step": 76300
    },
    {
      "epoch": 22.365181711606095,
      "grad_norm": 2.0286076068878174,
      "learning_rate": 7.533286254194167e-05,
      "loss": 0.0406,
      "step": 76310
    },
    {
      "epoch": 22.368112543962486,
      "grad_norm": 1.0168060064315796,
      "learning_rate": 7.52490662543533e-05,
      "loss": 0.0377,
      "step": 76320
    },
    {
      "epoch": 22.371043376318873,
      "grad_norm": 1.3714826107025146,
      "learning_rate": 7.516526996676494e-05,
      "loss": 0.039,
      "step": 76330
    },
    {
      "epoch": 22.373974208675264,
      "grad_norm": 0.7129397392272949,
      "learning_rate": 7.508147367917657e-05,
      "loss": 0.0408,
      "step": 76340
    },
    {
      "epoch": 22.37690504103165,
      "grad_norm": 0.8857085704803467,
      "learning_rate": 7.49976773915882e-05,
      "loss": 0.0232,
      "step": 76350
    },
    {
      "epoch": 22.379835873388043,
      "grad_norm": 0.5199773907661438,
      "learning_rate": 7.491388110399984e-05,
      "loss": 0.0253,
      "step": 76360
    },
    {
      "epoch": 22.38276670574443,
      "grad_norm": 0.7306983470916748,
      "learning_rate": 7.483008481641148e-05,
      "loss": 0.0289,
      "step": 76370
    },
    {
      "epoch": 22.38569753810082,
      "grad_norm": 0.6284741759300232,
      "learning_rate": 7.47462885288231e-05,
      "loss": 0.021,
      "step": 76380
    },
    {
      "epoch": 22.38862837045721,
      "grad_norm": 2.1148736476898193,
      "learning_rate": 7.466249224123474e-05,
      "loss": 0.0254,
      "step": 76390
    },
    {
      "epoch": 22.3915592028136,
      "grad_norm": 0.7937495112419128,
      "learning_rate": 7.457869595364637e-05,
      "loss": 0.0339,
      "step": 76400
    },
    {
      "epoch": 22.394490035169987,
      "grad_norm": 0.37905868887901306,
      "learning_rate": 7.4494899666058e-05,
      "loss": 0.023,
      "step": 76410
    },
    {
      "epoch": 22.397420867526378,
      "grad_norm": 0.9881190061569214,
      "learning_rate": 7.441110337846963e-05,
      "loss": 0.0251,
      "step": 76420
    },
    {
      "epoch": 22.400351699882766,
      "grad_norm": 2.2081756591796875,
      "learning_rate": 7.432730709088127e-05,
      "loss": 0.0388,
      "step": 76430
    },
    {
      "epoch": 22.403282532239157,
      "grad_norm": 1.4952995777130127,
      "learning_rate": 7.42435108032929e-05,
      "loss": 0.0216,
      "step": 76440
    },
    {
      "epoch": 22.406213364595544,
      "grad_norm": 0.2946474850177765,
      "learning_rate": 7.415971451570453e-05,
      "loss": 0.0302,
      "step": 76450
    },
    {
      "epoch": 22.409144196951935,
      "grad_norm": 1.2175496816635132,
      "learning_rate": 7.407591822811618e-05,
      "loss": 0.0291,
      "step": 76460
    },
    {
      "epoch": 22.412075029308323,
      "grad_norm": 0.06166795641183853,
      "learning_rate": 7.39921219405278e-05,
      "loss": 0.0661,
      "step": 76470
    },
    {
      "epoch": 22.415005861664714,
      "grad_norm": 1.9839822053909302,
      "learning_rate": 7.390832565293944e-05,
      "loss": 0.0437,
      "step": 76480
    },
    {
      "epoch": 22.4179366940211,
      "grad_norm": 2.024354934692383,
      "learning_rate": 7.382452936535106e-05,
      "loss": 0.0258,
      "step": 76490
    },
    {
      "epoch": 22.420867526377492,
      "grad_norm": 0.2658689022064209,
      "learning_rate": 7.37407330777627e-05,
      "loss": 0.019,
      "step": 76500
    },
    {
      "epoch": 22.42379835873388,
      "grad_norm": 2.812073230743408,
      "learning_rate": 7.365693679017434e-05,
      "loss": 0.0343,
      "step": 76510
    },
    {
      "epoch": 22.42672919109027,
      "grad_norm": 0.6291897296905518,
      "learning_rate": 7.357314050258596e-05,
      "loss": 0.0353,
      "step": 76520
    },
    {
      "epoch": 22.429660023446658,
      "grad_norm": 1.2615528106689453,
      "learning_rate": 7.348934421499761e-05,
      "loss": 0.0271,
      "step": 76530
    },
    {
      "epoch": 22.43259085580305,
      "grad_norm": 0.15661916136741638,
      "learning_rate": 7.340554792740923e-05,
      "loss": 0.0193,
      "step": 76540
    },
    {
      "epoch": 22.435521688159437,
      "grad_norm": 0.12463250756263733,
      "learning_rate": 7.332175163982087e-05,
      "loss": 0.0294,
      "step": 76550
    },
    {
      "epoch": 22.438452520515828,
      "grad_norm": 0.33361518383026123,
      "learning_rate": 7.323795535223251e-05,
      "loss": 0.0184,
      "step": 76560
    },
    {
      "epoch": 22.441383352872215,
      "grad_norm": 1.0057505369186401,
      "learning_rate": 7.315415906464414e-05,
      "loss": 0.0238,
      "step": 76570
    },
    {
      "epoch": 22.444314185228606,
      "grad_norm": 1.501242995262146,
      "learning_rate": 7.307036277705577e-05,
      "loss": 0.0515,
      "step": 76580
    },
    {
      "epoch": 22.447245017584994,
      "grad_norm": 3.6125354766845703,
      "learning_rate": 7.29865664894674e-05,
      "loss": 0.0377,
      "step": 76590
    },
    {
      "epoch": 22.450175849941385,
      "grad_norm": 1.0845310688018799,
      "learning_rate": 7.290277020187904e-05,
      "loss": 0.0402,
      "step": 76600
    },
    {
      "epoch": 22.453106682297772,
      "grad_norm": 0.1741013377904892,
      "learning_rate": 7.281897391429066e-05,
      "loss": 0.0299,
      "step": 76610
    },
    {
      "epoch": 22.456037514654163,
      "grad_norm": 0.9720737934112549,
      "learning_rate": 7.27351776267023e-05,
      "loss": 0.0256,
      "step": 76620
    },
    {
      "epoch": 22.45896834701055,
      "grad_norm": 0.7022634148597717,
      "learning_rate": 7.265138133911394e-05,
      "loss": 0.0459,
      "step": 76630
    },
    {
      "epoch": 22.46189917936694,
      "grad_norm": 0.2891787588596344,
      "learning_rate": 7.256758505152558e-05,
      "loss": 0.0353,
      "step": 76640
    },
    {
      "epoch": 22.46483001172333,
      "grad_norm": 0.7020077109336853,
      "learning_rate": 7.24837887639372e-05,
      "loss": 0.0552,
      "step": 76650
    },
    {
      "epoch": 22.46776084407972,
      "grad_norm": 0.7249178290367126,
      "learning_rate": 7.239999247634884e-05,
      "loss": 0.0305,
      "step": 76660
    },
    {
      "epoch": 22.470691676436108,
      "grad_norm": 0.08414976298809052,
      "learning_rate": 7.231619618876047e-05,
      "loss": 0.0294,
      "step": 76670
    },
    {
      "epoch": 22.4736225087925,
      "grad_norm": 0.8178848028182983,
      "learning_rate": 7.223239990117211e-05,
      "loss": 0.0263,
      "step": 76680
    },
    {
      "epoch": 22.476553341148886,
      "grad_norm": 0.2793274223804474,
      "learning_rate": 7.214860361358373e-05,
      "loss": 0.0303,
      "step": 76690
    },
    {
      "epoch": 22.479484173505277,
      "grad_norm": 0.16070875525474548,
      "learning_rate": 7.206480732599537e-05,
      "loss": 0.0223,
      "step": 76700
    },
    {
      "epoch": 22.482415005861665,
      "grad_norm": 0.2352077215909958,
      "learning_rate": 7.1981011038407e-05,
      "loss": 0.0241,
      "step": 76710
    },
    {
      "epoch": 22.485345838218056,
      "grad_norm": 0.7576959133148193,
      "learning_rate": 7.189721475081863e-05,
      "loss": 0.0327,
      "step": 76720
    },
    {
      "epoch": 22.488276670574443,
      "grad_norm": 0.676376461982727,
      "learning_rate": 7.181341846323027e-05,
      "loss": 0.0417,
      "step": 76730
    },
    {
      "epoch": 22.491207502930834,
      "grad_norm": 0.7354496717453003,
      "learning_rate": 7.17296221756419e-05,
      "loss": 0.0278,
      "step": 76740
    },
    {
      "epoch": 22.49413833528722,
      "grad_norm": 0.6082371473312378,
      "learning_rate": 7.164582588805354e-05,
      "loss": 0.0425,
      "step": 76750
    },
    {
      "epoch": 22.497069167643613,
      "grad_norm": 0.9969355463981628,
      "learning_rate": 7.156202960046516e-05,
      "loss": 0.041,
      "step": 76760
    },
    {
      "epoch": 22.5,
      "grad_norm": 0.5951428413391113,
      "learning_rate": 7.14782333128768e-05,
      "loss": 0.0285,
      "step": 76770
    },
    {
      "epoch": 22.50293083235639,
      "grad_norm": 2.8936588764190674,
      "learning_rate": 7.139443702528844e-05,
      "loss": 0.0413,
      "step": 76780
    },
    {
      "epoch": 22.50586166471278,
      "grad_norm": 0.17809490859508514,
      "learning_rate": 7.131064073770007e-05,
      "loss": 0.0175,
      "step": 76790
    },
    {
      "epoch": 22.508792497069166,
      "grad_norm": 0.6389768123626709,
      "learning_rate": 7.12268444501117e-05,
      "loss": 0.0341,
      "step": 76800
    },
    {
      "epoch": 22.511723329425557,
      "grad_norm": 2.487483024597168,
      "learning_rate": 7.114304816252333e-05,
      "loss": 0.0348,
      "step": 76810
    },
    {
      "epoch": 22.514654161781944,
      "grad_norm": 2.623178482055664,
      "learning_rate": 7.105925187493497e-05,
      "loss": 0.0347,
      "step": 76820
    },
    {
      "epoch": 22.517584994138335,
      "grad_norm": 1.195547103881836,
      "learning_rate": 7.097545558734659e-05,
      "loss": 0.0317,
      "step": 76830
    },
    {
      "epoch": 22.520515826494723,
      "grad_norm": 0.6302762627601624,
      "learning_rate": 7.089165929975824e-05,
      "loss": 0.0286,
      "step": 76840
    },
    {
      "epoch": 22.523446658851114,
      "grad_norm": 1.8065071105957031,
      "learning_rate": 7.080786301216987e-05,
      "loss": 0.0391,
      "step": 76850
    },
    {
      "epoch": 22.5263774912075,
      "grad_norm": 1.4614980220794678,
      "learning_rate": 7.07240667245815e-05,
      "loss": 0.0302,
      "step": 76860
    },
    {
      "epoch": 22.529308323563892,
      "grad_norm": 1.6120929718017578,
      "learning_rate": 7.064027043699314e-05,
      "loss": 0.0336,
      "step": 76870
    },
    {
      "epoch": 22.53223915592028,
      "grad_norm": 0.5298307538032532,
      "learning_rate": 7.055647414940478e-05,
      "loss": 0.0241,
      "step": 76880
    },
    {
      "epoch": 22.53516998827667,
      "grad_norm": 3.4347445964813232,
      "learning_rate": 7.04726778618164e-05,
      "loss": 0.0335,
      "step": 76890
    },
    {
      "epoch": 22.53810082063306,
      "grad_norm": 7.091597080230713,
      "learning_rate": 7.038888157422804e-05,
      "loss": 0.0268,
      "step": 76900
    },
    {
      "epoch": 22.54103165298945,
      "grad_norm": 1.3375626802444458,
      "learning_rate": 7.030508528663967e-05,
      "loss": 0.0207,
      "step": 76910
    },
    {
      "epoch": 22.543962485345837,
      "grad_norm": 0.28111615777015686,
      "learning_rate": 7.02212889990513e-05,
      "loss": 0.0385,
      "step": 76920
    },
    {
      "epoch": 22.546893317702228,
      "grad_norm": 1.3777213096618652,
      "learning_rate": 7.013749271146293e-05,
      "loss": 0.023,
      "step": 76930
    },
    {
      "epoch": 22.549824150058615,
      "grad_norm": 0.8026031851768494,
      "learning_rate": 7.005369642387457e-05,
      "loss": 0.0292,
      "step": 76940
    },
    {
      "epoch": 22.552754982415006,
      "grad_norm": 0.15978145599365234,
      "learning_rate": 6.996990013628621e-05,
      "loss": 0.0399,
      "step": 76950
    },
    {
      "epoch": 22.555685814771394,
      "grad_norm": 0.4239034056663513,
      "learning_rate": 6.988610384869783e-05,
      "loss": 0.0295,
      "step": 76960
    },
    {
      "epoch": 22.558616647127785,
      "grad_norm": 2.521984338760376,
      "learning_rate": 6.980230756110947e-05,
      "loss": 0.0384,
      "step": 76970
    },
    {
      "epoch": 22.561547479484172,
      "grad_norm": 1.3146288394927979,
      "learning_rate": 6.97185112735211e-05,
      "loss": 0.0401,
      "step": 76980
    },
    {
      "epoch": 22.564478311840563,
      "grad_norm": 1.6723401546478271,
      "learning_rate": 6.963471498593274e-05,
      "loss": 0.0448,
      "step": 76990
    },
    {
      "epoch": 22.56740914419695,
      "grad_norm": 0.7357746958732605,
      "learning_rate": 6.955091869834436e-05,
      "loss": 0.0414,
      "step": 77000
    },
    {
      "epoch": 22.570339976553342,
      "grad_norm": 0.16514770686626434,
      "learning_rate": 6.9467122410756e-05,
      "loss": 0.0434,
      "step": 77010
    },
    {
      "epoch": 22.57327080890973,
      "grad_norm": 0.41151437163352966,
      "learning_rate": 6.938332612316764e-05,
      "loss": 0.036,
      "step": 77020
    },
    {
      "epoch": 22.57620164126612,
      "grad_norm": 1.3042742013931274,
      "learning_rate": 6.929952983557926e-05,
      "loss": 0.0269,
      "step": 77030
    },
    {
      "epoch": 22.579132473622508,
      "grad_norm": 1.0694785118103027,
      "learning_rate": 6.92157335479909e-05,
      "loss": 0.0369,
      "step": 77040
    },
    {
      "epoch": 22.5820633059789,
      "grad_norm": 0.9414073824882507,
      "learning_rate": 6.913193726040254e-05,
      "loss": 0.0172,
      "step": 77050
    },
    {
      "epoch": 22.584994138335286,
      "grad_norm": 0.45271626114845276,
      "learning_rate": 6.904814097281417e-05,
      "loss": 0.0245,
      "step": 77060
    },
    {
      "epoch": 22.587924970691677,
      "grad_norm": 1.9962042570114136,
      "learning_rate": 6.89643446852258e-05,
      "loss": 0.052,
      "step": 77070
    },
    {
      "epoch": 22.590855803048065,
      "grad_norm": 1.5655198097229004,
      "learning_rate": 6.888054839763743e-05,
      "loss": 0.026,
      "step": 77080
    },
    {
      "epoch": 22.593786635404456,
      "grad_norm": 0.055850643664598465,
      "learning_rate": 6.879675211004907e-05,
      "loss": 0.0339,
      "step": 77090
    },
    {
      "epoch": 22.596717467760843,
      "grad_norm": 0.5469791889190674,
      "learning_rate": 6.87129558224607e-05,
      "loss": 0.0167,
      "step": 77100
    },
    {
      "epoch": 22.599648300117234,
      "grad_norm": 0.4913037121295929,
      "learning_rate": 6.862915953487233e-05,
      "loss": 0.0247,
      "step": 77110
    },
    {
      "epoch": 22.602579132473622,
      "grad_norm": 0.506846010684967,
      "learning_rate": 6.854536324728397e-05,
      "loss": 0.041,
      "step": 77120
    },
    {
      "epoch": 22.605509964830013,
      "grad_norm": 0.6168963313102722,
      "learning_rate": 6.84615669596956e-05,
      "loss": 0.0406,
      "step": 77130
    },
    {
      "epoch": 22.6084407971864,
      "grad_norm": 2.816479206085205,
      "learning_rate": 6.837777067210723e-05,
      "loss": 0.0504,
      "step": 77140
    },
    {
      "epoch": 22.61137162954279,
      "grad_norm": 0.0939960852265358,
      "learning_rate": 6.829397438451888e-05,
      "loss": 0.0282,
      "step": 77150
    },
    {
      "epoch": 22.61430246189918,
      "grad_norm": 0.2374541163444519,
      "learning_rate": 6.82101780969305e-05,
      "loss": 0.0333,
      "step": 77160
    },
    {
      "epoch": 22.61723329425557,
      "grad_norm": 1.0816054344177246,
      "learning_rate": 6.812638180934214e-05,
      "loss": 0.0311,
      "step": 77170
    },
    {
      "epoch": 22.620164126611957,
      "grad_norm": 1.2492330074310303,
      "learning_rate": 6.804258552175376e-05,
      "loss": 0.0208,
      "step": 77180
    },
    {
      "epoch": 22.62309495896835,
      "grad_norm": 0.19862738251686096,
      "learning_rate": 6.79587892341654e-05,
      "loss": 0.0175,
      "step": 77190
    },
    {
      "epoch": 22.626025791324736,
      "grad_norm": 0.21646930277347565,
      "learning_rate": 6.787499294657703e-05,
      "loss": 0.0268,
      "step": 77200
    },
    {
      "epoch": 22.628956623681127,
      "grad_norm": 0.9597550630569458,
      "learning_rate": 6.779119665898867e-05,
      "loss": 0.0362,
      "step": 77210
    },
    {
      "epoch": 22.631887456037514,
      "grad_norm": 0.08462067693471909,
      "learning_rate": 6.77074003714003e-05,
      "loss": 0.0246,
      "step": 77220
    },
    {
      "epoch": 22.634818288393905,
      "grad_norm": 0.7356668710708618,
      "learning_rate": 6.762360408381193e-05,
      "loss": 0.0232,
      "step": 77230
    },
    {
      "epoch": 22.637749120750293,
      "grad_norm": 0.5655399560928345,
      "learning_rate": 6.753980779622357e-05,
      "loss": 0.0249,
      "step": 77240
    },
    {
      "epoch": 22.640679953106684,
      "grad_norm": 1.271822214126587,
      "learning_rate": 6.74560115086352e-05,
      "loss": 0.021,
      "step": 77250
    },
    {
      "epoch": 22.64361078546307,
      "grad_norm": 1.5651665925979614,
      "learning_rate": 6.737221522104684e-05,
      "loss": 0.032,
      "step": 77260
    },
    {
      "epoch": 22.646541617819462,
      "grad_norm": 1.0472007989883423,
      "learning_rate": 6.728841893345846e-05,
      "loss": 0.0377,
      "step": 77270
    },
    {
      "epoch": 22.64947245017585,
      "grad_norm": 0.9618315100669861,
      "learning_rate": 6.72046226458701e-05,
      "loss": 0.0316,
      "step": 77280
    },
    {
      "epoch": 22.65240328253224,
      "grad_norm": 0.1246018335223198,
      "learning_rate": 6.712082635828174e-05,
      "loss": 0.0284,
      "step": 77290
    },
    {
      "epoch": 22.65533411488863,
      "grad_norm": 0.34790101647377014,
      "learning_rate": 6.703703007069336e-05,
      "loss": 0.0385,
      "step": 77300
    },
    {
      "epoch": 22.65826494724502,
      "grad_norm": 0.9749282598495483,
      "learning_rate": 6.6953233783105e-05,
      "loss": 0.0375,
      "step": 77310
    },
    {
      "epoch": 22.661195779601407,
      "grad_norm": 0.08916232734918594,
      "learning_rate": 6.686943749551663e-05,
      "loss": 0.0251,
      "step": 77320
    },
    {
      "epoch": 22.664126611957798,
      "grad_norm": 0.15243683755397797,
      "learning_rate": 6.678564120792827e-05,
      "loss": 0.034,
      "step": 77330
    },
    {
      "epoch": 22.667057444314185,
      "grad_norm": 0.7783880233764648,
      "learning_rate": 6.67018449203399e-05,
      "loss": 0.0521,
      "step": 77340
    },
    {
      "epoch": 22.669988276670573,
      "grad_norm": 0.7086834907531738,
      "learning_rate": 6.661804863275153e-05,
      "loss": 0.0375,
      "step": 77350
    },
    {
      "epoch": 22.672919109026964,
      "grad_norm": 0.7487243413925171,
      "learning_rate": 6.653425234516317e-05,
      "loss": 0.0179,
      "step": 77360
    },
    {
      "epoch": 22.67584994138335,
      "grad_norm": 0.5866463780403137,
      "learning_rate": 6.64504560575748e-05,
      "loss": 0.0427,
      "step": 77370
    },
    {
      "epoch": 22.678780773739742,
      "grad_norm": 0.2828024625778198,
      "learning_rate": 6.636665976998643e-05,
      "loss": 0.0361,
      "step": 77380
    },
    {
      "epoch": 22.68171160609613,
      "grad_norm": 1.0311123132705688,
      "learning_rate": 6.628286348239806e-05,
      "loss": 0.0251,
      "step": 77390
    },
    {
      "epoch": 22.68464243845252,
      "grad_norm": 0.42259228229522705,
      "learning_rate": 6.61990671948097e-05,
      "loss": 0.0327,
      "step": 77400
    },
    {
      "epoch": 22.687573270808908,
      "grad_norm": 1.1293127536773682,
      "learning_rate": 6.611527090722132e-05,
      "loss": 0.0412,
      "step": 77410
    },
    {
      "epoch": 22.6905041031653,
      "grad_norm": 0.9049785137176514,
      "learning_rate": 6.603147461963296e-05,
      "loss": 0.0176,
      "step": 77420
    },
    {
      "epoch": 22.693434935521687,
      "grad_norm": 0.8351466655731201,
      "learning_rate": 6.59476783320446e-05,
      "loss": 0.0381,
      "step": 77430
    },
    {
      "epoch": 22.696365767878078,
      "grad_norm": 1.2394393682479858,
      "learning_rate": 6.586388204445624e-05,
      "loss": 0.0272,
      "step": 77440
    },
    {
      "epoch": 22.699296600234465,
      "grad_norm": 0.6073122620582581,
      "learning_rate": 6.578008575686786e-05,
      "loss": 0.0305,
      "step": 77450
    },
    {
      "epoch": 22.702227432590856,
      "grad_norm": 1.5918530225753784,
      "learning_rate": 6.569628946927951e-05,
      "loss": 0.0291,
      "step": 77460
    },
    {
      "epoch": 22.705158264947244,
      "grad_norm": 0.044802531599998474,
      "learning_rate": 6.561249318169113e-05,
      "loss": 0.031,
      "step": 77470
    },
    {
      "epoch": 22.708089097303635,
      "grad_norm": 0.5777672529220581,
      "learning_rate": 6.552869689410277e-05,
      "loss": 0.0275,
      "step": 77480
    },
    {
      "epoch": 22.711019929660022,
      "grad_norm": 0.5129677057266235,
      "learning_rate": 6.544490060651439e-05,
      "loss": 0.0171,
      "step": 77490
    },
    {
      "epoch": 22.713950762016413,
      "grad_norm": 0.39710351824760437,
      "learning_rate": 6.536110431892603e-05,
      "loss": 0.0326,
      "step": 77500
    },
    {
      "epoch": 22.7168815943728,
      "grad_norm": 0.48614898324012756,
      "learning_rate": 6.527730803133767e-05,
      "loss": 0.0217,
      "step": 77510
    },
    {
      "epoch": 22.71981242672919,
      "grad_norm": 0.4118649661540985,
      "learning_rate": 6.519351174374929e-05,
      "loss": 0.0289,
      "step": 77520
    },
    {
      "epoch": 22.72274325908558,
      "grad_norm": 0.04562371224164963,
      "learning_rate": 6.510971545616094e-05,
      "loss": 0.0362,
      "step": 77530
    },
    {
      "epoch": 22.72567409144197,
      "grad_norm": 0.41650861501693726,
      "learning_rate": 6.502591916857256e-05,
      "loss": 0.0391,
      "step": 77540
    },
    {
      "epoch": 22.728604923798358,
      "grad_norm": 0.5890216827392578,
      "learning_rate": 6.49421228809842e-05,
      "loss": 0.0283,
      "step": 77550
    },
    {
      "epoch": 22.73153575615475,
      "grad_norm": 1.7970815896987915,
      "learning_rate": 6.485832659339584e-05,
      "loss": 0.0269,
      "step": 77560
    },
    {
      "epoch": 22.734466588511136,
      "grad_norm": 0.6039084196090698,
      "learning_rate": 6.477453030580747e-05,
      "loss": 0.0421,
      "step": 77570
    },
    {
      "epoch": 22.737397420867527,
      "grad_norm": 1.9189831018447876,
      "learning_rate": 6.46907340182191e-05,
      "loss": 0.029,
      "step": 77580
    },
    {
      "epoch": 22.740328253223915,
      "grad_norm": 1.5546166896820068,
      "learning_rate": 6.460693773063073e-05,
      "loss": 0.0264,
      "step": 77590
    },
    {
      "epoch": 22.743259085580306,
      "grad_norm": 2.517956018447876,
      "learning_rate": 6.452314144304237e-05,
      "loss": 0.0205,
      "step": 77600
    },
    {
      "epoch": 22.746189917936693,
      "grad_norm": 0.7664533257484436,
      "learning_rate": 6.443934515545399e-05,
      "loss": 0.0295,
      "step": 77610
    },
    {
      "epoch": 22.749120750293084,
      "grad_norm": 0.9175674915313721,
      "learning_rate": 6.435554886786563e-05,
      "loss": 0.038,
      "step": 77620
    },
    {
      "epoch": 22.75205158264947,
      "grad_norm": 0.5812087655067444,
      "learning_rate": 6.427175258027727e-05,
      "loss": 0.0427,
      "step": 77630
    },
    {
      "epoch": 22.754982415005863,
      "grad_norm": 0.9816759824752808,
      "learning_rate": 6.41879562926889e-05,
      "loss": 0.0273,
      "step": 77640
    },
    {
      "epoch": 22.75791324736225,
      "grad_norm": 0.5828749537467957,
      "learning_rate": 6.410416000510053e-05,
      "loss": 0.0194,
      "step": 77650
    },
    {
      "epoch": 22.76084407971864,
      "grad_norm": 1.1959623098373413,
      "learning_rate": 6.402036371751216e-05,
      "loss": 0.0546,
      "step": 77660
    },
    {
      "epoch": 22.76377491207503,
      "grad_norm": 0.5767025947570801,
      "learning_rate": 6.39365674299238e-05,
      "loss": 0.0325,
      "step": 77670
    },
    {
      "epoch": 22.76670574443142,
      "grad_norm": 0.9158709645271301,
      "learning_rate": 6.385277114233544e-05,
      "loss": 0.0209,
      "step": 77680
    },
    {
      "epoch": 22.769636576787807,
      "grad_norm": 1.6900993585586548,
      "learning_rate": 6.376897485474706e-05,
      "loss": 0.0364,
      "step": 77690
    },
    {
      "epoch": 22.772567409144198,
      "grad_norm": 0.9175493717193604,
      "learning_rate": 6.36851785671587e-05,
      "loss": 0.0439,
      "step": 77700
    },
    {
      "epoch": 22.775498241500586,
      "grad_norm": 1.8749792575836182,
      "learning_rate": 6.360138227957033e-05,
      "loss": 0.0219,
      "step": 77710
    },
    {
      "epoch": 22.778429073856977,
      "grad_norm": 0.07527124881744385,
      "learning_rate": 6.351758599198196e-05,
      "loss": 0.0478,
      "step": 77720
    },
    {
      "epoch": 22.781359906213364,
      "grad_norm": 3.222653388977051,
      "learning_rate": 6.34337897043936e-05,
      "loss": 0.0216,
      "step": 77730
    },
    {
      "epoch": 22.784290738569755,
      "grad_norm": 1.6627024412155151,
      "learning_rate": 6.334999341680523e-05,
      "loss": 0.0352,
      "step": 77740
    },
    {
      "epoch": 22.787221570926143,
      "grad_norm": 0.9848482012748718,
      "learning_rate": 6.326619712921687e-05,
      "loss": 0.0215,
      "step": 77750
    },
    {
      "epoch": 22.790152403282534,
      "grad_norm": 1.6198384761810303,
      "learning_rate": 6.318240084162849e-05,
      "loss": 0.0439,
      "step": 77760
    },
    {
      "epoch": 22.79308323563892,
      "grad_norm": 2.2319247722625732,
      "learning_rate": 6.309860455404014e-05,
      "loss": 0.0228,
      "step": 77770
    },
    {
      "epoch": 22.796014067995312,
      "grad_norm": 1.4159178733825684,
      "learning_rate": 6.301480826645176e-05,
      "loss": 0.0306,
      "step": 77780
    },
    {
      "epoch": 22.7989449003517,
      "grad_norm": 1.4463123083114624,
      "learning_rate": 6.29310119788634e-05,
      "loss": 0.0198,
      "step": 77790
    },
    {
      "epoch": 22.80187573270809,
      "grad_norm": 0.39996325969696045,
      "learning_rate": 6.284721569127502e-05,
      "loss": 0.0292,
      "step": 77800
    },
    {
      "epoch": 22.804806565064478,
      "grad_norm": 1.0694141387939453,
      "learning_rate": 6.276341940368666e-05,
      "loss": 0.035,
      "step": 77810
    },
    {
      "epoch": 22.80773739742087,
      "grad_norm": 1.2799099683761597,
      "learning_rate": 6.26796231160983e-05,
      "loss": 0.0288,
      "step": 77820
    },
    {
      "epoch": 22.810668229777256,
      "grad_norm": 0.3100424110889435,
      "learning_rate": 6.259582682850992e-05,
      "loss": 0.0185,
      "step": 77830
    },
    {
      "epoch": 22.813599062133648,
      "grad_norm": 2.0704596042633057,
      "learning_rate": 6.251203054092157e-05,
      "loss": 0.0331,
      "step": 77840
    },
    {
      "epoch": 22.816529894490035,
      "grad_norm": 0.7263233065605164,
      "learning_rate": 6.24282342533332e-05,
      "loss": 0.0468,
      "step": 77850
    },
    {
      "epoch": 22.819460726846426,
      "grad_norm": 0.09887556731700897,
      "learning_rate": 6.234443796574483e-05,
      "loss": 0.0412,
      "step": 77860
    },
    {
      "epoch": 22.822391559202813,
      "grad_norm": 2.2920966148376465,
      "learning_rate": 6.226064167815647e-05,
      "loss": 0.0334,
      "step": 77870
    },
    {
      "epoch": 22.825322391559205,
      "grad_norm": 0.29711905121803284,
      "learning_rate": 6.21768453905681e-05,
      "loss": 0.0265,
      "step": 77880
    },
    {
      "epoch": 22.828253223915592,
      "grad_norm": 1.087167501449585,
      "learning_rate": 6.209304910297973e-05,
      "loss": 0.0445,
      "step": 77890
    },
    {
      "epoch": 22.83118405627198,
      "grad_norm": 3.8154914379119873,
      "learning_rate": 6.200925281539137e-05,
      "loss": 0.0521,
      "step": 77900
    },
    {
      "epoch": 22.83411488862837,
      "grad_norm": 0.8423041701316833,
      "learning_rate": 6.1925456527803e-05,
      "loss": 0.0376,
      "step": 77910
    },
    {
      "epoch": 22.83704572098476,
      "grad_norm": 0.1699765920639038,
      "learning_rate": 6.184166024021463e-05,
      "loss": 0.0197,
      "step": 77920
    },
    {
      "epoch": 22.83997655334115,
      "grad_norm": 0.9752355813980103,
      "learning_rate": 6.175786395262626e-05,
      "loss": 0.0326,
      "step": 77930
    },
    {
      "epoch": 22.842907385697536,
      "grad_norm": 1.549550175666809,
      "learning_rate": 6.16740676650379e-05,
      "loss": 0.0277,
      "step": 77940
    },
    {
      "epoch": 22.845838218053927,
      "grad_norm": 0.40321388840675354,
      "learning_rate": 6.159027137744954e-05,
      "loss": 0.0576,
      "step": 77950
    },
    {
      "epoch": 22.848769050410315,
      "grad_norm": 0.47419464588165283,
      "learning_rate": 6.150647508986116e-05,
      "loss": 0.032,
      "step": 77960
    },
    {
      "epoch": 22.851699882766706,
      "grad_norm": 0.29099124670028687,
      "learning_rate": 6.14226788022728e-05,
      "loss": 0.0143,
      "step": 77970
    },
    {
      "epoch": 22.854630715123093,
      "grad_norm": 0.10002105683088303,
      "learning_rate": 6.133888251468443e-05,
      "loss": 0.032,
      "step": 77980
    },
    {
      "epoch": 22.857561547479484,
      "grad_norm": 0.7099614143371582,
      "learning_rate": 6.125508622709607e-05,
      "loss": 0.0305,
      "step": 77990
    },
    {
      "epoch": 22.860492379835872,
      "grad_norm": 0.3433893620967865,
      "learning_rate": 6.117128993950769e-05,
      "loss": 0.0413,
      "step": 78000
    },
    {
      "epoch": 22.863423212192263,
      "grad_norm": 2.555572271347046,
      "learning_rate": 6.108749365191933e-05,
      "loss": 0.0333,
      "step": 78010
    },
    {
      "epoch": 22.86635404454865,
      "grad_norm": 0.7536277174949646,
      "learning_rate": 6.1003697364330967e-05,
      "loss": 0.0255,
      "step": 78020
    },
    {
      "epoch": 22.86928487690504,
      "grad_norm": 0.12140711396932602,
      "learning_rate": 6.0919901076742597e-05,
      "loss": 0.0242,
      "step": 78030
    },
    {
      "epoch": 22.87221570926143,
      "grad_norm": 0.6291326284408569,
      "learning_rate": 6.0836104789154227e-05,
      "loss": 0.0195,
      "step": 78040
    },
    {
      "epoch": 22.87514654161782,
      "grad_norm": 0.5179078578948975,
      "learning_rate": 6.075230850156586e-05,
      "loss": 0.0369,
      "step": 78050
    },
    {
      "epoch": 22.878077373974207,
      "grad_norm": 7.038515567779541,
      "learning_rate": 6.0668512213977493e-05,
      "loss": 0.0241,
      "step": 78060
    },
    {
      "epoch": 22.8810082063306,
      "grad_norm": 0.3177845776081085,
      "learning_rate": 6.058471592638913e-05,
      "loss": 0.0399,
      "step": 78070
    },
    {
      "epoch": 22.883939038686986,
      "grad_norm": 1.4422950744628906,
      "learning_rate": 6.050091963880077e-05,
      "loss": 0.0486,
      "step": 78080
    },
    {
      "epoch": 22.886869871043377,
      "grad_norm": 0.7427237033843994,
      "learning_rate": 6.04171233512124e-05,
      "loss": 0.032,
      "step": 78090
    },
    {
      "epoch": 22.889800703399764,
      "grad_norm": 1.4829130172729492,
      "learning_rate": 6.033332706362403e-05,
      "loss": 0.0256,
      "step": 78100
    },
    {
      "epoch": 22.892731535756155,
      "grad_norm": 0.2807486951351166,
      "learning_rate": 6.024953077603566e-05,
      "loss": 0.0283,
      "step": 78110
    },
    {
      "epoch": 22.895662368112543,
      "grad_norm": 0.0906105563044548,
      "learning_rate": 6.01657344884473e-05,
      "loss": 0.0263,
      "step": 78120
    },
    {
      "epoch": 22.898593200468934,
      "grad_norm": 0.24965547025203705,
      "learning_rate": 6.008193820085893e-05,
      "loss": 0.0378,
      "step": 78130
    },
    {
      "epoch": 22.90152403282532,
      "grad_norm": 1.1029795408248901,
      "learning_rate": 5.999814191327056e-05,
      "loss": 0.0286,
      "step": 78140
    },
    {
      "epoch": 22.904454865181712,
      "grad_norm": 1.2753605842590332,
      "learning_rate": 5.99143456256822e-05,
      "loss": 0.0272,
      "step": 78150
    },
    {
      "epoch": 22.9073856975381,
      "grad_norm": 1.362276554107666,
      "learning_rate": 5.983054933809383e-05,
      "loss": 0.0408,
      "step": 78160
    },
    {
      "epoch": 22.91031652989449,
      "grad_norm": 0.6964362859725952,
      "learning_rate": 5.974675305050546e-05,
      "loss": 0.0322,
      "step": 78170
    },
    {
      "epoch": 22.91324736225088,
      "grad_norm": 1.740816354751587,
      "learning_rate": 5.9662956762917094e-05,
      "loss": 0.0344,
      "step": 78180
    },
    {
      "epoch": 22.91617819460727,
      "grad_norm": 0.7813062071800232,
      "learning_rate": 5.957916047532873e-05,
      "loss": 0.0395,
      "step": 78190
    },
    {
      "epoch": 22.919109026963657,
      "grad_norm": 0.45100605487823486,
      "learning_rate": 5.949536418774036e-05,
      "loss": 0.0236,
      "step": 78200
    },
    {
      "epoch": 22.922039859320048,
      "grad_norm": 1.279350757598877,
      "learning_rate": 5.941156790015199e-05,
      "loss": 0.0274,
      "step": 78210
    },
    {
      "epoch": 22.924970691676435,
      "grad_norm": 1.1411713361740112,
      "learning_rate": 5.9327771612563635e-05,
      "loss": 0.0369,
      "step": 78220
    },
    {
      "epoch": 22.927901524032826,
      "grad_norm": 0.19638921320438385,
      "learning_rate": 5.9243975324975265e-05,
      "loss": 0.0306,
      "step": 78230
    },
    {
      "epoch": 22.930832356389214,
      "grad_norm": 0.26975715160369873,
      "learning_rate": 5.9160179037386895e-05,
      "loss": 0.0288,
      "step": 78240
    },
    {
      "epoch": 22.933763188745605,
      "grad_norm": 0.30529144406318665,
      "learning_rate": 5.907638274979853e-05,
      "loss": 0.0276,
      "step": 78250
    },
    {
      "epoch": 22.936694021101992,
      "grad_norm": 2.11044979095459,
      "learning_rate": 5.899258646221016e-05,
      "loss": 0.0363,
      "step": 78260
    },
    {
      "epoch": 22.939624853458383,
      "grad_norm": 0.6216939091682434,
      "learning_rate": 5.890879017462179e-05,
      "loss": 0.0309,
      "step": 78270
    },
    {
      "epoch": 22.94255568581477,
      "grad_norm": 0.246243417263031,
      "learning_rate": 5.882499388703343e-05,
      "loss": 0.0224,
      "step": 78280
    },
    {
      "epoch": 22.945486518171162,
      "grad_norm": 0.8172674179077148,
      "learning_rate": 5.8741197599445065e-05,
      "loss": 0.0267,
      "step": 78290
    },
    {
      "epoch": 22.94841735052755,
      "grad_norm": 0.45621705055236816,
      "learning_rate": 5.8657401311856695e-05,
      "loss": 0.0286,
      "step": 78300
    },
    {
      "epoch": 22.95134818288394,
      "grad_norm": 0.6438254714012146,
      "learning_rate": 5.8573605024268325e-05,
      "loss": 0.04,
      "step": 78310
    },
    {
      "epoch": 22.954279015240328,
      "grad_norm": 1.1098436117172241,
      "learning_rate": 5.848980873667997e-05,
      "loss": 0.0194,
      "step": 78320
    },
    {
      "epoch": 22.95720984759672,
      "grad_norm": 0.603902280330658,
      "learning_rate": 5.84060124490916e-05,
      "loss": 0.0171,
      "step": 78330
    },
    {
      "epoch": 22.960140679953106,
      "grad_norm": 1.4598702192306519,
      "learning_rate": 5.832221616150323e-05,
      "loss": 0.0307,
      "step": 78340
    },
    {
      "epoch": 22.963071512309497,
      "grad_norm": 1.1113349199295044,
      "learning_rate": 5.823841987391486e-05,
      "loss": 0.0477,
      "step": 78350
    },
    {
      "epoch": 22.966002344665885,
      "grad_norm": 0.505707859992981,
      "learning_rate": 5.8154623586326496e-05,
      "loss": 0.0484,
      "step": 78360
    },
    {
      "epoch": 22.968933177022276,
      "grad_norm": 0.3128095269203186,
      "learning_rate": 5.8070827298738126e-05,
      "loss": 0.0254,
      "step": 78370
    },
    {
      "epoch": 22.971864009378663,
      "grad_norm": 0.06246136501431465,
      "learning_rate": 5.7987031011149756e-05,
      "loss": 0.033,
      "step": 78380
    },
    {
      "epoch": 22.974794841735054,
      "grad_norm": 1.5278254747390747,
      "learning_rate": 5.79032347235614e-05,
      "loss": 0.0427,
      "step": 78390
    },
    {
      "epoch": 22.97772567409144,
      "grad_norm": 1.7722479104995728,
      "learning_rate": 5.781943843597303e-05,
      "loss": 0.0277,
      "step": 78400
    },
    {
      "epoch": 22.980656506447833,
      "grad_norm": 0.6249996423721313,
      "learning_rate": 5.773564214838466e-05,
      "loss": 0.0371,
      "step": 78410
    },
    {
      "epoch": 22.98358733880422,
      "grad_norm": 1.3603066205978394,
      "learning_rate": 5.765184586079629e-05,
      "loss": 0.022,
      "step": 78420
    },
    {
      "epoch": 22.98651817116061,
      "grad_norm": 0.43263038992881775,
      "learning_rate": 5.756804957320793e-05,
      "loss": 0.0299,
      "step": 78430
    },
    {
      "epoch": 22.989449003517,
      "grad_norm": 0.9675309062004089,
      "learning_rate": 5.748425328561956e-05,
      "loss": 0.0315,
      "step": 78440
    },
    {
      "epoch": 22.99237983587339,
      "grad_norm": 1.0378905534744263,
      "learning_rate": 5.740045699803119e-05,
      "loss": 0.0447,
      "step": 78450
    },
    {
      "epoch": 22.995310668229777,
      "grad_norm": 1.309460997581482,
      "learning_rate": 5.731666071044283e-05,
      "loss": 0.0352,
      "step": 78460
    },
    {
      "epoch": 22.99824150058617,
      "grad_norm": 0.14077065885066986,
      "learning_rate": 5.723286442285446e-05,
      "loss": 0.0457,
      "step": 78470
    },
    {
      "epoch": 23.0,
      "eval_accuracy": 0.8212990438509726,
      "eval_f1_macro": 0.8641606706432948,
      "eval_f1_micro": 0.8785377358490565,
      "eval_f1_weighted": 0.8779094684495694,
      "eval_loss": 0.0679461732506752,
      "eval_roc_auc": 0.9298394619799786,
      "eval_runtime": 142.2308,
      "eval_samples_per_second": 21.324,
      "eval_steps_per_second": 2.672,
      "step": 78476
    },
    {
      "epoch": 23.001172332942556,
      "grad_norm": 0.5415257811546326,
      "learning_rate": 5.714906813526609e-05,
      "loss": 0.0556,
      "step": 78480
    },
    {
      "epoch": 23.004103165298943,
      "grad_norm": 1.3410007953643799,
      "learning_rate": 5.706527184767772e-05,
      "loss": 0.0443,
      "step": 78490
    },
    {
      "epoch": 23.007033997655334,
      "grad_norm": 1.7247759103775024,
      "learning_rate": 5.6981475560089364e-05,
      "loss": 0.0337,
      "step": 78500
    },
    {
      "epoch": 23.00996483001172,
      "grad_norm": 0.3433639705181122,
      "learning_rate": 5.6897679272500994e-05,
      "loss": 0.0203,
      "step": 78510
    },
    {
      "epoch": 23.012895662368113,
      "grad_norm": 1.9069874286651611,
      "learning_rate": 5.6813882984912624e-05,
      "loss": 0.027,
      "step": 78520
    },
    {
      "epoch": 23.0158264947245,
      "grad_norm": 0.6049423813819885,
      "learning_rate": 5.673008669732427e-05,
      "loss": 0.0203,
      "step": 78530
    },
    {
      "epoch": 23.01875732708089,
      "grad_norm": 0.3200902044773102,
      "learning_rate": 5.66462904097359e-05,
      "loss": 0.0321,
      "step": 78540
    },
    {
      "epoch": 23.02168815943728,
      "grad_norm": 0.995941698551178,
      "learning_rate": 5.656249412214753e-05,
      "loss": 0.0357,
      "step": 78550
    },
    {
      "epoch": 23.02461899179367,
      "grad_norm": 0.22804094851016998,
      "learning_rate": 5.6478697834559164e-05,
      "loss": 0.0329,
      "step": 78560
    },
    {
      "epoch": 23.027549824150057,
      "grad_norm": 0.4614274203777313,
      "learning_rate": 5.6394901546970794e-05,
      "loss": 0.0238,
      "step": 78570
    },
    {
      "epoch": 23.030480656506448,
      "grad_norm": 1.2331109046936035,
      "learning_rate": 5.6311105259382424e-05,
      "loss": 0.0261,
      "step": 78580
    },
    {
      "epoch": 23.033411488862836,
      "grad_norm": 2.1365416049957275,
      "learning_rate": 5.6227308971794054e-05,
      "loss": 0.0212,
      "step": 78590
    },
    {
      "epoch": 23.036342321219227,
      "grad_norm": 0.2276265174150467,
      "learning_rate": 5.61435126842057e-05,
      "loss": 0.0465,
      "step": 78600
    },
    {
      "epoch": 23.039273153575614,
      "grad_norm": 0.3527435064315796,
      "learning_rate": 5.605971639661733e-05,
      "loss": 0.0419,
      "step": 78610
    },
    {
      "epoch": 23.042203985932005,
      "grad_norm": 0.10178529471158981,
      "learning_rate": 5.597592010902896e-05,
      "loss": 0.0207,
      "step": 78620
    },
    {
      "epoch": 23.045134818288393,
      "grad_norm": 0.21840481460094452,
      "learning_rate": 5.58921238214406e-05,
      "loss": 0.0272,
      "step": 78630
    },
    {
      "epoch": 23.048065650644784,
      "grad_norm": 1.0721818208694458,
      "learning_rate": 5.580832753385223e-05,
      "loss": 0.0238,
      "step": 78640
    },
    {
      "epoch": 23.05099648300117,
      "grad_norm": 0.18235670030117035,
      "learning_rate": 5.572453124626386e-05,
      "loss": 0.0254,
      "step": 78650
    },
    {
      "epoch": 23.053927315357562,
      "grad_norm": 0.06883721053600311,
      "learning_rate": 5.564073495867549e-05,
      "loss": 0.0267,
      "step": 78660
    },
    {
      "epoch": 23.05685814771395,
      "grad_norm": 0.7813694477081299,
      "learning_rate": 5.555693867108713e-05,
      "loss": 0.0137,
      "step": 78670
    },
    {
      "epoch": 23.05978898007034,
      "grad_norm": 2.104241371154785,
      "learning_rate": 5.547314238349876e-05,
      "loss": 0.0364,
      "step": 78680
    },
    {
      "epoch": 23.062719812426728,
      "grad_norm": 0.869795024394989,
      "learning_rate": 5.538934609591039e-05,
      "loss": 0.0317,
      "step": 78690
    },
    {
      "epoch": 23.06565064478312,
      "grad_norm": 0.33616483211517334,
      "learning_rate": 5.530554980832203e-05,
      "loss": 0.0317,
      "step": 78700
    },
    {
      "epoch": 23.068581477139507,
      "grad_norm": 0.9713664650917053,
      "learning_rate": 5.522175352073366e-05,
      "loss": 0.0197,
      "step": 78710
    },
    {
      "epoch": 23.071512309495898,
      "grad_norm": 1.340419054031372,
      "learning_rate": 5.513795723314529e-05,
      "loss": 0.0335,
      "step": 78720
    },
    {
      "epoch": 23.074443141852285,
      "grad_norm": 2.082899570465088,
      "learning_rate": 5.505416094555692e-05,
      "loss": 0.0471,
      "step": 78730
    },
    {
      "epoch": 23.077373974208676,
      "grad_norm": 0.08833033591508865,
      "learning_rate": 5.4970364657968566e-05,
      "loss": 0.0179,
      "step": 78740
    },
    {
      "epoch": 23.080304806565064,
      "grad_norm": 1.9889570474624634,
      "learning_rate": 5.4886568370380196e-05,
      "loss": 0.0097,
      "step": 78750
    },
    {
      "epoch": 23.083235638921455,
      "grad_norm": 0.6674582362174988,
      "learning_rate": 5.4802772082791826e-05,
      "loss": 0.0316,
      "step": 78760
    },
    {
      "epoch": 23.086166471277842,
      "grad_norm": 0.262260764837265,
      "learning_rate": 5.471897579520346e-05,
      "loss": 0.0187,
      "step": 78770
    },
    {
      "epoch": 23.089097303634233,
      "grad_norm": 0.5466028451919556,
      "learning_rate": 5.463517950761509e-05,
      "loss": 0.0159,
      "step": 78780
    },
    {
      "epoch": 23.09202813599062,
      "grad_norm": 0.19972491264343262,
      "learning_rate": 5.455138322002672e-05,
      "loss": 0.0215,
      "step": 78790
    },
    {
      "epoch": 23.09495896834701,
      "grad_norm": 0.3948776125907898,
      "learning_rate": 5.446758693243835e-05,
      "loss": 0.0151,
      "step": 78800
    },
    {
      "epoch": 23.0978898007034,
      "grad_norm": 0.35483264923095703,
      "learning_rate": 5.4383790644849996e-05,
      "loss": 0.0257,
      "step": 78810
    },
    {
      "epoch": 23.10082063305979,
      "grad_norm": 0.5340518355369568,
      "learning_rate": 5.4299994357261626e-05,
      "loss": 0.0242,
      "step": 78820
    },
    {
      "epoch": 23.103751465416178,
      "grad_norm": 0.20640240609645844,
      "learning_rate": 5.4216198069673256e-05,
      "loss": 0.0174,
      "step": 78830
    },
    {
      "epoch": 23.10668229777257,
      "grad_norm": 1.9888601303100586,
      "learning_rate": 5.41324017820849e-05,
      "loss": 0.0259,
      "step": 78840
    },
    {
      "epoch": 23.109613130128956,
      "grad_norm": 0.05758193880319595,
      "learning_rate": 5.404860549449653e-05,
      "loss": 0.0306,
      "step": 78850
    },
    {
      "epoch": 23.112543962485347,
      "grad_norm": 2.294663429260254,
      "learning_rate": 5.396480920690816e-05,
      "loss": 0.0271,
      "step": 78860
    },
    {
      "epoch": 23.115474794841735,
      "grad_norm": 0.9614638686180115,
      "learning_rate": 5.38810129193198e-05,
      "loss": 0.0333,
      "step": 78870
    },
    {
      "epoch": 23.118405627198126,
      "grad_norm": 0.8281221389770508,
      "learning_rate": 5.379721663173143e-05,
      "loss": 0.0354,
      "step": 78880
    },
    {
      "epoch": 23.121336459554513,
      "grad_norm": 0.14884798228740692,
      "learning_rate": 5.371342034414306e-05,
      "loss": 0.028,
      "step": 78890
    },
    {
      "epoch": 23.124267291910904,
      "grad_norm": 0.05266872048377991,
      "learning_rate": 5.362962405655469e-05,
      "loss": 0.028,
      "step": 78900
    },
    {
      "epoch": 23.12719812426729,
      "grad_norm": 1.3766530752182007,
      "learning_rate": 5.354582776896633e-05,
      "loss": 0.0271,
      "step": 78910
    },
    {
      "epoch": 23.130128956623683,
      "grad_norm": 1.2149226665496826,
      "learning_rate": 5.346203148137796e-05,
      "loss": 0.0217,
      "step": 78920
    },
    {
      "epoch": 23.13305978898007,
      "grad_norm": 0.4488445818424225,
      "learning_rate": 5.337823519378959e-05,
      "loss": 0.0217,
      "step": 78930
    },
    {
      "epoch": 23.13599062133646,
      "grad_norm": 0.9119750261306763,
      "learning_rate": 5.3294438906201234e-05,
      "loss": 0.0221,
      "step": 78940
    },
    {
      "epoch": 23.13892145369285,
      "grad_norm": 0.3147179186344147,
      "learning_rate": 5.3210642618612864e-05,
      "loss": 0.029,
      "step": 78950
    },
    {
      "epoch": 23.14185228604924,
      "grad_norm": 1.874241590499878,
      "learning_rate": 5.3126846331024494e-05,
      "loss": 0.034,
      "step": 78960
    },
    {
      "epoch": 23.144783118405627,
      "grad_norm": 0.6844467520713806,
      "learning_rate": 5.3043050043436124e-05,
      "loss": 0.0221,
      "step": 78970
    },
    {
      "epoch": 23.147713950762018,
      "grad_norm": 0.2864852249622345,
      "learning_rate": 5.295925375584776e-05,
      "loss": 0.036,
      "step": 78980
    },
    {
      "epoch": 23.150644783118405,
      "grad_norm": 1.258102297782898,
      "learning_rate": 5.287545746825939e-05,
      "loss": 0.0266,
      "step": 78990
    },
    {
      "epoch": 23.153575615474796,
      "grad_norm": 0.2996669411659241,
      "learning_rate": 5.279166118067102e-05,
      "loss": 0.0154,
      "step": 79000
    },
    {
      "epoch": 23.156506447831184,
      "grad_norm": 0.19540904462337494,
      "learning_rate": 5.2707864893082665e-05,
      "loss": 0.0186,
      "step": 79010
    },
    {
      "epoch": 23.159437280187575,
      "grad_norm": 0.14575357735157013,
      "learning_rate": 5.2624068605494295e-05,
      "loss": 0.0409,
      "step": 79020
    },
    {
      "epoch": 23.162368112543962,
      "grad_norm": 0.4954207241535187,
      "learning_rate": 5.2540272317905925e-05,
      "loss": 0.0391,
      "step": 79030
    },
    {
      "epoch": 23.165298944900353,
      "grad_norm": 0.24927008152008057,
      "learning_rate": 5.2456476030317555e-05,
      "loss": 0.031,
      "step": 79040
    },
    {
      "epoch": 23.16822977725674,
      "grad_norm": 1.089491367340088,
      "learning_rate": 5.23726797427292e-05,
      "loss": 0.0441,
      "step": 79050
    },
    {
      "epoch": 23.17116060961313,
      "grad_norm": 0.3737379014492035,
      "learning_rate": 5.228888345514083e-05,
      "loss": 0.0125,
      "step": 79060
    },
    {
      "epoch": 23.17409144196952,
      "grad_norm": 0.13406085968017578,
      "learning_rate": 5.220508716755246e-05,
      "loss": 0.0255,
      "step": 79070
    },
    {
      "epoch": 23.177022274325907,
      "grad_norm": 0.11478263139724731,
      "learning_rate": 5.2121290879964095e-05,
      "loss": 0.0453,
      "step": 79080
    },
    {
      "epoch": 23.179953106682298,
      "grad_norm": 0.14739258587360382,
      "learning_rate": 5.2037494592375725e-05,
      "loss": 0.0265,
      "step": 79090
    },
    {
      "epoch": 23.182883939038685,
      "grad_norm": 0.18751409649848938,
      "learning_rate": 5.1953698304787355e-05,
      "loss": 0.0234,
      "step": 79100
    },
    {
      "epoch": 23.185814771395076,
      "grad_norm": 3.3980917930603027,
      "learning_rate": 5.1869902017198985e-05,
      "loss": 0.0395,
      "step": 79110
    },
    {
      "epoch": 23.188745603751464,
      "grad_norm": 0.49284297227859497,
      "learning_rate": 5.178610572961063e-05,
      "loss": 0.0438,
      "step": 79120
    },
    {
      "epoch": 23.191676436107855,
      "grad_norm": 0.5695896148681641,
      "learning_rate": 5.170230944202226e-05,
      "loss": 0.0347,
      "step": 79130
    },
    {
      "epoch": 23.194607268464242,
      "grad_norm": 0.24831317365169525,
      "learning_rate": 5.161851315443389e-05,
      "loss": 0.0145,
      "step": 79140
    },
    {
      "epoch": 23.197538100820633,
      "grad_norm": 0.41104841232299805,
      "learning_rate": 5.153471686684553e-05,
      "loss": 0.0197,
      "step": 79150
    },
    {
      "epoch": 23.20046893317702,
      "grad_norm": 0.26371967792510986,
      "learning_rate": 5.145092057925716e-05,
      "loss": 0.0542,
      "step": 79160
    },
    {
      "epoch": 23.203399765533412,
      "grad_norm": 1.0698561668395996,
      "learning_rate": 5.136712429166879e-05,
      "loss": 0.0412,
      "step": 79170
    },
    {
      "epoch": 23.2063305978898,
      "grad_norm": 1.6571815013885498,
      "learning_rate": 5.128332800408042e-05,
      "loss": 0.0324,
      "step": 79180
    },
    {
      "epoch": 23.20926143024619,
      "grad_norm": 2.704853057861328,
      "learning_rate": 5.119953171649206e-05,
      "loss": 0.0551,
      "step": 79190
    },
    {
      "epoch": 23.212192262602578,
      "grad_norm": 2.7203476428985596,
      "learning_rate": 5.111573542890369e-05,
      "loss": 0.052,
      "step": 79200
    },
    {
      "epoch": 23.21512309495897,
      "grad_norm": 0.9187982082366943,
      "learning_rate": 5.103193914131532e-05,
      "loss": 0.0166,
      "step": 79210
    },
    {
      "epoch": 23.218053927315356,
      "grad_norm": 0.8639582395553589,
      "learning_rate": 5.094814285372696e-05,
      "loss": 0.0492,
      "step": 79220
    },
    {
      "epoch": 23.220984759671747,
      "grad_norm": 0.6084309816360474,
      "learning_rate": 5.086434656613859e-05,
      "loss": 0.0316,
      "step": 79230
    },
    {
      "epoch": 23.223915592028135,
      "grad_norm": 0.7412095069885254,
      "learning_rate": 5.078055027855022e-05,
      "loss": 0.0316,
      "step": 79240
    },
    {
      "epoch": 23.226846424384526,
      "grad_norm": 0.4801616370677948,
      "learning_rate": 5.069675399096186e-05,
      "loss": 0.0243,
      "step": 79250
    },
    {
      "epoch": 23.229777256740913,
      "grad_norm": 0.8747185468673706,
      "learning_rate": 5.06129577033735e-05,
      "loss": 0.0339,
      "step": 79260
    },
    {
      "epoch": 23.232708089097304,
      "grad_norm": 2.2329399585723877,
      "learning_rate": 5.052916141578513e-05,
      "loss": 0.0451,
      "step": 79270
    },
    {
      "epoch": 23.235638921453692,
      "grad_norm": 0.2552102208137512,
      "learning_rate": 5.044536512819676e-05,
      "loss": 0.0178,
      "step": 79280
    },
    {
      "epoch": 23.238569753810083,
      "grad_norm": 1.0572587251663208,
      "learning_rate": 5.0361568840608394e-05,
      "loss": 0.0314,
      "step": 79290
    },
    {
      "epoch": 23.24150058616647,
      "grad_norm": 1.7699040174484253,
      "learning_rate": 5.0277772553020024e-05,
      "loss": 0.0262,
      "step": 79300
    },
    {
      "epoch": 23.24443141852286,
      "grad_norm": 0.687578558921814,
      "learning_rate": 5.0193976265431654e-05,
      "loss": 0.0292,
      "step": 79310
    },
    {
      "epoch": 23.24736225087925,
      "grad_norm": 0.2741089463233948,
      "learning_rate": 5.01101799778433e-05,
      "loss": 0.018,
      "step": 79320
    },
    {
      "epoch": 23.25029308323564,
      "grad_norm": 0.3203374743461609,
      "learning_rate": 5.002638369025493e-05,
      "loss": 0.0131,
      "step": 79330
    },
    {
      "epoch": 23.253223915592027,
      "grad_norm": 0.3556375503540039,
      "learning_rate": 4.994258740266656e-05,
      "loss": 0.0217,
      "step": 79340
    },
    {
      "epoch": 23.25615474794842,
      "grad_norm": 1.4136525392532349,
      "learning_rate": 4.985879111507819e-05,
      "loss": 0.0334,
      "step": 79350
    },
    {
      "epoch": 23.259085580304806,
      "grad_norm": 0.813008725643158,
      "learning_rate": 4.9774994827489824e-05,
      "loss": 0.0277,
      "step": 79360
    },
    {
      "epoch": 23.262016412661197,
      "grad_norm": 0.6635152101516724,
      "learning_rate": 4.969119853990146e-05,
      "loss": 0.0232,
      "step": 79370
    },
    {
      "epoch": 23.264947245017584,
      "grad_norm": 0.18842172622680664,
      "learning_rate": 4.960740225231309e-05,
      "loss": 0.0286,
      "step": 79380
    },
    {
      "epoch": 23.267878077373975,
      "grad_norm": 2.2233059406280518,
      "learning_rate": 4.952360596472473e-05,
      "loss": 0.0284,
      "step": 79390
    },
    {
      "epoch": 23.270808909730363,
      "grad_norm": 1.5318344831466675,
      "learning_rate": 4.943980967713636e-05,
      "loss": 0.0349,
      "step": 79400
    },
    {
      "epoch": 23.273739742086754,
      "grad_norm": 0.9260410666465759,
      "learning_rate": 4.935601338954799e-05,
      "loss": 0.0334,
      "step": 79410
    },
    {
      "epoch": 23.27667057444314,
      "grad_norm": 0.5230221152305603,
      "learning_rate": 4.927221710195962e-05,
      "loss": 0.0325,
      "step": 79420
    },
    {
      "epoch": 23.279601406799532,
      "grad_norm": 0.47835278511047363,
      "learning_rate": 4.918842081437126e-05,
      "loss": 0.0133,
      "step": 79430
    },
    {
      "epoch": 23.28253223915592,
      "grad_norm": 0.7805165648460388,
      "learning_rate": 4.910462452678289e-05,
      "loss": 0.0157,
      "step": 79440
    },
    {
      "epoch": 23.28546307151231,
      "grad_norm": 0.6110041737556458,
      "learning_rate": 4.902082823919452e-05,
      "loss": 0.0278,
      "step": 79450
    },
    {
      "epoch": 23.2883939038687,
      "grad_norm": 1.6576011180877686,
      "learning_rate": 4.893703195160616e-05,
      "loss": 0.0229,
      "step": 79460
    },
    {
      "epoch": 23.29132473622509,
      "grad_norm": 0.7561817765235901,
      "learning_rate": 4.885323566401779e-05,
      "loss": 0.0291,
      "step": 79470
    },
    {
      "epoch": 23.294255568581477,
      "grad_norm": 0.285919725894928,
      "learning_rate": 4.8769439376429425e-05,
      "loss": 0.0182,
      "step": 79480
    },
    {
      "epoch": 23.297186400937868,
      "grad_norm": 1.9008055925369263,
      "learning_rate": 4.8685643088841055e-05,
      "loss": 0.0485,
      "step": 79490
    },
    {
      "epoch": 23.300117233294255,
      "grad_norm": 0.10605543851852417,
      "learning_rate": 4.860184680125269e-05,
      "loss": 0.0168,
      "step": 79500
    },
    {
      "epoch": 23.303048065650646,
      "grad_norm": 0.2809959650039673,
      "learning_rate": 4.851805051366432e-05,
      "loss": 0.0351,
      "step": 79510
    },
    {
      "epoch": 23.305978898007034,
      "grad_norm": 0.2803899049758911,
      "learning_rate": 4.843425422607595e-05,
      "loss": 0.0199,
      "step": 79520
    },
    {
      "epoch": 23.308909730363425,
      "grad_norm": 2.032829999923706,
      "learning_rate": 4.8350457938487596e-05,
      "loss": 0.0322,
      "step": 79530
    },
    {
      "epoch": 23.311840562719812,
      "grad_norm": 1.5944156646728516,
      "learning_rate": 4.8266661650899226e-05,
      "loss": 0.0323,
      "step": 79540
    },
    {
      "epoch": 23.314771395076203,
      "grad_norm": 1.6945825815200806,
      "learning_rate": 4.8182865363310856e-05,
      "loss": 0.0423,
      "step": 79550
    },
    {
      "epoch": 23.31770222743259,
      "grad_norm": 0.40644338726997375,
      "learning_rate": 4.809906907572249e-05,
      "loss": 0.0163,
      "step": 79560
    },
    {
      "epoch": 23.32063305978898,
      "grad_norm": 0.8257849216461182,
      "learning_rate": 4.801527278813412e-05,
      "loss": 0.0173,
      "step": 79570
    },
    {
      "epoch": 23.32356389214537,
      "grad_norm": 1.3528612852096558,
      "learning_rate": 4.793147650054576e-05,
      "loss": 0.029,
      "step": 79580
    },
    {
      "epoch": 23.32649472450176,
      "grad_norm": 0.4227139949798584,
      "learning_rate": 4.784768021295739e-05,
      "loss": 0.0224,
      "step": 79590
    },
    {
      "epoch": 23.329425556858148,
      "grad_norm": 1.6898138523101807,
      "learning_rate": 4.7763883925369026e-05,
      "loss": 0.022,
      "step": 79600
    },
    {
      "epoch": 23.33235638921454,
      "grad_norm": 3.054797649383545,
      "learning_rate": 4.7680087637780656e-05,
      "loss": 0.0233,
      "step": 79610
    },
    {
      "epoch": 23.335287221570926,
      "grad_norm": 1.0834200382232666,
      "learning_rate": 4.7596291350192286e-05,
      "loss": 0.0418,
      "step": 79620
    },
    {
      "epoch": 23.338218053927314,
      "grad_norm": 0.6181129813194275,
      "learning_rate": 4.751249506260393e-05,
      "loss": 0.0302,
      "step": 79630
    },
    {
      "epoch": 23.341148886283705,
      "grad_norm": 0.043900810182094574,
      "learning_rate": 4.742869877501556e-05,
      "loss": 0.0204,
      "step": 79640
    },
    {
      "epoch": 23.344079718640092,
      "grad_norm": 0.43061479926109314,
      "learning_rate": 4.734490248742719e-05,
      "loss": 0.0348,
      "step": 79650
    },
    {
      "epoch": 23.347010550996483,
      "grad_norm": 0.35927000641822815,
      "learning_rate": 4.726110619983882e-05,
      "loss": 0.0281,
      "step": 79660
    },
    {
      "epoch": 23.34994138335287,
      "grad_norm": 0.07230611890554428,
      "learning_rate": 4.717730991225046e-05,
      "loss": 0.0275,
      "step": 79670
    },
    {
      "epoch": 23.35287221570926,
      "grad_norm": 0.099661685526371,
      "learning_rate": 4.709351362466209e-05,
      "loss": 0.0311,
      "step": 79680
    },
    {
      "epoch": 23.35580304806565,
      "grad_norm": 0.36010220646858215,
      "learning_rate": 4.7009717337073724e-05,
      "loss": 0.0208,
      "step": 79690
    },
    {
      "epoch": 23.35873388042204,
      "grad_norm": 1.2156946659088135,
      "learning_rate": 4.692592104948536e-05,
      "loss": 0.0217,
      "step": 79700
    },
    {
      "epoch": 23.361664712778428,
      "grad_norm": 0.9782662391662598,
      "learning_rate": 4.684212476189699e-05,
      "loss": 0.0201,
      "step": 79710
    },
    {
      "epoch": 23.36459554513482,
      "grad_norm": 2.148668050765991,
      "learning_rate": 4.675832847430862e-05,
      "loss": 0.0232,
      "step": 79720
    },
    {
      "epoch": 23.367526377491206,
      "grad_norm": 1.5259628295898438,
      "learning_rate": 4.667453218672025e-05,
      "loss": 0.04,
      "step": 79730
    },
    {
      "epoch": 23.370457209847597,
      "grad_norm": 0.4925563633441925,
      "learning_rate": 4.6590735899131894e-05,
      "loss": 0.01,
      "step": 79740
    },
    {
      "epoch": 23.373388042203985,
      "grad_norm": 0.3510333001613617,
      "learning_rate": 4.6506939611543524e-05,
      "loss": 0.0312,
      "step": 79750
    },
    {
      "epoch": 23.376318874560376,
      "grad_norm": 0.15382343530654907,
      "learning_rate": 4.6423143323955154e-05,
      "loss": 0.0477,
      "step": 79760
    },
    {
      "epoch": 23.379249706916763,
      "grad_norm": 0.40616074204444885,
      "learning_rate": 4.633934703636679e-05,
      "loss": 0.0134,
      "step": 79770
    },
    {
      "epoch": 23.382180539273154,
      "grad_norm": 0.1928929090499878,
      "learning_rate": 4.625555074877842e-05,
      "loss": 0.0183,
      "step": 79780
    },
    {
      "epoch": 23.38511137162954,
      "grad_norm": 0.34586238861083984,
      "learning_rate": 4.617175446119005e-05,
      "loss": 0.0239,
      "step": 79790
    },
    {
      "epoch": 23.388042203985933,
      "grad_norm": 3.062138557434082,
      "learning_rate": 4.608795817360169e-05,
      "loss": 0.0374,
      "step": 79800
    },
    {
      "epoch": 23.39097303634232,
      "grad_norm": 1.144070029258728,
      "learning_rate": 4.6004161886013325e-05,
      "loss": 0.0186,
      "step": 79810
    },
    {
      "epoch": 23.39390386869871,
      "grad_norm": 1.3210768699645996,
      "learning_rate": 4.5920365598424955e-05,
      "loss": 0.0472,
      "step": 79820
    },
    {
      "epoch": 23.3968347010551,
      "grad_norm": 0.8392707705497742,
      "learning_rate": 4.5836569310836585e-05,
      "loss": 0.0353,
      "step": 79830
    },
    {
      "epoch": 23.39976553341149,
      "grad_norm": 1.1785274744033813,
      "learning_rate": 4.575277302324823e-05,
      "loss": 0.0362,
      "step": 79840
    },
    {
      "epoch": 23.402696365767877,
      "grad_norm": 0.33038875460624695,
      "learning_rate": 4.566897673565986e-05,
      "loss": 0.0307,
      "step": 79850
    },
    {
      "epoch": 23.405627198124268,
      "grad_norm": 0.5605430603027344,
      "learning_rate": 4.558518044807149e-05,
      "loss": 0.0137,
      "step": 79860
    },
    {
      "epoch": 23.408558030480656,
      "grad_norm": 0.4259430766105652,
      "learning_rate": 4.5501384160483125e-05,
      "loss": 0.0283,
      "step": 79870
    },
    {
      "epoch": 23.411488862837047,
      "grad_norm": 1.7176127433776855,
      "learning_rate": 4.5417587872894755e-05,
      "loss": 0.0239,
      "step": 79880
    },
    {
      "epoch": 23.414419695193434,
      "grad_norm": 1.7870773077011108,
      "learning_rate": 4.5333791585306385e-05,
      "loss": 0.0186,
      "step": 79890
    },
    {
      "epoch": 23.417350527549825,
      "grad_norm": 0.22406765818595886,
      "learning_rate": 4.524999529771802e-05,
      "loss": 0.021,
      "step": 79900
    },
    {
      "epoch": 23.420281359906213,
      "grad_norm": 0.5649499893188477,
      "learning_rate": 4.516619901012966e-05,
      "loss": 0.0174,
      "step": 79910
    },
    {
      "epoch": 23.423212192262604,
      "grad_norm": 0.7550340890884399,
      "learning_rate": 4.508240272254129e-05,
      "loss": 0.0315,
      "step": 79920
    },
    {
      "epoch": 23.42614302461899,
      "grad_norm": 1.2537895441055298,
      "learning_rate": 4.499860643495292e-05,
      "loss": 0.03,
      "step": 79930
    },
    {
      "epoch": 23.429073856975382,
      "grad_norm": 0.3349919617176056,
      "learning_rate": 4.491481014736456e-05,
      "loss": 0.0228,
      "step": 79940
    },
    {
      "epoch": 23.43200468933177,
      "grad_norm": 1.317739486694336,
      "learning_rate": 4.483101385977619e-05,
      "loss": 0.0304,
      "step": 79950
    },
    {
      "epoch": 23.43493552168816,
      "grad_norm": 0.7501387596130371,
      "learning_rate": 4.474721757218782e-05,
      "loss": 0.0163,
      "step": 79960
    },
    {
      "epoch": 23.437866354044548,
      "grad_norm": 1.9331731796264648,
      "learning_rate": 4.466342128459946e-05,
      "loss": 0.0172,
      "step": 79970
    },
    {
      "epoch": 23.44079718640094,
      "grad_norm": 1.018259882926941,
      "learning_rate": 4.457962499701109e-05,
      "loss": 0.0214,
      "step": 79980
    },
    {
      "epoch": 23.443728018757326,
      "grad_norm": 0.5201302170753479,
      "learning_rate": 4.449582870942272e-05,
      "loss": 0.056,
      "step": 79990
    },
    {
      "epoch": 23.446658851113718,
      "grad_norm": 0.48433080315589905,
      "learning_rate": 4.4412032421834356e-05,
      "loss": 0.0501,
      "step": 80000
    },
    {
      "epoch": 23.449589683470105,
      "grad_norm": 0.10110227018594742,
      "learning_rate": 4.4328236134245986e-05,
      "loss": 0.0316,
      "step": 80010
    },
    {
      "epoch": 23.452520515826496,
      "grad_norm": 0.8897385001182556,
      "learning_rate": 4.424443984665762e-05,
      "loss": 0.0369,
      "step": 80020
    },
    {
      "epoch": 23.455451348182883,
      "grad_norm": 0.5264390707015991,
      "learning_rate": 4.416064355906925e-05,
      "loss": 0.0364,
      "step": 80030
    },
    {
      "epoch": 23.458382180539274,
      "grad_norm": 2.132861375808716,
      "learning_rate": 4.407684727148089e-05,
      "loss": 0.0395,
      "step": 80040
    },
    {
      "epoch": 23.461313012895662,
      "grad_norm": 1.241381287574768,
      "learning_rate": 4.399305098389252e-05,
      "loss": 0.0623,
      "step": 80050
    },
    {
      "epoch": 23.464243845252053,
      "grad_norm": 0.8796247243881226,
      "learning_rate": 4.390925469630416e-05,
      "loss": 0.0266,
      "step": 80060
    },
    {
      "epoch": 23.46717467760844,
      "grad_norm": 0.9437854886054993,
      "learning_rate": 4.3825458408715793e-05,
      "loss": 0.0196,
      "step": 80070
    },
    {
      "epoch": 23.47010550996483,
      "grad_norm": 0.02824249677360058,
      "learning_rate": 4.3741662121127424e-05,
      "loss": 0.0216,
      "step": 80080
    },
    {
      "epoch": 23.47303634232122,
      "grad_norm": 0.6639083027839661,
      "learning_rate": 4.3657865833539054e-05,
      "loss": 0.0359,
      "step": 80090
    },
    {
      "epoch": 23.47596717467761,
      "grad_norm": 0.6482324600219727,
      "learning_rate": 4.3574069545950684e-05,
      "loss": 0.0214,
      "step": 80100
    },
    {
      "epoch": 23.478898007033997,
      "grad_norm": 0.021547956392169,
      "learning_rate": 4.349027325836232e-05,
      "loss": 0.0202,
      "step": 80110
    },
    {
      "epoch": 23.48182883939039,
      "grad_norm": 0.7119014859199524,
      "learning_rate": 4.340647697077396e-05,
      "loss": 0.0388,
      "step": 80120
    },
    {
      "epoch": 23.484759671746776,
      "grad_norm": 1.7467719316482544,
      "learning_rate": 4.332268068318559e-05,
      "loss": 0.0353,
      "step": 80130
    },
    {
      "epoch": 23.487690504103167,
      "grad_norm": 0.26186510920524597,
      "learning_rate": 4.3238884395597224e-05,
      "loss": 0.0224,
      "step": 80140
    },
    {
      "epoch": 23.490621336459554,
      "grad_norm": 0.42409995198249817,
      "learning_rate": 4.3155088108008854e-05,
      "loss": 0.0291,
      "step": 80150
    },
    {
      "epoch": 23.493552168815945,
      "grad_norm": 0.8773800730705261,
      "learning_rate": 4.307129182042049e-05,
      "loss": 0.0307,
      "step": 80160
    },
    {
      "epoch": 23.496483001172333,
      "grad_norm": 1.8621630668640137,
      "learning_rate": 4.298749553283212e-05,
      "loss": 0.0284,
      "step": 80170
    },
    {
      "epoch": 23.49941383352872,
      "grad_norm": 0.713457465171814,
      "learning_rate": 4.290369924524376e-05,
      "loss": 0.0438,
      "step": 80180
    },
    {
      "epoch": 23.50234466588511,
      "grad_norm": 2.5122439861297607,
      "learning_rate": 4.281990295765539e-05,
      "loss": 0.0436,
      "step": 80190
    },
    {
      "epoch": 23.5052754982415,
      "grad_norm": 1.6855580806732178,
      "learning_rate": 4.273610667006702e-05,
      "loss": 0.0391,
      "step": 80200
    },
    {
      "epoch": 23.50820633059789,
      "grad_norm": 0.7925161719322205,
      "learning_rate": 4.2652310382478655e-05,
      "loss": 0.0199,
      "step": 80210
    },
    {
      "epoch": 23.511137162954277,
      "grad_norm": 0.06531544774770737,
      "learning_rate": 4.2568514094890285e-05,
      "loss": 0.0262,
      "step": 80220
    },
    {
      "epoch": 23.51406799531067,
      "grad_norm": 0.9294017553329468,
      "learning_rate": 4.248471780730192e-05,
      "loss": 0.0448,
      "step": 80230
    },
    {
      "epoch": 23.516998827667056,
      "grad_norm": 0.48010027408599854,
      "learning_rate": 4.240092151971355e-05,
      "loss": 0.0376,
      "step": 80240
    },
    {
      "epoch": 23.519929660023447,
      "grad_norm": 1.143272042274475,
      "learning_rate": 4.231712523212519e-05,
      "loss": 0.0218,
      "step": 80250
    },
    {
      "epoch": 23.522860492379834,
      "grad_norm": 0.512103259563446,
      "learning_rate": 4.2233328944536825e-05,
      "loss": 0.0196,
      "step": 80260
    },
    {
      "epoch": 23.525791324736225,
      "grad_norm": 0.3750925362110138,
      "learning_rate": 4.2149532656948455e-05,
      "loss": 0.0275,
      "step": 80270
    },
    {
      "epoch": 23.528722157092613,
      "grad_norm": 0.7049350142478943,
      "learning_rate": 4.206573636936009e-05,
      "loss": 0.0146,
      "step": 80280
    },
    {
      "epoch": 23.531652989449004,
      "grad_norm": 1.1456189155578613,
      "learning_rate": 4.198194008177172e-05,
      "loss": 0.0295,
      "step": 80290
    },
    {
      "epoch": 23.53458382180539,
      "grad_norm": 0.07792016118764877,
      "learning_rate": 4.189814379418335e-05,
      "loss": 0.0208,
      "step": 80300
    },
    {
      "epoch": 23.537514654161782,
      "grad_norm": 1.7491822242736816,
      "learning_rate": 4.181434750659499e-05,
      "loss": 0.0431,
      "step": 80310
    },
    {
      "epoch": 23.54044548651817,
      "grad_norm": 3.635368824005127,
      "learning_rate": 4.173055121900662e-05,
      "loss": 0.0234,
      "step": 80320
    },
    {
      "epoch": 23.54337631887456,
      "grad_norm": 1.1503894329071045,
      "learning_rate": 4.1646754931418256e-05,
      "loss": 0.0309,
      "step": 80330
    },
    {
      "epoch": 23.54630715123095,
      "grad_norm": 0.9666202068328857,
      "learning_rate": 4.1562958643829886e-05,
      "loss": 0.0366,
      "step": 80340
    },
    {
      "epoch": 23.54923798358734,
      "grad_norm": 1.2386623620986938,
      "learning_rate": 4.147916235624152e-05,
      "loss": 0.0412,
      "step": 80350
    },
    {
      "epoch": 23.552168815943727,
      "grad_norm": 0.9999879598617554,
      "learning_rate": 4.139536606865315e-05,
      "loss": 0.0524,
      "step": 80360
    },
    {
      "epoch": 23.555099648300118,
      "grad_norm": 0.26589658856391907,
      "learning_rate": 4.131156978106479e-05,
      "loss": 0.038,
      "step": 80370
    },
    {
      "epoch": 23.558030480656505,
      "grad_norm": 3.8468198776245117,
      "learning_rate": 4.122777349347642e-05,
      "loss": 0.0363,
      "step": 80380
    },
    {
      "epoch": 23.560961313012896,
      "grad_norm": 0.7661413550376892,
      "learning_rate": 4.1143977205888056e-05,
      "loss": 0.0298,
      "step": 80390
    },
    {
      "epoch": 23.563892145369284,
      "grad_norm": 0.515569269657135,
      "learning_rate": 4.1060180918299686e-05,
      "loss": 0.031,
      "step": 80400
    },
    {
      "epoch": 23.566822977725675,
      "grad_norm": 0.7298415899276733,
      "learning_rate": 4.0976384630711316e-05,
      "loss": 0.0334,
      "step": 80410
    },
    {
      "epoch": 23.569753810082062,
      "grad_norm": 1.1505582332611084,
      "learning_rate": 4.089258834312295e-05,
      "loss": 0.0172,
      "step": 80420
    },
    {
      "epoch": 23.572684642438453,
      "grad_norm": 0.39351311326026917,
      "learning_rate": 4.080879205553459e-05,
      "loss": 0.0175,
      "step": 80430
    },
    {
      "epoch": 23.57561547479484,
      "grad_norm": 2.257641315460205,
      "learning_rate": 4.072499576794622e-05,
      "loss": 0.0338,
      "step": 80440
    },
    {
      "epoch": 23.578546307151232,
      "grad_norm": 0.12062282115221024,
      "learning_rate": 4.0641199480357857e-05,
      "loss": 0.0251,
      "step": 80450
    },
    {
      "epoch": 23.58147713950762,
      "grad_norm": 0.17997142672538757,
      "learning_rate": 4.055740319276949e-05,
      "loss": 0.0175,
      "step": 80460
    },
    {
      "epoch": 23.58440797186401,
      "grad_norm": 5.4530158042907715,
      "learning_rate": 4.0473606905181123e-05,
      "loss": 0.0272,
      "step": 80470
    },
    {
      "epoch": 23.587338804220398,
      "grad_norm": 1.560461163520813,
      "learning_rate": 4.0389810617592753e-05,
      "loss": 0.0319,
      "step": 80480
    },
    {
      "epoch": 23.59026963657679,
      "grad_norm": 0.7792278528213501,
      "learning_rate": 4.0306014330004384e-05,
      "loss": 0.0252,
      "step": 80490
    },
    {
      "epoch": 23.593200468933176,
      "grad_norm": 0.6690607666969299,
      "learning_rate": 4.022221804241602e-05,
      "loss": 0.0188,
      "step": 80500
    },
    {
      "epoch": 23.596131301289567,
      "grad_norm": 0.41620731353759766,
      "learning_rate": 4.013842175482765e-05,
      "loss": 0.0581,
      "step": 80510
    },
    {
      "epoch": 23.599062133645955,
      "grad_norm": 1.3935855627059937,
      "learning_rate": 4.005462546723929e-05,
      "loss": 0.0314,
      "step": 80520
    },
    {
      "epoch": 23.601992966002346,
      "grad_norm": 0.3640642762184143,
      "learning_rate": 3.997082917965092e-05,
      "loss": 0.0139,
      "step": 80530
    },
    {
      "epoch": 23.604923798358733,
      "grad_norm": 0.6630096435546875,
      "learning_rate": 3.9887032892062554e-05,
      "loss": 0.0358,
      "step": 80540
    },
    {
      "epoch": 23.607854630715124,
      "grad_norm": 0.6105098724365234,
      "learning_rate": 3.9803236604474184e-05,
      "loss": 0.0246,
      "step": 80550
    },
    {
      "epoch": 23.61078546307151,
      "grad_norm": 2.0359301567077637,
      "learning_rate": 3.971944031688582e-05,
      "loss": 0.0184,
      "step": 80560
    },
    {
      "epoch": 23.613716295427903,
      "grad_norm": 0.2655836343765259,
      "learning_rate": 3.963564402929746e-05,
      "loss": 0.0291,
      "step": 80570
    },
    {
      "epoch": 23.61664712778429,
      "grad_norm": 0.274157851934433,
      "learning_rate": 3.955184774170909e-05,
      "loss": 0.0279,
      "step": 80580
    },
    {
      "epoch": 23.61957796014068,
      "grad_norm": 1.090882658958435,
      "learning_rate": 3.946805145412072e-05,
      "loss": 0.0354,
      "step": 80590
    },
    {
      "epoch": 23.62250879249707,
      "grad_norm": 0.41689595580101013,
      "learning_rate": 3.938425516653235e-05,
      "loss": 0.0299,
      "step": 80600
    },
    {
      "epoch": 23.62543962485346,
      "grad_norm": 1.2440009117126465,
      "learning_rate": 3.9300458878943985e-05,
      "loss": 0.0196,
      "step": 80610
    },
    {
      "epoch": 23.628370457209847,
      "grad_norm": 0.150607168674469,
      "learning_rate": 3.921666259135562e-05,
      "loss": 0.0288,
      "step": 80620
    },
    {
      "epoch": 23.631301289566238,
      "grad_norm": 0.7791667580604553,
      "learning_rate": 3.913286630376725e-05,
      "loss": 0.0249,
      "step": 80630
    },
    {
      "epoch": 23.634232121922626,
      "grad_norm": 0.9256755113601685,
      "learning_rate": 3.904907001617889e-05,
      "loss": 0.0152,
      "step": 80640
    },
    {
      "epoch": 23.637162954279017,
      "grad_norm": 0.8895565271377563,
      "learning_rate": 3.896527372859052e-05,
      "loss": 0.026,
      "step": 80650
    },
    {
      "epoch": 23.640093786635404,
      "grad_norm": 2.67997407913208,
      "learning_rate": 3.8881477441002155e-05,
      "loss": 0.042,
      "step": 80660
    },
    {
      "epoch": 23.643024618991795,
      "grad_norm": 0.03849601000547409,
      "learning_rate": 3.8797681153413785e-05,
      "loss": 0.013,
      "step": 80670
    },
    {
      "epoch": 23.645955451348183,
      "grad_norm": 0.09211628884077072,
      "learning_rate": 3.871388486582542e-05,
      "loss": 0.0324,
      "step": 80680
    },
    {
      "epoch": 23.648886283704574,
      "grad_norm": 0.7581915259361267,
      "learning_rate": 3.863008857823705e-05,
      "loss": 0.0279,
      "step": 80690
    },
    {
      "epoch": 23.65181711606096,
      "grad_norm": 1.1265652179718018,
      "learning_rate": 3.854629229064868e-05,
      "loss": 0.0419,
      "step": 80700
    },
    {
      "epoch": 23.654747948417352,
      "grad_norm": 1.0118980407714844,
      "learning_rate": 3.846249600306032e-05,
      "loss": 0.0244,
      "step": 80710
    },
    {
      "epoch": 23.65767878077374,
      "grad_norm": 0.09890596568584442,
      "learning_rate": 3.837869971547195e-05,
      "loss": 0.0242,
      "step": 80720
    },
    {
      "epoch": 23.66060961313013,
      "grad_norm": 2.430537223815918,
      "learning_rate": 3.8294903427883586e-05,
      "loss": 0.0207,
      "step": 80730
    },
    {
      "epoch": 23.663540445486518,
      "grad_norm": 0.8746746778488159,
      "learning_rate": 3.8211107140295216e-05,
      "loss": 0.0182,
      "step": 80740
    },
    {
      "epoch": 23.66647127784291,
      "grad_norm": 1.4026001691818237,
      "learning_rate": 3.812731085270685e-05,
      "loss": 0.0189,
      "step": 80750
    },
    {
      "epoch": 23.669402110199297,
      "grad_norm": 0.22693300247192383,
      "learning_rate": 3.804351456511849e-05,
      "loss": 0.0191,
      "step": 80760
    },
    {
      "epoch": 23.672332942555684,
      "grad_norm": 1.2578469514846802,
      "learning_rate": 3.795971827753012e-05,
      "loss": 0.045,
      "step": 80770
    },
    {
      "epoch": 23.675263774912075,
      "grad_norm": 0.39467233419418335,
      "learning_rate": 3.7875921989941756e-05,
      "loss": 0.0408,
      "step": 80780
    },
    {
      "epoch": 23.678194607268463,
      "grad_norm": 0.23971517384052277,
      "learning_rate": 3.7792125702353386e-05,
      "loss": 0.0347,
      "step": 80790
    },
    {
      "epoch": 23.681125439624854,
      "grad_norm": 1.7068352699279785,
      "learning_rate": 3.7708329414765016e-05,
      "loss": 0.0237,
      "step": 80800
    },
    {
      "epoch": 23.68405627198124,
      "grad_norm": 0.8594342470169067,
      "learning_rate": 3.762453312717665e-05,
      "loss": 0.0244,
      "step": 80810
    },
    {
      "epoch": 23.686987104337632,
      "grad_norm": 1.064431071281433,
      "learning_rate": 3.754073683958828e-05,
      "loss": 0.0187,
      "step": 80820
    },
    {
      "epoch": 23.68991793669402,
      "grad_norm": 0.6481691002845764,
      "learning_rate": 3.745694055199992e-05,
      "loss": 0.0292,
      "step": 80830
    },
    {
      "epoch": 23.69284876905041,
      "grad_norm": 0.9117177128791809,
      "learning_rate": 3.737314426441155e-05,
      "loss": 0.0379,
      "step": 80840
    },
    {
      "epoch": 23.695779601406798,
      "grad_norm": 5.069456100463867,
      "learning_rate": 3.7289347976823187e-05,
      "loss": 0.0415,
      "step": 80850
    },
    {
      "epoch": 23.69871043376319,
      "grad_norm": 0.5972018837928772,
      "learning_rate": 3.7205551689234817e-05,
      "loss": 0.04,
      "step": 80860
    },
    {
      "epoch": 23.701641266119577,
      "grad_norm": 0.3287962079048157,
      "learning_rate": 3.712175540164645e-05,
      "loss": 0.0349,
      "step": 80870
    },
    {
      "epoch": 23.704572098475968,
      "grad_norm": 0.6338711977005005,
      "learning_rate": 3.703795911405809e-05,
      "loss": 0.0403,
      "step": 80880
    },
    {
      "epoch": 23.707502930832355,
      "grad_norm": 0.3765285909175873,
      "learning_rate": 3.695416282646972e-05,
      "loss": 0.0144,
      "step": 80890
    },
    {
      "epoch": 23.710433763188746,
      "grad_norm": 2.1769258975982666,
      "learning_rate": 3.687036653888135e-05,
      "loss": 0.0191,
      "step": 80900
    },
    {
      "epoch": 23.713364595545134,
      "grad_norm": 1.116152286529541,
      "learning_rate": 3.678657025129298e-05,
      "loss": 0.0327,
      "step": 80910
    },
    {
      "epoch": 23.716295427901525,
      "grad_norm": 0.18634672462940216,
      "learning_rate": 3.670277396370462e-05,
      "loss": 0.0207,
      "step": 80920
    },
    {
      "epoch": 23.719226260257912,
      "grad_norm": 1.1874457597732544,
      "learning_rate": 3.6618977676116254e-05,
      "loss": 0.0451,
      "step": 80930
    },
    {
      "epoch": 23.722157092614303,
      "grad_norm": 0.7085072994232178,
      "learning_rate": 3.6535181388527884e-05,
      "loss": 0.0174,
      "step": 80940
    },
    {
      "epoch": 23.72508792497069,
      "grad_norm": 1.291761875152588,
      "learning_rate": 3.645138510093952e-05,
      "loss": 0.028,
      "step": 80950
    },
    {
      "epoch": 23.72801875732708,
      "grad_norm": 1.503506064414978,
      "learning_rate": 3.636758881335115e-05,
      "loss": 0.0275,
      "step": 80960
    },
    {
      "epoch": 23.73094958968347,
      "grad_norm": 1.296705722808838,
      "learning_rate": 3.628379252576279e-05,
      "loss": 0.0236,
      "step": 80970
    },
    {
      "epoch": 23.73388042203986,
      "grad_norm": 0.1855156123638153,
      "learning_rate": 3.619999623817442e-05,
      "loss": 0.04,
      "step": 80980
    },
    {
      "epoch": 23.736811254396248,
      "grad_norm": 0.6032256484031677,
      "learning_rate": 3.6116199950586054e-05,
      "loss": 0.0373,
      "step": 80990
    },
    {
      "epoch": 23.73974208675264,
      "grad_norm": 1.480576515197754,
      "learning_rate": 3.6032403662997684e-05,
      "loss": 0.0136,
      "step": 81000
    },
    {
      "epoch": 23.742672919109026,
      "grad_norm": 1.8856557607650757,
      "learning_rate": 3.5948607375409314e-05,
      "loss": 0.0474,
      "step": 81010
    },
    {
      "epoch": 23.745603751465417,
      "grad_norm": 1.9067997932434082,
      "learning_rate": 3.586481108782095e-05,
      "loss": 0.0428,
      "step": 81020
    },
    {
      "epoch": 23.748534583821804,
      "grad_norm": 1.2949055433273315,
      "learning_rate": 3.578101480023258e-05,
      "loss": 0.0386,
      "step": 81030
    },
    {
      "epoch": 23.751465416178196,
      "grad_norm": 0.14054714143276215,
      "learning_rate": 3.569721851264422e-05,
      "loss": 0.0244,
      "step": 81040
    },
    {
      "epoch": 23.754396248534583,
      "grad_norm": 0.19743943214416504,
      "learning_rate": 3.561342222505585e-05,
      "loss": 0.0297,
      "step": 81050
    },
    {
      "epoch": 23.757327080890974,
      "grad_norm": 0.10872264206409454,
      "learning_rate": 3.5529625937467485e-05,
      "loss": 0.019,
      "step": 81060
    },
    {
      "epoch": 23.76025791324736,
      "grad_norm": 1.9592422246932983,
      "learning_rate": 3.544582964987912e-05,
      "loss": 0.0292,
      "step": 81070
    },
    {
      "epoch": 23.763188745603752,
      "grad_norm": 0.049653418362140656,
      "learning_rate": 3.536203336229075e-05,
      "loss": 0.0361,
      "step": 81080
    },
    {
      "epoch": 23.76611957796014,
      "grad_norm": 0.47597938776016235,
      "learning_rate": 3.527823707470239e-05,
      "loss": 0.0217,
      "step": 81090
    },
    {
      "epoch": 23.76905041031653,
      "grad_norm": 0.14516353607177734,
      "learning_rate": 3.519444078711402e-05,
      "loss": 0.0253,
      "step": 81100
    },
    {
      "epoch": 23.77198124267292,
      "grad_norm": 0.37652021646499634,
      "learning_rate": 3.511064449952565e-05,
      "loss": 0.036,
      "step": 81110
    },
    {
      "epoch": 23.77491207502931,
      "grad_norm": 1.7688353061676025,
      "learning_rate": 3.5026848211937285e-05,
      "loss": 0.0352,
      "step": 81120
    },
    {
      "epoch": 23.777842907385697,
      "grad_norm": 0.1420034021139145,
      "learning_rate": 3.4943051924348915e-05,
      "loss": 0.0226,
      "step": 81130
    },
    {
      "epoch": 23.780773739742088,
      "grad_norm": 1.2641386985778809,
      "learning_rate": 3.485925563676055e-05,
      "loss": 0.042,
      "step": 81140
    },
    {
      "epoch": 23.783704572098475,
      "grad_norm": 0.1748489886522293,
      "learning_rate": 3.477545934917218e-05,
      "loss": 0.0329,
      "step": 81150
    },
    {
      "epoch": 23.786635404454866,
      "grad_norm": 0.533422589302063,
      "learning_rate": 3.469166306158382e-05,
      "loss": 0.019,
      "step": 81160
    },
    {
      "epoch": 23.789566236811254,
      "grad_norm": 0.2572067975997925,
      "learning_rate": 3.460786677399545e-05,
      "loss": 0.0232,
      "step": 81170
    },
    {
      "epoch": 23.792497069167645,
      "grad_norm": 0.6175975799560547,
      "learning_rate": 3.4524070486407086e-05,
      "loss": 0.0193,
      "step": 81180
    },
    {
      "epoch": 23.795427901524032,
      "grad_norm": 2.1101067066192627,
      "learning_rate": 3.4440274198818716e-05,
      "loss": 0.0328,
      "step": 81190
    },
    {
      "epoch": 23.798358733880423,
      "grad_norm": 2.671217441558838,
      "learning_rate": 3.435647791123035e-05,
      "loss": 0.025,
      "step": 81200
    },
    {
      "epoch": 23.80128956623681,
      "grad_norm": 0.30900371074676514,
      "learning_rate": 3.427268162364198e-05,
      "loss": 0.0237,
      "step": 81210
    },
    {
      "epoch": 23.804220398593202,
      "grad_norm": 0.730736494064331,
      "learning_rate": 3.418888533605361e-05,
      "loss": 0.0232,
      "step": 81220
    },
    {
      "epoch": 23.80715123094959,
      "grad_norm": 1.3304945230484009,
      "learning_rate": 3.410508904846525e-05,
      "loss": 0.0246,
      "step": 81230
    },
    {
      "epoch": 23.81008206330598,
      "grad_norm": 1.9691585302352905,
      "learning_rate": 3.402129276087688e-05,
      "loss": 0.0419,
      "step": 81240
    },
    {
      "epoch": 23.813012895662368,
      "grad_norm": 1.6104810237884521,
      "learning_rate": 3.3937496473288516e-05,
      "loss": 0.0468,
      "step": 81250
    },
    {
      "epoch": 23.81594372801876,
      "grad_norm": 0.9067032337188721,
      "learning_rate": 3.385370018570015e-05,
      "loss": 0.0193,
      "step": 81260
    },
    {
      "epoch": 23.818874560375146,
      "grad_norm": 1.7233294248580933,
      "learning_rate": 3.376990389811178e-05,
      "loss": 0.0417,
      "step": 81270
    },
    {
      "epoch": 23.821805392731537,
      "grad_norm": 1.290397047996521,
      "learning_rate": 3.368610761052342e-05,
      "loss": 0.0298,
      "step": 81280
    },
    {
      "epoch": 23.824736225087925,
      "grad_norm": 1.6916217803955078,
      "learning_rate": 3.360231132293505e-05,
      "loss": 0.0343,
      "step": 81290
    },
    {
      "epoch": 23.827667057444316,
      "grad_norm": 0.4380699396133423,
      "learning_rate": 3.351851503534668e-05,
      "loss": 0.0202,
      "step": 81300
    },
    {
      "epoch": 23.830597889800703,
      "grad_norm": 0.9092881679534912,
      "learning_rate": 3.343471874775832e-05,
      "loss": 0.0406,
      "step": 81310
    },
    {
      "epoch": 23.83352872215709,
      "grad_norm": 0.4152500331401825,
      "learning_rate": 3.335092246016995e-05,
      "loss": 0.0192,
      "step": 81320
    },
    {
      "epoch": 23.836459554513482,
      "grad_norm": 0.23308534920215607,
      "learning_rate": 3.3267126172581584e-05,
      "loss": 0.0217,
      "step": 81330
    },
    {
      "epoch": 23.83939038686987,
      "grad_norm": 0.14210744202136993,
      "learning_rate": 3.3183329884993214e-05,
      "loss": 0.0096,
      "step": 81340
    },
    {
      "epoch": 23.84232121922626,
      "grad_norm": 0.9386674761772156,
      "learning_rate": 3.309953359740485e-05,
      "loss": 0.019,
      "step": 81350
    },
    {
      "epoch": 23.845252051582648,
      "grad_norm": 0.9725406765937805,
      "learning_rate": 3.301573730981648e-05,
      "loss": 0.0321,
      "step": 81360
    },
    {
      "epoch": 23.84818288393904,
      "grad_norm": 1.6962172985076904,
      "learning_rate": 3.293194102222812e-05,
      "loss": 0.0211,
      "step": 81370
    },
    {
      "epoch": 23.851113716295426,
      "grad_norm": 0.9935634136199951,
      "learning_rate": 3.2848144734639754e-05,
      "loss": 0.0222,
      "step": 81380
    },
    {
      "epoch": 23.854044548651817,
      "grad_norm": 2.3515164852142334,
      "learning_rate": 3.2764348447051384e-05,
      "loss": 0.0215,
      "step": 81390
    },
    {
      "epoch": 23.856975381008205,
      "grad_norm": 0.46050387620925903,
      "learning_rate": 3.2680552159463014e-05,
      "loss": 0.0133,
      "step": 81400
    },
    {
      "epoch": 23.859906213364596,
      "grad_norm": 0.19327087700366974,
      "learning_rate": 3.2596755871874644e-05,
      "loss": 0.0272,
      "step": 81410
    },
    {
      "epoch": 23.862837045720983,
      "grad_norm": 0.645699679851532,
      "learning_rate": 3.251295958428628e-05,
      "loss": 0.0186,
      "step": 81420
    },
    {
      "epoch": 23.865767878077374,
      "grad_norm": 1.3593367338180542,
      "learning_rate": 3.242916329669792e-05,
      "loss": 0.0344,
      "step": 81430
    },
    {
      "epoch": 23.868698710433762,
      "grad_norm": 0.1368304193019867,
      "learning_rate": 3.234536700910955e-05,
      "loss": 0.0205,
      "step": 81440
    },
    {
      "epoch": 23.871629542790153,
      "grad_norm": 0.9204866290092468,
      "learning_rate": 3.2261570721521185e-05,
      "loss": 0.0218,
      "step": 81450
    },
    {
      "epoch": 23.87456037514654,
      "grad_norm": 0.15767091512680054,
      "learning_rate": 3.2177774433932815e-05,
      "loss": 0.0318,
      "step": 81460
    },
    {
      "epoch": 23.87749120750293,
      "grad_norm": 0.2709095776081085,
      "learning_rate": 3.209397814634445e-05,
      "loss": 0.0297,
      "step": 81470
    },
    {
      "epoch": 23.88042203985932,
      "grad_norm": 2.205153703689575,
      "learning_rate": 3.201018185875608e-05,
      "loss": 0.0342,
      "step": 81480
    },
    {
      "epoch": 23.88335287221571,
      "grad_norm": 0.2947511076927185,
      "learning_rate": 3.192638557116772e-05,
      "loss": 0.0415,
      "step": 81490
    },
    {
      "epoch": 23.886283704572097,
      "grad_norm": 0.6496641635894775,
      "learning_rate": 3.184258928357935e-05,
      "loss": 0.0444,
      "step": 81500
    },
    {
      "epoch": 23.88921453692849,
      "grad_norm": 0.21284395456314087,
      "learning_rate": 3.175879299599098e-05,
      "loss": 0.0128,
      "step": 81510
    },
    {
      "epoch": 23.892145369284876,
      "grad_norm": 1.8063896894454956,
      "learning_rate": 3.1674996708402615e-05,
      "loss": 0.0201,
      "step": 81520
    },
    {
      "epoch": 23.895076201641267,
      "grad_norm": 0.16763007640838623,
      "learning_rate": 3.1591200420814245e-05,
      "loss": 0.0321,
      "step": 81530
    },
    {
      "epoch": 23.898007033997654,
      "grad_norm": 1.1277599334716797,
      "learning_rate": 3.150740413322588e-05,
      "loss": 0.033,
      "step": 81540
    },
    {
      "epoch": 23.900937866354045,
      "grad_norm": 1.4287033081054688,
      "learning_rate": 3.142360784563751e-05,
      "loss": 0.0348,
      "step": 81550
    },
    {
      "epoch": 23.903868698710433,
      "grad_norm": 1.2237852811813354,
      "learning_rate": 3.133981155804915e-05,
      "loss": 0.0223,
      "step": 81560
    },
    {
      "epoch": 23.906799531066824,
      "grad_norm": 0.29697951674461365,
      "learning_rate": 3.1256015270460786e-05,
      "loss": 0.0226,
      "step": 81570
    },
    {
      "epoch": 23.90973036342321,
      "grad_norm": 1.0251327753067017,
      "learning_rate": 3.1172218982872416e-05,
      "loss": 0.0275,
      "step": 81580
    },
    {
      "epoch": 23.912661195779602,
      "grad_norm": 0.3669368326663971,
      "learning_rate": 3.108842269528405e-05,
      "loss": 0.0204,
      "step": 81590
    },
    {
      "epoch": 23.91559202813599,
      "grad_norm": 1.3902748823165894,
      "learning_rate": 3.100462640769568e-05,
      "loss": 0.0188,
      "step": 81600
    },
    {
      "epoch": 23.91852286049238,
      "grad_norm": 0.800636887550354,
      "learning_rate": 3.092083012010731e-05,
      "loss": 0.0243,
      "step": 81610
    },
    {
      "epoch": 23.921453692848768,
      "grad_norm": 2.01741099357605,
      "learning_rate": 3.083703383251895e-05,
      "loss": 0.0336,
      "step": 81620
    },
    {
      "epoch": 23.92438452520516,
      "grad_norm": 0.8194165825843811,
      "learning_rate": 3.075323754493058e-05,
      "loss": 0.0321,
      "step": 81630
    },
    {
      "epoch": 23.927315357561547,
      "grad_norm": 1.864258885383606,
      "learning_rate": 3.0669441257342216e-05,
      "loss": 0.0179,
      "step": 81640
    },
    {
      "epoch": 23.930246189917938,
      "grad_norm": 0.43629539012908936,
      "learning_rate": 3.0585644969753846e-05,
      "loss": 0.0239,
      "step": 81650
    },
    {
      "epoch": 23.933177022274325,
      "grad_norm": 0.885260820388794,
      "learning_rate": 3.0501848682165483e-05,
      "loss": 0.0167,
      "step": 81660
    },
    {
      "epoch": 23.936107854630716,
      "grad_norm": 1.46500563621521,
      "learning_rate": 3.0418052394577113e-05,
      "loss": 0.0391,
      "step": 81670
    },
    {
      "epoch": 23.939038686987104,
      "grad_norm": 0.5442917943000793,
      "learning_rate": 3.0334256106988747e-05,
      "loss": 0.0251,
      "step": 81680
    },
    {
      "epoch": 23.941969519343495,
      "grad_norm": 0.06264209002256393,
      "learning_rate": 3.0250459819400384e-05,
      "loss": 0.0259,
      "step": 81690
    },
    {
      "epoch": 23.944900351699882,
      "grad_norm": 3.7658233642578125,
      "learning_rate": 3.0166663531812014e-05,
      "loss": 0.0253,
      "step": 81700
    },
    {
      "epoch": 23.947831184056273,
      "grad_norm": 0.43637749552726746,
      "learning_rate": 3.008286724422365e-05,
      "loss": 0.0309,
      "step": 81710
    },
    {
      "epoch": 23.95076201641266,
      "grad_norm": 1.8555642366409302,
      "learning_rate": 2.999907095663528e-05,
      "loss": 0.0425,
      "step": 81720
    },
    {
      "epoch": 23.95369284876905,
      "grad_norm": 0.03310401737689972,
      "learning_rate": 2.9915274669046914e-05,
      "loss": 0.0085,
      "step": 81730
    },
    {
      "epoch": 23.95662368112544,
      "grad_norm": 0.3728002905845642,
      "learning_rate": 2.9831478381458547e-05,
      "loss": 0.0171,
      "step": 81740
    },
    {
      "epoch": 23.95955451348183,
      "grad_norm": 0.7025614976882935,
      "learning_rate": 2.974768209387018e-05,
      "loss": 0.0234,
      "step": 81750
    },
    {
      "epoch": 23.962485345838218,
      "grad_norm": 1.872747778892517,
      "learning_rate": 2.9663885806281817e-05,
      "loss": 0.0329,
      "step": 81760
    },
    {
      "epoch": 23.96541617819461,
      "grad_norm": 0.6915417313575745,
      "learning_rate": 2.9580089518693447e-05,
      "loss": 0.022,
      "step": 81770
    },
    {
      "epoch": 23.968347010550996,
      "grad_norm": 0.1074371486902237,
      "learning_rate": 2.949629323110508e-05,
      "loss": 0.0378,
      "step": 81780
    },
    {
      "epoch": 23.971277842907387,
      "grad_norm": 1.154077172279358,
      "learning_rate": 2.9412496943516714e-05,
      "loss": 0.0239,
      "step": 81790
    },
    {
      "epoch": 23.974208675263775,
      "grad_norm": 0.38502082228660583,
      "learning_rate": 2.9328700655928348e-05,
      "loss": 0.0206,
      "step": 81800
    },
    {
      "epoch": 23.977139507620166,
      "grad_norm": 1.359257459640503,
      "learning_rate": 2.9244904368339985e-05,
      "loss": 0.0219,
      "step": 81810
    },
    {
      "epoch": 23.980070339976553,
      "grad_norm": 0.1869165301322937,
      "learning_rate": 2.9161108080751615e-05,
      "loss": 0.0257,
      "step": 81820
    },
    {
      "epoch": 23.983001172332944,
      "grad_norm": 1.7775952816009521,
      "learning_rate": 2.9077311793163248e-05,
      "loss": 0.0281,
      "step": 81830
    },
    {
      "epoch": 23.98593200468933,
      "grad_norm": 3.1554982662200928,
      "learning_rate": 2.8993515505574878e-05,
      "loss": 0.0256,
      "step": 81840
    },
    {
      "epoch": 23.988862837045723,
      "grad_norm": 1.0580414533615112,
      "learning_rate": 2.8909719217986515e-05,
      "loss": 0.0514,
      "step": 81850
    },
    {
      "epoch": 23.99179366940211,
      "grad_norm": 0.021618995815515518,
      "learning_rate": 2.8825922930398145e-05,
      "loss": 0.0158,
      "step": 81860
    },
    {
      "epoch": 23.9947245017585,
      "grad_norm": 1.6847822666168213,
      "learning_rate": 2.874212664280978e-05,
      "loss": 0.0276,
      "step": 81870
    },
    {
      "epoch": 23.99765533411489,
      "grad_norm": 0.8211912512779236,
      "learning_rate": 2.8658330355221415e-05,
      "loss": 0.0284,
      "step": 81880
    },
    {
      "epoch": 24.0,
      "eval_accuracy": 0.8308605341246291,
      "eval_f1_macro": 0.870636686219174,
      "eval_f1_micro": 0.8823616674083964,
      "eval_f1_weighted": 0.8813052777044461,
      "eval_loss": 0.06582047790288925,
      "eval_roc_auc": 0.9294382619131848,
      "eval_runtime": 232.9574,
      "eval_samples_per_second": 13.02,
      "eval_steps_per_second": 1.631,
      "step": 81888
    },
    {
      "epoch": 24.000586166471276,
      "grad_norm": 1.3548541069030762,
      "learning_rate": 2.8574534067633045e-05,
      "loss": 0.0164,
      "step": 81890
    },
    {
      "epoch": 24.003516998827667,
      "grad_norm": 0.38620713353157043,
      "learning_rate": 2.8490737780044682e-05,
      "loss": 0.0162,
      "step": 81900
    },
    {
      "epoch": 24.006447831184055,
      "grad_norm": 1.4623689651489258,
      "learning_rate": 2.8406941492456312e-05,
      "loss": 0.0331,
      "step": 81910
    },
    {
      "epoch": 24.009378663540446,
      "grad_norm": 0.4967309534549713,
      "learning_rate": 2.832314520486795e-05,
      "loss": 0.0191,
      "step": 81920
    },
    {
      "epoch": 24.012309495896833,
      "grad_norm": 0.42984718084335327,
      "learning_rate": 2.8239348917279582e-05,
      "loss": 0.0284,
      "step": 81930
    },
    {
      "epoch": 24.015240328253224,
      "grad_norm": 0.5073032379150391,
      "learning_rate": 2.8155552629691212e-05,
      "loss": 0.0219,
      "step": 81940
    },
    {
      "epoch": 24.01817116060961,
      "grad_norm": 3.0074689388275146,
      "learning_rate": 2.807175634210285e-05,
      "loss": 0.0302,
      "step": 81950
    },
    {
      "epoch": 24.021101992966003,
      "grad_norm": 0.03404337540268898,
      "learning_rate": 2.798796005451448e-05,
      "loss": 0.0236,
      "step": 81960
    },
    {
      "epoch": 24.02403282532239,
      "grad_norm": 0.255434513092041,
      "learning_rate": 2.7904163766926116e-05,
      "loss": 0.0248,
      "step": 81970
    },
    {
      "epoch": 24.02696365767878,
      "grad_norm": 1.0408228635787964,
      "learning_rate": 2.7820367479337746e-05,
      "loss": 0.0119,
      "step": 81980
    },
    {
      "epoch": 24.02989449003517,
      "grad_norm": 1.0477523803710938,
      "learning_rate": 2.773657119174938e-05,
      "loss": 0.0286,
      "step": 81990
    },
    {
      "epoch": 24.03282532239156,
      "grad_norm": 2.638695478439331,
      "learning_rate": 2.7652774904161016e-05,
      "loss": 0.037,
      "step": 82000
    },
    {
      "epoch": 24.035756154747947,
      "grad_norm": 0.7138276100158691,
      "learning_rate": 2.7568978616572646e-05,
      "loss": 0.0498,
      "step": 82010
    },
    {
      "epoch": 24.038686987104338,
      "grad_norm": 0.36665019392967224,
      "learning_rate": 2.7485182328984283e-05,
      "loss": 0.0153,
      "step": 82020
    },
    {
      "epoch": 24.041617819460726,
      "grad_norm": 0.2814614474773407,
      "learning_rate": 2.7401386041395913e-05,
      "loss": 0.0187,
      "step": 82030
    },
    {
      "epoch": 24.044548651817117,
      "grad_norm": 0.6437047123908997,
      "learning_rate": 2.7317589753807546e-05,
      "loss": 0.0301,
      "step": 82040
    },
    {
      "epoch": 24.047479484173504,
      "grad_norm": 0.801631510257721,
      "learning_rate": 2.7233793466219176e-05,
      "loss": 0.0212,
      "step": 82050
    },
    {
      "epoch": 24.050410316529895,
      "grad_norm": 0.04434121772646904,
      "learning_rate": 2.7149997178630813e-05,
      "loss": 0.0091,
      "step": 82060
    },
    {
      "epoch": 24.053341148886282,
      "grad_norm": 2.589895009994507,
      "learning_rate": 2.706620089104245e-05,
      "loss": 0.013,
      "step": 82070
    },
    {
      "epoch": 24.056271981242674,
      "grad_norm": 0.052908241748809814,
      "learning_rate": 2.698240460345408e-05,
      "loss": 0.0332,
      "step": 82080
    },
    {
      "epoch": 24.05920281359906,
      "grad_norm": 0.11615122854709625,
      "learning_rate": 2.6898608315865713e-05,
      "loss": 0.0174,
      "step": 82090
    },
    {
      "epoch": 24.062133645955452,
      "grad_norm": 0.1522461622953415,
      "learning_rate": 2.6814812028277343e-05,
      "loss": 0.031,
      "step": 82100
    },
    {
      "epoch": 24.06506447831184,
      "grad_norm": 3.451592445373535,
      "learning_rate": 2.673101574068898e-05,
      "loss": 0.0307,
      "step": 82110
    },
    {
      "epoch": 24.06799531066823,
      "grad_norm": 0.34119126200675964,
      "learning_rate": 2.6647219453100617e-05,
      "loss": 0.0149,
      "step": 82120
    },
    {
      "epoch": 24.070926143024618,
      "grad_norm": 2.3429877758026123,
      "learning_rate": 2.6563423165512247e-05,
      "loss": 0.0291,
      "step": 82130
    },
    {
      "epoch": 24.07385697538101,
      "grad_norm": 1.8657996654510498,
      "learning_rate": 2.647962687792388e-05,
      "loss": 0.0193,
      "step": 82140
    },
    {
      "epoch": 24.076787807737396,
      "grad_norm": 0.6529282331466675,
      "learning_rate": 2.639583059033551e-05,
      "loss": 0.029,
      "step": 82150
    },
    {
      "epoch": 24.079718640093787,
      "grad_norm": 1.5788159370422363,
      "learning_rate": 2.6312034302747147e-05,
      "loss": 0.0379,
      "step": 82160
    },
    {
      "epoch": 24.082649472450175,
      "grad_norm": 0.5211772322654724,
      "learning_rate": 2.6228238015158777e-05,
      "loss": 0.0165,
      "step": 82170
    },
    {
      "epoch": 24.085580304806566,
      "grad_norm": 0.9485530853271484,
      "learning_rate": 2.6144441727570414e-05,
      "loss": 0.0236,
      "step": 82180
    },
    {
      "epoch": 24.088511137162953,
      "grad_norm": 0.5324559807777405,
      "learning_rate": 2.6060645439982048e-05,
      "loss": 0.012,
      "step": 82190
    },
    {
      "epoch": 24.091441969519344,
      "grad_norm": 0.11198584735393524,
      "learning_rate": 2.5976849152393678e-05,
      "loss": 0.028,
      "step": 82200
    },
    {
      "epoch": 24.094372801875732,
      "grad_norm": 0.8114615678787231,
      "learning_rate": 2.5893052864805314e-05,
      "loss": 0.0265,
      "step": 82210
    },
    {
      "epoch": 24.097303634232123,
      "grad_norm": 0.25952908396720886,
      "learning_rate": 2.5809256577216944e-05,
      "loss": 0.0286,
      "step": 82220
    },
    {
      "epoch": 24.10023446658851,
      "grad_norm": 1.1662819385528564,
      "learning_rate": 2.572546028962858e-05,
      "loss": 0.0249,
      "step": 82230
    },
    {
      "epoch": 24.1031652989449,
      "grad_norm": 1.0711395740509033,
      "learning_rate": 2.564166400204021e-05,
      "loss": 0.0276,
      "step": 82240
    },
    {
      "epoch": 24.10609613130129,
      "grad_norm": 0.689355194568634,
      "learning_rate": 2.5557867714451845e-05,
      "loss": 0.0168,
      "step": 82250
    },
    {
      "epoch": 24.10902696365768,
      "grad_norm": 0.17024607956409454,
      "learning_rate": 2.547407142686348e-05,
      "loss": 0.0245,
      "step": 82260
    },
    {
      "epoch": 24.111957796014067,
      "grad_norm": 0.04419758543372154,
      "learning_rate": 2.539027513927511e-05,
      "loss": 0.024,
      "step": 82270
    },
    {
      "epoch": 24.11488862837046,
      "grad_norm": 0.41885682940483093,
      "learning_rate": 2.530647885168675e-05,
      "loss": 0.0335,
      "step": 82280
    },
    {
      "epoch": 24.117819460726846,
      "grad_norm": 0.27698084712028503,
      "learning_rate": 2.522268256409838e-05,
      "loss": 0.0129,
      "step": 82290
    },
    {
      "epoch": 24.120750293083237,
      "grad_norm": 0.22402146458625793,
      "learning_rate": 2.5138886276510012e-05,
      "loss": 0.0258,
      "step": 82300
    },
    {
      "epoch": 24.123681125439624,
      "grad_norm": 0.0586041584610939,
      "learning_rate": 2.505508998892165e-05,
      "loss": 0.0202,
      "step": 82310
    },
    {
      "epoch": 24.126611957796015,
      "grad_norm": 0.8311253190040588,
      "learning_rate": 2.497129370133328e-05,
      "loss": 0.034,
      "step": 82320
    },
    {
      "epoch": 24.129542790152403,
      "grad_norm": 1.3086124658584595,
      "learning_rate": 2.4887497413744912e-05,
      "loss": 0.0182,
      "step": 82330
    },
    {
      "epoch": 24.132473622508794,
      "grad_norm": 0.6384287476539612,
      "learning_rate": 2.4803701126156546e-05,
      "loss": 0.0313,
      "step": 82340
    },
    {
      "epoch": 24.13540445486518,
      "grad_norm": 1.613176941871643,
      "learning_rate": 2.471990483856818e-05,
      "loss": 0.036,
      "step": 82350
    },
    {
      "epoch": 24.138335287221572,
      "grad_norm": 0.07290764898061752,
      "learning_rate": 2.463610855097981e-05,
      "loss": 0.0269,
      "step": 82360
    },
    {
      "epoch": 24.14126611957796,
      "grad_norm": 2.317833185195923,
      "learning_rate": 2.4552312263391446e-05,
      "loss": 0.0264,
      "step": 82370
    },
    {
      "epoch": 24.14419695193435,
      "grad_norm": 0.5557095408439636,
      "learning_rate": 2.446851597580308e-05,
      "loss": 0.0282,
      "step": 82380
    },
    {
      "epoch": 24.14712778429074,
      "grad_norm": 0.2642395496368408,
      "learning_rate": 2.4384719688214713e-05,
      "loss": 0.0162,
      "step": 82390
    },
    {
      "epoch": 24.15005861664713,
      "grad_norm": 0.41570207476615906,
      "learning_rate": 2.4300923400626346e-05,
      "loss": 0.0281,
      "step": 82400
    },
    {
      "epoch": 24.152989449003517,
      "grad_norm": 0.9423584342002869,
      "learning_rate": 2.4217127113037976e-05,
      "loss": 0.0399,
      "step": 82410
    },
    {
      "epoch": 24.155920281359908,
      "grad_norm": 0.27317607402801514,
      "learning_rate": 2.4133330825449613e-05,
      "loss": 0.0215,
      "step": 82420
    },
    {
      "epoch": 24.158851113716295,
      "grad_norm": 0.3574954867362976,
      "learning_rate": 2.4049534537861246e-05,
      "loss": 0.0141,
      "step": 82430
    },
    {
      "epoch": 24.161781946072686,
      "grad_norm": 0.038215771317481995,
      "learning_rate": 2.396573825027288e-05,
      "loss": 0.0281,
      "step": 82440
    },
    {
      "epoch": 24.164712778429074,
      "grad_norm": 1.14464271068573,
      "learning_rate": 2.3881941962684513e-05,
      "loss": 0.0435,
      "step": 82450
    },
    {
      "epoch": 24.16764361078546,
      "grad_norm": 0.26328739523887634,
      "learning_rate": 2.3798145675096143e-05,
      "loss": 0.0343,
      "step": 82460
    },
    {
      "epoch": 24.170574443141852,
      "grad_norm": 1.4970756769180298,
      "learning_rate": 2.371434938750778e-05,
      "loss": 0.0356,
      "step": 82470
    },
    {
      "epoch": 24.17350527549824,
      "grad_norm": 1.251417875289917,
      "learning_rate": 2.363055309991941e-05,
      "loss": 0.0401,
      "step": 82480
    },
    {
      "epoch": 24.17643610785463,
      "grad_norm": 0.06022089719772339,
      "learning_rate": 2.3546756812331043e-05,
      "loss": 0.0095,
      "step": 82490
    },
    {
      "epoch": 24.17936694021102,
      "grad_norm": 0.45500892400741577,
      "learning_rate": 2.346296052474268e-05,
      "loss": 0.0271,
      "step": 82500
    },
    {
      "epoch": 24.18229777256741,
      "grad_norm": 0.8899641633033752,
      "learning_rate": 2.337916423715431e-05,
      "loss": 0.0354,
      "step": 82510
    },
    {
      "epoch": 24.185228604923797,
      "grad_norm": 0.1041395366191864,
      "learning_rate": 2.3295367949565947e-05,
      "loss": 0.0273,
      "step": 82520
    },
    {
      "epoch": 24.188159437280188,
      "grad_norm": 0.16555824875831604,
      "learning_rate": 2.3211571661977577e-05,
      "loss": 0.0291,
      "step": 82530
    },
    {
      "epoch": 24.191090269636575,
      "grad_norm": 1.0822325944900513,
      "learning_rate": 2.312777537438921e-05,
      "loss": 0.0158,
      "step": 82540
    },
    {
      "epoch": 24.194021101992966,
      "grad_norm": 0.9491503238677979,
      "learning_rate": 2.3043979086800844e-05,
      "loss": 0.0297,
      "step": 82550
    },
    {
      "epoch": 24.196951934349354,
      "grad_norm": 0.8938847780227661,
      "learning_rate": 2.2960182799212477e-05,
      "loss": 0.0267,
      "step": 82560
    },
    {
      "epoch": 24.199882766705745,
      "grad_norm": 0.4304359555244446,
      "learning_rate": 2.2876386511624114e-05,
      "loss": 0.034,
      "step": 82570
    },
    {
      "epoch": 24.202813599062132,
      "grad_norm": 0.5250212550163269,
      "learning_rate": 2.2792590224035744e-05,
      "loss": 0.016,
      "step": 82580
    },
    {
      "epoch": 24.205744431418523,
      "grad_norm": 1.1600630283355713,
      "learning_rate": 2.2708793936447378e-05,
      "loss": 0.0273,
      "step": 82590
    },
    {
      "epoch": 24.20867526377491,
      "grad_norm": 2.170970916748047,
      "learning_rate": 2.262499764885901e-05,
      "loss": 0.0488,
      "step": 82600
    },
    {
      "epoch": 24.2116060961313,
      "grad_norm": 0.014045946300029755,
      "learning_rate": 2.2541201361270644e-05,
      "loss": 0.0213,
      "step": 82610
    },
    {
      "epoch": 24.21453692848769,
      "grad_norm": 1.6992814540863037,
      "learning_rate": 2.245740507368228e-05,
      "loss": 0.0284,
      "step": 82620
    },
    {
      "epoch": 24.21746776084408,
      "grad_norm": 0.853198230266571,
      "learning_rate": 2.237360878609391e-05,
      "loss": 0.0306,
      "step": 82630
    },
    {
      "epoch": 24.220398593200468,
      "grad_norm": 0.12009132653474808,
      "learning_rate": 2.2289812498505545e-05,
      "loss": 0.0214,
      "step": 82640
    },
    {
      "epoch": 24.22332942555686,
      "grad_norm": 0.6141057014465332,
      "learning_rate": 2.2206016210917178e-05,
      "loss": 0.0213,
      "step": 82650
    },
    {
      "epoch": 24.226260257913246,
      "grad_norm": 0.10604600608348846,
      "learning_rate": 2.212221992332881e-05,
      "loss": 0.012,
      "step": 82660
    },
    {
      "epoch": 24.229191090269637,
      "grad_norm": 5.371553421020508,
      "learning_rate": 2.2038423635740445e-05,
      "loss": 0.0246,
      "step": 82670
    },
    {
      "epoch": 24.232121922626025,
      "grad_norm": 1.9478116035461426,
      "learning_rate": 2.195462734815208e-05,
      "loss": 0.0249,
      "step": 82680
    },
    {
      "epoch": 24.235052754982416,
      "grad_norm": 0.2797713279724121,
      "learning_rate": 2.1870831060563712e-05,
      "loss": 0.0391,
      "step": 82690
    },
    {
      "epoch": 24.237983587338803,
      "grad_norm": 0.22140444815158844,
      "learning_rate": 2.1787034772975342e-05,
      "loss": 0.0148,
      "step": 82700
    },
    {
      "epoch": 24.240914419695194,
      "grad_norm": 0.9435824751853943,
      "learning_rate": 2.170323848538698e-05,
      "loss": 0.0262,
      "step": 82710
    },
    {
      "epoch": 24.24384525205158,
      "grad_norm": 0.5489417910575867,
      "learning_rate": 2.1619442197798612e-05,
      "loss": 0.0155,
      "step": 82720
    },
    {
      "epoch": 24.246776084407973,
      "grad_norm": 3.102276086807251,
      "learning_rate": 2.1535645910210245e-05,
      "loss": 0.0347,
      "step": 82730
    },
    {
      "epoch": 24.24970691676436,
      "grad_norm": 3.4122142791748047,
      "learning_rate": 2.145184962262188e-05,
      "loss": 0.0391,
      "step": 82740
    },
    {
      "epoch": 24.25263774912075,
      "grad_norm": 0.5470790266990662,
      "learning_rate": 2.136805333503351e-05,
      "loss": 0.0418,
      "step": 82750
    },
    {
      "epoch": 24.25556858147714,
      "grad_norm": 0.11424192786216736,
      "learning_rate": 2.1284257047445142e-05,
      "loss": 0.041,
      "step": 82760
    },
    {
      "epoch": 24.25849941383353,
      "grad_norm": 0.7676790952682495,
      "learning_rate": 2.1200460759856776e-05,
      "loss": 0.0273,
      "step": 82770
    },
    {
      "epoch": 24.261430246189917,
      "grad_norm": 3.0055525302886963,
      "learning_rate": 2.1116664472268413e-05,
      "loss": 0.0229,
      "step": 82780
    },
    {
      "epoch": 24.264361078546308,
      "grad_norm": 0.5008074045181274,
      "learning_rate": 2.1032868184680046e-05,
      "loss": 0.0169,
      "step": 82790
    },
    {
      "epoch": 24.267291910902696,
      "grad_norm": 1.5089877843856812,
      "learning_rate": 2.0949071897091676e-05,
      "loss": 0.0419,
      "step": 82800
    },
    {
      "epoch": 24.270222743259087,
      "grad_norm": 0.37949642539024353,
      "learning_rate": 2.086527560950331e-05,
      "loss": 0.0155,
      "step": 82810
    },
    {
      "epoch": 24.273153575615474,
      "grad_norm": 0.3844662606716156,
      "learning_rate": 2.0781479321914943e-05,
      "loss": 0.0227,
      "step": 82820
    },
    {
      "epoch": 24.276084407971865,
      "grad_norm": 3.249290943145752,
      "learning_rate": 2.0697683034326576e-05,
      "loss": 0.0364,
      "step": 82830
    },
    {
      "epoch": 24.279015240328253,
      "grad_norm": 0.8055345416069031,
      "learning_rate": 2.061388674673821e-05,
      "loss": 0.0212,
      "step": 82840
    },
    {
      "epoch": 24.281946072684644,
      "grad_norm": 0.4647958278656006,
      "learning_rate": 2.0530090459149843e-05,
      "loss": 0.0102,
      "step": 82850
    },
    {
      "epoch": 24.28487690504103,
      "grad_norm": 1.4387643337249756,
      "learning_rate": 2.0446294171561476e-05,
      "loss": 0.0209,
      "step": 82860
    },
    {
      "epoch": 24.287807737397422,
      "grad_norm": 1.1008795499801636,
      "learning_rate": 2.036249788397311e-05,
      "loss": 0.0292,
      "step": 82870
    },
    {
      "epoch": 24.29073856975381,
      "grad_norm": 0.9585704207420349,
      "learning_rate": 2.0278701596384743e-05,
      "loss": 0.018,
      "step": 82880
    },
    {
      "epoch": 24.2936694021102,
      "grad_norm": 1.7405256032943726,
      "learning_rate": 2.0194905308796377e-05,
      "loss": 0.0171,
      "step": 82890
    },
    {
      "epoch": 24.296600234466588,
      "grad_norm": 2.5629184246063232,
      "learning_rate": 2.011110902120801e-05,
      "loss": 0.0185,
      "step": 82900
    },
    {
      "epoch": 24.29953106682298,
      "grad_norm": 0.3011866807937622,
      "learning_rate": 2.0027312733619644e-05,
      "loss": 0.0256,
      "step": 82910
    },
    {
      "epoch": 24.302461899179367,
      "grad_norm": 0.15875187516212463,
      "learning_rate": 1.9943516446031277e-05,
      "loss": 0.0155,
      "step": 82920
    },
    {
      "epoch": 24.305392731535758,
      "grad_norm": 0.01480520237237215,
      "learning_rate": 1.985972015844291e-05,
      "loss": 0.0121,
      "step": 82930
    },
    {
      "epoch": 24.308323563892145,
      "grad_norm": 1.0337390899658203,
      "learning_rate": 1.9775923870854544e-05,
      "loss": 0.0207,
      "step": 82940
    },
    {
      "epoch": 24.311254396248536,
      "grad_norm": 4.046578884124756,
      "learning_rate": 1.9692127583266174e-05,
      "loss": 0.0206,
      "step": 82950
    },
    {
      "epoch": 24.314185228604924,
      "grad_norm": 2.8525750637054443,
      "learning_rate": 1.960833129567781e-05,
      "loss": 0.0474,
      "step": 82960
    },
    {
      "epoch": 24.317116060961315,
      "grad_norm": 0.6014140844345093,
      "learning_rate": 1.9524535008089444e-05,
      "loss": 0.0311,
      "step": 82970
    },
    {
      "epoch": 24.320046893317702,
      "grad_norm": 0.5091791749000549,
      "learning_rate": 1.9440738720501077e-05,
      "loss": 0.0179,
      "step": 82980
    },
    {
      "epoch": 24.322977725674093,
      "grad_norm": 0.15170562267303467,
      "learning_rate": 1.935694243291271e-05,
      "loss": 0.0254,
      "step": 82990
    },
    {
      "epoch": 24.32590855803048,
      "grad_norm": 0.2259730100631714,
      "learning_rate": 1.927314614532434e-05,
      "loss": 0.0351,
      "step": 83000
    },
    {
      "epoch": 24.32883939038687,
      "grad_norm": 0.9227339029312134,
      "learning_rate": 1.9189349857735974e-05,
      "loss": 0.0182,
      "step": 83010
    },
    {
      "epoch": 24.33177022274326,
      "grad_norm": 0.32872721552848816,
      "learning_rate": 1.9105553570147608e-05,
      "loss": 0.0291,
      "step": 83020
    },
    {
      "epoch": 24.33470105509965,
      "grad_norm": 1.6693978309631348,
      "learning_rate": 1.9021757282559245e-05,
      "loss": 0.0239,
      "step": 83030
    },
    {
      "epoch": 24.337631887456038,
      "grad_norm": 0.4827059209346771,
      "learning_rate": 1.8937960994970878e-05,
      "loss": 0.0312,
      "step": 83040
    },
    {
      "epoch": 24.340562719812425,
      "grad_norm": 1.066057562828064,
      "learning_rate": 1.8854164707382508e-05,
      "loss": 0.0285,
      "step": 83050
    },
    {
      "epoch": 24.343493552168816,
      "grad_norm": 0.8769757151603699,
      "learning_rate": 1.877036841979414e-05,
      "loss": 0.016,
      "step": 83060
    },
    {
      "epoch": 24.346424384525204,
      "grad_norm": 0.6411844491958618,
      "learning_rate": 1.8686572132205775e-05,
      "loss": 0.032,
      "step": 83070
    },
    {
      "epoch": 24.349355216881595,
      "grad_norm": 0.2206268459558487,
      "learning_rate": 1.8602775844617408e-05,
      "loss": 0.013,
      "step": 83080
    },
    {
      "epoch": 24.352286049237982,
      "grad_norm": 0.6753710508346558,
      "learning_rate": 1.8518979557029045e-05,
      "loss": 0.0133,
      "step": 83090
    },
    {
      "epoch": 24.355216881594373,
      "grad_norm": 0.07357001304626465,
      "learning_rate": 1.8435183269440675e-05,
      "loss": 0.022,
      "step": 83100
    },
    {
      "epoch": 24.35814771395076,
      "grad_norm": 2.580976724624634,
      "learning_rate": 1.835138698185231e-05,
      "loss": 0.0289,
      "step": 83110
    },
    {
      "epoch": 24.36107854630715,
      "grad_norm": 2.3022825717926025,
      "learning_rate": 1.8267590694263942e-05,
      "loss": 0.0219,
      "step": 83120
    },
    {
      "epoch": 24.36400937866354,
      "grad_norm": 1.0061626434326172,
      "learning_rate": 1.8183794406675575e-05,
      "loss": 0.0271,
      "step": 83130
    },
    {
      "epoch": 24.36694021101993,
      "grad_norm": 0.25331130623817444,
      "learning_rate": 1.809999811908721e-05,
      "loss": 0.0164,
      "step": 83140
    },
    {
      "epoch": 24.369871043376317,
      "grad_norm": 0.7233123779296875,
      "learning_rate": 1.8016201831498842e-05,
      "loss": 0.0325,
      "step": 83150
    },
    {
      "epoch": 24.37280187573271,
      "grad_norm": 0.6124604344367981,
      "learning_rate": 1.7932405543910476e-05,
      "loss": 0.0221,
      "step": 83160
    },
    {
      "epoch": 24.375732708089096,
      "grad_norm": 0.37815916538238525,
      "learning_rate": 1.784860925632211e-05,
      "loss": 0.0115,
      "step": 83170
    },
    {
      "epoch": 24.378663540445487,
      "grad_norm": 2.1291615962982178,
      "learning_rate": 1.7764812968733742e-05,
      "loss": 0.0141,
      "step": 83180
    },
    {
      "epoch": 24.381594372801874,
      "grad_norm": 2.3494839668273926,
      "learning_rate": 1.7681016681145376e-05,
      "loss": 0.0255,
      "step": 83190
    },
    {
      "epoch": 24.384525205158265,
      "grad_norm": 0.021229395642876625,
      "learning_rate": 1.759722039355701e-05,
      "loss": 0.026,
      "step": 83200
    },
    {
      "epoch": 24.387456037514653,
      "grad_norm": 1.785079002380371,
      "learning_rate": 1.7513424105968643e-05,
      "loss": 0.0217,
      "step": 83210
    },
    {
      "epoch": 24.390386869871044,
      "grad_norm": 0.9593594670295715,
      "learning_rate": 1.7429627818380276e-05,
      "loss": 0.0205,
      "step": 83220
    },
    {
      "epoch": 24.39331770222743,
      "grad_norm": 0.37481099367141724,
      "learning_rate": 1.734583153079191e-05,
      "loss": 0.0217,
      "step": 83230
    },
    {
      "epoch": 24.396248534583822,
      "grad_norm": 0.7866774797439575,
      "learning_rate": 1.7262035243203543e-05,
      "loss": 0.0154,
      "step": 83240
    },
    {
      "epoch": 24.39917936694021,
      "grad_norm": 0.8457353115081787,
      "learning_rate": 1.7178238955615176e-05,
      "loss": 0.0228,
      "step": 83250
    },
    {
      "epoch": 24.4021101992966,
      "grad_norm": 0.7066356539726257,
      "learning_rate": 1.7094442668026806e-05,
      "loss": 0.035,
      "step": 83260
    },
    {
      "epoch": 24.40504103165299,
      "grad_norm": 1.8665046691894531,
      "learning_rate": 1.701064638043844e-05,
      "loss": 0.0199,
      "step": 83270
    },
    {
      "epoch": 24.40797186400938,
      "grad_norm": 0.5689883232116699,
      "learning_rate": 1.6926850092850077e-05,
      "loss": 0.0284,
      "step": 83280
    },
    {
      "epoch": 24.410902696365767,
      "grad_norm": 0.2471376210451126,
      "learning_rate": 1.684305380526171e-05,
      "loss": 0.0391,
      "step": 83290
    },
    {
      "epoch": 24.413833528722158,
      "grad_norm": 0.9478886723518372,
      "learning_rate": 1.675925751767334e-05,
      "loss": 0.0336,
      "step": 83300
    },
    {
      "epoch": 24.416764361078545,
      "grad_norm": 0.139070525765419,
      "learning_rate": 1.6675461230084974e-05,
      "loss": 0.0286,
      "step": 83310
    },
    {
      "epoch": 24.419695193434936,
      "grad_norm": 1.2539949417114258,
      "learning_rate": 1.6591664942496607e-05,
      "loss": 0.0196,
      "step": 83320
    },
    {
      "epoch": 24.422626025791324,
      "grad_norm": 1.1621536016464233,
      "learning_rate": 1.650786865490824e-05,
      "loss": 0.0212,
      "step": 83330
    },
    {
      "epoch": 24.425556858147715,
      "grad_norm": 0.2571384608745575,
      "learning_rate": 1.6424072367319877e-05,
      "loss": 0.0457,
      "step": 83340
    },
    {
      "epoch": 24.428487690504102,
      "grad_norm": 0.8613483905792236,
      "learning_rate": 1.6340276079731507e-05,
      "loss": 0.0271,
      "step": 83350
    },
    {
      "epoch": 24.431418522860493,
      "grad_norm": 2.1016087532043457,
      "learning_rate": 1.625647979214314e-05,
      "loss": 0.0178,
      "step": 83360
    },
    {
      "epoch": 24.43434935521688,
      "grad_norm": 1.5709494352340698,
      "learning_rate": 1.6172683504554774e-05,
      "loss": 0.0234,
      "step": 83370
    },
    {
      "epoch": 24.437280187573272,
      "grad_norm": 2.636406421661377,
      "learning_rate": 1.6088887216966407e-05,
      "loss": 0.0298,
      "step": 83380
    },
    {
      "epoch": 24.44021101992966,
      "grad_norm": 0.841468334197998,
      "learning_rate": 1.600509092937804e-05,
      "loss": 0.0278,
      "step": 83390
    },
    {
      "epoch": 24.44314185228605,
      "grad_norm": 0.02569900080561638,
      "learning_rate": 1.5921294641789674e-05,
      "loss": 0.0269,
      "step": 83400
    },
    {
      "epoch": 24.446072684642438,
      "grad_norm": 0.3140905499458313,
      "learning_rate": 1.5837498354201308e-05,
      "loss": 0.0072,
      "step": 83410
    },
    {
      "epoch": 24.44900351699883,
      "grad_norm": 0.5991693139076233,
      "learning_rate": 1.575370206661294e-05,
      "loss": 0.0236,
      "step": 83420
    },
    {
      "epoch": 24.451934349355216,
      "grad_norm": 0.45085954666137695,
      "learning_rate": 1.5669905779024575e-05,
      "loss": 0.031,
      "step": 83430
    },
    {
      "epoch": 24.454865181711607,
      "grad_norm": 1.758870005607605,
      "learning_rate": 1.5586109491436208e-05,
      "loss": 0.0245,
      "step": 83440
    },
    {
      "epoch": 24.457796014067995,
      "grad_norm": 0.508563220500946,
      "learning_rate": 1.550231320384784e-05,
      "loss": 0.0235,
      "step": 83450
    },
    {
      "epoch": 24.460726846424386,
      "grad_norm": 0.06638125330209732,
      "learning_rate": 1.5418516916259475e-05,
      "loss": 0.0177,
      "step": 83460
    },
    {
      "epoch": 24.463657678780773,
      "grad_norm": 1.4993879795074463,
      "learning_rate": 1.5334720628671108e-05,
      "loss": 0.0268,
      "step": 83470
    },
    {
      "epoch": 24.466588511137164,
      "grad_norm": 0.6190133094787598,
      "learning_rate": 1.5250924341082742e-05,
      "loss": 0.0339,
      "step": 83480
    },
    {
      "epoch": 24.469519343493552,
      "grad_norm": 0.4550352394580841,
      "learning_rate": 1.5167128053494373e-05,
      "loss": 0.029,
      "step": 83490
    },
    {
      "epoch": 24.472450175849943,
      "grad_norm": 0.1324431598186493,
      "learning_rate": 1.5083331765906007e-05,
      "loss": 0.0347,
      "step": 83500
    },
    {
      "epoch": 24.47538100820633,
      "grad_norm": 1.8768244981765747,
      "learning_rate": 1.499953547831764e-05,
      "loss": 0.0247,
      "step": 83510
    },
    {
      "epoch": 24.47831184056272,
      "grad_norm": 0.12471089512109756,
      "learning_rate": 1.4915739190729274e-05,
      "loss": 0.0347,
      "step": 83520
    },
    {
      "epoch": 24.48124267291911,
      "grad_norm": 1.0894709825515747,
      "learning_rate": 1.4831942903140909e-05,
      "loss": 0.0203,
      "step": 83530
    },
    {
      "epoch": 24.4841735052755,
      "grad_norm": 0.08087197691202164,
      "learning_rate": 1.474814661555254e-05,
      "loss": 0.0168,
      "step": 83540
    },
    {
      "epoch": 24.487104337631887,
      "grad_norm": 0.3584511876106262,
      "learning_rate": 1.4664350327964174e-05,
      "loss": 0.0242,
      "step": 83550
    },
    {
      "epoch": 24.49003516998828,
      "grad_norm": 0.6130995750427246,
      "learning_rate": 1.4580554040375807e-05,
      "loss": 0.0278,
      "step": 83560
    },
    {
      "epoch": 24.492966002344666,
      "grad_norm": 0.866786003112793,
      "learning_rate": 1.4496757752787439e-05,
      "loss": 0.0278,
      "step": 83570
    },
    {
      "epoch": 24.495896834701057,
      "grad_norm": 0.5855085849761963,
      "learning_rate": 1.4412961465199072e-05,
      "loss": 0.0182,
      "step": 83580
    },
    {
      "epoch": 24.498827667057444,
      "grad_norm": 0.039396513253450394,
      "learning_rate": 1.4329165177610708e-05,
      "loss": 0.012,
      "step": 83590
    },
    {
      "epoch": 24.50175849941383,
      "grad_norm": 0.6203441619873047,
      "learning_rate": 1.4245368890022341e-05,
      "loss": 0.0216,
      "step": 83600
    },
    {
      "epoch": 24.504689331770223,
      "grad_norm": 1.42734956741333,
      "learning_rate": 1.4161572602433974e-05,
      "loss": 0.0237,
      "step": 83610
    },
    {
      "epoch": 24.50762016412661,
      "grad_norm": 0.1680813580751419,
      "learning_rate": 1.4077776314845606e-05,
      "loss": 0.0259,
      "step": 83620
    },
    {
      "epoch": 24.510550996483,
      "grad_norm": 0.9529184699058533,
      "learning_rate": 1.399398002725724e-05,
      "loss": 0.0282,
      "step": 83630
    },
    {
      "epoch": 24.51348182883939,
      "grad_norm": 0.5548253059387207,
      "learning_rate": 1.3910183739668873e-05,
      "loss": 0.0252,
      "step": 83640
    },
    {
      "epoch": 24.51641266119578,
      "grad_norm": 0.060074951499700546,
      "learning_rate": 1.3826387452080508e-05,
      "loss": 0.0159,
      "step": 83650
    },
    {
      "epoch": 24.519343493552167,
      "grad_norm": 2.5414960384368896,
      "learning_rate": 1.3742591164492141e-05,
      "loss": 0.0186,
      "step": 83660
    },
    {
      "epoch": 24.52227432590856,
      "grad_norm": 1.109542727470398,
      "learning_rate": 1.3658794876903773e-05,
      "loss": 0.0161,
      "step": 83670
    },
    {
      "epoch": 24.525205158264946,
      "grad_norm": 0.9561223983764648,
      "learning_rate": 1.3574998589315407e-05,
      "loss": 0.0217,
      "step": 83680
    },
    {
      "epoch": 24.528135990621337,
      "grad_norm": 1.2722368240356445,
      "learning_rate": 1.349120230172704e-05,
      "loss": 0.019,
      "step": 83690
    },
    {
      "epoch": 24.531066822977724,
      "grad_norm": 1.7138453722000122,
      "learning_rate": 1.3407406014138672e-05,
      "loss": 0.0156,
      "step": 83700
    },
    {
      "epoch": 24.533997655334115,
      "grad_norm": 0.5527670979499817,
      "learning_rate": 1.3323609726550309e-05,
      "loss": 0.0377,
      "step": 83710
    },
    {
      "epoch": 24.536928487690503,
      "grad_norm": 0.7416990399360657,
      "learning_rate": 1.323981343896194e-05,
      "loss": 0.0247,
      "step": 83720
    },
    {
      "epoch": 24.539859320046894,
      "grad_norm": 1.5494803190231323,
      "learning_rate": 1.3156017151373574e-05,
      "loss": 0.0236,
      "step": 83730
    },
    {
      "epoch": 24.54279015240328,
      "grad_norm": 0.5617102384567261,
      "learning_rate": 1.3072220863785207e-05,
      "loss": 0.022,
      "step": 83740
    },
    {
      "epoch": 24.545720984759672,
      "grad_norm": 1.1888232231140137,
      "learning_rate": 1.2988424576196839e-05,
      "loss": 0.0231,
      "step": 83750
    },
    {
      "epoch": 24.54865181711606,
      "grad_norm": 0.807251513004303,
      "learning_rate": 1.2904628288608472e-05,
      "loss": 0.0264,
      "step": 83760
    },
    {
      "epoch": 24.55158264947245,
      "grad_norm": 2.503520965576172,
      "learning_rate": 1.2820832001020106e-05,
      "loss": 0.0291,
      "step": 83770
    },
    {
      "epoch": 24.554513481828838,
      "grad_norm": 1.0871332883834839,
      "learning_rate": 1.273703571343174e-05,
      "loss": 0.0292,
      "step": 83780
    },
    {
      "epoch": 24.55744431418523,
      "grad_norm": 0.5307851433753967,
      "learning_rate": 1.2653239425843374e-05,
      "loss": 0.0211,
      "step": 83790
    },
    {
      "epoch": 24.560375146541617,
      "grad_norm": 1.5662070512771606,
      "learning_rate": 1.2569443138255006e-05,
      "loss": 0.0236,
      "step": 83800
    },
    {
      "epoch": 24.563305978898008,
      "grad_norm": 1.0852175951004028,
      "learning_rate": 1.248564685066664e-05,
      "loss": 0.0212,
      "step": 83810
    },
    {
      "epoch": 24.566236811254395,
      "grad_norm": 1.6725670099258423,
      "learning_rate": 1.2401850563078273e-05,
      "loss": 0.0221,
      "step": 83820
    },
    {
      "epoch": 24.569167643610786,
      "grad_norm": 1.5737009048461914,
      "learning_rate": 1.2318054275489904e-05,
      "loss": 0.0348,
      "step": 83830
    },
    {
      "epoch": 24.572098475967174,
      "grad_norm": 0.1732659637928009,
      "learning_rate": 1.223425798790154e-05,
      "loss": 0.0311,
      "step": 83840
    },
    {
      "epoch": 24.575029308323565,
      "grad_norm": 1.0215821266174316,
      "learning_rate": 1.2150461700313173e-05,
      "loss": 0.0146,
      "step": 83850
    },
    {
      "epoch": 24.577960140679952,
      "grad_norm": 0.37305328249931335,
      "learning_rate": 1.2066665412724806e-05,
      "loss": 0.0318,
      "step": 83860
    },
    {
      "epoch": 24.580890973036343,
      "grad_norm": 0.2839422821998596,
      "learning_rate": 1.198286912513644e-05,
      "loss": 0.0266,
      "step": 83870
    },
    {
      "epoch": 24.58382180539273,
      "grad_norm": 0.031158756464719772,
      "learning_rate": 1.1899072837548072e-05,
      "loss": 0.0219,
      "step": 83880
    },
    {
      "epoch": 24.58675263774912,
      "grad_norm": 0.3748302161693573,
      "learning_rate": 1.1815276549959705e-05,
      "loss": 0.0217,
      "step": 83890
    },
    {
      "epoch": 24.58968347010551,
      "grad_norm": 1.5328466892242432,
      "learning_rate": 1.173148026237134e-05,
      "loss": 0.0367,
      "step": 83900
    },
    {
      "epoch": 24.5926143024619,
      "grad_norm": 0.8738579750061035,
      "learning_rate": 1.1647683974782974e-05,
      "loss": 0.0149,
      "step": 83910
    },
    {
      "epoch": 24.595545134818288,
      "grad_norm": 0.6920410394668579,
      "learning_rate": 1.1563887687194605e-05,
      "loss": 0.0249,
      "step": 83920
    },
    {
      "epoch": 24.59847596717468,
      "grad_norm": 2.0470917224884033,
      "learning_rate": 1.1480091399606239e-05,
      "loss": 0.0293,
      "step": 83930
    },
    {
      "epoch": 24.601406799531066,
      "grad_norm": 2.3561360836029053,
      "learning_rate": 1.1396295112017872e-05,
      "loss": 0.0238,
      "step": 83940
    },
    {
      "epoch": 24.604337631887457,
      "grad_norm": 1.3712018728256226,
      "learning_rate": 1.1312498824429505e-05,
      "loss": 0.0263,
      "step": 83950
    },
    {
      "epoch": 24.607268464243845,
      "grad_norm": 1.2635903358459473,
      "learning_rate": 1.122870253684114e-05,
      "loss": 0.0169,
      "step": 83960
    },
    {
      "epoch": 24.610199296600236,
      "grad_norm": 0.31057462096214294,
      "learning_rate": 1.1144906249252772e-05,
      "loss": 0.0209,
      "step": 83970
    },
    {
      "epoch": 24.613130128956623,
      "grad_norm": 2.6796789169311523,
      "learning_rate": 1.1061109961664406e-05,
      "loss": 0.0512,
      "step": 83980
    },
    {
      "epoch": 24.616060961313014,
      "grad_norm": 0.24503539502620697,
      "learning_rate": 1.097731367407604e-05,
      "loss": 0.0162,
      "step": 83990
    },
    {
      "epoch": 24.6189917936694,
      "grad_norm": 0.3024456799030304,
      "learning_rate": 1.0893517386487671e-05,
      "loss": 0.0314,
      "step": 84000
    },
    {
      "epoch": 24.621922626025793,
      "grad_norm": 0.6575837135314941,
      "learning_rate": 1.0809721098899306e-05,
      "loss": 0.0102,
      "step": 84010
    },
    {
      "epoch": 24.62485345838218,
      "grad_norm": 1.0835685729980469,
      "learning_rate": 1.072592481131094e-05,
      "loss": 0.0352,
      "step": 84020
    },
    {
      "epoch": 24.62778429073857,
      "grad_norm": 0.035899560898542404,
      "learning_rate": 1.0642128523722571e-05,
      "loss": 0.0251,
      "step": 84030
    },
    {
      "epoch": 24.63071512309496,
      "grad_norm": 0.05779790133237839,
      "learning_rate": 1.0558332236134206e-05,
      "loss": 0.0281,
      "step": 84040
    },
    {
      "epoch": 24.63364595545135,
      "grad_norm": 0.5614833235740662,
      "learning_rate": 1.0474535948545838e-05,
      "loss": 0.0312,
      "step": 84050
    },
    {
      "epoch": 24.636576787807737,
      "grad_norm": 0.5443491339683533,
      "learning_rate": 1.0390739660957471e-05,
      "loss": 0.0311,
      "step": 84060
    },
    {
      "epoch": 24.639507620164128,
      "grad_norm": 0.05262070894241333,
      "learning_rate": 1.0306943373369105e-05,
      "loss": 0.0387,
      "step": 84070
    },
    {
      "epoch": 24.642438452520516,
      "grad_norm": 0.34711605310440063,
      "learning_rate": 1.0223147085780738e-05,
      "loss": 0.0349,
      "step": 84080
    },
    {
      "epoch": 24.645369284876907,
      "grad_norm": 0.11749918013811111,
      "learning_rate": 1.0139350798192372e-05,
      "loss": 0.0251,
      "step": 84090
    },
    {
      "epoch": 24.648300117233294,
      "grad_norm": 0.11127074062824249,
      "learning_rate": 1.0055554510604005e-05,
      "loss": 0.0321,
      "step": 84100
    },
    {
      "epoch": 24.651230949589685,
      "grad_norm": 0.08811774104833603,
      "learning_rate": 9.971758223015638e-06,
      "loss": 0.0198,
      "step": 84110
    },
    {
      "epoch": 24.654161781946073,
      "grad_norm": 0.1633005142211914,
      "learning_rate": 9.887961935427272e-06,
      "loss": 0.0291,
      "step": 84120
    },
    {
      "epoch": 24.657092614302464,
      "grad_norm": 0.3562290668487549,
      "learning_rate": 9.804165647838905e-06,
      "loss": 0.0266,
      "step": 84130
    },
    {
      "epoch": 24.66002344665885,
      "grad_norm": 1.8712313175201416,
      "learning_rate": 9.720369360250539e-06,
      "loss": 0.0317,
      "step": 84140
    },
    {
      "epoch": 24.66295427901524,
      "grad_norm": 0.10153830051422119,
      "learning_rate": 9.63657307266217e-06,
      "loss": 0.0223,
      "step": 84150
    },
    {
      "epoch": 24.66588511137163,
      "grad_norm": 0.7054822444915771,
      "learning_rate": 9.552776785073804e-06,
      "loss": 0.0207,
      "step": 84160
    },
    {
      "epoch": 24.66881594372802,
      "grad_norm": 1.603234052658081,
      "learning_rate": 9.468980497485439e-06,
      "loss": 0.0132,
      "step": 84170
    },
    {
      "epoch": 24.671746776084408,
      "grad_norm": 1.7090502977371216,
      "learning_rate": 9.38518420989707e-06,
      "loss": 0.0258,
      "step": 84180
    },
    {
      "epoch": 24.674677608440795,
      "grad_norm": 0.37228432297706604,
      "learning_rate": 9.301387922308704e-06,
      "loss": 0.0436,
      "step": 84190
    },
    {
      "epoch": 24.677608440797187,
      "grad_norm": 0.44538581371307373,
      "learning_rate": 9.217591634720338e-06,
      "loss": 0.014,
      "step": 84200
    },
    {
      "epoch": 24.680539273153574,
      "grad_norm": 0.45398131012916565,
      "learning_rate": 9.133795347131971e-06,
      "loss": 0.0097,
      "step": 84210
    },
    {
      "epoch": 24.683470105509965,
      "grad_norm": 0.5350373387336731,
      "learning_rate": 9.049999059543604e-06,
      "loss": 0.0218,
      "step": 84220
    },
    {
      "epoch": 24.686400937866352,
      "grad_norm": 1.1174405813217163,
      "learning_rate": 8.966202771955238e-06,
      "loss": 0.0166,
      "step": 84230
    },
    {
      "epoch": 24.689331770222744,
      "grad_norm": 0.3744645416736603,
      "learning_rate": 8.882406484366871e-06,
      "loss": 0.0146,
      "step": 84240
    },
    {
      "epoch": 24.69226260257913,
      "grad_norm": 0.40322384238243103,
      "learning_rate": 8.798610196778505e-06,
      "loss": 0.0201,
      "step": 84250
    },
    {
      "epoch": 24.695193434935522,
      "grad_norm": 0.9308449625968933,
      "learning_rate": 8.714813909190138e-06,
      "loss": 0.0175,
      "step": 84260
    },
    {
      "epoch": 24.69812426729191,
      "grad_norm": 1.050312876701355,
      "learning_rate": 8.631017621601771e-06,
      "loss": 0.0274,
      "step": 84270
    },
    {
      "epoch": 24.7010550996483,
      "grad_norm": 0.7332882285118103,
      "learning_rate": 8.547221334013403e-06,
      "loss": 0.0336,
      "step": 84280
    },
    {
      "epoch": 24.703985932004688,
      "grad_norm": 0.1850413978099823,
      "learning_rate": 8.463425046425038e-06,
      "loss": 0.0135,
      "step": 84290
    },
    {
      "epoch": 24.70691676436108,
      "grad_norm": 0.32607200741767883,
      "learning_rate": 8.37962875883667e-06,
      "loss": 0.0239,
      "step": 84300
    },
    {
      "epoch": 24.709847596717466,
      "grad_norm": 0.48442313075065613,
      "learning_rate": 8.295832471248303e-06,
      "loss": 0.044,
      "step": 84310
    },
    {
      "epoch": 24.712778429073857,
      "grad_norm": 1.0425288677215576,
      "learning_rate": 8.212036183659939e-06,
      "loss": 0.0562,
      "step": 84320
    },
    {
      "epoch": 24.715709261430245,
      "grad_norm": 0.45066219568252563,
      "learning_rate": 8.12823989607157e-06,
      "loss": 0.0134,
      "step": 84330
    },
    {
      "epoch": 24.718640093786636,
      "grad_norm": 1.790501356124878,
      "learning_rate": 8.044443608483204e-06,
      "loss": 0.0226,
      "step": 84340
    },
    {
      "epoch": 24.721570926143023,
      "grad_norm": 0.7071529626846313,
      "learning_rate": 7.960647320894837e-06,
      "loss": 0.032,
      "step": 84350
    },
    {
      "epoch": 24.724501758499414,
      "grad_norm": 0.4389192759990692,
      "learning_rate": 7.87685103330647e-06,
      "loss": 0.018,
      "step": 84360
    },
    {
      "epoch": 24.727432590855802,
      "grad_norm": 2.0500011444091797,
      "learning_rate": 7.793054745718104e-06,
      "loss": 0.0319,
      "step": 84370
    },
    {
      "epoch": 24.730363423212193,
      "grad_norm": 0.6485096216201782,
      "learning_rate": 7.709258458129737e-06,
      "loss": 0.0169,
      "step": 84380
    },
    {
      "epoch": 24.73329425556858,
      "grad_norm": 0.34439340233802795,
      "learning_rate": 7.625462170541371e-06,
      "loss": 0.022,
      "step": 84390
    },
    {
      "epoch": 24.73622508792497,
      "grad_norm": 0.8817057609558105,
      "learning_rate": 7.541665882953003e-06,
      "loss": 0.026,
      "step": 84400
    },
    {
      "epoch": 24.73915592028136,
      "grad_norm": 0.73481684923172,
      "learning_rate": 7.457869595364637e-06,
      "loss": 0.0186,
      "step": 84410
    },
    {
      "epoch": 24.74208675263775,
      "grad_norm": 0.5391360521316528,
      "learning_rate": 7.37407330777627e-06,
      "loss": 0.0285,
      "step": 84420
    },
    {
      "epoch": 24.745017584994137,
      "grad_norm": 1.8160693645477295,
      "learning_rate": 7.290277020187904e-06,
      "loss": 0.025,
      "step": 84430
    },
    {
      "epoch": 24.74794841735053,
      "grad_norm": 0.2584334909915924,
      "learning_rate": 7.206480732599536e-06,
      "loss": 0.0327,
      "step": 84440
    },
    {
      "epoch": 24.750879249706916,
      "grad_norm": 0.5098480582237244,
      "learning_rate": 7.1226844450111705e-06,
      "loss": 0.027,
      "step": 84450
    },
    {
      "epoch": 24.753810082063307,
      "grad_norm": 0.22378213703632355,
      "learning_rate": 7.038888157422803e-06,
      "loss": 0.0302,
      "step": 84460
    },
    {
      "epoch": 24.756740914419694,
      "grad_norm": 0.2322121113538742,
      "learning_rate": 6.9550918698344365e-06,
      "loss": 0.015,
      "step": 84470
    },
    {
      "epoch": 24.759671746776085,
      "grad_norm": 1.5629847049713135,
      "learning_rate": 6.871295582246071e-06,
      "loss": 0.0262,
      "step": 84480
    },
    {
      "epoch": 24.762602579132473,
      "grad_norm": 0.5768057703971863,
      "learning_rate": 6.787499294657703e-06,
      "loss": 0.025,
      "step": 84490
    },
    {
      "epoch": 24.765533411488864,
      "grad_norm": 0.3628084063529968,
      "learning_rate": 6.703703007069336e-06,
      "loss": 0.0166,
      "step": 84500
    },
    {
      "epoch": 24.76846424384525,
      "grad_norm": 1.0582098960876465,
      "learning_rate": 6.61990671948097e-06,
      "loss": 0.0219,
      "step": 84510
    },
    {
      "epoch": 24.771395076201642,
      "grad_norm": 1.2872450351715088,
      "learning_rate": 6.5361104318926036e-06,
      "loss": 0.0255,
      "step": 84520
    },
    {
      "epoch": 24.77432590855803,
      "grad_norm": 0.453517347574234,
      "learning_rate": 6.452314144304236e-06,
      "loss": 0.026,
      "step": 84530
    },
    {
      "epoch": 24.77725674091442,
      "grad_norm": 0.20140308141708374,
      "learning_rate": 6.36851785671587e-06,
      "loss": 0.0269,
      "step": 84540
    },
    {
      "epoch": 24.78018757327081,
      "grad_norm": 1.0090152025222778,
      "learning_rate": 6.284721569127503e-06,
      "loss": 0.0258,
      "step": 84550
    },
    {
      "epoch": 24.7831184056272,
      "grad_norm": 0.8443788290023804,
      "learning_rate": 6.200925281539136e-06,
      "loss": 0.0254,
      "step": 84560
    },
    {
      "epoch": 24.786049237983587,
      "grad_norm": 0.41512274742126465,
      "learning_rate": 6.11712899395077e-06,
      "loss": 0.019,
      "step": 84570
    },
    {
      "epoch": 24.788980070339978,
      "grad_norm": 1.4700684547424316,
      "learning_rate": 6.033332706362403e-06,
      "loss": 0.0256,
      "step": 84580
    },
    {
      "epoch": 24.791910902696365,
      "grad_norm": 0.6627609729766846,
      "learning_rate": 5.949536418774036e-06,
      "loss": 0.0168,
      "step": 84590
    },
    {
      "epoch": 24.794841735052756,
      "grad_norm": 0.328334778547287,
      "learning_rate": 5.86574013118567e-06,
      "loss": 0.0198,
      "step": 84600
    },
    {
      "epoch": 24.797772567409144,
      "grad_norm": 0.09106513112783432,
      "learning_rate": 5.781943843597303e-06,
      "loss": 0.0305,
      "step": 84610
    },
    {
      "epoch": 24.800703399765535,
      "grad_norm": 1.9408531188964844,
      "learning_rate": 5.698147556008936e-06,
      "loss": 0.0167,
      "step": 84620
    },
    {
      "epoch": 24.803634232121922,
      "grad_norm": 0.7233114242553711,
      "learning_rate": 5.61435126842057e-06,
      "loss": 0.0219,
      "step": 84630
    },
    {
      "epoch": 24.806565064478313,
      "grad_norm": 0.5252534747123718,
      "learning_rate": 5.530554980832203e-06,
      "loss": 0.026,
      "step": 84640
    },
    {
      "epoch": 24.8094958968347,
      "grad_norm": 0.3541986644268036,
      "learning_rate": 5.4467586932438354e-06,
      "loss": 0.0148,
      "step": 84650
    },
    {
      "epoch": 24.812426729191092,
      "grad_norm": 2.6744441986083984,
      "learning_rate": 5.36296240565547e-06,
      "loss": 0.0569,
      "step": 84660
    },
    {
      "epoch": 24.81535756154748,
      "grad_norm": 2.492887496948242,
      "learning_rate": 5.279166118067103e-06,
      "loss": 0.0336,
      "step": 84670
    },
    {
      "epoch": 24.81828839390387,
      "grad_norm": 0.043496016412973404,
      "learning_rate": 5.195369830478736e-06,
      "loss": 0.0166,
      "step": 84680
    },
    {
      "epoch": 24.821219226260258,
      "grad_norm": 0.6486577987670898,
      "learning_rate": 5.111573542890369e-06,
      "loss": 0.0286,
      "step": 84690
    },
    {
      "epoch": 24.82415005861665,
      "grad_norm": 0.21913006901741028,
      "learning_rate": 5.0277772553020025e-06,
      "loss": 0.0239,
      "step": 84700
    },
    {
      "epoch": 24.827080890973036,
      "grad_norm": 1.6345961093902588,
      "learning_rate": 4.943980967713636e-06,
      "loss": 0.0171,
      "step": 84710
    },
    {
      "epoch": 24.830011723329427,
      "grad_norm": 1.1496412754058838,
      "learning_rate": 4.860184680125269e-06,
      "loss": 0.0178,
      "step": 84720
    },
    {
      "epoch": 24.832942555685815,
      "grad_norm": 0.3830260932445526,
      "learning_rate": 4.776388392536902e-06,
      "loss": 0.013,
      "step": 84730
    },
    {
      "epoch": 24.835873388042202,
      "grad_norm": 1.8419069051742554,
      "learning_rate": 4.692592104948535e-06,
      "loss": 0.0368,
      "step": 84740
    },
    {
      "epoch": 24.838804220398593,
      "grad_norm": 0.2510327398777008,
      "learning_rate": 4.608795817360169e-06,
      "loss": 0.0395,
      "step": 84750
    },
    {
      "epoch": 24.84173505275498,
      "grad_norm": 0.9630908966064453,
      "learning_rate": 4.524999529771802e-06,
      "loss": 0.0251,
      "step": 84760
    },
    {
      "epoch": 24.84466588511137,
      "grad_norm": 2.4749550819396973,
      "learning_rate": 4.441203242183436e-06,
      "loss": 0.0077,
      "step": 84770
    },
    {
      "epoch": 24.84759671746776,
      "grad_norm": 1.6419676542282104,
      "learning_rate": 4.357406954595069e-06,
      "loss": 0.0375,
      "step": 84780
    },
    {
      "epoch": 24.85052754982415,
      "grad_norm": 1.8118923902511597,
      "learning_rate": 4.273610667006702e-06,
      "loss": 0.0444,
      "step": 84790
    },
    {
      "epoch": 24.853458382180538,
      "grad_norm": 0.40943604707717896,
      "learning_rate": 4.189814379418335e-06,
      "loss": 0.0283,
      "step": 84800
    },
    {
      "epoch": 24.85638921453693,
      "grad_norm": 2.2454185485839844,
      "learning_rate": 4.106018091829969e-06,
      "loss": 0.0214,
      "step": 84810
    },
    {
      "epoch": 24.859320046893316,
      "grad_norm": 1.4653109312057495,
      "learning_rate": 4.022221804241602e-06,
      "loss": 0.0224,
      "step": 84820
    },
    {
      "epoch": 24.862250879249707,
      "grad_norm": 0.822593629360199,
      "learning_rate": 3.938425516653235e-06,
      "loss": 0.0143,
      "step": 84830
    },
    {
      "epoch": 24.865181711606095,
      "grad_norm": 0.3930501937866211,
      "learning_rate": 3.854629229064869e-06,
      "loss": 0.0122,
      "step": 84840
    },
    {
      "epoch": 24.868112543962486,
      "grad_norm": 0.572486937046051,
      "learning_rate": 3.7708329414765017e-06,
      "loss": 0.0313,
      "step": 84850
    },
    {
      "epoch": 24.871043376318873,
      "grad_norm": 0.6660852432250977,
      "learning_rate": 3.687036653888135e-06,
      "loss": 0.0211,
      "step": 84860
    },
    {
      "epoch": 24.873974208675264,
      "grad_norm": 0.6251897811889648,
      "learning_rate": 3.603240366299768e-06,
      "loss": 0.0291,
      "step": 84870
    },
    {
      "epoch": 24.87690504103165,
      "grad_norm": 0.6518165469169617,
      "learning_rate": 3.5194440787114015e-06,
      "loss": 0.0296,
      "step": 84880
    },
    {
      "epoch": 24.879835873388043,
      "grad_norm": 1.5689091682434082,
      "learning_rate": 3.4356477911230354e-06,
      "loss": 0.0297,
      "step": 84890
    },
    {
      "epoch": 24.88276670574443,
      "grad_norm": 0.3451649248600006,
      "learning_rate": 3.351851503534668e-06,
      "loss": 0.0331,
      "step": 84900
    },
    {
      "epoch": 24.88569753810082,
      "grad_norm": 1.9046704769134521,
      "learning_rate": 3.2680552159463018e-06,
      "loss": 0.0261,
      "step": 84910
    },
    {
      "epoch": 24.88862837045721,
      "grad_norm": 0.1539064645767212,
      "learning_rate": 3.184258928357935e-06,
      "loss": 0.0192,
      "step": 84920
    },
    {
      "epoch": 24.8915592028136,
      "grad_norm": 0.3163469135761261,
      "learning_rate": 3.100462640769568e-06,
      "loss": 0.0423,
      "step": 84930
    },
    {
      "epoch": 24.894490035169987,
      "grad_norm": 0.16886603832244873,
      "learning_rate": 3.0166663531812016e-06,
      "loss": 0.0123,
      "step": 84940
    },
    {
      "epoch": 24.897420867526378,
      "grad_norm": 0.6260176301002502,
      "learning_rate": 2.932870065592835e-06,
      "loss": 0.0245,
      "step": 84950
    },
    {
      "epoch": 24.900351699882766,
      "grad_norm": 1.0783658027648926,
      "learning_rate": 2.849073778004468e-06,
      "loss": 0.02,
      "step": 84960
    },
    {
      "epoch": 24.903282532239157,
      "grad_norm": 0.06512989848852158,
      "learning_rate": 2.7652774904161014e-06,
      "loss": 0.0141,
      "step": 84970
    },
    {
      "epoch": 24.906213364595544,
      "grad_norm": 0.549052894115448,
      "learning_rate": 2.681481202827735e-06,
      "loss": 0.0232,
      "step": 84980
    },
    {
      "epoch": 24.909144196951935,
      "grad_norm": 1.9184221029281616,
      "learning_rate": 2.597684915239368e-06,
      "loss": 0.0491,
      "step": 84990
    },
    {
      "epoch": 24.912075029308323,
      "grad_norm": 0.5183461308479309,
      "learning_rate": 2.5138886276510013e-06,
      "loss": 0.0319,
      "step": 85000
    },
    {
      "epoch": 24.915005861664714,
      "grad_norm": 1.8486502170562744,
      "learning_rate": 2.4300923400626347e-06,
      "loss": 0.0229,
      "step": 85010
    },
    {
      "epoch": 24.9179366940211,
      "grad_norm": 0.3920603394508362,
      "learning_rate": 2.3462960524742677e-06,
      "loss": 0.0181,
      "step": 85020
    },
    {
      "epoch": 24.920867526377492,
      "grad_norm": 0.757060170173645,
      "learning_rate": 2.262499764885901e-06,
      "loss": 0.0234,
      "step": 85030
    },
    {
      "epoch": 24.92379835873388,
      "grad_norm": 0.30246230959892273,
      "learning_rate": 2.1787034772975345e-06,
      "loss": 0.0253,
      "step": 85040
    },
    {
      "epoch": 24.92672919109027,
      "grad_norm": 0.2658131718635559,
      "learning_rate": 2.0949071897091675e-06,
      "loss": 0.0287,
      "step": 85050
    },
    {
      "epoch": 24.929660023446658,
      "grad_norm": 0.6755660772323608,
      "learning_rate": 2.011110902120801e-06,
      "loss": 0.0246,
      "step": 85060
    },
    {
      "epoch": 24.93259085580305,
      "grad_norm": 4.0618486404418945,
      "learning_rate": 1.9273146145324343e-06,
      "loss": 0.0286,
      "step": 85070
    },
    {
      "epoch": 24.935521688159437,
      "grad_norm": 0.015036406926810741,
      "learning_rate": 1.8435183269440676e-06,
      "loss": 0.0111,
      "step": 85080
    },
    {
      "epoch": 24.938452520515828,
      "grad_norm": 0.6666197180747986,
      "learning_rate": 1.7597220393557008e-06,
      "loss": 0.0195,
      "step": 85090
    },
    {
      "epoch": 24.941383352872215,
      "grad_norm": 0.298425555229187,
      "learning_rate": 1.675925751767334e-06,
      "loss": 0.0392,
      "step": 85100
    },
    {
      "epoch": 24.944314185228606,
      "grad_norm": 1.4396064281463623,
      "learning_rate": 1.5921294641789676e-06,
      "loss": 0.0225,
      "step": 85110
    },
    {
      "epoch": 24.947245017584994,
      "grad_norm": 1.2561378479003906,
      "learning_rate": 1.5083331765906008e-06,
      "loss": 0.051,
      "step": 85120
    },
    {
      "epoch": 24.950175849941385,
      "grad_norm": 0.6545940041542053,
      "learning_rate": 1.424536889002234e-06,
      "loss": 0.0222,
      "step": 85130
    },
    {
      "epoch": 24.953106682297772,
      "grad_norm": 0.014506424777209759,
      "learning_rate": 1.3407406014138674e-06,
      "loss": 0.0194,
      "step": 85140
    },
    {
      "epoch": 24.956037514654163,
      "grad_norm": 2.61501145362854,
      "learning_rate": 1.2569443138255006e-06,
      "loss": 0.0266,
      "step": 85150
    },
    {
      "epoch": 24.95896834701055,
      "grad_norm": 1.0077863931655884,
      "learning_rate": 1.1731480262371338e-06,
      "loss": 0.0195,
      "step": 85160
    },
    {
      "epoch": 24.96189917936694,
      "grad_norm": 0.020323794335126877,
      "learning_rate": 1.0893517386487673e-06,
      "loss": 0.0178,
      "step": 85170
    },
    {
      "epoch": 24.96483001172333,
      "grad_norm": 0.09274201095104218,
      "learning_rate": 1.0055554510604005e-06,
      "loss": 0.02,
      "step": 85180
    },
    {
      "epoch": 24.96776084407972,
      "grad_norm": 0.36913639307022095,
      "learning_rate": 9.217591634720338e-07,
      "loss": 0.0159,
      "step": 85190
    },
    {
      "epoch": 24.970691676436108,
      "grad_norm": 1.0027945041656494,
      "learning_rate": 8.37962875883667e-07,
      "loss": 0.024,
      "step": 85200
    },
    {
      "epoch": 24.9736225087925,
      "grad_norm": 2.538501262664795,
      "learning_rate": 7.541665882953004e-07,
      "loss": 0.0455,
      "step": 85210
    },
    {
      "epoch": 24.976553341148886,
      "grad_norm": 0.14375872910022736,
      "learning_rate": 6.703703007069337e-07,
      "loss": 0.0178,
      "step": 85220
    },
    {
      "epoch": 24.979484173505277,
      "grad_norm": 0.5131025314331055,
      "learning_rate": 5.865740131185669e-07,
      "loss": 0.0141,
      "step": 85230
    },
    {
      "epoch": 24.982415005861665,
      "grad_norm": 1.405912160873413,
      "learning_rate": 5.027777255302002e-07,
      "loss": 0.0281,
      "step": 85240
    },
    {
      "epoch": 24.985345838218056,
      "grad_norm": 0.47641080617904663,
      "learning_rate": 4.189814379418335e-07,
      "loss": 0.0313,
      "step": 85250
    },
    {
      "epoch": 24.988276670574443,
      "grad_norm": 0.32684382796287537,
      "learning_rate": 3.3518515035346686e-07,
      "loss": 0.0158,
      "step": 85260
    },
    {
      "epoch": 24.991207502930834,
      "grad_norm": 0.6937317848205566,
      "learning_rate": 2.513888627651001e-07,
      "loss": 0.0208,
      "step": 85270
    },
    {
      "epoch": 24.99413833528722,
      "grad_norm": 2.1309914588928223,
      "learning_rate": 1.6759257517673343e-07,
      "loss": 0.0336,
      "step": 85280
    },
    {
      "epoch": 24.99706916764361,
      "grad_norm": 0.1959410309791565,
      "learning_rate": 8.379628758836671e-08,
      "loss": 0.0225,
      "step": 85290
    },
    {
      "epoch": 25.0,
      "grad_norm": 0.4359987676143646,
      "learning_rate": 0.0,
      "loss": 0.0213,
      "step": 85300
    },
    {
      "epoch": 25.0,
      "eval_accuracy": 0.8318496538081108,
      "eval_f1_macro": 0.8719101343536,
      "eval_f1_micro": 0.8823963405636713,
      "eval_f1_weighted": 0.8815371485166827,
      "eval_loss": 0.06628153473138809,
      "eval_roc_auc": 0.9315157972862602,
      "eval_runtime": 232.8574,
      "eval_samples_per_second": 13.025,
      "eval_steps_per_second": 1.632,
      "step": 85300
    }
  ],
  "logging_steps": 10,
  "max_steps": 85300,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 25,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 6.156178176915004e+17,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": {
    "_wandb": {},
    "assignments": {},
    "learning_rate": 0.000714782333128768,
    "max_grad_norm": 0.5,
    "metric": "eval/loss",
    "per_device_train_batch_size": 4,
    "weight_decay": 0.2
  }
}
